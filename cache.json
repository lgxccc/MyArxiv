{"2025-10-24T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2506.16055v2","updated":"2025-10-24T17:50:04Z","published":"2025-06-19T06:27:54Z","title":"Knee-Deep in C-RASP: A Transformer Depth Hierarchy","summary":"  It has been observed that transformers with greater depth (that is, more\nlayers) have more capabilities, but can we establish formally which\ncapabilities are gained? We answer this question with a theoretical proof\nfollowed by an empirical study. First, we consider transformers that round to\nfixed precision except inside attention. We show that this subclass of\ntransformers is expressively equivalent to the programming language C-RASP and\nthis equivalence preserves depth. Second, we prove that deeper C-RASP programs\nare more expressive than shallower C-RASP programs, implying that deeper\ntransformers are more expressive than shallower transformers (within the\nsubclass mentioned above). The same is also proven for transformers with\npositional encodings (like RoPE and ALiBi). These results are established by\nstudying a temporal logic with counting operators equivalent to C-RASP.\nFinally, we provide empirical evidence that our theory predicts the depth\nrequired for transformers without positional encodings to length-generalize on\na family of sequential dependency tasks.\n","authors":["Andy Yang","MichaÃ«l Cadilhac","David Chiang"],"pdf_url":"https://arxiv.org/pdf/2506.16055v2.pdf","comment":"35 pages, 5 figures"},{"id":"http://arxiv.org/abs/2507.23773v2","updated":"2025-10-24T17:44:52Z","published":"2025-07-31T17:57:20Z","title":"SimuRA: A World-Model-Driven Simulative Reasoning Architecture for\n  General Goal-Oriented Agents","summary":"  AI agents built on foundation models hold enormous promise. Current practice,\nhowever, focuses on a one-task-one-agent approach, which not only falls short\nof scalability and generality, but also faces practical limitations from\nblack-box autoregressive reasoning, where decisions unfold token by token\nwithout explicit simulation or counterfactual evaluation of outcomes. Humans,\non the other hand, reason and plan by mentally simulating the consequences of\nactions within an internal model of the world -- a capability that supports\nflexible, goal-directed behavior across diverse contexts. Moving towards a more\ngeneral and powerful AI agent, we introduce SimuRA, a goal-oriented\narchitecture for generalized agentic reasoning. Based on a principled\nformulation of an optimal agent in any general environment, SimuRA addresses\nthe limitations of black-box autoregressive reasoning by incorporating the\nworld model for planning via simulation. Our prototype world model is\nimplemented using LLMs as a substrate, leveraging the natural language as a\ndiscrete, hierarchical representation grounded in concepts for planning, while\nremaining model-agnostic. On complex web-browsing tasks such as flight search,\nSimuRA improves the success rate from 0% to 32.2% compared to a representative\nopen-web agent baseline. Across tasks, world-model-based planning achieves up\nto 124% higher task completion rates than a matched black-box autoregressive\nbaseline, demonstrating the advantages of simulative reasoning. We release\nReasonerAgent-Web, a web-browsing agent built on SimuRA, as an open-source\nresearch demo.\n","authors":["Mingkai Deng","Jinyu Hou","Zhiting Hu","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2507.23773v2.pdf","comment":"This submission has been updated to adjust the scope and presentation\n  of the work"},{"id":"http://arxiv.org/abs/2510.06186v2","updated":"2025-10-24T17:20:26Z","published":"2025-10-07T17:45:35Z","title":"RECODE-H: A Benchmark for Research Code Development with Interactive\n  Human Feedback","summary":"  Large language models (LLMs) show the promise in supporting scientific\nresearch implementation, yet their ability to generate correct and executable\ncode remains limited. Existing works largely adopt one-shot settings, ignoring\nthe iterative and feedback-driven nature of realistic workflows of scientific\nresearch development. To address this gap, we present RECODE-H, a benchmark of\n102 tasks from research papers and repositories that evaluates LLM agents\nthrough multi-turn interactions with LLM-simulated human feedback. It includes\nstructured instructions,unit tests, and a five-level feedback hierarchy to\nreflect realistic researcher-agent collaboration. We further present\nReCodeAgent, a framework that integrates feedback into iterative code\ngeneration. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,\nDeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer\nfeedback, while also highlighting ongoing challenges in the generation of\ncomplex research code. RECODE-H establishes a foundation for developing\nadaptive, feedback-driven LLM agents in scientific research implementation\n","authors":["Chunyu Miao","Henry Peng Zou","Yangning Li","Yankai Chen","Yibo Wang","Fangxin Wang","Yifan Li","Wooseong Yang","Bowei He","Xinni Zhang","Dianzhi Yu","Hanchen Yang","Hoang H Nguyen","Yue Zhou","Jie Yang","Jizhou Guo","Wenzhe Fan","Chin-Yuan Yeh","Panpan Meng","Liancheng Fang","Jinhu Qi","Wei-Chieh Huang","Zhengyao Gu","Yuwei Han","Langzhou He","Yuyao Yang","Yinghui Li","Hai-Tao Zheng","Xue Liu","Irwin King","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2510.06186v2.pdf","comment":"Code and dataset are available at github.com/ChunyuMiao98/RECODE"},{"id":"http://arxiv.org/abs/2510.21652v1","updated":"2025-10-24T17:10:26Z","published":"2025-10-24T17:10:26Z","title":"AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research\n  Suite","summary":"  AI agents hold the potential to revolutionize scientific productivity by\nautomating literature reviews, replicating experiments, analyzing data, and\neven proposing new directions of inquiry; indeed, there are now many such\nagents, ranging from general-purpose \"deep research\" systems to specialized\nscience-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of\nthese agents is critical for progress. Yet existing benchmarks fall short on\nseveral fronts: they (1) fail to provide holistic, product-informed measures of\nreal-world use cases such as science research; (2) lack reproducible agent\ntools necessary for a controlled comparison of core agentic capabilities; (3)\ndo not account for confounding variables such as model cost and tool access;\n(4) do not provide standardized interfaces for quick agent prototyping and\nevaluation; and (5) lack comprehensive baseline agents necessary to identify\ntrue advances. In response, we define principles and tooling for more\nrigorously benchmarking agents. Using these, we present AstaBench, a suite that\nprovides the first holistic measure of agentic ability to perform scientific\nresearch, comprising 2400+ problems spanning the entire scientific discovery\nprocess and multiple scientific domains, and including many problems inspired\nby actual user requests to deployed Asta agents. Our suite comes with the first\nscientific research environment with production-grade search tools that enable\ncontrolled, reproducible evaluation, better accounting for confounders.\nAlongside, we provide a comprehensive suite of nine science-optimized classes\nof Asta agents and numerous baselines. Our extensive evaluation of 57 agents\nacross 22 agent classes reveals several interesting findings, most importantly\nthat despite meaningful progress on certain individual aspects, AI remains far\nfrom solving the challenge of science research assistance.\n","authors":["Jonathan Bragg","Mike D'Arcy","Nishant Balepur","Dan Bareket","Bhavana Dalvi","Sergey Feldman","Dany Haddad","Jena D. Hwang","Peter Jansen","Varsha Kishore","Bodhisattwa Prasad Majumder","Aakanksha Naik","Sigal Rahamimov","Kyle Richardson","Amanpreet Singh","Harshit Surana","Aryeh Tiktinsky","Rosni Vasu","Guy Wiener","Chloe Anastasiades","Stefan Candra","Jason Dunkelberger","Dan Emery","Rob Evans","Malachi Hamada","Regan Huff","Rodney Kinney","Matt Latzke","Jaron Lochner","Ruben Lozano-Aguilera","Cecile Nguyen","Smita Rao","Amber Tanaka","Brooke Vlahos","Peter Clark","Doug Downey","Yoav Goldberg","Ashish Sabharwal","Daniel S. Weld"],"pdf_url":"https://arxiv.org/pdf/2510.21652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.20787v2","updated":"2025-10-24T16:56:22Z","published":"2025-10-23T17:53:03Z","title":"Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction","summary":"  Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches.\n","authors":["Mutian He","Philip N. Garner"],"pdf_url":"https://arxiv.org/pdf/2510.20787v2.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.03325v2","updated":"2025-10-24T16:55:30Z","published":"2025-02-05T16:22:33Z","title":"Electronic Circuit Principles of Large Language Models","summary":"  Large language models (LLMs) such as DeepSeek-R1 have achieved remarkable\nperformance across diverse reasoning tasks. To uncover the principles that\ngovern their behaviour, we introduce the Electronic Circuit Principles (ECP),\nwhich maps inference-time learning (ITL) onto a semantic electromotive force\nand inference-time reasoning (ITR) onto a resistive network governed by Ohm's\nand Faraday's laws. This circuit-based modelling yields closed-form predictions\nof task performance and reveals how modular prompt components interact to shape\naccuracy. We validated ECP on 70,000 samples spanning 350 reasoning tasks and 9\nadvanced LLMs, observing a about 60% improvement in Pearson correlation\nrelative to the conventional inference-time scaling law. Moreover, ECP explains\nthe efficacy of 15 established prompting strategies and directs the development\nof new modular interventions that exceed the median score of the top 80% of\nparticipants in both the International Olympiad in Informatics and the\nInternational Mathematical Olympiad. By grounding LLM reasoning in\nelectronic-circuit principles, ECP provides a rigorous framework for predicting\nperformance and optimising modular components.\n","authors":["Qiguang Chen","Libo Qin","Jinhao Liu","Dengyun Peng","Jiaqi Wang","Mengkang Hu","Zhi Chen","Wanxiang Che","Ting Liu"],"pdf_url":"https://arxiv.org/pdf/2502.03325v2.pdf","comment":"Manuscript"},{"id":"http://arxiv.org/abs/2510.21631v1","updated":"2025-10-24T16:36:34Z","published":"2025-10-24T16:36:34Z","title":"Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations","summary":"  Knowledge distillation is a promising approach to transfer capabilities from\ncomplex teacher models to smaller, resource-efficient student models that can\nbe deployed easily, particularly in task-aware scenarios. However, existing\nmethods of task-aware distillation typically require substantial quantities of\ndata which may be unavailable or expensive to obtain in many practical\nscenarios. In this paper, we address this challenge by introducing a novel\nstrategy called Counterfactual-explanation-infused Distillation CoD for\nfew-shot task-aware knowledge distillation by systematically infusing\ncounterfactual explanations. Counterfactual explanations (CFEs) refer to inputs\nthat can flip the output prediction of the teacher model with minimum\nperturbation. Our strategy CoD leverages these CFEs to precisely map the\nteacher's decision boundary with significantly fewer samples. We provide\ntheoretical guarantees for motivating the role of CFEs in distillation, from\nboth statistical and geometric perspectives. We mathematically show that CFEs\ncan improve parameter estimation by providing more informative examples near\nthe teacher's decision boundary. We also derive geometric insights on how CFEs\neffectively act as knowledge probes, helping the students mimic the teacher's\ndecision boundaries more effectively than standard data. We perform experiments\nacross various datasets and LLMs to show that CoD outperforms standard\ndistillation approaches in few-shot regimes (as low as 8-512 samples). Notably,\nCoD only uses half of the original samples used by the baselines, paired with\ntheir corresponding CFEs and still improves performance.\n","authors":["Faisal Hamman","Pasan Dissanayake","Yanjun Fu","Sanghamitra Dutta"],"pdf_url":"https://arxiv.org/pdf/2510.21631v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.21623v1","updated":"2025-10-24T16:26:36Z","published":"2025-10-24T16:26:36Z","title":"The Universal Landscape of Human Reasoning","summary":"  Understanding how information is dynamically accumulated and transformed in\nhuman reasoning has long challenged cognitive psychology, philosophy, and\nartificial intelligence. Existing accounts, from classical logic to\nprobabilistic models, illuminate aspects of output or individual modelling, but\ndo not offer a unified, quantitative description of general human reasoning\ndynamics. To solve this, we introduce Information Flow Tracking (IF-Track),\nthat uses large language models (LLMs) as probabilistic encoder to quantify\ninformation entropy and gain at each reasoning step. Through fine-grained\nanalyses across diverse tasks, our method is the first successfully models the\nuniversal landscape of human reasoning behaviors within a single metric space.\nWe show that IF-Track captures essential reasoning features, identifies\nsystematic error patterns, and characterizes individual differences. Applied to\ndiscussion of advanced psychological theory, we first reconcile single- versus\ndual-process theories in IF-Track and discover the alignment of artificial and\nhuman cognition and how LLMs reshaping human reasoning process. This approach\nestablishes a quantitative bridge between theory and measurement, offering\nmechanistic insights into the architecture of reasoning.\n","authors":["Qiguang Chen","Jinhao Liu","Libo Qin","Yimeng Zhang","Yihao Liang","Shangxu Ren","Chengyu Luan","Dengyun Peng","Hanjing Li","Jiannan Guan","Zheng Yan","Jiaqi Wang","Mengkang Hu","Yantao Du","Zhi Chen","Xie Chen","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2510.21623v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2507.14198v2","updated":"2025-10-24T16:26:00Z","published":"2025-07-14T15:51:19Z","title":"Retention analysis of edited knowledge after fine-tuning","summary":"  Large language models (LLMs) store vast amounts of knowledge, which often\nrequires updates to correct factual errors, incorporate newly acquired\ninformation, or adapt model behavior. Model editing methods have emerged as\nefficient solutions for such updates, offering localized and precise knowledge\nmodification at significantly lower computational cost than continual training.\nIn parallel, LLMs are frequently fine-tuned for a wide range of downstream\ntasks. However, the effect of fine-tuning on previously edited knowledge\nremains poorly understood. In this work, we systematically investigate how\ndifferent fine-tuning objectives interact with various model editing\ntechniques. Our findings show that edited knowledge is substantially more\nsusceptible to forgetting during fine-tuning than intrinsic knowledge acquired\nthrough pre-training. This analysis highlights a key limitation of current\nediting approaches and suggests that evaluating edit robustness under\ndownstream fine-tuning is critical for their practical deployment. We further\nfind that knowledge retention can be significantly improved by either\naugmenting edit knowledge with paraphrases or by freezing layers associated\nwith edited content in fine-tuning stage, offering insight for developing more\nrobust editing algorithms.\n","authors":["Fufang Wen","Shichang Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.14198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21618v1","updated":"2025-10-24T16:24:01Z","published":"2025-10-24T16:24:01Z","title":"DeepAgent: A General Reasoning Agent with Scalable Toolsets","summary":"  Large reasoning models have demonstrated strong problem-solving abilities,\nyet real-world tasks often require external tools and long-horizon\ninteractions. Existing agent frameworks typically follow predefined workflows,\nwhich limit autonomous and global task completion. In this paper, we introduce\nDeepAgent, an end-to-end deep reasoning agent that performs autonomous\nthinking, tool discovery, and action execution within a single, coherent\nreasoning process. To address the challenges of long-horizon interactions,\nparticularly the context length explosion from multiple tool calls and the\naccumulation of interaction history, we introduce an autonomous memory folding\nmechanism that compresses past interactions into structured episodic, working,\nand tool memories, reducing error accumulation while preserving critical\ninformation. To teach general-purpose tool use efficiently and stably, we\ndevelop an end-to-end reinforcement learning strategy, namely ToolPO, that\nleverages LLM-simulated APIs and applies tool-call advantage attribution to\nassign fine-grained credit to the tool invocation tokens. Extensive experiments\non eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,\nTMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,\nHLE), demonstrate that DeepAgent consistently outperforms baselines across both\nlabeled-tool and open-set tool retrieval scenarios. This work takes a step\ntoward more general and capable agents for real-world applications. The code\nand demo are available at https://github.com/RUC-NLPIR/DeepAgent.\n","authors":["Xiaoxi Li","Wenxiang Jiao","Jiarui Jin","Guanting Dong","Jiajie Jin","Yinuo Wang","Hao Wang","Yutao Zhu","Ji-Rong Wen","Yuan Lu","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2510.21618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06485v2","updated":"2025-10-24T16:19:27Z","published":"2025-07-09T02:06:13Z","title":"Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for\n  Efficient and Enhanced Video Reasoning","summary":"  Despite advances in reinforcement learning (RL)-based video reasoning with\nlarge language models (LLMs), data collection and fine-tuning remain\nsignificant challenges. These methods often rely on large-scale supervised\nfine-tuning (SFT) with extensive video data and long Chain-of-Thought (CoT)\nannotations, making them costly and hard to scale. To address this, we present\nVideo-RTS, a new approach to improve video reasoning capability with\ndrastically improved data efficiency by combining data-efficient RL with a\nvideo-adaptive test-time scaling (TTS) strategy. Building on observations about\nthe data scaling, we skip the resource-intensive SFT step and employ efficient\npure-RL training with output-based rewards, requiring no additional annotations\nor extensive fine-tuning. Furthermore, to utilize computational resources more\nefficiently, we introduce a sparse-to-dense video TTS strategy that improves\ninference by iteratively adding frames based on output consistency. We validate\nour approach on multiple video reasoning benchmarks, showing that Video-RTS\nsurpasses existing video reasoning models by 2.4% in accuracy using only 3.6%\ntraining samples. Specifically, Video-RTS achieves a 4.2% improvement on\nVideo-Holmes, a recent and challenging video reasoning benchmark. Notably, our\npure RL training and adaptive video TTS offer complementary strengths, enabling\nVideo-RTS's strong reasoning performance.\n","authors":["Ziyang Wang","Jaehong Yoon","Shoubin Yu","Md Mohaiminul Islam","Gedas Bertasius","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2507.06485v2.pdf","comment":"EMNLP 2025. The first two authors contributed equally. Project page:\n  https://sites.google.com/cs.unc.edu/videorts2025/"},{"id":"http://arxiv.org/abs/2510.21604v1","updated":"2025-10-24T16:08:33Z","published":"2025-10-24T16:08:33Z","title":"RETuning: Upgrading Inference-Time Scaling for Stock Movement Prediction\n  with Large Language Models","summary":"  Recently, large language models (LLMs) have demonstrated outstanding\nreasoning capabilities on mathematical and coding tasks. However, their\napplication to financial tasks-especially the most fundamental task of stock\nmovement prediction-remains underexplored. We study a three-class\nclassification problem (up, hold, down) and, by analyzing existing reasoning\nresponses, observe that: (1) LLMs follow analysts' opinions rather than exhibit\na systematic, independent analytical logic (CoTs). (2) LLMs list summaries from\ndifferent sources without weighing adversarial evidence, yet such\ncounterevidence is crucial for reliable prediction. It shows that the model\ndoes not make good use of its reasoning ability to complete the task. To\naddress this, we propose Reflective Evidence Tuning (RETuning), a cold-start\nmethod prior to reinforcement learning, to enhance prediction ability. While\ngenerating CoT, RETuning encourages dynamically constructing an analytical\nframework from diverse information sources, organizing and scoring evidence for\nprice up or down based on that framework-rather than on contextual\nviewpoints-and finally reflecting to derive the prediction. This approach\nmaximally aligns the model with its learned analytical framework, ensuring\nindependent logical reasoning and reducing undue influence from context. We\nalso build a large-scale dataset spanning all of 2024 for 5,123 A-share stocks,\nwith long contexts (32K tokens) and over 200K samples. In addition to price and\nnews, it incorporates analysts' opinions, quantitative reports, fundamental\ndata, macroeconomic indicators, and similar stocks. Experiments show that\nRETuning successfully unlocks the model's reasoning ability in the financial\ndomain. Inference-time scaling still works even after 6 months or on\nout-of-distribution stocks, since the models gain valuable insights about stock\nmovement prediction.\n","authors":["Xueyuan Lin","Cehao Yang","Ye Ma","Ming Li","Rongjunchen Zhang","Yang Ni","Xiaojun Wu","Chengjin Xu","Jian Guo","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2510.21604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21603v1","updated":"2025-10-24T16:07:54Z","published":"2025-10-24T16:07:54Z","title":"Doc-Researcher: A Unified System for Multimodal Document Parsing and\n  Deep Research","summary":"  Deep Research systems have revolutionized how LLMs solve complex questions\nthrough iterative reasoning and evidence gathering. However, current systems\nremain fundamentally constrained to textual web data, overlooking the vast\nknowledge embedded in multimodal documents Processing such documents demands\nsophisticated parsing to preserve visual semantics (figures, tables, charts,\nand equations), intelligent chunking to maintain structural coherence, and\nadaptive retrieval across modalities, which are capabilities absent in existing\nsystems. In response, we present Doc-Researcher, a unified system that bridges\nthis gap through three integrated components: (i) deep multimodal parsing that\npreserves layout structure and visual semantics while creating multi-granular\nrepresentations from chunk to document level, (ii) systematic retrieval\narchitecture supporting text-only, vision-only, and hybrid paradigms with\ndynamic granularity selection, and (iii) iterative multi-agent workflows that\ndecompose complex queries, progressively accumulate evidence, and synthesize\ncomprehensive answers across documents and modalities. To enable rigorous\nevaluation, we introduce M4DocBench, the first benchmark for Multi-modal,\nMulti-hop, Multi-document, and Multi-turn deep research. Featuring 158\nexpert-annotated questions with complete evidence chains across 304 documents,\nM4DocBench tests capabilities that existing benchmarks cannot assess.\nExperiments demonstrate that Doc-Researcher achieves 50.6% accuracy, 3.4xbetter\nthan state-of-the-art baselines, validating that effective document research\nrequires not just better retrieval, but fundamentally deep parsing that\npreserve multimodal integrity and support iterative research. Our work\nestablishes a new paradigm for conducting deep research on multimodal document\ncollections.\n","authors":["Kuicai Dong","Shurui Huang","Fangda Ye","Wei Han","Zhi Zhang","Dexun Li","Wenjun Li","Qu Yang","Gang Wang","Yichao Wang","Chen Zhang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2510.21603v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2407.07612v3","updated":"2025-10-24T16:07:28Z","published":"2024-07-10T12:50:44Z","title":"Teaching Transformers Causal Reasoning through Axiomatic Training","summary":"  For text-based AI systems to interact in the real world, causal reasoning is\nan essential skill. Since active interventions are costly, we study to what\nextent a system can learn causal reasoning from symbolic demonstrations of\ncausal axioms. Specifically, we present an axiomatic training method where the\nsystem learns from multiple demonstrations of a causal axiom (or rule), rather\nthan incorporating the axiom as an inductive bias or inferring it from data\nvalues. A key question is whether the system would learn to generalize from the\naxiom demonstrations to more complex scenarios. Our results, based on applying\naxiomatic training to learn the transitivity axiom and d-separation rule,\nindicate that such generalization is possible. To avoid data contamination\nissues, we start with a 67 million parameter transformer model and train it\nfrom scratch. On both tasks, we find that a model trained on linear causal\nchains (along with some noisy variations) can generalize well to complex\ngraphs, including longer causal chains, causal chains with reversed order, and\ngraphs with branching.To handle diverse text inputs, the same method is\nextended to finetune language models. Finetuning Llama-3-8B-Instruct model on\nour axiomatic data leads to significant gains on causal benchmarks such as\nCorr2Cause and CLEAR, in some cases providing state-of-the-art performance\nsurpassing GPT-4.\n","authors":["Aniket Vashishtha","Abhinav Kumar","Atharva Pandey","Abbavaram Gowtham Reddy","Kabir Ahuja","Vineeth N Balasubramanian","Amit Sharma"],"pdf_url":"https://arxiv.org/pdf/2407.07612v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23794v2","updated":"2025-10-24T15:52:23Z","published":"2025-05-26T12:25:37Z","title":"R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via\n  Reinforcement Learning","summary":"  Retrieval-Augmented Generation (RAG) integrates external knowledge with Large\nLanguage Models (LLMs) to enhance factual correctness and mitigate\nhallucination. However, dense retrievers often become the bottleneck of RAG\nsystems due to their limited parameters compared to LLMs and their inability to\nperform step-by-step reasoning. While prompt-based iterative RAG attempts to\naddress these limitations, it is constrained by human-designed workflows. To\naddress these limitations, we propose $\\textbf{R3-RAG}$, which uses\n$\\textbf{R}$einforcement learning to make the LLM learn how to\n$\\textbf{R}$eason and $\\textbf{R}$etrieve step by step, thus retrieving\ncomprehensive external knowledge and leading to correct answers. R3-RAG is\ndivided into two stages. We first use cold start to make the model learn the\nmanner of iteratively interleaving reasoning and retrieval. Then we use\nreinforcement learning to further harness its ability to better explore the\nexternal retrieval environment. Specifically, we propose two rewards for\nR3-RAG: 1) answer correctness for outcome reward, which judges whether the\ntrajectory leads to a correct answer; 2) relevance-based document verification\nfor process reward, encouraging the model to retrieve documents that are\nrelevant to the user question, through which we can let the model learn how to\niteratively reason and retrieve relevant documents to get the correct answer.\nExperimental results show that R3-RAG significantly outperforms baselines and\ncan transfer well to different retrievers. We release R3-RAG at\nhttps://github.com/Yuan-Li-FNLP/R3-RAG.\n","authors":["Yuan Li","Qi Luo","Xiaonan Li","Bufan Li","Qinyuan Cheng","Bo Wang","Yining Zheng","Yuxin Wang","Zhangyue Yin","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2505.23794v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21584v1","updated":"2025-10-24T15:51:10Z","published":"2025-10-24T15:51:10Z","title":"Automated Quality Control for Language Documentation: Detecting\n  Phonotactic Inconsistencies in a Kokborok Wordlist","summary":"  Lexical data collection in language documentation often contains\ntranscription errors and undocumented borrowings that can mislead linguistic\nanalysis. We present unsupervised anomaly detection methods to identify\nphonotactic inconsistencies in wordlists, applying them to a multilingual\ndataset of Kokborok varieties with Bangla. Using character-level and\nsyllable-level phonotactic features, our algorithms identify potential\ntranscription errors and borrowings. While precision and recall remain modest\ndue to the subtle nature of these anomalies, syllable-aware features\nsignificantly outperform character-level baselines. The high-recall approach\nprovides fieldworkers with a systematic method to flag entries requiring\nverification, supporting data quality improvement in low-resourced language\ndocumentation.\n","authors":["Kellen Parker van Dam","Abishek Stephen"],"pdf_url":"https://arxiv.org/pdf/2510.21584v1.pdf","comment":"Submitted to The 5th Workshop on Evaluation and Comparison for NLP\n  systems (Eval4NLP) 2025"},{"id":"http://arxiv.org/abs/2510.21575v1","updated":"2025-10-24T15:43:42Z","published":"2025-10-24T15:43:42Z","title":"From Polyester Girlfriends to Blind Mice: Creating the First Pragmatics\n  Understanding Benchmarks for Slovene","summary":"  Large language models are demonstrating increasing capabilities, excelling at\nbenchmarks once considered very difficult. As their capabilities grow, there is\na need for more challenging evaluations that go beyond surface-level linguistic\ncompetence. Namely, language competence involves not only syntax and semantics\nbut also pragmatics, i.e., understanding situational meaning as shaped by\ncontext as well as linguistic and cultural norms. To contribute to this line of\nresearch, we introduce SloPragEval and SloPragMega, the first pragmatics\nunderstanding benchmarks for Slovene that contain altogether 405\nmultiple-choice questions. We discuss the difficulties of translation, describe\nthe campaign to establish a human baseline, and report pilot evaluations with\nLLMs. Our results indicate that current models have greatly improved in\nunderstanding nuanced language but may still fail to infer implied speaker\nmeaning in non-literal utterances, especially those that are culture-specific.\nWe also observe a significant gap between proprietary and open-source models.\nFinally, we argue that benchmarks targeting nuanced language understanding and\nknowledge of the target culture must be designed with care, preferably\nconstructed from native data, and validated with human responses.\n","authors":["Mojca Brglez","Å pela Vintar"],"pdf_url":"https://arxiv.org/pdf/2510.21575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.20176v2","updated":"2025-10-24T15:36:31Z","published":"2025-10-23T03:51:17Z","title":"Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table\n  Understanding","summary":"  Understanding and reasoning over tables is a critical capability for many\nreal-world applications. Large language models (LLMs) have shown promise on\nthis task, but current approaches remain limited. Fine-tuning based methods\nstrengthen language reasoning; yet they are prone to arithmetic errors and\nhallucination. In contrast, tool-based methods enable precise table\nmanipulation but rely on rigid schemas and lack semantic understanding. These\ncomplementary drawbacks highlight the need for approaches that integrate robust\nreasoning with reliable table processing. In this work, we propose\nMixture-of-Minds, a multi-agent framework that decomposes table reasoning into\nthree specialized roles: planning, coding, and answering. This design enables\neach agent to focus on a specific aspect of the task while leveraging code\nexecution for precise table manipulation. Building on this workflow, we\nintroduce a self-improvement training framework that employs Monte Carlo Tree\nSearch (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents\nwith reinforcement learning (RL). Extensive experiments show that\nMixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and\nsurpassing OpenAI-o4-mini-high. These results demonstrate the promise of\ncombining structured multi-agent workflows with RL to advance table\nunderstanding.\n","authors":["Yuhang Zhou","Mingrui Zhang","Ke Li","Mingyi Wang","Qiao Liu","Qifei Wang","Jiayi Liu","Fei Liu","Serena Li","Weiwei Li","Mingze Gao","Abhishek Kumar","Xiangjun Fan","Zhuokai Zhao","Lizhu Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.20176v2.pdf","comment":"18 pages, 4 figures"},{"id":"http://arxiv.org/abs/2510.21566v1","updated":"2025-10-24T15:26:30Z","published":"2025-10-24T15:26:30Z","title":"ColorEcosystem: Powering Personalized, Standardized, and Trustworthy\n  Agentic Service in massive-agent Ecosystem","summary":"  With the rapid development of (multimodal) large language model-based agents,\nthe landscape of agentic service management has evolved from single-agent\nsystems to multi-agent systems, and now to massive-agent ecosystems. Current\nmassive-agent ecosystems face growing challenges, including impersonal service\nexperiences, a lack of standardization, and untrustworthy behavior. To address\nthese issues, we propose ColorEcosystem, a novel blueprint designed to enable\npersonalized, standardized, and trustworthy agentic service at scale.\nConcretely, ColorEcosystem consists of three key components: agent carrier,\nagent store, and agent audit. The agent carrier provides personalized service\nexperiences by utilizing user-specific data and creating a digital twin, while\nthe agent store serves as a centralized, standardized platform for managing\ndiverse agentic services. The agent audit, based on the supervision of\ndeveloper and user activities, ensures the integrity and credibility of both\nservice providers and users. Through the analysis of challenges, transitional\nforms, and practical considerations, the ColorEcosystem is poised to power\npersonalized, standardized, and trustworthy agentic service across\nmassive-agent ecosystems. Meanwhile, we have also implemented part of\nColorEcosystem's functionality, and the relevant code is open-sourced at\nhttps://github.com/opas-lab/color-ecosystem.\n","authors":["Fangwen Wu","Zheng Wu","Jihong Wang","Yunku Chen","Ruiguang Pei","Heyuan Huang","Xin Liao","Xingyu Lou","Huarong Deng","Zhihui Fu","Weiwen Liu","Zhuosheng Zhang","Weinan Zhang","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2510.21566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.16753v2","updated":"2025-10-24T15:20:55Z","published":"2025-08-22T19:13:21Z","title":"GAICo: A Deployed and Extensible Framework for Evaluating Diverse and\n  Multimodal Generative AI Outputs","summary":"  The rapid proliferation of Generative AI (GenAI) into diverse, high-stakes\ndomains necessitates robust and reproducible evaluation methods. However,\npractitioners often resort to ad-hoc, non-standardized scripts, as common\nmetrics are often unsuitable for specialized, structured outputs (e.g.,\nautomated plans, time-series) or holistic comparison across modalities (e.g.,\ntext, audio, and image). This fragmentation hinders comparability and slows AI\nsystem development. To address this challenge, we present GAICo (Generative AI\nComparator): a deployed, open-source Python library that streamlines and\nstandardizes GenAI output comparison. GAICo provides a unified, extensible\nframework supporting a comprehensive suite of reference-based metrics for\nunstructured text, specialized structured data formats, and multimedia (images,\naudio). Its architecture features a high-level API for rapid, end-to-end\nanalysis, from multi-model comparison to visualization and reporting, alongside\ndirect metric access for granular control. We demonstrate GAICo's utility\nthrough a detailed case study evaluating and debugging complex, multi-modal AI\nTravel Assistant pipelines. GAICo empowers AI researchers and developers to\nefficiently assess system performance, make evaluation reproducible, improve\ndevelopment velocity, and ultimately build more trustworthy AI systems,\naligning with the goal of moving faster and safer in AI deployment. Since its\nrelease on PyPI in Jun 2025, the tool has been downloaded over 13K times,\nacross versions, by Aug 2025, demonstrating growing community interest.\n","authors":["Nitin Gupta","Pallav Koppisetti","Kausik Lakkaraju","Biplav Srivastava"],"pdf_url":"https://arxiv.org/pdf/2508.16753v2.pdf","comment":"11 pages, 7 figures, accepted at IAAI/AAAI 2026; updated with\n  figures, captions, and acknowledgments"},{"id":"http://arxiv.org/abs/2510.21561v1","updated":"2025-10-24T15:20:40Z","published":"2025-10-24T15:20:40Z","title":"Are the LLMs Capable of Maintaining at Least the Language Genus?","summary":"  Large Language Models (LLMs) display notable variation in multilingual\nbehavior, yet the role of genealogical language structure in shaping this\nvariation remains underexplored. In this paper, we investigate whether LLMs\nexhibit sensitivity to linguistic genera by extending prior analyses on the\nMultiQ dataset. We first check if models prefer to switch to genealogically\nrelated languages when prompt language fidelity is not maintained. Next, we\ninvestigate whether knowledge consistency is better preserved within than\nacross genera. We show that genus-level effects are present but strongly\nconditioned by training resource availability. We further observe distinct\nmultilingual strategies across LLMs families. Our findings suggest that LLMs\nencode aspects of genus-level structure, but training data imbalances remain\nthe primary factor shaping their multilingual performance.\n","authors":["Sandra MitroviÄ","David Kletz","Ljiljana Dolamic","Fabio Rinaldi"],"pdf_url":"https://arxiv.org/pdf/2510.21561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03525v2","updated":"2025-10-24T15:17:29Z","published":"2025-06-04T03:18:01Z","title":"Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video\n  Reasoning","summary":"  Recent advances in Chain-of-Thought (CoT) reasoning have improved complex\nvideo understanding, but existing methods often struggle to adapt to\ndomain-specific skills (e.g., event detection, spatial relation understanding,\nemotion understanding) over various video content. To address this, we propose\nVideo-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs\nand leverages skill-aware CoT supervisions for domain-adaptive video reasoning.\nFirst, we construct skill-based CoT annotations: we extract domain-relevant\nreasoning skills from training questions, cluster them into a shared skill\ntaxonomy, and create detailed multi-step CoT rationale tailored to each\nvideo-question pair for training. Second, we introduce a skill-specific expert\nlearning framework. Each expert module specializes in a subset of reasoning\nskills and is trained with lightweight adapters using the collected CoT\nsupervision. We demonstrate the effectiveness of the proposed approach on three\nvideo understanding benchmarks, where Video-SKoT consistently outperforms\nstrong baselines. We also provide in-depth analyses on comparing different CoT\nannotation pipelines and learned skills over multiple video domains.\n","authors":["Daeun Lee","Jaehong Yoon","Jaemin Cho","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2506.03525v2.pdf","comment":"Project website: https://video-skill-cot.github.io/"},{"id":"http://arxiv.org/abs/2510.21553v1","updated":"2025-10-24T15:12:08Z","published":"2025-10-24T15:12:08Z","title":"Document Understanding, Measurement, and Manipulation Using Category\n  Theory","summary":"  We apply category theory to extract multimodal document structure which leads\nus to develop information theoretic measures, content summarization and\nextension, and self-supervised improvement of large pretrained models. We first\ndevelop a mathematical representation of a document as a category of\nquestion-answer pairs. Second, we develop an orthogonalization procedure to\ndivide the information contained in one or more documents into non-overlapping\npieces. The structures extracted in the first and second steps lead us to\ndevelop methods to measure and enumerate the information contained in a\ndocument. We also build on those steps to develop new summarization techniques,\nas well as to develop a solution to a new problem viz. exegesis resulting in an\nextension of the original document. Our question-answer pair methodology\nenables a novel rate distortion analysis of summarization techniques. We\nimplement our techniques using large pretrained models, and we propose a\nmultimodal extension of our overall mathematical framework. Finally, we develop\na novel self-supervised method using RLVR to improve large pretrained models\nusing consistency constraints such as composability and closure under certain\noperations that stem naturally from our category theoretic framework.\n","authors":["Jared Claypoole","Yunye Gong","Noson S. Yanofsky","Ajay Divakaran"],"pdf_url":"https://arxiv.org/pdf/2510.21553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.08221v2","updated":"2025-10-24T15:10:11Z","published":"2025-08-11T17:39:45Z","title":"Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning","summary":"  Reinforcement learning for LLM reasoning has rapidly emerged as a prominent\nresearch area, marked by a significant surge in related studies on both\nalgorithmic innovations and practical applications. Despite this progress,\nseveral critical challenges remain, including the absence of standardized\nguidelines for employing RL techniques and a fragmented understanding of their\nunderlying mechanisms. Additionally, inconsistent experimental settings,\nvariations in training data, and differences in model initialization have led\nto conflicting conclusions, obscuring the key characteristics of these\ntechniques and creating confusion among practitioners when selecting\nappropriate techniques. This paper systematically reviews widely adopted RL\ntechniques through rigorous reproductions and isolated evaluations within a\nunified open-source framework. We analyze the internal mechanisms, applicable\nscenarios, and core principles of each technique through fine-grained\nexperiments, including datasets of varying difficulty, model sizes, and\narchitectures. Based on these insights, we present clear guidelines for\nselecting RL techniques tailored to specific setups, and provide a reliable\nroadmap for practitioners navigating the RL for the LLM domain. Finally, we\nreveal that a minimalist combination of two techniques can unlock the learning\ncapability of critic-free policies using vanilla PPO loss. The results\ndemonstrate that our simple combination consistently improves performance,\nsurpassing strategies like GRPO and DAPO.\n","authors":["Zihe Liu","Jiashun Liu","Yancheng He","Weixun Wang","Jiaheng Liu","Ling Pan","Xinyu Hu","Shaopan Xiong","Ju Huang","Jian Hu","Shengyi Huang","Siran Yang","Jiamang Wang","Wenbo Su","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2508.08221v2.pdf","comment":"26 pages, 21 figures"},{"id":"http://arxiv.org/abs/2505.21749v2","updated":"2025-10-24T15:06:33Z","published":"2025-05-27T20:38:19Z","title":"Revisiting Bi-Linear State Transitions in Recurrent Neural Networks","summary":"  The role of hidden units in recurrent neural networks is typically seen as\nmodeling memory, with research focusing on enhancing information retention\nthrough gating mechanisms. A less explored perspective views hidden units as\nactive participants in the computation performed by the network, rather than\npassive memory stores. In this work, we revisit bilinear operations, which\ninvolve multiplicative interactions between hidden units and input embeddings.\nWe demonstrate theoretically and empirically that they constitute a natural\ninductive bias for representing the evolution of hidden states in state\ntracking tasks. These are the simplest type of tasks that require hidden units\nto actively contribute to the behavior of the network. We also show that\nbilinear state updates form a natural hierarchy corresponding to state tracking\ntasks of increasing complexity, with popular linear recurrent networks such as\nMamba residing at the lowest-complexity center of that hierarchy.\n","authors":["M. Reza Ebrahimi","Roland Memisevic"],"pdf_url":"https://arxiv.org/pdf/2505.21749v2.pdf","comment":"Accepted to NeurIPS 2025"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2510.21704v1","updated":"2025-10-24T17:59:02Z","published":"2025-10-24T17:59:02Z","title":"Automated Detection of Visual Attribute Reliance with a Self-Reflective\n  Agent","summary":"  When a vision model performs image recognition, which visual attributes drive\nits predictions? Detecting unintended reliance on specific visual features is\ncritical for ensuring model robustness, preventing overfitting, and avoiding\nspurious correlations. We introduce an automated framework for detecting such\ndependencies in trained vision models. At the core of our method is a\nself-reflective agent that systematically generates and tests hypotheses about\nvisual attributes that a model may rely on. This process is iterative: the\nagent refines its hypotheses based on experimental outcomes and uses a\nself-evaluation protocol to assess whether its findings accurately explain\nmodel behavior. When inconsistencies arise, the agent self-reflects over its\nfindings and triggers a new cycle of experimentation. We evaluate our approach\non a novel benchmark of 130 models designed to exhibit diverse visual attribute\ndependencies across 18 categories. Our results show that the agent's\nperformance consistently improves with self-reflection, with a significant\nperformance increase over non-reflective baselines. We further demonstrate that\nthe agent identifies real-world visual attribute dependencies in\nstate-of-the-art models, including CLIP's vision encoder and the YOLOv8 object\ndetector.\n","authors":["Christy Li","Josep Lopez CamuÃ±as","Jake Thomas Touchet","Jacob Andreas","Agata Lapedriza","Antonio Torralba","Tamar Rott Shaham"],"pdf_url":"https://arxiv.org/pdf/2510.21704v1.pdf","comment":"32 pages, 10 figures, Neurips 2025"},{"id":"http://arxiv.org/abs/2510.21697v1","updated":"2025-10-24T17:57:31Z","published":"2025-10-24T17:57:31Z","title":"Visual Diffusion Models are Geometric Solvers","summary":"  In this paper we show that visual diffusion models can serve as effective\ngeometric solvers: they can directly reason about geometric problems by working\nin pixel space. We first demonstrate this on the Inscribed Square Problem, a\nlong-standing problem in geometry that asks whether every Jordan curve contains\nfour points forming a square. We then extend the approach to two other\nwell-known hard geometric problems: the Steiner Tree Problem and the Simple\nPolygon Problem.\n  Our method treats each problem instance as an image and trains a standard\nvisual diffusion model that transforms Gaussian noise into an image\nrepresenting a valid approximate solution that closely matches the exact one.\nThe model learns to transform noisy geometric structures into correct\nconfigurations, effectively recasting geometric reasoning as image generation.\n  Unlike prior work that necessitates specialized architectures and\ndomain-specific adaptations when applying diffusion to parametric geometric\nrepresentations, we employ a standard visual diffusion model that operates on\nthe visual representation of the problem. This simplicity highlights a\nsurprising bridge between generative modeling and geometric problem solving.\nBeyond the specific problems studied here, our results point toward a broader\nparadigm: operating in image space provides a general and practical framework\nfor approximating notoriously hard problems, and opens the door to tackling a\nfar wider class of challenging geometric tasks.\n","authors":["Nir Goren","Shai Yehezkel","Omer Dahary","Andrey Voynov","Or Patashnik","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2510.21697v1.pdf","comment":"Project page: https://kariander1.github.io/visual-geo-solver/"},{"id":"http://arxiv.org/abs/2510.21696v1","updated":"2025-10-24T17:56:37Z","published":"2025-10-24T17:56:37Z","title":"BachVid: Training-Free Video Generation with Consistent Background and\n  Character","summary":"  Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training.\n","authors":["Han Yan","Xibin Song","Yifu Wang","Hongdong Li","Pan Ji","Chao Ma"],"pdf_url":"https://arxiv.org/pdf/2510.21696v1.pdf","comment":"Project page: https://wolfball.github.io/bachvid"},{"id":"http://arxiv.org/abs/2510.21689v1","updated":"2025-10-24T17:46:24Z","published":"2025-10-24T17:46:24Z","title":"On Thin Ice: Towards Explainable Conservation Monitoring via Attribution\n  and Perturbations","summary":"  Computer vision can accelerate ecological research and conservation\nmonitoring, yet adoption in ecology lags in part because of a lack of trust in\nblack-box neural-network-based models. We seek to address this challenge by\napplying post-hoc explanations to provide evidence for predictions and document\nlimitations that are important to field deployment. Using aerial imagery from\nGlacier Bay National Park, we train a Faster R-CNN to detect pinnipeds (harbor\nseals) and generate explanations via gradient-based class activation mapping\n(HiResCAM, LayerCAM), local interpretable model-agnostic explanations (LIME),\nand perturbation-based explanations. We assess explanations along three axes\nrelevant to field use: (i) localization fidelity: whether high-attribution\nregions coincide with the animal rather than background context; (ii)\nfaithfulness: whether deletion/insertion tests produce changes in detector\nconfidence; and (iii) diagnostic utility: whether explanations reveal\nsystematic failure modes. Explanations concentrate on seal torsos and contours\nrather than surrounding ice/rock, and removal of the seals reduces detection\nconfidence, providing model-evidence for true positives. The analysis also\nuncovers recurrent error sources, including confusion between seals and black\nice and rocks. We translate these findings into actionable next steps for model\ndevelopment, including more targeted data curation and augmentation. By pairing\nobject detection with post-hoc explainability, we can move beyond \"black-box\"\npredictions toward auditable, decision-supporting tools for conservation\nmonitoring.\n","authors":["Jiayi Zhou","GÃ¼nel Aghakishiyeva","Saagar Arya","Julian Dale","James David Poling","Holly R. Houliston","Jamie N. Womble","Gregory D. Larsen","David W. Johnston","Brinnae Bent"],"pdf_url":"https://arxiv.org/pdf/2510.21689v1.pdf","comment":"NeurIPS Imageomics Workshop 2025"},{"id":"http://arxiv.org/abs/2510.18813v2","updated":"2025-10-24T17:42:45Z","published":"2025-10-21T17:10:48Z","title":"A Geometric Approach to Steerable Convolutions","summary":"  In contrast to the somewhat abstract, group theoretical approach adopted by\nmany papers, our work provides a new and more intuitive derivation of steerable\nconvolutional neural networks in $d$ dimensions. This derivation is based on\ngeometric arguments and fundamental principles of pattern matching. We offer an\nintuitive explanation for the appearance of the Clebsch--Gordan decomposition\nand spherical harmonic basis functions. Furthermore, we suggest a novel way to\nconstruct steerable convolution layers using interpolation kernels that improve\nupon existing implementation, and offer greater robustness to noisy data.\n","authors":["Soumyabrata Kundu","Risi Kondor"],"pdf_url":"https://arxiv.org/pdf/2510.18813v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08053v3","updated":"2025-10-24T17:42:08Z","published":"2024-12-11T03:00:15Z","title":"DynamicPAE: Generating Scene-Aware Physical Adversarial Examples in\n  Real-Time","summary":"  Physical adversarial examples (PAEs) are regarded as whistle-blowers of\nreal-world risks in deep-learning applications, thus worth further\ninvestigation. However, current PAE generation studies show limited adaptive\nattacking ability to diverse and varying scenes, revealing the urgent\nrequirement of dynamic PAEs that are generated in real time and conditioned on\nthe observation from the attacker. The key challenge in generating dynamic PAEs\nis learning the sparse relation between PAEs and the observation of attackers\nunder the noisy feedback of attack training. To address the challenge, we\npresent DynamicPAE, the first generative framework that enables scene-aware\nreal-time physical attacks. Specifically, to address the noisy feedback problem\nthat obfuscates the exploration of scene-related PAEs, we introduce the\nresidual-guided adversarial pattern exploration technique. Residual-guided\ntraining, which relaxes the attack training with a reconstruction task, is\nproposed to enrich the feedback information, thereby achieving a more\ncomprehensive exploration of PAEs. To address the alignment problem between the\ntrained generator and the real-world scenario, we introduce the\ndistribution-matched attack scenario alignment, consisting of the\nconditional-uncertainty-aligned data module and the skewness-aligned objective\nre-weighting module. The former aligns the training environment with the\nincomplete observation of the real-world attacker. The latter facilitates\nconsistent stealth control across different attack targets with the skewness\ncontroller. Extensive digital and physical evaluations demonstrate the superior\nattack performance of DynamicPAE, attaining a 2.07 $\\times$ boost (58.8%\naverage AP drop under attack) on representative object detectors (e.g., DETR)\nover state-of-the-art static PAE generating methods. Overall, our work opens\nthe door to end-to-end modeling of dynamic PAEs.\n","authors":["Jin Hu","Xianglong Liu","Jiakai Wang","Junkai Zhang","Xianqi Yang","Haotong Qin","Yuqing Ma","Ke Xu"],"pdf_url":"https://arxiv.org/pdf/2412.08053v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21682v1","updated":"2025-10-24T17:39:52Z","published":"2025-10-24T17:39:52Z","title":"WorldGrow: Generating Infinite 3D World","summary":"  We tackle the challenge of generating the infinitely extendable 3D world --\nlarge, continuous environments with coherent geometry and realistic appearance.\nExisting methods face key challenges: 2D-lifting approaches suffer from\ngeometric and appearance inconsistencies across views, 3D implicit\nrepresentations are hard to scale up, and current 3D foundation models are\nmostly object-centric, limiting their applicability to scene-level generation.\nOur key insight is leveraging strong generation priors from pre-trained 3D\nmodels for structured scene block generation. To this end, we propose\nWorldGrow, a hierarchical framework for unbounded 3D scene synthesis. Our\nmethod features three core components: (1) a data curation pipeline that\nextracts high-quality scene blocks for training, making the 3D structured\nlatent representations suitable for scene generation; (2) a 3D block inpainting\nmechanism that enables context-aware scene extension; and (3) a coarse-to-fine\ngeneration strategy that ensures both global layout plausibility and local\ngeometric/textural fidelity. Evaluated on the large-scale 3D-FRONT dataset,\nWorldGrow achieves SOTA performance in geometry reconstruction, while uniquely\nsupporting infinite scene generation with photorealistic and structurally\nconsistent outputs. These results highlight its capability for constructing\nlarge-scale virtual environments and potential for building future world\nmodels.\n","authors":["Sikuang Li","Chen Yang","Jiemin Fang","Taoran Yi","Jia Lu","Jiazhong Cen","Lingxi Xie","Wei Shen","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2510.21682v1.pdf","comment":"Project page: https://world-grow.github.io/ Code:\n  https://github.com/world-grow/WorldGrow"},{"id":"http://arxiv.org/abs/2411.18322v2","updated":"2025-10-24T17:36:28Z","published":"2024-11-27T13:23:11Z","title":"Mixture of Experts in Image Classification: What's the Sweet Spot?","summary":"  Mixture-of-Experts (MoE) models have shown promising potential for\nparameter-efficient scaling across domains. However, their application to image\nclassification remains limited, often requiring billion-scale datasets to be\ncompetitive. In this work, we explore the integration of MoE layers into image\nclassification architectures using open datasets. We conduct a systematic\nanalysis across different MoE configurations and model scales. We find that\nmoderate parameter activation per sample provides the best trade-off between\nperformance and efficiency. However, as the number of activated parameters\nincreases, the benefits of MoE diminish. Our analysis yields several practical\ninsights for vision MoE design. First, MoE layers most effectively strengthen\ntiny and mid-sized models, while gains taper off for large-capacity networks\nand do not redefine state-of-the-art ImageNet performance. Second, a Last-2\nplacement heuristic offers the most robust cross-architecture choice, with\nEvery-2 slightly better for Vision Transform (ViT), and both remaining\neffective as data and model scale increase. Third, larger datasets (e.g.,\nImageNet-21k) allow more experts, up to 16, for ConvNeXt to be utilized\neffectively without changing placement, as increased data reduces overfitting\nand promotes broader expert specialization. Finally, a simple linear router\nperforms best, suggesting that additional routing complexity yields no\nconsistent benefit.\n","authors":["Mathurin Videau","Alessandro Leite","Marc Schoenauer","Olivier Teytaud"],"pdf_url":"https://arxiv.org/pdf/2411.18322v2.pdf","comment":"Published in Transactions on Machine Learning Research"},{"id":"http://arxiv.org/abs/2506.12945v2","updated":"2025-10-24T17:23:51Z","published":"2025-06-15T19:12:37Z","title":"Metropolis-Hastings Sampling for 3D Gaussian Reconstruction","summary":"  We propose an adaptive sampling framework for 3D Gaussian Splatting (3DGS)\nthat leverages comprehensive multi-view photometric error signals within a\nunified Metropolis-Hastings approach. Vanilla 3DGS heavily relies on\nheuristic-based density-control mechanisms (e.g., cloning, splitting, and\npruning), which can lead to redundant computations or premature removal of\nbeneficial Gaussians. Our framework overcomes these limitations by\nreformulating densification and pruning as a probabilistic sampling process,\ndynamically inserting and relocating Gaussians based on aggregated multi-view\nerrors and opacity scores. Guided by Bayesian acceptance tests derived from\nthese error-based importance scores, our method substantially reduces reliance\non heuristics, offers greater flexibility, and adaptively infers Gaussian\ndistributions without requiring predefined scene complexity. Experiments on\nbenchmark datasets, including Mip-NeRF360, Tanks and Temples and Deep Blending,\nshow that our approach reduces the number of Gaussians needed, achieving faster\nconvergence while matching or modestly surpassing the view-synthesis quality of\nstate-of-the-art models.\n","authors":["Hyunjin Kim","Haebeom Jung","Jaesik Park"],"pdf_url":"https://arxiv.org/pdf/2506.12945v2.pdf","comment":"NeurIPS 2025. Project Page: https://hjhyunjinkim.github.io/MH-3DGS"},{"id":"http://arxiv.org/abs/2510.21664v1","updated":"2025-10-24T17:21:43Z","published":"2025-10-24T17:21:43Z","title":"Foundation Models in Dermatopathology: Skin Tissue Classification","summary":"  The rapid generation of whole-slide images (WSIs) in dermatopathology\nnecessitates automated methods for efficient processing and accurate\nclassification. This study evaluates the performance of two foundation models,\nUNI and Virchow2, as feature extractors for classifying WSIs into three\ndiagnostic categories: melanocytic, basaloid, and squamous lesions. Patch-level\nembeddings were aggregated into slide-level features using a mean-aggregation\nstrategy and subsequently used to train multiple machine learning classifiers,\nincluding logistic regression, gradient-boosted trees, and random forest\nmodels. Performance was assessed using precision, recall, true positive rate,\nfalse positive rate, and the area under the receiver operating characteristic\ncurve (AUROC) on the test set. Results demonstrate that patch-level features\nextracted using Virchow2 outperformed those extracted via UNI across most\nslide-level classifiers, with logistic regression achieving the highest\naccuracy (90%) for Virchow2, though the difference was not statistically\nsignificant. The study also explored data augmentation techniques and image\nnormalization to enhance model robustness and generalizability. The\nmean-aggregation approach provided reliable slide-level feature\nrepresentations. All experimental results and metrics were tracked and\nvisualized using WandB.ai, facilitating reproducibility and interpretability.\nThis research highlights the potential of foundation models for automated WSI\nclassification, providing a scalable and effective approach for\ndermatopathological diagnosis while paving the way for future advancements in\nslide-level representation learning.\n","authors":["Riya Gupta","Yiwei Zong","Dennis H. Murphree"],"pdf_url":"https://arxiv.org/pdf/2510.21664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21663v1","updated":"2025-10-24T17:17:46Z","published":"2025-10-24T17:17:46Z","title":"Self-Supervised Learning of Synapse Types from EM Images","summary":"  Separating synapses into different classes based on their appearance in EM\nimages has many applications in biology. Examples may include assigning a\nneurotransmitter to a particular class, or separating synapses whose strength\ncan be modulated from those whose strength is fixed. Traditionally, this has\nbeen done in a supervised manner, giving the classification algorithm examples\nof the different classes. Here we instead separate synapses into classes based\nonly on the observation that nearby synapses in the same neuron are likely more\nsimilar than synapses chosen randomly from different cells. We apply our\nmethodology to data from {\\it Drosophila}. Our approach has the advantage that\nthe number of synapse types does not need to be known in advance. It may also\nprovide a principled way to select ground-truth that spans the range of synapse\nstructure.\n","authors":["Aarav Shetty","Gary B Huang"],"pdf_url":"https://arxiv.org/pdf/2510.21663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.11128v2","updated":"2025-10-24T17:14:46Z","published":"2025-10-13T08:19:56Z","title":"Lightweight Facial Landmark Detection in Thermal Images via Multi-Level\n  Cross-Modal Knowledge Transfer","summary":"  Facial Landmark Detection (FLD) in thermal imagery is critical for\napplications in challenging lighting conditions, but it is hampered by the lack\nof rich visual cues. Conventional cross-modal solutions, like feature fusion or\nimage translation from RGB data, are often computationally expensive or\nintroduce structural artifacts, limiting their practical deployment. To address\nthis, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a\nnovel framework that decouples high-fidelity RGB-to-thermal knowledge transfer\nfrom model compression to create both accurate and efficient thermal FLD\nmodels. A central challenge during knowledge transfer is the profound modality\ngap between RGB and thermal data, where traditional unidirectional distillation\nfails to enforce semantic consistency across disparate feature spaces. To\novercome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a\nbidirectional mechanism designed specifically for this task. DIKD establishes a\nconnection between modalities: it not only guides the thermal student with rich\nRGB features but also validates the student's learned representations by\nfeeding them back into the frozen teacher's prediction head. This closed-loop\nsupervision forces the student to learn modality-invariant features that are\nsemantically aligned with the teacher, ensuring a robust and profound knowledge\ntransfer. Experiments show that our approach sets a new state-of-the-art on\npublic thermal FLD benchmarks, notably outperforming previous methods while\ndrastically reducing computational overhead.\n","authors":["Qiyi Tong","Olivia Nocentini","Marta Lagomarsino","Kuanqi Cai","Marta Lorenzini","Arash Ajoudani"],"pdf_url":"https://arxiv.org/pdf/2510.11128v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21657v1","updated":"2025-10-24T17:13:37Z","published":"2025-10-24T17:13:37Z","title":"Long-tailed Species Recognition in the NACTI Wildlife Dataset","summary":"  As most ''in the wild'' data collections of the natural world, the North\nAmerica Camera Trap Images (NACTI) dataset shows severe long-tailed class\nimbalance, noting that the largest 'Head' class alone covers >50% of the 3.7M\nimages in the corpus. Building on the PyTorch Wildlife model, we present a\nsystematic study of Long-Tail Recognition methodologies for species recognition\non the NACTI dataset covering experiments on various LTR loss functions plus\nLTR-sensitive regularisation. Our best configuration achieves 99.40% Top-1\naccuracy on our NACTI test data split, substantially improving over a 95.51%\nbaseline using standard cross-entropy with Adam. This also improves on\npreviously reported top performance in MLWIC2 at 96.8% albeit using partly\nunpublished (potentially different) partitioning, optimiser, and evaluation\nprotocols. To evaluate domain shifts (e.g. night-time captures, occlusion,\nmotion-blur) towards other datasets we construct a Reduced-Bias Test set from\nthe ENA-Detection dataset where our experimentally optimised long-tail enhanced\nmodel achieves leading 52.55% accuracy (up from 51.20% with WCE loss),\ndemonstrating stronger generalisation capabilities under distribution shift. We\ndocument the consistent improvements of LTR-enhancing scheduler choices in this\nNACTI wildlife domain, particularly when in tandem with state-of-the-art LTR\nlosses. We finally discuss qualitative and quantitative shortcomings that LTR\nmethods cannot sufficiently address, including catastrophic breakdown for\n'Tail' classes under severe domain shift. For maximum reproducibility we\npublish all dataset splits, key code, and full network weights.\n","authors":["Zehua Liu","Tilo Burghardt"],"pdf_url":"https://arxiv.org/pdf/2510.21657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21654v1","updated":"2025-10-24T17:11:50Z","published":"2025-10-24T17:11:50Z","title":"Group Inertial Poser: Multi-Person Pose and Global Translation from\n  Sparse Inertial Sensors and Ultra-Wideband Ranging","summary":"  Tracking human full-body motion using sparse wearable inertial measurement\nunits (IMUs) overcomes the limitations of occlusion and instrumentation of the\nenvironment inherent in vision-based approaches. However, purely IMU-based\ntracking compromises translation estimates and accurate relative positioning\nbetween individuals, as inertial cues are inherently self-referential and\nprovide no direct spatial reference for others. In this paper, we present a\nnovel approach for robustly estimating body poses and global translation for\nmultiple individuals by leveraging the distances between sparse wearable\nsensors - both on each individual and across multiple individuals. Our method\nGroup Inertial Poser estimates these absolute distances between pairs of\nsensors from ultra-wideband ranging (UWB) and fuses them with inertial\nobservations as input into structured state-space models to integrate temporal\nmotion patterns for precise 3D pose estimation. Our novel two-step optimization\nfurther leverages the estimated distances for accurately tracking people's\nglobal trajectories through the world. We also introduce GIP-DB, the first\nIMU+UWB dataset for two-person tracking, which comprises 200 minutes of motion\nrecordings from 14 participants. In our evaluation, Group Inertial Poser\noutperforms previous state-of-the-art methods in accuracy and robustness across\nsynthetic and real-world data, showing the promise of IMU+UWB-based multi-human\nmotion capture in the wild. Code, models, dataset:\nhttps://github.com/eth-siplab/GroupInertialPoser\n","authors":["Ying Xue","Jiaxi Jiang","Rayan Armani","Dominik Hollidt","Yi-Chi Liao","Christian Holz"],"pdf_url":"https://arxiv.org/pdf/2510.21654v1.pdf","comment":"Accepted by ICCV 2025, Code:\n  https://github.com/eth-siplab/GroupInertialPoser"},{"id":"http://arxiv.org/abs/2510.21649v1","updated":"2025-10-24T17:07:27Z","published":"2025-10-24T17:07:27Z","title":"A Dynamic Knowledge Distillation Method Based on the Gompertz Curve","summary":"  This paper introduces a novel dynamic knowledge distillation framework,\nGompertz-CNN, which integrates the Gompertz growth model into the training\nprocess to address the limitations of traditional knowledge distillation.\nConventional methods often fail to capture the evolving cognitive capacity of\nstudent models, leading to suboptimal knowledge transfer. To overcome this, we\npropose a stage-aware distillation strategy that dynamically adjusts the weight\nof distillation loss based on the Gompertz curve, reflecting the student's\nlearning progression: slow initial growth, rapid mid-phase improvement, and\nlate-stage saturation. Our framework incorporates Wasserstein distance to\nmeasure feature-level discrepancies and gradient matching to align backward\npropagation behaviors between teacher and student models. These components are\nunified under a multi-loss objective, where the Gompertz curve modulates the\ninfluence of distillation losses over time. Extensive experiments on CIFAR-10\nand CIFAR-100 using various teacher-student architectures (e.g., ResNet50 and\nMobileNet_v2) demonstrate that Gompertz-CNN consistently outperforms\ntraditional distillation methods, achieving up to 8% and 4% accuracy gains on\nCIFAR-10 and CIFAR-100, respectively.\n","authors":["Han Yang","Guangjun Qin"],"pdf_url":"https://arxiv.org/pdf/2510.21649v1.pdf","comment":"15 pages, 2 figures"},{"id":"http://arxiv.org/abs/2510.21635v1","updated":"2025-10-24T16:44:40Z","published":"2025-10-24T16:44:40Z","title":"DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective\n  Cross-Domain Learning","summary":"  Compared to 2D data, the scale of point cloud data in different domains\navailable for training, is quite limited. Researchers have been trying to\ncombine these data of different domains for masked autoencoder (MAE)\npre-training to leverage such a data scarcity issue. However, the prior\nknowledge learned from mixed domains may not align well with the downstream 3D\npoint cloud analysis tasks, leading to degraded performance. To address such an\nissue, we propose the Domain-Adaptive Point Cloud Masked Autoencoder (DAP-MAE),\nan MAE pre-training method, to adaptively integrate the knowledge of\ncross-domain datasets for general point cloud analysis. In DAP-MAE, we design a\nheterogeneous domain adapter that utilizes an adaptation mode during\npre-training, enabling the model to comprehensively learn information from\npoint clouds across different domains, while employing a fusion mode in the\nfine-tuning to enhance point cloud features. Meanwhile, DAP-MAE incorporates a\ndomain feature generator to guide the adaptation of point cloud features to\nvarious downstream tasks. With only one pre-training, DAP-MAE achieves\nexcellent performance across four different point cloud analysis tasks,\nreaching 95.18% in object classification on ScanObjectNN and 88.45% in facial\nexpression recognition on Bosphorus.\n","authors":["Ziqi Gao","Qiufu Li","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2510.21635v1.pdf","comment":"14 pages, 7 figures, conference"},{"id":"http://arxiv.org/abs/2506.02964v2","updated":"2025-10-24T16:25:21Z","published":"2025-06-03T14:59:22Z","title":"FORLA: Federated Object-centric Representation Learning with Slot\n  Attention","summary":"  Learning efficient visual representations across heterogeneous unlabeled\ndatasets remains a central challenge in federated learning. Effective federated\nrepresentations require features that are jointly informative across clients\nwhile disentangling domain-specific factors without supervision. We introduce\nFORLA, a novel framework for federated object-centric representation learning\nand feature adaptation across clients using unsupervised slot attention. At the\ncore of our method is a shared feature adapter, trained collaboratively across\nclients to adapt features from foundation models, and a shared slot attention\nmodule that learns to reconstruct the adapted features. To optimize this\nadapter, we design a two-branch student-teacher architecture. In each client, a\nstudent decoder learns to reconstruct full features from foundation models,\nwhile a teacher decoder reconstructs their adapted, low-dimensional\ncounterpart. The shared slot attention module bridges cross-domain learning by\naligning object-level representations across clients. Experiments in multiple\nreal-world datasets show that our framework not only outperforms centralized\nbaselines on object discovery but also learns a compact, universal\nrepresentation that generalizes well across domains. This work highlights\nfederated slot attention as an effective tool for scalable, unsupervised visual\nrepresentation learning from cross-domain data with distributed concepts.\n","authors":["Guiqiu Liao","Matjaz Jogan","Eric Eaton","Daniel A. Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2506.02964v2.pdf","comment":"Accepted by Neurips2025"},{"id":"http://arxiv.org/abs/2510.05051v2","updated":"2025-10-24T16:24:47Z","published":"2025-10-06T17:31:32Z","title":"SegMASt3R: Geometry Grounded Segment Matching","summary":"  Segment matching is an important intermediate task in computer vision that\nestablishes correspondences between semantically or geometrically coherent\nregions across images. Unlike keypoint matching, which focuses on localized\nfeatures, segment matching captures structured regions, offering greater\nrobustness to occlusions, lighting variations, and viewpoint changes. In this\npaper, we leverage the spatial understanding of 3D foundation models to tackle\nwide-baseline segment matching, a challenging setting involving extreme\nviewpoint shifts. We propose an architecture that uses the inductive bias of\nthese 3D foundation models to match segments across image pairs with up to 180\ndegree view-point change rotation. Extensive experiments show that our approach\noutperforms state-of-the-art methods, including the SAM2 video propagator and\nlocal feature matching methods, by up to 30% on the AUPRC metric, on ScanNet++\nand Replica datasets. We further demonstrate benefits of the proposed model on\nrelevant downstream tasks, including 3D instance mapping and object-relative\nnavigation. Project Page: https://segmast3r.github.io/\n","authors":["Rohit Jayanti","Swayam Agrawal","Vansh Garg","Siddharth Tourani","Muhammad Haris Khan","Sourav Garg","Madhava Krishna"],"pdf_url":"https://arxiv.org/pdf/2510.05051v2.pdf","comment":"Accepted to The 39th Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2025) as a Spotlight (top 3.5%)"},{"id":"http://arxiv.org/abs/2412.06646v3","updated":"2025-10-24T16:24:17Z","published":"2024-12-09T16:39:40Z","title":"The Narrow Gate: Localized Image-Text Communication in Native Multimodal\n  Models","summary":"  Recent advances in multimodal training have significantly improved the\nintegration of image understanding and generation within a unified model. This\nstudy investigates how vision-language models (VLMs) handle image-understanding\ntasks, focusing on how visual information is processed and transferred to the\ntextual domain. We compare native multimodal VLMs, models trained from scratch\non multimodal data to generate both text and images, and non-native multimodal\nVLMs, models adapted from pre-trained large language models or capable of\ngenerating only text, highlighting key differences in information flow. We find\nthat in native multimodal VLMs, image and text embeddings are more separated\nwithin the residual stream. Moreover, VLMs differ in how visual information\nreaches text: non-native multimodal VLMs exhibit a distributed communication\npattern, where information is exchanged through multiple image tokens, whereas\nmodels trained natively for joint image and text generation tend to rely on a\nsingle post-image token that acts as a narrow gate for visual information. We\nshow that ablating this single token significantly deteriorates\nimage-understanding performance, whereas targeted, token-level interventions\nreliably steer image semantics and downstream text with fine-grained control.\n","authors":["Alessandro Serra","Francesco Ortu","Emanuele Panizon","Lucrezia Valeriani","Lorenzo Basile","Alessio Ansuini","Diego Doimo","Alberto Cazzaniga"],"pdf_url":"https://arxiv.org/pdf/2412.06646v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21615v1","updated":"2025-10-24T16:21:37Z","published":"2025-10-24T16:21:37Z","title":"Epipolar Geometry Improves Video Generation Models","summary":"  Video generation models have progressed tremendously through large latent\ndiffusion transformers trained with rectified flow techniques. Yet these models\nstill struggle with geometric inconsistencies, unstable motion, and visual\nartifacts that break the illusion of realistic 3D scenes. 3D-consistent video\ngeneration could significantly impact numerous downstream applications in\ngeneration and reconstruction tasks. We explore how epipolar geometry\nconstraints improve modern video diffusion models. Despite massive training\ndata, these models fail to capture fundamental geometric principles underlying\nvisual content. We align diffusion models using pairwise epipolar geometry\nconstraints via preference-based optimization, directly addressing unstable\ncamera trajectories and geometric artifacts through mathematically principled\ngeometric enforcement. Our approach efficiently enforces geometric principles\nwithout requiring end-to-end differentiability. Evaluation demonstrates that\nclassical geometric constraints provide more stable optimization signals than\nmodern learned metrics, which produce noisy targets that compromise alignment\nquality. Training on static scenes with dynamic cameras ensures high-quality\nmeasurements while the model generalizes effectively to diverse dynamic\ncontent. By bridging data-driven deep learning with classical geometric\ncomputer vision, we present a practical method for generating spatially\nconsistent videos without compromising visual quality.\n","authors":["Orest Kupyn","Fabian Manhardt","Federico Tombari","Christian Rupprecht"],"pdf_url":"https://arxiv.org/pdf/2510.21615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14109v2","updated":"2025-10-24T16:20:41Z","published":"2025-03-18T10:25:28Z","title":"Operational Change Detection for Geographical Information: Overview and\n  Challenges","summary":"  Rapid evolution of territories due to climate change and human impact\nrequires prompt and effective updates to geospatial databases maintained by the\nNational Mapping Agency. This paper presents a comprehensive overview of change\ndetection methods tailored for the operational updating of large-scale\ngeographic databases. This review first outlines the fundamental definition of\nchange, emphasizing its multifaceted nature, from temporal to semantic\ncharacterization. It categorizes automatic change detection methods into four\nmain families: rule-based, statistical, machine learning, and simulation\nmethods. The strengths, limitations, and applicability of every family are\ndiscussed in the context of various input data. Then, key applications for\nNational Mapping Agencies are identified, particularly the optimization of\ngeospatial database updating, change-based phenomena, and dynamics monitoring.\nFinally, the paper highlights the current challenges for leveraging change\ndetection such as the variability of change definition, the missing of relevant\nlarge-scale datasets, the diversity of input data, the unstudied no-change\ndetection, the human in the loop integration and the operational constraints.\nThe discussion underscores the necessity for ongoing innovation in change\ndetection techniques to address the future needs of geographic information\nsystems for national mapping agencies.\n","authors":["Nicolas Gonthier"],"pdf_url":"https://arxiv.org/pdf/2503.14109v2.pdf","comment":"Preprint under review"},{"id":"http://arxiv.org/abs/2507.06485v2","updated":"2025-10-24T16:19:27Z","published":"2025-07-09T02:06:13Z","title":"Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for\n  Efficient and Enhanced Video Reasoning","summary":"  Despite advances in reinforcement learning (RL)-based video reasoning with\nlarge language models (LLMs), data collection and fine-tuning remain\nsignificant challenges. These methods often rely on large-scale supervised\nfine-tuning (SFT) with extensive video data and long Chain-of-Thought (CoT)\nannotations, making them costly and hard to scale. To address this, we present\nVideo-RTS, a new approach to improve video reasoning capability with\ndrastically improved data efficiency by combining data-efficient RL with a\nvideo-adaptive test-time scaling (TTS) strategy. Building on observations about\nthe data scaling, we skip the resource-intensive SFT step and employ efficient\npure-RL training with output-based rewards, requiring no additional annotations\nor extensive fine-tuning. Furthermore, to utilize computational resources more\nefficiently, we introduce a sparse-to-dense video TTS strategy that improves\ninference by iteratively adding frames based on output consistency. We validate\nour approach on multiple video reasoning benchmarks, showing that Video-RTS\nsurpasses existing video reasoning models by 2.4% in accuracy using only 3.6%\ntraining samples. Specifically, Video-RTS achieves a 4.2% improvement on\nVideo-Holmes, a recent and challenging video reasoning benchmark. Notably, our\npure RL training and adaptive video TTS offer complementary strengths, enabling\nVideo-RTS's strong reasoning performance.\n","authors":["Ziyang Wang","Jaehong Yoon","Shoubin Yu","Md Mohaiminul Islam","Gedas Bertasius","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2507.06485v2.pdf","comment":"EMNLP 2025. The first two authors contributed equally. Project page:\n  https://sites.google.com/cs.unc.edu/videorts2025/"},{"id":"http://arxiv.org/abs/2510.21606v1","updated":"2025-10-24T16:11:10Z","published":"2025-10-24T16:11:10Z","title":"Modest-Align: Data-Efficient Alignment for Vision-Language Models","summary":"  Cross-modal alignment aims to map heterogeneous modalities into a shared\nlatent space, as exemplified by models like CLIP, which benefit from\nlarge-scale image-text pretraining for strong recognition capabilities.\nHowever, when operating in resource-constrained settings with limited or\nlow-quality data, these models often suffer from overconfidence and degraded\nperformance due to the prevalence of ambiguous or weakly correlated image-text\npairs. Current contrastive learning approaches, which rely on single positive\npairs, further exacerbate this issue by reinforcing overconfidence on uncertain\nsamples. To address these challenges, we propose Modest-Align, a lightweight\nalignment framework designed for robustness and efficiency. Our approach\nleverages two complementary strategies -- Random Perturbation, which introduces\ncontrolled noise to simulate uncertainty, and Embedding Smoothing, which\ncalibrates similarity distributions in the embedding space. These mechanisms\ncollectively reduce overconfidence and improve performance on noisy or weakly\naligned samples. Extensive experiments across multiple benchmark datasets\ndemonstrate that Modest-Align outperforms state-of-the-art methods in retrieval\ntasks, achieving competitive results with over 100x less training data and 600x\nless GPU time than CLIP. Our method offers a practical and scalable solution\nfor cross-modal alignment in real-world, low-resource scenarios.\n","authors":["Jiaxiang Liu","Yuan Wang","Jiawei Du","Joey Tianyi Zhou","Mingkun Xu","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2510.21606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21605v1","updated":"2025-10-24T16:10:09Z","published":"2025-10-24T16:10:09Z","title":"S3OD: Towards Generalizable Salient Object Detection with Synthetic Data","summary":"  Salient object detection exemplifies data-bounded tasks where expensive\npixel-precise annotations force separate model training for related subtasks\nlike DIS and HR-SOD. We present a method that dramatically improves\ngeneralization through large-scale synthetic data generation and\nambiguity-aware architecture. We introduce S3OD, a dataset of over 139,000\nhigh-resolution images created through our multi-modal diffusion pipeline that\nextracts labels from diffusion and DINO-v3 features. The iterative generation\nframework prioritizes challenging categories based on model performance. We\npropose a streamlined multi-mask decoder that naturally handles the inherent\nambiguity in salient object detection by predicting multiple valid\ninterpretations. Models trained solely on synthetic data achieve 20-50% error\nreduction in cross-dataset generalization, while fine-tuned versions reach\nstate-of-the-art performance across DIS and HR-SOD benchmarks.\n","authors":["Orest Kupyn","Hirokatsu Kataoka","Christian Rupprecht"],"pdf_url":"https://arxiv.org/pdf/2510.21605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21596v1","updated":"2025-10-24T16:02:05Z","published":"2025-10-24T16:02:05Z","title":"Automated interictal epileptic spike detection from simple and noisy\n  annotations in MEG data","summary":"  In drug-resistant epilepsy, presurgical evaluation of epilepsy can be\nconsidered. Magnetoencephalography (MEG) has been shown to be an effective exam\nto inform the localization of the epileptogenic zone through the localization\nof interictal epileptic spikes. Manual detection of these pathological\nbiomarkers remains a fastidious and error-prone task due to the high\ndimensionality of MEG recordings, and interrater agreement has been reported to\nbe only moderate. Current automated methods are unsuitable for clinical\npractice, either requiring extensively annotated data or lacking robustness on\nnon-typical data. In this work, we demonstrate that deep learning models can be\nused for detecting interictal spikes in MEG recordings, even when only temporal\nand single-expert annotations are available, which represents real-world\nclinical practice. We propose two model architectures: a feature-based\nartificial neural network (ANN) and a convolutional neural network (CNN),\ntrained on a database of 59 patients, and evaluated against a state-of-the-art\nmodel to classify short time windows of signal. In addition, we employ an\ninteractive machine learning strategy to iteratively improve our data\nannotation quality using intermediary model outputs. Both proposed models\noutperform the state-of-the-art model (F1-scores: CNN=0.46, ANN=0.44) when\ntested on 10 holdout test patients. The interactive machine learning strategy\ndemonstrates that our models are robust to noisy annotations. Overall, results\nhighlight the robustness of models with simple architectures when analyzing\ncomplex and imperfectly annotated data. Our method of interactive machine\nlearning offers great potential for faster data annotation, while our models\nrepresent useful and efficient tools for automated interictal spikes detection.\n","authors":["Pauline Mouches","Julien Jung","Armand Demasson","AgnÃ¨s Guinard","Romain Bouet","Rosalie Marchal","Romain Quentin"],"pdf_url":"https://arxiv.org/pdf/2510.21596v1.pdf","comment":"17 pages, 7 Figures"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2510.21671v1","updated":"2025-10-24T17:27:35Z","published":"2025-10-24T17:27:35Z","title":"A Data-Centric Approach to Multilingual E-Commerce Product Search: Case\n  Study on Query-Category and Query-Item Relevance","summary":"  Multilingual e-commerce search suffers from severe data imbalance across\nlanguages, label noise, and limited supervision for low-resource\nlanguages--challenges that impede the cross-lingual generalization of relevance\nmodels despite the strong capabilities of large language models (LLMs). In this\nwork, we present a practical, architecture-agnostic, data-centric framework to\nenhance performance on two core tasks: Query-Category (QC) relevance (matching\nqueries to product categories) and Query-Item (QI) relevance (matching queries\nto product titles). Rather than altering the model, we redesign the training\ndata through three complementary strategies: (1) translation-based augmentation\nto synthesize examples for languages absent in training, (2) semantic negative\nsampling to generate hard negatives and mitigate class imbalance, and (3)\nself-validation filtering to detect and remove likely mislabeled instances.\nEvaluated on the CIKM AnalytiCup 2025 dataset, our approach consistently yields\nsubstantial F1 score improvements over strong LLM baselines, achieving\ncompetitive results in the official competition. Our findings demonstrate that\nsystematic data engineering can be as impactful as--and often more deployable\nthan--complex model modifications, offering actionable guidance for building\nrobust multilingual search systems in the real-world e-commerce settings.\n","authors":["Yabo Yin","Yang Xi","Jialong Wang","Shanqi Wang","Jiateng Hu"],"pdf_url":"https://arxiv.org/pdf/2510.21671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21618v1","updated":"2025-10-24T16:24:01Z","published":"2025-10-24T16:24:01Z","title":"DeepAgent: A General Reasoning Agent with Scalable Toolsets","summary":"  Large reasoning models have demonstrated strong problem-solving abilities,\nyet real-world tasks often require external tools and long-horizon\ninteractions. Existing agent frameworks typically follow predefined workflows,\nwhich limit autonomous and global task completion. In this paper, we introduce\nDeepAgent, an end-to-end deep reasoning agent that performs autonomous\nthinking, tool discovery, and action execution within a single, coherent\nreasoning process. To address the challenges of long-horizon interactions,\nparticularly the context length explosion from multiple tool calls and the\naccumulation of interaction history, we introduce an autonomous memory folding\nmechanism that compresses past interactions into structured episodic, working,\nand tool memories, reducing error accumulation while preserving critical\ninformation. To teach general-purpose tool use efficiently and stably, we\ndevelop an end-to-end reinforcement learning strategy, namely ToolPO, that\nleverages LLM-simulated APIs and applies tool-call advantage attribution to\nassign fine-grained credit to the tool invocation tokens. Extensive experiments\non eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,\nTMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,\nHLE), demonstrate that DeepAgent consistently outperforms baselines across both\nlabeled-tool and open-set tool retrieval scenarios. This work takes a step\ntoward more general and capable agents for real-world applications. The code\nand demo are available at https://github.com/RUC-NLPIR/DeepAgent.\n","authors":["Xiaoxi Li","Wenxiang Jiao","Jiarui Jin","Guanting Dong","Jiajie Jin","Yinuo Wang","Hao Wang","Yutao Zhu","Ji-Rong Wen","Yuan Lu","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2510.21618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09682v2","updated":"2025-10-24T16:19:07Z","published":"2025-08-13T15:03:38Z","title":"Faster and Memory-Efficient Training of Sequential Recommendation Models\n  for Large Catalogs","summary":"  Sequential recommendations (SR) with transformer-based architectures are\nwidely adopted in real-world applications, where SR models require frequent\nretraining to adapt to ever-changing user preferences. However, training\ntransformer-based SR models often encounters a high computational cost\nassociated with scoring extensive item catalogs, often exceeding thousands of\nitems. This occurs mainly due to the use of cross-entropy loss, where peak\nmemory scales proportionally to catalog size, batch size, and sequence length.\nRecognizing this, practitioners in the field of recommendation systems\ntypically address memory consumption by integrating the cross-entropy (CE) loss\nwith negative sampling, thereby reducing the explicit memory demands of the\nfinal layer. However, a small number of negative samples would degrade model\nperformance, and as we demonstrate in our work, increasing the number of\nnegative samples and the batch size further improves the model's performance,\nbut rapidly starts to exceed industrial GPUs' size (~40Gb).\n  In this work, we introduce the CCE- method, which offers a GPU-efficient\nimplementation of the CE loss with negative sampling. Our method accelerates\ntraining by up to two times while reducing memory consumption by more than 10\ntimes. Leveraging the memory savings afforded by using CCE- for model training,\nit becomes feasible to enhance its accuracy on datasets with a large item\ncatalog compared to those trained with original PyTorch-implemented loss\nfunctions. Finally, we perform an analysis of key memory-related\nhyperparameters and highlight the necessity of a delicate balance among these\nfactors. We demonstrate that scaling both the number of negative samples and\nbatch size leads to better results rather than maximizing only one of them. To\nfacilitate further adoption of CCE-, we release a Triton kernel that\nefficiently implements the proposed method.\n","authors":["Maxim Zhelnin","Dmitry Redko","Volkov Daniil","Anna Volodkevich","Petr Sokerin","Valeriy Shevchenko","Egor Shvetsov","Alexey Vasilev","Darya Denisova","Ruslan Izmailov","Alexey Zaytsev"],"pdf_url":"https://arxiv.org/pdf/2509.09682v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21603v1","updated":"2025-10-24T16:07:54Z","published":"2025-10-24T16:07:54Z","title":"Doc-Researcher: A Unified System for Multimodal Document Parsing and\n  Deep Research","summary":"  Deep Research systems have revolutionized how LLMs solve complex questions\nthrough iterative reasoning and evidence gathering. However, current systems\nremain fundamentally constrained to textual web data, overlooking the vast\nknowledge embedded in multimodal documents Processing such documents demands\nsophisticated parsing to preserve visual semantics (figures, tables, charts,\nand equations), intelligent chunking to maintain structural coherence, and\nadaptive retrieval across modalities, which are capabilities absent in existing\nsystems. In response, we present Doc-Researcher, a unified system that bridges\nthis gap through three integrated components: (i) deep multimodal parsing that\npreserves layout structure and visual semantics while creating multi-granular\nrepresentations from chunk to document level, (ii) systematic retrieval\narchitecture supporting text-only, vision-only, and hybrid paradigms with\ndynamic granularity selection, and (iii) iterative multi-agent workflows that\ndecompose complex queries, progressively accumulate evidence, and synthesize\ncomprehensive answers across documents and modalities. To enable rigorous\nevaluation, we introduce M4DocBench, the first benchmark for Multi-modal,\nMulti-hop, Multi-document, and Multi-turn deep research. Featuring 158\nexpert-annotated questions with complete evidence chains across 304 documents,\nM4DocBench tests capabilities that existing benchmarks cannot assess.\nExperiments demonstrate that Doc-Researcher achieves 50.6% accuracy, 3.4xbetter\nthan state-of-the-art baselines, validating that effective document research\nrequires not just better retrieval, but fundamentally deep parsing that\npreserve multimodal integrity and support iterative research. Our work\nestablishes a new paradigm for conducting deep research on multimodal document\ncollections.\n","authors":["Kuicai Dong","Shurui Huang","Fangda Ye","Wei Han","Zhi Zhang","Dexun Li","Wenjun Li","Qu Yang","Gang Wang","Yichao Wang","Chen Zhang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2510.21603v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2510.21440v1","updated":"2025-10-24T13:17:00Z","published":"2025-10-24T13:17:00Z","title":"Redefining Retrieval Evaluation in the Era of LLMs","summary":"  Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR,\nassume that human users sequentially examine documents with diminishing\nattention to lower ranks. This assumption breaks down in Retrieval Augmented\nGeneration (RAG) systems, where search results are consumed by Large Language\nModels (LLMs), which, unlike humans, process all retrieved documents as a whole\nrather than sequentially. Additionally, traditional IR metrics do not account\nfor related but irrelevant documents that actively degrade generation quality,\nrather than merely being ignored. Due to these two major misalignments, namely\nhuman vs. machine position discount and human relevance vs. machine utility,\nclassical IR metrics do not accurately predict RAG performance. We introduce a\nutility-based annotation schema that quantifies both the positive contribution\nof relevant passages and the negative impact of distracting ones. Building on\nthis foundation, we propose UDCG (Utility and Distraction-aware Cumulative\nGain), a metric using an LLM-oriented positional discount to directly optimize\nthe correlation with the end-to-end answer accuracy. Experiments on five\ndatasets and six LLMs demonstrate that UDCG improves correlation by up to 36%\ncompared to traditional metrics. Our work provides a critical step toward\naligning IR evaluation with LLM consumers and enables more reliable assessment\nof RAG components\n","authors":["Giovanni Trappolini","Florin Cuconasu","Simone Filice","Yoelle Maarek","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2510.21440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21352v1","updated":"2025-10-24T11:28:08Z","published":"2025-10-24T11:28:08Z","title":"SciNUP: Natural Language User Interest Profiles for Scientific\n  Literature Recommendation","summary":"  The use of natural language (NL) user profiles in recommender systems offers\ngreater transparency and user control compared to traditional representations.\nHowever, there is scarcity of large-scale, publicly available test collections\nfor evaluating NL profile-based recommendation. To address this gap, we\nintroduce SciNUP, a novel synthetic dataset for scholarly recommendation that\nleverages authors' publication histories to generate NL profiles and\ncorresponding ground truth items. We use this dataset to conduct a comparison\nof baseline methods, ranging from sparse and dense retrieval approaches to\nstate-of-the-art LLM-based rerankers. Our results show that while baseline\nmethods achieve comparable performance, they often retrieve different items,\nindicating complementary behaviors. At the same time, considerable headroom for\nimprovement remains, highlighting the need for effective NL-based\nrecommendation approaches. The SciNUP dataset thus serves as a valuable\nresource for fostering future research and development in this area.\n","authors":["Mariam Arustashvili","Krisztian Balog"],"pdf_url":"https://arxiv.org/pdf/2510.21352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21333v1","updated":"2025-10-24T10:49:50Z","published":"2025-10-24T10:49:50Z","title":"CausalRec: A CausalBoost Attention Model for Sequential Recommendation","summary":"  Recent advances in correlation-based sequential recommendation systems have\ndemonstrated substantial success. Specifically, the attention-based model\noutperforms other RNN-based and Markov chains-based models by capturing both\nshort- and long-term dependencies more effectively. However, solely focusing on\nitem co-occurrences overlooks the underlying motivations behind user behaviors,\nleading to spurious correlations and potentially inaccurate recommendations. To\naddress this limitation, we present a novel framework that integrates causal\nattention for sequential recommendation, CausalRec. It incorporates a causal\ndiscovery block and a CausalBooster. The causal discovery block learns the\ncausal graph in user behavior sequences, and we provide a theory to guarantee\nthe identifiability of the learned causal graph. The CausalBooster utilizes the\ndiscovered causal graph to refine the attention mechanism, prioritizing\nbehaviors with causal significance. Experimental evaluations on real-world\ndatasets indicate that CausalRec outperforms several state-of-the-art methods,\nwith average improvements of 7.21% in Hit Rate (HR) and 8.65% in Normalized\nDiscounted Cumulative Gain (NDCG). To the best of our knowledge, this is the\nfirst model to incorporate causality through the attention mechanism in\nsequential recommendation, demonstrating the value of causality in generating\nmore accurate and reliable recommendations.\n","authors":["Yunbo Hou","Tianle Yang","Ruijie Li","Li He","Liang Wang","Weiping Li","Bo Zheng","Guojie Song"],"pdf_url":"https://arxiv.org/pdf/2510.21333v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2507.04888v2","updated":"2025-10-24T10:07:26Z","published":"2025-07-07T11:19:28Z","title":"SimLab: A Platform for Simulation-based Evaluation of Conversational\n  Information Access Systems","summary":"  Progress in conversational information access (CIA) systems has been hindered\nby the difficulty of evaluating such systems with reproducible experiments.\nWhile user simulation offers a promising solution, the lack of infrastructure\nand tooling to support this evaluation paradigm remains a significant barrier.\nTo address this gap, we introduce SimLab, the first cloud-based platform\nproviding a centralized solution for the community to benchmark both\nconversational systems and user simulators in a controlled and reproducible\nsetting. We articulate the requirements for such a platform and propose a\ngeneral infrastructure to meet them. We then present the design and\nimplementation of an initial version of SimLab and showcase its features\nthrough an initial simulation-based evaluation task in conversational movie\nrecommendation. Furthermore, we discuss the platform's sustainability and\nfuture opportunities for development, inviting the community to drive further\nprogress in the fields of CIA and user simulation.\n","authors":["Nolwenn Bernard","Sharath Chandra Etagi Suresh","Krisztian Balog","ChengXiang Zhai"],"pdf_url":"https://arxiv.org/pdf/2507.04888v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21276v1","updated":"2025-10-24T09:22:04Z","published":"2025-10-24T09:22:04Z","title":"Pctx: Tokenizing Personalized Context for Generative Recommendation","summary":"  Generative recommendation (GR) models tokenize each action into a few\ndiscrete tokens (called semantic IDs) and autoregressively generate the next\ntokens as predictions, showing advantages such as memory efficiency,\nscalability, and the potential to unify retrieval and ranking. Despite these\nbenefits, existing tokenization methods are static and non-personalized. They\ntypically derive semantic IDs solely from item features, assuming a universal\nitem similarity that overlooks user-specific perspectives. However, under the\nautoregressive paradigm, semantic IDs with the same prefixes always receive\nsimilar probabilities, so a single fixed mapping implicitly enforces a\nuniversal item similarity standard across all users. In practice, the same item\nmay be interpreted differently depending on user intentions and preferences. To\naddress this issue, we propose a personalized context-aware tokenizer that\nincorporates a user's historical interactions when generating semantic IDs.\nThis design allows the same item to be tokenized into different semantic IDs\nunder different user contexts, enabling GR models to capture multiple\ninterpretive standards and produce more personalized predictions. Experiments\non three public datasets demonstrate up to 11.44% improvement in NDCG@10 over\nnon-personalized action tokenization baselines. Our code is available at\nhttps://github.com/YoungZ365/Pctx.\n","authors":["Qiyong Zhong","Jiajie Su","Yunshan Ma","Julian McAuley","Yupeng Hou"],"pdf_url":"https://arxiv.org/pdf/2510.21276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21242v1","updated":"2025-10-24T08:25:56Z","published":"2025-10-24T08:25:56Z","title":"Bi-Level Optimization for Generative Recommendation: Bridging\n  Tokenization and Generation","summary":"  Generative recommendation is emerging as a transformative paradigm by\ndirectly generating recommended items, rather than relying on matching.\nBuilding such a system typically involves two key components: (1) optimizing\nthe tokenizer to derive suitable item identifiers, and (2) training the\nrecommender based on those identifiers. Existing approaches often treat these\ncomponents separately--either sequentially or in alternation--overlooking their\ninterdependence. This separation can lead to misalignment: the tokenizer is\ntrained without direct guidance from the recommendation objective, potentially\nyielding suboptimal identifiers that degrade recommendation performance.\n  To address this, we propose BLOGER, a Bi-Level Optimization for GEnerative\nRecommendation framework, which explicitly models the interdependence between\nthe tokenizer and the recommender in a unified optimization process. The lower\nlevel trains the recommender using tokenized sequences, while the upper level\noptimizes the tokenizer based on both the tokenization loss and recommendation\nloss. We adopt a meta-learning approach to solve this bi-level optimization\nefficiently, and introduce gradient surgery to mitigate gradient conflicts in\nthe upper-level updates, thereby ensuring that item identifiers are both\ninformative and recommendation-aligned. Extensive experiments on real-world\ndatasets demonstrate that BLOGER consistently outperforms state-of-the-art\ngenerative recommendation methods while maintaining practical efficiency with\nno significant additional computational overhead, effectively bridging the gap\nbetween item tokenization and autoregressive generation.\n","authors":["Yimeng Bai","Chang Liu","Yang Zhang","Dingxian Wang","Frank Yang","Andrew Rabinovich","Wenge Rong","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2510.21242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18512v2","updated":"2025-10-24T06:55:54Z","published":"2025-05-24T05:15:49Z","title":"AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking","summary":"  Listwise reranking with large language models (LLMs) enhances top-ranked\nresults in retrieval-based applications. Due to the limit in context size and\nhigh inference cost of long context, reranking is typically performed over a\nfixed size of small subsets, with the final ranking aggregated from these\npartial results. This fixed computation disregards query difficulty and\ndocument distribution, leading to inefficiencies. We propose AcuRank, an\nadaptive reranking framework that dynamically adjusts both the amount and\ntarget of computation based on uncertainty estimates over document relevance.\nUsing a Bayesian TrueSkill model, we iteratively refine relevance estimates\nuntil reaching sufficient confidence levels, and our explicit modeling of\nranking uncertainty enables principled control over reranking behavior and\navoids unnecessary updates to confident predictions. Results on the TREC-DL and\nBEIR benchmarks show that our method consistently achieves a superior\naccuracy-efficiency trade-off and scales better with compute than\nfixed-computation baselines. These results highlight the effectiveness and\ngeneralizability of our method across diverse retrieval tasks and LLM-based\nreranking models.\n","authors":["Soyoung Yoon","Gyuwan Kim","Gyu-Hwung Cho","Seung-won Hwang"],"pdf_url":"https://arxiv.org/pdf/2505.18512v2.pdf","comment":"Accepted at NeurIPS 2025. The first two authors contributed equally.\n  Author order is randomly determined via coin toss"},{"id":"http://arxiv.org/abs/2510.21151v1","updated":"2025-10-24T04:45:29Z","published":"2025-10-24T04:45:29Z","title":"VOGUE: A Multimodal Dataset for Conversational Recommendation in Fashion","summary":"  Multimodal conversational recommendation has emerged as a promising paradigm\nfor delivering personalized experiences through natural dialogue enriched by\nvisual and contextual grounding. Yet, current multimodal conversational\nrecommendation datasets remain limited: existing resources either simulate\nconversations, omit user history, or fail to collect sufficiently detailed\nfeedback, all of which constrain the types of research and evaluation they\nsupport.\n  To address these gaps, we introduce VOGUE, a novel dataset of 60 humanhuman\ndialogues in realistic fashion shopping scenarios. Each dialogue is paired with\na shared visual catalogue, item metadata, user fashion profiles and histories,\nand post-conversation ratings from both Seekers and Assistants. This design\nenables rigorous evaluation of conversational inference, including not only\nalignment between predicted and ground-truth preferences, but also calibration\nagainst full rating distributions and comparison with explicit and implicit\nuser satisfaction signals.\n  Our initial analyses of VOGUE reveal distinctive dynamics of visually\ngrounded dialogue. For example, recommenders frequently suggest items\nsimultaneously in feature-based groups, which creates distinct conversational\nphases bridged by Seeker critiques and refinements. Benchmarking multimodal\nlarge language models against human recommenders shows that while MLLMs\napproach human-level alignment in aggregate, they exhibit systematic\ndistribution errors in reproducing human ratings and struggle to generalize\npreference inference beyond explicitly discussed items. These findings\nestablish VOGUE as both a unique resource for studying multimodal\nconversational systems and as a challenge dataset beyond the current\nrecommendation capabilities of existing top-tier multimodal foundation models\nsuch as GPT-4o-mini, GPT-5-mini, and Gemini-2.5-Flash.\n","authors":["David Guo","Minqi Sun","Yilun Jiang","Jiazhou Liang","Scott Sanner"],"pdf_url":"https://arxiv.org/pdf/2510.21151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.16925v2","updated":"2025-10-24T03:04:35Z","published":"2025-10-19T16:46:11Z","title":"Towards Context-aware Reasoning-enhanced Generative Searching in\n  E-commerce","summary":"  Search-based recommendation is one of the most critical application scenarios\nin e-commerce platforms. Users' complex search contexts--such as spatiotemporal\nfactors, historical interactions, and current query's information--constitute\nan essential part of their decision-making, reflecting implicit preferences\nthat complement explicit query terms. Modeling such rich contextual signals and\ntheir intricate associations with candidate items remains a key challenge.\nAlthough numerous efforts have been devoted to building more effective search\nmethods, existing approaches still show limitations in integrating contextual\ninformation, which hinders their ability to fully capture user intent.\n  To address these challenges, we propose a context-aware reasoning-enhanced\ngenerative search framework for better \\textbf{understanding the complicated\ncontext}. Specifically, the framework first unifies heterogeneous user and item\ncontexts into textual representations or text-based semantic identifiers and\naligns them. To overcome the lack of explicit reasoning trajectories, we\nintroduce a self-evolving post-training paradigm that iteratively combines\nsupervised fine-tuning and reinforcement learning to progressively enhance the\nmodel's reasoning capability. In addition, we identify potential biases in\nexisting RL algorithms when applied to search scenarios and present a debiased\nvariant of GRPO to improve ranking performance. Extensive experiments on search\nlog data collected from a real-world e-commerce platform demonstrate that our\napproach achieves superior performance compared with strong baselines,\nvalidating its effectiveness for search-based recommendation.\n","authors":["Zhiding Liu","Ben Chen","Mingyue Cheng","Enhong Chen","Li Li","Chenyi Lei","Wenwu Ou","Han Li","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2510.16925v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.20150v2","updated":"2025-10-24T02:11:31Z","published":"2025-10-23T02:56:00Z","title":"Rank-GRPO: Training LLM-based Conversational Recommender Systems with\n  Reinforcement Learning","summary":"  Large language models (LLMs) are reshaping the recommender system paradigm by\nenabling users to express preferences and receive recommendations through\nconversations. Yet, aligning LLMs to the recommendation task remains\nchallenging: pretrained LLMs often generate out-of-catalog items, violate\nrequired output formats, and their ranking quality degrades sharply toward the\nend of the generated list. To this end, we propose ConvRec-R1, a two-stage\nframework for end-to-end training of LLM-based conversational recommender\nsystems. In Stage 1, we construct a behavioral-cloning dataset with a\nRemap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded\ndemonstrations from powerful blackbox LLMs to warm-start the RL training. In\nStage 2, we propose Rank-GRPO, a principled extension of group relative policy\noptimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats\neach rank in the recommendation list as the unit instead of token (too\nfine-grained) or sequence (too coarse), redefining rewards to remove non-causal\ncredit assignment and introducing a rank-level importance ratio based on the\ngeometric mean of rank-wise token probabilities to stabilize policy updates.\nExperiments on the public Reddit-v2 dataset show that ConvRec-R1 converges\nfaster and achieves higher Recall and NDCG than GRPO-style baselines. Code and\ndatasets are released at https://github.com/yaochenzhu/Rank-GRPO.\n","authors":["Yaochen Zhu","Harald Steck","Dawen Liang","Yinhan He","Vito Ostuni","Jundong Li","Nathan Kallus"],"pdf_url":"https://arxiv.org/pdf/2510.20150v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01659v2","updated":"2025-10-24T23:14:58Z","published":"2025-06-02T13:30:39Z","title":"Engram Memory Encoding and Retrieval: A Neurocomputational Perspective","summary":"  Despite substantial research into the biological basis of memory, the precise\nmechanisms by which experiences are encoded, stored, and retrieved in the brain\nremain incompletely understood. A growing body of evidence supports the engram\ntheory, which posits that sparse populations of neurons undergo lasting\nphysical and biochemical changes to support long-term memory. Yet, a\ncomprehensive computational framework that integrates biological findings with\nmechanistic models remains elusive. This work synthesizes insights from\ncellular neuroscience and computational modeling to address key challenges in\nengram research: how engram neurons are identified and manipulated; how\nsynaptic plasticity mechanisms contribute to stable memory traces; and how\nsparsity promotes efficient, interference-resistant representations. Relevant\ncomputational approaches -- such as sparse regularization, engram gating, and\nbiologically inspired architectures like Sparse Distributed Memory and spiking\nneural networks -- are also examined. Together, these findings suggest that\nmemory efficiency, capacity, and stability emerge from the interaction of\nplasticity and sparsity constraints. By integrating neurobiological and\ncomputational perspectives, this paper provides a comprehensive theoretical\nfoundation for engram research and proposes a roadmap for future inquiry into\nthe mechanisms underlying memory, with implications for the diagnosis and\ntreatment of memory-related disorders.\n","authors":["Daniel Szelogowski"],"pdf_url":"https://arxiv.org/pdf/2506.01659v2.pdf","comment":"18 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2510.22055v1","updated":"2025-10-24T22:37:13Z","published":"2025-10-24T22:37:13Z","title":"A Benchmark for Open-Domain Numerical Fact-Checking Enhanced by Claim\n  Decomposition","summary":"  Fact-checking numerical claims is critical as the presence of numbers provide\nmirage of veracity despite being fake potentially causing catastrophic impacts\non society. The prior works in automatic fact verification do not primarily\nfocus on natural numerical claims. A typical human fact-checker first retrieves\nrelevant evidence addressing the different numerical aspects of the claim and\nthen reasons about them to predict the veracity of the claim. Hence, the search\nprocess of a human fact-checker is a crucial skill that forms the foundation of\nthe verification process. Emulating a real-world setting is essential to aid in\nthe development of automated methods that encompass such skills. However,\nexisting benchmarks employ heuristic claim decomposition approaches augmented\nwith weakly supervised web search to collect evidences for verifying claims.\nThis sometimes results in less relevant evidences and noisy sources with\ntemporal leakage rendering a less realistic retrieval setting for claim\nverification. Hence, we introduce QuanTemp++: a dataset consisting of natural\nnumerical claims, an open domain corpus, with the corresponding relevant\nevidence for each claim. The evidences are collected through a claim\ndecomposition process approximately emulating the approach of human\nfact-checker and veracity labels ensuring there is no temporal leakage. Given\nthis dataset, we also characterize the retrieval performance of key claim\ndecomposition paradigms. Finally, we observe their effect on the outcome of the\nverification pipeline and draw insights. The code for data pipeline along with\nlink to data can be found at https://github.com/VenkteshV/QuanTemp_Plus\n","authors":["V Venktesh","Deepali Prabhu","Avishek Anand"],"pdf_url":"https://arxiv.org/pdf/2510.22055v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2510.22049v1","updated":"2025-10-24T22:17:49Z","published":"2025-10-24T22:17:49Z","title":"Massive Memorization with Hundreds of Trillions of Parameters for\n  Sequential Transducer Generative Recommenders","summary":"  Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users.\n","authors":["Zhimin Chen","Chenyu Zhao","Ka Chun Mo","Yunjiang Jiang","Jane H. Lee","Shouwei Chen","Khushhall Chandra Mahajan","Ning Jiang","Kai Ren","Jinhui Li","Wen-Yun Yang"],"pdf_url":"https://arxiv.org/pdf/2510.22049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22023v1","updated":"2025-10-24T21:03:20Z","published":"2025-10-24T21:03:20Z","title":"Multimodal Item Scoring for Natural Language Recommendation via Gaussian\n  Process Regression with LLM Relevance Judgments","summary":"  Natural Language Recommendation (NLRec) generates item suggestions based on\nthe relevance between user-issued NL requests and NL item description passages.\nExisting NLRec approaches often use Dense Retrieval (DR) to compute item\nrelevance scores from aggregation of inner products between user request\nembeddings and relevant passage embeddings. However, DR views the request as\nthe sole relevance label, thus leading to a unimodal scoring function centered\non the query embedding that is often a weak proxy for query relevance. To\nbetter capture the potential multimodal distribution of the relevance scoring\nfunction that may arise from complex NLRec data, we propose GPR-LLM that uses\nGaussian Process Regression (GPR) with LLM relevance judgments for a subset of\ncandidate passages. Experiments on four NLRec datasets and two LLM backbones\ndemonstrate that GPR-LLM with an RBF kernel, capable of modeling multimodal\nrelevance scoring functions, consistently outperforms simpler unimodal kernels\n(dot product, cosine similarity), as well as baseline methods including DR,\ncross-encoder, and pointwise LLM-based relevance scoring by up to 65%. Overall,\nGPR-LLM provides an efficient and effective approach to NLRec within a minimal\nLLM labeling budget.\n","authors":["Yifan Liu","Qianfeng Wen","Jiazhou Liang","Mark Zhao","Justin Cui","Anton Korikov","Armin Torogh","Junyoung Kim","Scott Sanner"],"pdf_url":"https://arxiv.org/pdf/2510.22023v1.pdf","comment":"16 pages,20 figures"},{"id":"http://arxiv.org/abs/2510.21962v1","updated":"2025-10-24T18:41:36Z","published":"2025-10-24T18:41:36Z","title":"Temporal Graph Theoretic Analysis of Geopolitical Dynamics in the U.S.\n  Entity List","summary":"  Export controls have become one of America's most prominent tools of economic\nstatecraft. They aim to block rival countries' access to sensitive\ntechnologies, safeguard U.S. supply chains, protect national security, and\nshape geopolitical competition. Among various instruments, the U.S. Entity List\nhas emerged as the most salient, yet its dynamics remain underexplored. This\npaper introduces a novel temporal graph framework that transforms the Entity\nList documents from a static registry of foreign entities of concern into a\ndynamic representation of geopolitical strategy. We construct the first\nevent-based dataset of U.S. government foreign entity designations and model\nthem as a temporal bipartite graph. Building on this representation, we develop\na multi-level analytical approach that reveals shifting roles, enforcement\nstrategy, and broader sanction ecosystems. Applied to 25 years of data, the\nframework uncovers dynamic patterns of escalation, persistence, and\ncoordination that static views cannot capture. More broadly, our study\ndemonstrates how temporal graph analysis offers systematic computational\ninsights into the geopolitical dynamics of export controls.\n","authors":["Yunsen Lei","Kexin Bai","Quan Li","H. Howie Huang"],"pdf_url":"https://arxiv.org/pdf/2510.21962v1.pdf","comment":"13 pages, 9 figures. Under review"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2510.21706v1","updated":"2025-10-24T17:59:46Z","published":"2025-10-24T17:59:46Z","title":"Equivariance by Contrast: Identifiable Equivariant Embeddings from\n  Unlabeled Finite Group Actions","summary":"  We propose Equivariance by Contrast (EbC) to learn equivariant embeddings\nfrom observation pairs $(\\mathbf{y}, g \\cdot \\mathbf{y})$, where $g$ is drawn\nfrom a finite group acting on the data. Our method jointly learns a latent\nspace and a group representation in which group actions correspond to\ninvertible linear maps -- without relying on group-specific inductive biases.\nWe validate our approach on the infinite dSprites dataset with structured\ntransformations defined by the finite group $G:= (R_m \\times \\mathbb{Z}_n\n\\times \\mathbb{Z}_n)$, combining discrete rotations and periodic translations.\nThe resulting embeddings exhibit high-fidelity equivariance, with group\noperations faithfully reproduced in latent space. On synthetic data, we further\nvalidate the approach on the non-abelian orthogonal group $O(n)$ and the\ngeneral linear group $GL(n)$. We also provide a theoretical proof for\nidentifiability. While broad evaluation across diverse group types on\nreal-world data remains future work, our results constitute the first\nsuccessful demonstration of general-purpose encoder-only equivariant learning\nfrom group action observations alone, including non-trivial non-abelian groups\nand a product group motivated by modeling affine equivariances in computer\nvision.\n","authors":["Tobias Schmidt","Steffen Schneider","Matthias Bethge"],"pdf_url":"https://arxiv.org/pdf/2510.21706v1.pdf","comment":"Accepted at NeurIPS 2025. The last two authors contributed equally.\n  Code is available at https://github.com/dynamical-inference/ebc"},{"id":"http://arxiv.org/abs/2510.21697v1","updated":"2025-10-24T17:57:31Z","published":"2025-10-24T17:57:31Z","title":"Visual Diffusion Models are Geometric Solvers","summary":"  In this paper we show that visual diffusion models can serve as effective\ngeometric solvers: they can directly reason about geometric problems by working\nin pixel space. We first demonstrate this on the Inscribed Square Problem, a\nlong-standing problem in geometry that asks whether every Jordan curve contains\nfour points forming a square. We then extend the approach to two other\nwell-known hard geometric problems: the Steiner Tree Problem and the Simple\nPolygon Problem.\n  Our method treats each problem instance as an image and trains a standard\nvisual diffusion model that transforms Gaussian noise into an image\nrepresenting a valid approximate solution that closely matches the exact one.\nThe model learns to transform noisy geometric structures into correct\nconfigurations, effectively recasting geometric reasoning as image generation.\n  Unlike prior work that necessitates specialized architectures and\ndomain-specific adaptations when applying diffusion to parametric geometric\nrepresentations, we employ a standard visual diffusion model that operates on\nthe visual representation of the problem. This simplicity highlights a\nsurprising bridge between generative modeling and geometric problem solving.\nBeyond the specific problems studied here, our results point toward a broader\nparadigm: operating in image space provides a general and practical framework\nfor approximating notoriously hard problems, and opens the door to tackling a\nfar wider class of challenging geometric tasks.\n","authors":["Nir Goren","Shai Yehezkel","Omer Dahary","Andrey Voynov","Or Patashnik","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2510.21697v1.pdf","comment":"Project page: https://kariander1.github.io/visual-geo-solver/"},{"id":"http://arxiv.org/abs/2506.09891v2","updated":"2025-10-24T17:57:09Z","published":"2025-06-11T16:00:55Z","title":"Causal Climate Emulation with Bayesian Filtering","summary":"  Traditional models of climate change use complex systems of coupled equations\nto simulate physical processes across the Earth system. These simulations are\nhighly computationally expensive, limiting our predictions of climate change\nand analyses of its causes and effects. Machine learning has the potential to\nquickly emulate data from climate models, but current approaches are not able\nto incorporate physically-based causal relationships. Here, we develop an\ninterpretable climate model emulator based on causal representation learning.\nWe derive a novel approach including a Bayesian filter for stable long-term\nautoregressive emulation. We demonstrate that our emulator learns accurate\nclimate dynamics, and we show the importance of each one of its components on a\nrealistic synthetic dataset and data from two widely deployed climate models.\n","authors":["Sebastian Hickman","Ilija Trajkovic","Julia Kaltenborn","Francis Pelletier","Alex Archibald","Yaniv Gurwicz","Peer Nowack","David Rolnick","Julien Boussard"],"pdf_url":"https://arxiv.org/pdf/2506.09891v2.pdf","comment":"37 pages, 26 figures"},{"id":"http://arxiv.org/abs/2510.21693v1","updated":"2025-10-24T17:54:19Z","published":"2025-10-24T17:54:19Z","title":"Mechanistic Interpretability for Neural TSP Solvers","summary":"  Neural networks have advanced combinatorial optimization, with\nTransformer-based solvers achieving near-optimal solutions on the Traveling\nSalesman Problem (TSP) in milliseconds. However, these models operate as black\nboxes, providing no insight into the geometric patterns they learn or the\nheuristics they employ during tour construction. We address this opacity by\napplying sparse autoencoders (SAEs), a mechanistic interpretability technique,\nto a Transformer-based TSP solver, representing the first application of\nactivation-based interpretability methods to operations research models. We\ntrain a pointer network with reinforcement learning on 100-node instances, then\nfit an SAE to the encoder's residual stream to discover an overcomplete\ndictionary of interpretable features. Our analysis reveals that the solver\nnaturally develops features mirroring fundamental TSP concepts: boundary\ndetectors that activate on convex-hull nodes, cluster-sensitive features\nresponding to locally dense regions, and separator features encoding geometric\npartitions. These findings provide the first model-internal account of what\nneural TSP solvers compute before node selection, demonstrate that geometric\nstructure emerges without explicit supervision, and suggest pathways toward\ntransparent hybrid systems that combine neural efficiency with algorithmic\ninterpretability. Interactive feature explorer:\nhttps://reubennarad.github.io/TSP_interp\n","authors":["Reuben Narad","Leonard Boussioux","Michael Wagner"],"pdf_url":"https://arxiv.org/pdf/2510.21693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00138v2","updated":"2025-10-24T17:52:29Z","published":"2025-05-30T18:21:40Z","title":"Intrinsic Goals for Autonomous Agents: Model-Based Exploration in\n  Virtual Zebrafish Predicts Ethological Behavior and Whole-Brain Dynamics","summary":"  Autonomy is a hallmark of animal intelligence, enabling adaptive and\nintelligent behavior in complex environments without relying on external reward\nor task structure. Existing reinforcement learning approaches to exploration in\nreward-free environments, including a class of methods known as model-based\nintrinsic motivation, exhibit inconsistent exploration patterns and do not\nconverge to an exploratory policy, thus failing to capture robust autonomous\nbehaviors observed in animals. Moreover, systems neuroscience has largely\noverlooked the neural basis of autonomy, focusing instead on experimental\nparadigms where animals are motivated by external reward rather than engaging\nin ethological, naturalistic and task-independent behavior. To bridge these\ngaps, we introduce a novel model-based intrinsic drive explicitly designed\nafter the principles of autonomous exploration in animals. Our method\n(3M-Progress) achieves animal-like exploration by tracking divergence between\nan online world model and a fixed prior learned from an ecological niche. To\nthe best of our knowledge, we introduce the first autonomous embodied agent\nthat predicts brain data entirely from self-supervised optimization of an\nintrinsic goal -- without any behavioral or neural training data --\ndemonstrating that 3M-Progress agents capture the explainable variance in\nbehavioral patterns and whole-brain neural-glial dynamics recorded from\nautonomously behaving larval zebrafish, thereby providing the first\ngoal-driven, population-level model of neural-glial computation. Our findings\nestablish a computational framework connecting model-based intrinsic motivation\nto naturalistic behavior, providing a foundation for building artificial agents\nwith animal-like autonomy.\n","authors":["Reece Keller","Alyn Kirsch","Felix Pei","Xaq Pitkow","Leo Kozachkov","Aran Nayebi"],"pdf_url":"https://arxiv.org/pdf/2506.00138v2.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2510.21691v1","updated":"2025-10-24T17:50:41Z","published":"2025-10-24T17:50:41Z","title":"On Uncertainty Calibration for Equivariant Functions","summary":"  Data-sparse settings such as robotic manipulation, molecular physics, and\ngalaxy morphology classification are some of the hardest domains for deep\nlearning. For these problems, equivariant networks can help improve modeling\nacross undersampled parts of the input space, and uncertainty estimation can\nguard against overconfidence. However, until now, the relationships between\nequivariance and model confidence, and more generally equivariance and model\ncalibration, has yet to be studied. Since traditional classification and\nregression error terms show up in the definitions of calibration error, it is\nnatural to suspect that previous work can be used to help understand the\nrelationship between equivariance and calibration error. In this work, we\npresent a theory relating equivariance to uncertainty estimation. By proving\nlower and upper bounds on uncertainty calibration errors (ECE and ENCE) under\nvarious equivariance conditions, we elucidate the generalization limits of\nequivariant models and illustrate how symmetry mismatch can result in\nmiscalibration in both classification and regression. We complement our\ntheoretical framework with numerical experiments that clarify the relationship\nbetween equivariance and uncertainty using a variety of real and simulated\ndatasets, and we comment on trends with symmetry mismatch, group size, and\naleatoric and epistemic uncertainties.\n","authors":["Edward Berman","Jacob Ginesin","Marco Pacini","Robin Walters"],"pdf_url":"https://arxiv.org/pdf/2510.21691v1.pdf","comment":"Under review at Transactions on Machine Learning Research (TMLR).\n  Code is available at https://github.com/EdwardBerman/EquiUQ . Excited to\n  share this paper, comments welcome :D"},{"id":"http://arxiv.org/abs/2506.23726v2","updated":"2025-10-24T17:47:21Z","published":"2025-06-30T10:58:49Z","title":"System-Embedded Diffusion Bridge Models","summary":"  Solving inverse problems -- recovering signals from incomplete or noisy\nmeasurements -- is fundamental in science and engineering. Score-based\ngenerative models (SGMs) have recently emerged as a powerful framework for this\ntask. Two main paradigms have formed: unsupervised approaches that adapt\npretrained generative models to inverse problems, and supervised bridge methods\nthat train stochastic processes conditioned on paired clean and corrupted data.\nWhile the former typically assume knowledge of the measurement model, the\nlatter have largely overlooked this structural information. We introduce System\nembedded Diffusion Bridge Models (SDBs), a new class of supervised bridge\nmethods that explicitly embed the known linear measurement system into the\ncoefficients of a matrix-valued SDE. This principled integration yields\nconsistent improvements across diverse linear inverse problems and demonstrates\nrobust generalization under system misspecification between training and\ndeployment, offering a promising solution to real-world applications.\n","authors":["Bartlomiej Sobieski","Matthew Tivnan","Yuang Wang","Siyeop Yoon","Pengfei Jin","Dufan Wu","Quanzheng Li","Przemyslaw Biecek"],"pdf_url":"https://arxiv.org/pdf/2506.23726v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2507.23773v2","updated":"2025-10-24T17:44:52Z","published":"2025-07-31T17:57:20Z","title":"SimuRA: A World-Model-Driven Simulative Reasoning Architecture for\n  General Goal-Oriented Agents","summary":"  AI agents built on foundation models hold enormous promise. Current practice,\nhowever, focuses on a one-task-one-agent approach, which not only falls short\nof scalability and generality, but also faces practical limitations from\nblack-box autoregressive reasoning, where decisions unfold token by token\nwithout explicit simulation or counterfactual evaluation of outcomes. Humans,\non the other hand, reason and plan by mentally simulating the consequences of\nactions within an internal model of the world -- a capability that supports\nflexible, goal-directed behavior across diverse contexts. Moving towards a more\ngeneral and powerful AI agent, we introduce SimuRA, a goal-oriented\narchitecture for generalized agentic reasoning. Based on a principled\nformulation of an optimal agent in any general environment, SimuRA addresses\nthe limitations of black-box autoregressive reasoning by incorporating the\nworld model for planning via simulation. Our prototype world model is\nimplemented using LLMs as a substrate, leveraging the natural language as a\ndiscrete, hierarchical representation grounded in concepts for planning, while\nremaining model-agnostic. On complex web-browsing tasks such as flight search,\nSimuRA improves the success rate from 0% to 32.2% compared to a representative\nopen-web agent baseline. Across tasks, world-model-based planning achieves up\nto 124% higher task completion rates than a matched black-box autoregressive\nbaseline, demonstrating the advantages of simulative reasoning. We release\nReasonerAgent-Web, a web-browsing agent built on SimuRA, as an open-source\nresearch demo.\n","authors":["Mingkai Deng","Jinyu Hou","Zhiting Hu","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2507.23773v2.pdf","comment":"This submission has been updated to adjust the scope and presentation\n  of the work"},{"id":"http://arxiv.org/abs/2510.21686v1","updated":"2025-10-24T17:44:40Z","published":"2025-10-24T17:44:40Z","title":"Multimodal Datasets with Controllable Mutual Information","summary":"  We introduce a framework for generating highly multimodal datasets with\nexplicitly calculable mutual information between modalities. This enables the\nconstruction of benchmark datasets that provide a novel testbed for systematic\nstudies of mutual information estimators and multimodal self-supervised\nlearning techniques. Our framework constructs realistic datasets with known\nmutual information using a flow-based generative model and a structured causal\nframework for generating correlated latent variables.\n","authors":["Raheem Karim Hashmani","Garrett W. Merz","Helen Qu","Mariel Pettee","Kyle Cranmer"],"pdf_url":"https://arxiv.org/pdf/2510.21686v1.pdf","comment":"15 pages, 4 figures, 1 table. Our code is publicly available at\n  https://github.com/RKHashmani/MmMi-Datasets"},{"id":"http://arxiv.org/abs/2504.05822v2","updated":"2025-10-24T17:44:21Z","published":"2025-04-08T09:05:33Z","title":"Federated Unlearning Made Practical: Seamless Integration via Negated\n  Pseudo-Gradients","summary":"  The right to be forgotten is a fundamental principle of privacy-preserving\nregulations and extends to Machine Learning (ML) paradigms such as Federated\nLearning (FL). While FL enhances privacy by enabling collaborative model\ntraining without sharing private data, trained models still retain the\ninfluence of training data. Federated Unlearning (FU) methods recently proposed\noften rely on impractical assumptions for real-world FL deployments, such as\nstoring client update histories or requiring access to a publicly available\ndataset. To address these constraints, this paper introduces a novel method\nthat leverages negated Pseudo-gradients Updates for Federated Unlearning (PUF).\nOur approach only uses standard client model updates, which are employed during\nregular FL rounds, and interprets them as pseudo-gradients. When a client needs\nto be forgotten, we apply the negation of their pseudo-gradients, appropriately\nscaled, to the global model. Unlike state-of-the-art mechanisms, PUF seamlessly\nintegrates with FL workflows, incurs no additional computational and\ncommunication overhead beyond standard FL rounds, and supports concurrent\nunlearning requests. We extensively evaluated the proposed method on two\nwell-known benchmark image classification datasets (CIFAR-10 and CIFAR-100) and\na real-world medical imaging dataset for segmentation (ProstateMRI), using\nthree different neural architectures: two residual networks and a vision\ntransformer. The experimental results across various settings demonstrate that\nPUF achieves state-of-the-art forgetting effectiveness and recovery time,\nwithout relying on any additional assumptions.\n","authors":["Alessio Mora","Carlo Mazzocca","Rebecca Montanari","Paolo Bellavista"],"pdf_url":"https://arxiv.org/pdf/2504.05822v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07969v3","updated":"2025-10-24T17:37:23Z","published":"2025-07-10T17:48:03Z","title":"Reinforcement Learning with Action Chunking","summary":"  We present Q-chunking, a simple yet effective recipe for improving\nreinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks.\nOur recipe is designed for the offline-to-online RL setting, where the goal is\nto leverage an offline prior dataset to maximize the sample-efficiency of\nonline learning. Effective exploration and sample-efficient learning remain\ncentral challenges in this setting, as it is not obvious how the offline data\nshould be utilized to acquire a good exploratory policy. Our key insight is\nthat action chunking, a technique popularized in imitation learning where\nsequences of future actions are predicted rather than a single action at each\ntimestep, can be applied to temporal difference (TD)-based RL methods to\nmitigate the exploration challenge. Q-chunking adopts action chunking by\ndirectly running RL in a 'chunked' action space, enabling the agent to (1)\nleverage temporally consistent behaviors from offline data for more effective\nonline exploration and (2) use unbiased $n$-step backups for more stable and\nefficient TD learning. Our experimental results demonstrate that Q-chunking\nexhibits strong offline performance and online sample efficiency, outperforming\nprior best offline-to-online methods on a range of long-horizon, sparse-reward\nmanipulation tasks.\n","authors":["Qiyang Li","Zhiyuan Zhou","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2507.07969v3.pdf","comment":"The Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems (NeurIPS 2025); 36 pages, 17 figures"},{"id":"http://arxiv.org/abs/2508.04048v2","updated":"2025-10-24T17:37:07Z","published":"2025-08-06T03:21:20Z","title":"Quantum Temporal Fusion Transformer","summary":"  The \\textit{Temporal Fusion Transformer} (TFT), proposed by Lim \\textit{et\nal.}, published in \\textit{International Journal of Forecasting} (2021), is a\nstate-of-the-art attention-based deep neural network architecture specifically\ndesigned for multi-horizon time series forecasting. It has demonstrated\nsignificant performance improvements over existing benchmarks. In this work, we\nintroduce the Quantum Temporal Fusion Transformer (QTFT), a quantum-enhanced\nhybrid quantum-classical architecture that extends the capabilities of the\nclassical TFT framework. The core idea of this work is inspired by the\nfoundation studies, \\textit{The Power of Quantum Neural Networks} by Amira\nAbbas \\textit{et al.} and \\textit{Quantum Vision Transformers} by El Amine\nCherrat \\textit{et al.}, published in \\textit{ Nature Computational Science}\n(2021) and \\textit{Quantum} (2024), respectively. A key advantage of our\napproach lies in its foundation on a variational quantum algorithm, enabling\nimplementation on current noisy intermediate-scale quantum (NISQ) devices\nwithout strict requirements on the number of qubits or circuit depth. Our\nresults demonstrate that QTFT is successfully trained on the forecasting\ndatasets and is capable of accurately predicting future values. In particular,\nour experimental results on two different datasets display that the model\noutperforms its classical counterpart in terms of both training and test loss.\nThese results indicate the prospect of using quantum computing to boost deep\nlearning architectures in complex machine learning tasks.\n","authors":["Krishnakanta Barik","Goutam Paul"],"pdf_url":"https://arxiv.org/pdf/2508.04048v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13678v4","updated":"2025-10-24T17:36:52Z","published":"2025-06-16T16:32:51Z","title":"A Gravity-informed Spatiotemporal Transformer for Human Activity\n  Intensity Prediction","summary":"  Human activity intensity prediction is crucial to many location-based\nservices. Despite tremendous progress in modeling dynamics of human activity,\nmost existing methods overlook physical constraints of spatial interaction,\nleading to uninterpretable spatial correlations and over-smoothing phenomenon.\nTo address these limitations, this work proposes a physics-informed deep\nlearning framework, namely Gravity-informed Spatiotemporal Transformer\n(Gravityformer) by integrating the universal law of gravitation to refine\ntransformer attention. Specifically, it (1) estimates two spatially explicit\nmass parameters based on spatiotemporal embedding feature, (2) models the\nspatial interaction in end-to-end neural network using proposed adaptive\ngravity model to learn the physical constraint, and (3) utilizes the learned\nspatial interaction to guide and mitigate the over-smoothing phenomenon in\ntransformer attention. Moreover, a parallel spatiotemporal graph convolution\ntransformer is proposed for achieving a balance between coupled spatial and\ntemporal learning. Systematic experiments on six real-world large-scale\nactivity datasets demonstrate the quantitative and qualitative superiority of\nour model over state-of-the-art benchmarks. Additionally, the learned gravity\nattention matrix can be not only disentangled and interpreted based on\ngeographical laws, but also improved the generalization in zero-shot\ncross-region inference. This work provides a novel insight into integrating\nphysical laws with deep learning for spatiotemporal prediction.\n","authors":["Yi Wang","Zhenghong Wang","Fan Zhang","Chaogui Kang","Sijie Ruan","Di Zhu","Chengling Tang","Zhongfu Ma","Weiyu Zhang","Yu Zheng","Philip S. Yu","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2506.13678v4.pdf","comment":"IEEE TPAMI 2025. 18 pages, 14 figures"},{"id":"http://arxiv.org/abs/2411.18322v2","updated":"2025-10-24T17:36:28Z","published":"2024-11-27T13:23:11Z","title":"Mixture of Experts in Image Classification: What's the Sweet Spot?","summary":"  Mixture-of-Experts (MoE) models have shown promising potential for\nparameter-efficient scaling across domains. However, their application to image\nclassification remains limited, often requiring billion-scale datasets to be\ncompetitive. In this work, we explore the integration of MoE layers into image\nclassification architectures using open datasets. We conduct a systematic\nanalysis across different MoE configurations and model scales. We find that\nmoderate parameter activation per sample provides the best trade-off between\nperformance and efficiency. However, as the number of activated parameters\nincreases, the benefits of MoE diminish. Our analysis yields several practical\ninsights for vision MoE design. First, MoE layers most effectively strengthen\ntiny and mid-sized models, while gains taper off for large-capacity networks\nand do not redefine state-of-the-art ImageNet performance. Second, a Last-2\nplacement heuristic offers the most robust cross-architecture choice, with\nEvery-2 slightly better for Vision Transform (ViT), and both remaining\neffective as data and model scale increase. Third, larger datasets (e.g.,\nImageNet-21k) allow more experts, up to 16, for ConvNeXt to be utilized\neffectively without changing placement, as increased data reduces overfitting\nand promotes broader expert specialization. Finally, a simple linear router\nperforms best, suggesting that additional routing complexity yields no\nconsistent benefit.\n","authors":["Mathurin Videau","Alessandro Leite","Marc Schoenauer","Olivier Teytaud"],"pdf_url":"https://arxiv.org/pdf/2411.18322v2.pdf","comment":"Published in Transactions on Machine Learning Research"},{"id":"http://arxiv.org/abs/2510.18878v2","updated":"2025-10-24T17:31:44Z","published":"2025-09-13T18:16:29Z","title":"CityAQVis: Integrated ML-Visualization Sandbox Tool for Pollutant\n  Estimation in Urban Regions Using Multi-Source Data (Software Article)","summary":"  Urban air pollution poses significant risks to public health, environmental\nsustainability, and policy planning. Effective air quality management requires\npredictive tools that can integrate diverse datasets and communicate complex\nspatial and temporal pollution patterns. There is a gap in interactive tools\nwith seamless integration of forecasting and visualization of spatial\ndistributions of air pollutant concentrations. We present CityAQVis, an\ninteractive machine learning ML sandbox tool designed to predict and visualize\npollutant concentrations at the ground level using multi-source data, which\nincludes satellite observations, meteorological parameters, population density,\nelevation, and nighttime lights. While traditional air quality visualization\ntools often lack forecasting capabilities, CityAQVis enables users to build and\ncompare predictive models, visualizing the model outputs and offering insights\ninto pollution dynamics at the ground level. The pilot implementation of the\ntool is tested through case studies predicting nitrogen dioxide (NO2)\nconcentrations in metropolitan regions, highlighting its adaptability to\nvarious pollutants. Through an intuitive graphical user interface (GUI), the\nuser can perform comparative visualizations of the spatial distribution of\nsurface-level pollutant concentration in two different urban scenarios. Our\nresults highlight the potential of ML-driven visual analytics to improve\nsituational awareness and support data-driven decision-making in air quality\nmanagement.\n","authors":["Brij Bidhin Desai","Yukta Arvind Rajapur","Aswathi Mundayatt","Jaya Sreevalsan-Nair"],"pdf_url":"https://arxiv.org/pdf/2510.18878v2.pdf","comment":"19 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2503.10799v3","updated":"2025-10-24T17:27:32Z","published":"2025-03-13T18:50:22Z","title":"Fixed-Point RNNs: Interpolating from Diagonal to Dense","summary":"  Linear recurrent neural networks (RNNs) and state-space models (SSMs) such as\nMamba have become promising alternatives to softmax-attention as sequence\nmixing layers in Transformer architectures. Current models, however, do not\nexhibit the full state-tracking expressivity of RNNs because they rely on\nchannel-wise (i.e. diagonal) sequence mixing. In this paper, we investigate\nparameterizations of a large class of dense linear RNNs as fixed-points of\nparallelizable diagonal linear RNNs. The resulting models can naturally trade\nexpressivity for efficiency at a fixed number of parameters and achieve\nstate-of-the-art results on the state-tracking benchmarks $A_5$ and $S_5$,\nwhile matching performance on copying and other tasks.\n","authors":["Sajad Movahedi","Felix Sarnthein","Nicola Muca Cirone","Antonio Orvieto"],"pdf_url":"https://arxiv.org/pdf/2503.10799v3.pdf","comment":"NeurIPS 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2510.21669v1","updated":"2025-10-24T17:24:26Z","published":"2025-10-24T17:24:26Z","title":"Optimal Graph Clustering without Edge Density Signals","summary":"  This paper establishes the theoretical limits of graph clustering under the\nPopularity-Adjusted Block Model (PABM), addressing limitations of existing\nmodels. In contrast to the Stochastic Block Model (SBM), which assumes uniform\nvertex degrees, and to the Degree-Corrected Block Model (DCBM), which applies\nuniform degree corrections across clusters, PABM introduces separate popularity\nparameters for intra- and inter-cluster connections. Our main contribution is\nthe characterization of the optimal error rate for clustering under PABM, which\nprovides novel insights on clustering hardness: we demonstrate that unlike SBM\nand DCBM, cluster recovery remains possible in PABM even when traditional\nedge-density signals vanish, provided intra- and inter-cluster popularity\ncoefficients differ. This highlights a dimension of degree heterogeneity\ncaptured by PABM but overlooked by DCBM: local differences in connectivity\npatterns can enhance cluster separability independently of global edge\ndensities. Finally, because PABM exhibits a richer structure, its expected\nadjacency matrix has rank between $k$ and $k^2$, where $k$ is the number of\nclusters. As a result, spectral embeddings based on the top $k$ eigenvectors\nmay fail to capture important structural information. Our numerical experiments\non both synthetic and real datasets confirm that spectral clustering algorithms\nincorporating $k^2$ eigenvectors outperform traditional spectral approaches.\n","authors":["Maximilien Dreveton","Elaine Siyu Liu","Matthias Grossglauser","Patrick Thiran"],"pdf_url":"https://arxiv.org/pdf/2510.21669v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.17412v3","updated":"2025-10-24T17:22:51Z","published":"2025-08-24T15:34:17Z","title":"Convergence and Generalization of Anti-Regularization for Parametric\n  Models","summary":"  Anti-regularization introduces a reward term with a reversed sign into the\nloss function, deliberately amplifying model expressivity in small-sample\nregimes while ensuring that the intervention gradually vanishes as the sample\nsize grows through a power-law decay schedule. We formalize spectral safety\nconditions and trust-region constraints, and we design a lightweight safeguard\nthat combines a projection operator with gradient clipping to guarantee stable\nintervention. Theoretical analysis extends to linear smoothers and the Neural\nTangent Kernel regime, providing practical guidance on the choice of decay\nexponents through the balance between empirical risk and variance. Empirical\nresults show that Anti-regularization mitigates underfitting in both regression\nand classification while preserving generalization and improving calibration.\nAblation studies confirm that the decay schedule and safeguards are essential\nto avoiding overfitting and instability. As an alternative, we also propose a\ndegrees-of-freedom targeting schedule that maintains constant per-sample\ncomplexity. Anti-regularization constitutes a simple and reproducible procedure\nthat integrates seamlessly into standard empirical risk minimization pipelines,\nenabling robust learning under limited data and resource constraints by\nintervening only when necessary and vanishing otherwise.\n","authors":["Dongseok Kim","Wonjun Jeong","Gisung Oh"],"pdf_url":"https://arxiv.org/pdf/2508.17412v3.pdf","comment":"v3: Revised the paragraph under Theoretical Analysis (English\n  translation and typo corrections)"},{"id":"http://arxiv.org/abs/2505.23579v2","updated":"2025-10-24T17:16:49Z","published":"2025-05-29T15:49:27Z","title":"BioReason: Incentivizing Multimodal Biological Reasoning within a\n  DNA-LLM Model","summary":"  Unlocking deep and interpretable biological reasoning from complex genomic\ndata remains a major AI challenge limiting scientific progress. While current\nDNA foundation models excel at representing sequences, they struggle with\nmulti-step reasoning and lack transparent, biologically meaningful\nexplanations. BioReason addresses this by tightly integrating a DNA foundation\nmodel with a large language model (LLM), enabling the LLM to directly interpret\nand reason over genomic information. Through supervised fine-tuning and\nreinforcement learning, BioReason learns to produce logical, biologically\ncoherent deductions. It achieves major performance gains, boosting KEGG-based\ndisease pathway prediction accuracy from 86% to 98% and improving variant\neffect prediction by an average of 15% over strong baselines. BioReason can\nreason over unseen biological entities and explain its decisions step by step,\noffering a transformative framework for interpretable, mechanistic AI in\nbiology. All data, code, and checkpoints are available at\nhttps://github.com/bowang-lab/BioReason\n","authors":["Adibvafa Fallahpour","Andrew Magnuson","Purav Gupta","Shihao Ma","Jack Naimer","Arnav Shah","Haonan Duan","Omar Ibrahim","Hani Goodarzi","Chris J. Maddison","Bo Wang"],"pdf_url":"https://arxiv.org/pdf/2505.23579v2.pdf","comment":"28 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2510.11128v2","updated":"2025-10-24T17:14:46Z","published":"2025-10-13T08:19:56Z","title":"Lightweight Facial Landmark Detection in Thermal Images via Multi-Level\n  Cross-Modal Knowledge Transfer","summary":"  Facial Landmark Detection (FLD) in thermal imagery is critical for\napplications in challenging lighting conditions, but it is hampered by the lack\nof rich visual cues. Conventional cross-modal solutions, like feature fusion or\nimage translation from RGB data, are often computationally expensive or\nintroduce structural artifacts, limiting their practical deployment. To address\nthis, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a\nnovel framework that decouples high-fidelity RGB-to-thermal knowledge transfer\nfrom model compression to create both accurate and efficient thermal FLD\nmodels. A central challenge during knowledge transfer is the profound modality\ngap between RGB and thermal data, where traditional unidirectional distillation\nfails to enforce semantic consistency across disparate feature spaces. To\novercome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a\nbidirectional mechanism designed specifically for this task. DIKD establishes a\nconnection between modalities: it not only guides the thermal student with rich\nRGB features but also validates the student's learned representations by\nfeeding them back into the frozen teacher's prediction head. This closed-loop\nsupervision forces the student to learn modality-invariant features that are\nsemantically aligned with the teacher, ensuring a robust and profound knowledge\ntransfer. Experiments show that our approach sets a new state-of-the-art on\npublic thermal FLD benchmarks, notably outperforming previous methods while\ndrastically reducing computational overhead.\n","authors":["Qiyi Tong","Olivia Nocentini","Marta Lagomarsino","Kuanqi Cai","Marta Lorenzini","Arash Ajoudani"],"pdf_url":"https://arxiv.org/pdf/2510.11128v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.18119v2","updated":"2025-10-24T17:13:05Z","published":"2025-09-10T13:09:27Z","title":"MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents","summary":"  Building general-purpose graphical user interface (GUI) agents has become\nincreasingly promising with the progress in vision language models. However,\ndeveloping effective mobile GUI agents with reinforcement learning (RL) remains\nchallenging due to the heavy-tailed distribution of task difficulty and the\ninefficiency of large-scale environment sampling. We present an online agentic\nreinforcement learning framework MobileRL to enhance GUI agents in mobile\nenvironments. Its core component is the Difficulty-ADAptive GRPO (ADAGRPO)\nalgorithm. In ADAGRPO, we design difficulty-adaptive positive replay and\nfailure curriculum filtering to adapt the model to different task difficulties.\nWe introduce the shortest-path reward adjustment strategy to reshape rewards\nconcerning the task length in multi-turn agentic tasks. Those strategies\njointly stabilize RL training, improve sample efficiency, and generate strong\nperformance across diverse mobile apps and tasks. We apply MOBILERL to two open\nmodels (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B\nmodel achieves state-of-the-art results in terms of success rates on both\nAndroidWorld (80.2%) and AndroidLab (53.6%). The MOBILERL framework is\nopen-sourced at: https://github.com/THUDM/MobileRL.\n","authors":["Yifan Xu","Xiao Liu","Xinghan Liu","Jiaqi Fu","Hanchen Zhang","Bohao Jing","Shudan Zhang","Yuting Wang","Wenyi Zhao","Yuxiao Dong"],"pdf_url":"https://arxiv.org/pdf/2509.18119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.17470v2","updated":"2025-10-24T17:04:17Z","published":"2025-09-22T08:05:44Z","title":"Transformer-Gather, Fuzzy-Reconsider: A Scalable Hybrid Framework for\n  Entity Resolution","summary":"  Entity resolution plays a significant role in enterprise systems where data\nintegrity must be rigorously maintained. Traditional methods often struggle\nwith handling noisy data or semantic understanding, while modern methods suffer\nfrom computational costs or the excessive need for parallel computation. In\nthis study, we introduce a scalable hybrid framework, which is designed to\naddress several important problems, including scalability, noise robustness,\nand reliable results. We utilized a pre-trained language model to encode each\nstructured data into corresponding semantic embedding vectors. Subsequently,\nafter retrieving a semantically relevant subset of candidates, we apply a\nsyntactic verification stage using fuzzy string matching techniques to refine\nclassification on the unlabeled data. This approach was applied to a real-world\nentity resolution task, which exposed a linkage between a central user\nmanagement database and numerous shared hosting server records. Compared to\nother methods, this approach exhibits an outstanding performance in terms of\nboth processing time and robustness, making it a reliable solution for a\nserver-side product. Crucially, this efficiency does not compromise results, as\nthe system maintains a high retrieval recall of approximately 0.97. The\nscalability of the framework makes it deployable on standard CPU-based\ninfrastructure, offering a practical and effective solution for\nenterprise-level data integrity auditing.\n","authors":["Mohammadreza Sharifi","Danial Ahmadzadeh"],"pdf_url":"https://arxiv.org/pdf/2509.17470v2.pdf","comment":"Accepted at ICCKE 2025 Conference. 6 tables, 7 figures"},{"id":"http://arxiv.org/abs/2510.20787v2","updated":"2025-10-24T16:56:22Z","published":"2025-10-23T17:53:03Z","title":"Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction","summary":"  Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches.\n","authors":["Mutian He","Philip N. Garner"],"pdf_url":"https://arxiv.org/pdf/2510.20787v2.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.21638v1","updated":"2025-10-24T16:51:17Z","published":"2025-10-24T16:51:17Z","title":"DEEDEE: Fast and Scalable Out-of-Distribution Dynamics Detection","summary":"  Deploying reinforcement learning (RL) in safety-critical settings is\nconstrained by brittleness under distribution shift. We study\nout-of-distribution (OOD) detection for RL time series and introduce DEEDEE, a\ntwo-statistic detector that revisits representation-heavy pipelines with a\nminimal alternative. DEEDEE uses only an episodewise mean and an RBF kernel\nsimilarity to a training summary, capturing complementary global and local\ndeviations. Despite its simplicity, DEEDEE matches or surpasses contemporary\ndetectors across standard RL OOD suites, delivering a 600-fold reduction in\ncompute (FLOPs / wall-time) and an average 5% absolute accuracy gain over\nstrong baselines. Conceptually, our results indicate that diverse anomaly types\noften imprint on RL trajectories through a small set of low-order statistics,\nsuggesting a compact foundation for OOD detection in complex environments.\n","authors":["Tala Aljaafari","Varun Kanade","Philip Torr","Christian Schroeder de Witt"],"pdf_url":"https://arxiv.org/pdf/2510.21638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21631v1","updated":"2025-10-24T16:36:34Z","published":"2025-10-24T16:36:34Z","title":"Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations","summary":"  Knowledge distillation is a promising approach to transfer capabilities from\ncomplex teacher models to smaller, resource-efficient student models that can\nbe deployed easily, particularly in task-aware scenarios. However, existing\nmethods of task-aware distillation typically require substantial quantities of\ndata which may be unavailable or expensive to obtain in many practical\nscenarios. In this paper, we address this challenge by introducing a novel\nstrategy called Counterfactual-explanation-infused Distillation CoD for\nfew-shot task-aware knowledge distillation by systematically infusing\ncounterfactual explanations. Counterfactual explanations (CFEs) refer to inputs\nthat can flip the output prediction of the teacher model with minimum\nperturbation. Our strategy CoD leverages these CFEs to precisely map the\nteacher's decision boundary with significantly fewer samples. We provide\ntheoretical guarantees for motivating the role of CFEs in distillation, from\nboth statistical and geometric perspectives. We mathematically show that CFEs\ncan improve parameter estimation by providing more informative examples near\nthe teacher's decision boundary. We also derive geometric insights on how CFEs\neffectively act as knowledge probes, helping the students mimic the teacher's\ndecision boundaries more effectively than standard data. We perform experiments\nacross various datasets and LLMs to show that CoD outperforms standard\ndistillation approaches in few-shot regimes (as low as 8-512 samples). Notably,\nCoD only uses half of the original samples used by the baselines, paired with\ntheir corresponding CFEs and still improves performance.\n","authors":["Faisal Hamman","Pasan Dissanayake","Yanjun Fu","Sanghamitra Dutta"],"pdf_url":"https://arxiv.org/pdf/2510.21631v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2503.17629v2","updated":"2025-10-24T16:33:12Z","published":"2025-03-22T03:18:09Z","title":"Planning and Learning in Average Risk-aware MDPs","summary":"  For continuing tasks, average cost Markov decision processes have\nwell-documented value and can be solved using efficient algorithms. However, it\nexplicitly assumes that the agent is risk-neutral. In this work, we extend\nrisk-neutral algorithms to accommodate the more general class of dynamic risk\nmeasures. Specifically, we propose a relative value iteration (RVI) algorithm\nfor planning and design two model-free Q-learning algorithms, namely a generic\nalgorithm based on the multi-level Monte Carlo (MLMC) method, and an off-policy\nalgorithm dedicated to utility-based shortfall risk measures. Both the RVI and\nMLMC-based Q-learning algorithms are proven to converge to optimality.\nNumerical experiments validate our analysis, confirm empirically the\nconvergence of the off-policy algorithm, and demonstrate that our approach\nenables the identification of policies that are finely tuned to the intricate\nrisk-awareness of the agent that they serve.\n","authors":["Weikai Wang","Erick Delage"],"pdf_url":"https://arxiv.org/pdf/2503.17629v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.21043v3","updated":"2025-10-24T16:26:22Z","published":"2025-09-25T11:48:37Z","title":"Combinatorial Creativity: A New Frontier in Generalization Abilities","summary":"  Artificial intelligence (AI) systems, and Large Language Models (LLMs) in\nparticular, are increasingly employed for creative tasks like scientific idea\ngeneration, constituting a form of generalization from training data\nunaddressed by existing conceptual frameworks. Despite its similarities to\ncompositional generalization (CG), combinatorial creativity (CC) is an\nopen-ended ability. Instead of evaluating for accuracy or correctness against\nfixed targets, which would contradict the open-ended nature of CC, we propose a\ntheoretical framework and algorithmic task for evaluating outputs by their\ndegrees of novelty and utility. From here, we make several important empirical\ncontributions: (1) We obtain the first insights into the scaling behavior of\ncreativity for LLMs. (2) We discover that, for fixed compute budgets, there\nexist optimal model depths and widths for creative ability. (3) We find that\nthe ideation-execution gap, whereby LLMs excel at generating novel scientific\nideas but struggle to ensure their practical feasibility, may be explained by a\nmore fundamental novelty-utility tradeoff characteristic of creativity\nalgorithms in general. Importantly, this tradeoff remains persistent even at\nscale, casting doubt on the long-term creative potential of LLMs in their\ncurrent form. Together, our conceptual framework and empirical findings provide\na foundation for understanding and improving creativity in modern AI models,\nbridging the gap between human and machine intelligence.\n","authors":["Samuel Schapiro","Sumuk Shashidhar","Alexi Gladstone","Jonah Black","Royce Moon","Dilek Hakkani-Tur","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2509.21043v3.pdf","comment":"Preprint. The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2506.02964v2","updated":"2025-10-24T16:25:21Z","published":"2025-06-03T14:59:22Z","title":"FORLA: Federated Object-centric Representation Learning with Slot\n  Attention","summary":"  Learning efficient visual representations across heterogeneous unlabeled\ndatasets remains a central challenge in federated learning. Effective federated\nrepresentations require features that are jointly informative across clients\nwhile disentangling domain-specific factors without supervision. We introduce\nFORLA, a novel framework for federated object-centric representation learning\nand feature adaptation across clients using unsupervised slot attention. At the\ncore of our method is a shared feature adapter, trained collaboratively across\nclients to adapt features from foundation models, and a shared slot attention\nmodule that learns to reconstruct the adapted features. To optimize this\nadapter, we design a two-branch student-teacher architecture. In each client, a\nstudent decoder learns to reconstruct full features from foundation models,\nwhile a teacher decoder reconstructs their adapted, low-dimensional\ncounterpart. The shared slot attention module bridges cross-domain learning by\naligning object-level representations across clients. Experiments in multiple\nreal-world datasets show that our framework not only outperforms centralized\nbaselines on object discovery but also learns a compact, universal\nrepresentation that generalizes well across domains. This work highlights\nfederated slot attention as an effective tool for scalable, unsupervised visual\nrepresentation learning from cross-domain data with distributed concepts.\n","authors":["Guiqiu Liao","Matjaz Jogan","Eric Eaton","Daniel A. Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2506.02964v2.pdf","comment":"Accepted by Neurips2025"},{"id":"http://arxiv.org/abs/2412.06646v3","updated":"2025-10-24T16:24:17Z","published":"2024-12-09T16:39:40Z","title":"The Narrow Gate: Localized Image-Text Communication in Native Multimodal\n  Models","summary":"  Recent advances in multimodal training have significantly improved the\nintegration of image understanding and generation within a unified model. This\nstudy investigates how vision-language models (VLMs) handle image-understanding\ntasks, focusing on how visual information is processed and transferred to the\ntextual domain. We compare native multimodal VLMs, models trained from scratch\non multimodal data to generate both text and images, and non-native multimodal\nVLMs, models adapted from pre-trained large language models or capable of\ngenerating only text, highlighting key differences in information flow. We find\nthat in native multimodal VLMs, image and text embeddings are more separated\nwithin the residual stream. Moreover, VLMs differ in how visual information\nreaches text: non-native multimodal VLMs exhibit a distributed communication\npattern, where information is exchanged through multiple image tokens, whereas\nmodels trained natively for joint image and text generation tend to rely on a\nsingle post-image token that acts as a narrow gate for visual information. We\nshow that ablating this single token significantly deteriorates\nimage-understanding performance, whereas targeted, token-level interventions\nreliably steer image semantics and downstream text with fine-grained control.\n","authors":["Alessandro Serra","Francesco Ortu","Emanuele Panizon","Lucrezia Valeriani","Lorenzo Basile","Alessio Ansuini","Diego Doimo","Alberto Cazzaniga"],"pdf_url":"https://arxiv.org/pdf/2412.06646v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21618v1","updated":"2025-10-24T16:24:01Z","published":"2025-10-24T16:24:01Z","title":"DeepAgent: A General Reasoning Agent with Scalable Toolsets","summary":"  Large reasoning models have demonstrated strong problem-solving abilities,\nyet real-world tasks often require external tools and long-horizon\ninteractions. Existing agent frameworks typically follow predefined workflows,\nwhich limit autonomous and global task completion. In this paper, we introduce\nDeepAgent, an end-to-end deep reasoning agent that performs autonomous\nthinking, tool discovery, and action execution within a single, coherent\nreasoning process. To address the challenges of long-horizon interactions,\nparticularly the context length explosion from multiple tool calls and the\naccumulation of interaction history, we introduce an autonomous memory folding\nmechanism that compresses past interactions into structured episodic, working,\nand tool memories, reducing error accumulation while preserving critical\ninformation. To teach general-purpose tool use efficiently and stably, we\ndevelop an end-to-end reinforcement learning strategy, namely ToolPO, that\nleverages LLM-simulated APIs and applies tool-call advantage attribution to\nassign fine-grained credit to the tool invocation tokens. Extensive experiments\non eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,\nTMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,\nHLE), demonstrate that DeepAgent consistently outperforms baselines across both\nlabeled-tool and open-set tool retrieval scenarios. This work takes a step\ntoward more general and capable agents for real-world applications. The code\nand demo are available at https://github.com/RUC-NLPIR/DeepAgent.\n","authors":["Xiaoxi Li","Wenxiang Jiao","Jiarui Jin","Guanting Dong","Jiajie Jin","Yinuo Wang","Hao Wang","Yutao Zhu","Ji-Rong Wen","Yuan Lu","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2510.21618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21610v1","updated":"2025-10-24T16:15:53Z","published":"2025-10-24T16:15:53Z","title":"Generative Correlation Manifolds: Generating Synthetic Data with\n  Preserved Higher-Order Correlations","summary":"  The increasing need for data privacy and the demand for robust machine\nlearning models have fueled the development of synthetic data generation\ntechniques. However, current methods often succeed in replicating simple\nsummary statistics but fail to preserve both the pairwise and higher-order\ncorrelation structure of the data that define the complex, multi-variable\ninteractions inherent in real-world systems. This limitation can lead to\nsynthetic data that is superficially realistic but fails when used for\nsophisticated modeling tasks. In this white paper, we introduce Generative\nCorrelation Manifolds (GCM), a computationally efficient method for generating\nsynthetic data. The technique uses Cholesky decomposition of a target\ncorrelation matrix to produce datasets that, by mathematical proof, preserve\nthe entire correlation structure -- from simple pairwise relationships to\nhigher-order interactions -- of the source dataset. We argue that this method\nprovides a new approach to synthetic data generation with potential\napplications in privacy-preserving data sharing, robust model training, and\nsimulation.\n","authors":["Jens E. d'Hondt","Wieger R. Punter","Odysseas Papapetrou"],"pdf_url":"https://arxiv.org/pdf/2510.21610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21609v1","updated":"2025-10-24T16:15:05Z","published":"2025-10-24T16:15:05Z","title":"Enhancing Tactile-based Reinforcement Learning for Robotic Control","summary":"  Achieving safe, reliable real-world robotic manipulation requires agents to\nevolve beyond vision and incorporate tactile sensing to overcome sensory\ndeficits and reliance on idealised state information. Despite its potential,\nthe efficacy of tactile sensing in reinforcement learning (RL) remains\ninconsistent. We address this by developing self-supervised learning (SSL)\nmethodologies to more effectively harness tactile observations, focusing on a\nscalable setup of proprioception and sparse binary contacts. We empirically\ndemonstrate that sparse binary tactile signals are critical for dexterity,\nparticularly for interactions that proprioceptive control errors do not\nregister, such as decoupled robot-object motions. Our agents achieve superhuman\ndexterity in complex contact tasks (ball bouncing and Baoding ball rotation).\nFurthermore, we find that decoupling the SSL memory from the on-policy memory\ncan improve performance. We release the Robot Tactile Olympiad (RoTO) benchmark\nto standardise and promote future research in tactile-based manipulation.\nProject page: https://elle-miller.github.io/tactile_rl\n","authors":["Elle Miller","Trevor McInroe","David Abel","Oisin Mac Aodha","Sethu Vijayakumar"],"pdf_url":"https://arxiv.org/pdf/2510.21609v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2505.11217v2","updated":"2025-10-24T14:21:57Z","published":"2025-05-16T13:13:25Z","title":"Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI\n  models in Sound Localization","summary":"  Imagine hearing a dog bark and turning toward the sound only to see a parked\ncar, while the real, silent dog sits elsewhere. Such sensory conflicts test\nperception, yet humans reliably resolve them by prioritizing sound over\nmisleading visuals. Despite advances in multimodal AI integrating vision and\naudio, little is known about how these systems handle cross-modal conflicts or\nwhether they favor one modality. In this study, we systematically examine\nmodality bias and conflict resolution in AI sound localization. We assess\nleading multimodal models and benchmark them against human performance in\npsychophysics experiments across six audiovisual conditions, including\ncongruent, conflicting, and absent cues. Humans consistently outperform AI,\ndemonstrating superior resilience to conflicting or missing visuals by relying\non auditory information. In contrast, AI models often default to visual input,\ndegrading performance to near chance levels. To address this, we propose a\nneuroscience-inspired model, EchoPin, which uses a stereo audio-image dataset\ngenerated via 3D simulations. Even with limited training data, EchoPin\nsurpasses existing benchmarks. Notably, it also mirrors human-like horizontal\nlocalization bias favoring left-right precision-likely due to the stereo audio\nstructure reflecting human ear placement. These findings underscore how sensory\ninput quality and system architecture shape multimodal representation accuracy.\n","authors":["Yanhao Jia","Ji Xie","S Jivaganesh","Hao Li","Xu Wu","Mengmi Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.11217v2.pdf","comment":"NeurIPS 2025, Spotlight"},{"id":"http://arxiv.org/abs/2406.03317v3","updated":"2025-10-24T02:20:55Z","published":"2024-06-05T14:29:44Z","title":"Save It for the \"Hot\" Day: An LLM-Empowered Visual Analytics System for\n  Heat Risk Management","summary":"  The escalating frequency and intensity of heat-related climate events,\nparticularly heatwaves, emphasize the pressing need for advanced heat risk\nmanagement strategies. Current approaches, primarily relying on numerical\nmodels, face challenges in spatial-temporal resolution and in capturing the\ndynamic interplay of environmental, social, and behavioral factors affecting\nheat risks. This has led to difficulties in translating risk assessments into\neffective mitigation actions. Recognizing these problems, we introduce a novel\napproach leveraging the burgeoning capabilities of Large Language Models (LLMs)\nto extract rich and contextual insights from news reports. We hence propose an\nLLM-empowered visual analytics system, Havior, that integrates the precise,\ndata-driven insights of numerical models with nuanced news report information.\nThis hybrid approach enables a more comprehensive assessment of heat risks and\nbetter identification, assessment, and mitigation of heat-related threats. The\nsystem incorporates novel visualization designs, such as \"thermoglyph\" and news\nglyph, enhancing intuitive understanding and analysis of heat risks. The\nintegration of LLM-based techniques also enables advanced information retrieval\nand semantic knowledge extraction that can be guided by experts' analytics\nneeds. Our case studies on two cities that faced significant heatwave events\nand interviews with five experts have demonstrated the usefulness of our system\nin providing in-depth and actionable insights for heat risk management.\n","authors":["Haobo Li","Wong Kam-Kwai","Yan Luo","Juntong Chen","Chengzhong Liu","Yaxuan Zhang","Alexis Kai Hon Lau","Huamin Qu","Dongyu Liu"],"pdf_url":"https://arxiv.org/pdf/2406.03317v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24769v1","updated":"2025-10-24T11:27:14Z","published":"2025-10-24T11:27:14Z","title":"YTLive: A Dataset of Real-World YouTube Live Streaming Sessions","summary":"  Live streaming plays a major role in today's digital platforms, supporting\nentertainment, education, social media, etc. However, research in this field is\nlimited by the lack of large, publicly available datasets that capture\nreal-time viewer behavior at scale. To address this gap, we introduce YTLive, a\npublic dataset focused on YouTube Live. Collected through the YouTube\nResearcher Program over May and June 2024, YTLive includes more than 507000\nrecords from 12156 live streams, tracking concurrent viewer counts at\nfive-minute intervals along with precise broadcast durations. We describe the\ndataset design and collection process and present an initial analysis of\ntemporal viewing patterns. Results show that viewer counts are higher and more\nstable on weekends, especially during afternoon hours. Shorter streams attract\nlarger and more consistent audiences, while longer streams tend to grow slowly\nand exhibit greater variability. These insights have direct implications for\nadaptive streaming, resource allocation, and Quality of Experience (QoE)\nmodeling. YTLive offers a timely, open resource to support reproducible\nresearch and system-level innovation in live streaming. The dataset is publicly\navailable at github.\n","authors":["Mojtaba Mozhganfar","Pooya Jamshidi","Seyyed Ali Aghamiri","Mohsen Ghasemi","Mahdi Dolati","Farzad Tashtarian","Ahmad Khonsari","Christian Timmerer"],"pdf_url":"https://arxiv.org/pdf/2510.24769v1.pdf","comment":null}]},"2025-10-27T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2510.23606v1","updated":"2025-10-27T17:59:57Z","published":"2025-10-27T17:59:57Z","title":"Variational Masked Diffusion Models","summary":"  Masked diffusion models have recently emerged as a flexible framework for\ndiscrete generative modeling. However, a key limitation of standard masked\ndiffusion is its inability to effectively capture dependencies among tokens\nthat are predicted concurrently, leading to degraded generation quality when\ndependencies among tokens are important. To explicitly model dependencies among\ntokens, we propose Variational Masked Diffusion (VMD), a framework that\nintroduces latent variables into the masked diffusion process. Through\ncontrolled experiments on synthetic datasets, we demonstrate that VMD\nsuccessfully learns dependencies that conventional masked diffusion fails to\ncapture. We further validate the effectiveness of our approach on Sudoku\npuzzles and text datasets, where learning of dependencies among tokens improves\nglobal consistency. Across these domains, VMD enhances both generation quality\nand dependency awareness, highlighting the value of integrating variational\ninference into masked diffusion. Our code is available at:\nhttps://riccizz.github.io/VMD.\n","authors":["Yichi Zhang","Alex Schwing","Zhizhen Zhao"],"pdf_url":"https://arxiv.org/pdf/2510.23606v1.pdf","comment":"Project Page: https://riccizz.github.io/VMD"},{"id":"http://arxiv.org/abs/2506.05314v2","updated":"2025-10-27T17:59:13Z","published":"2025-06-05T17:55:23Z","title":"Constrained Entropic Unlearning: A Primal-Dual Framework for Large\n  Language Models","summary":"  Large Language Models (LLMs) deployed in real-world settings increasingly\nface the need to unlearn sensitive, outdated, or proprietary information.\nExisting unlearning methods typically formulate forgetting and retention as a\nregularized trade-off, combining both objectives into a single scalarized loss.\nThis often leads to unstable optimization and degraded performance on retained\ndata, especially under aggressive forgetting. We propose a new formulation of\nLLM unlearning as a constrained optimization problem: forgetting is enforced\nvia a novel logit-margin flattening loss that explicitly drives the output\ndistribution toward uniformity on a designated forget set, while retention is\npreserved through a hard constraint on a separate retain set. Compared to\nentropy-based objectives, our loss is softmax-free, numerically stable, and\nmaintains non-vanishing gradients, enabling more efficient and robust\noptimization. We solve the constrained problem using a scalable primal-dual\nalgorithm that exposes the trade-off between forgetting and retention through\nthe dynamics of the dual variable, all without any extra computational\noverhead. Evaluations on the TOFU and MUSE benchmarks across diverse LLM\narchitectures demonstrate that our approach consistently matches or exceeds\nstate-of-the-art baselines, effectively removing targeted information while\npreserving downstream utility.\n","authors":["Taha Entesari","Arman Hatami","Rinat Khaziev","Anil Ramakrishna","Mahyar Fazlyab"],"pdf_url":"https://arxiv.org/pdf/2506.05314v2.pdf","comment":"The Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems"},{"id":"http://arxiv.org/abs/2510.23596v1","updated":"2025-10-27T17:58:07Z","published":"2025-10-27T17:58:07Z","title":"Think Twice: Branch-and-Rethink Reasoning Reward Model","summary":"  Large language models (LLMs) increasingly rely on thinking models that\nexternalize intermediate steps and allocate extra test-time compute, with\nthink-twice strategies showing that a deliberate second pass can elicit\nstronger reasoning. In contrast, most reward models (RMs) still compress many\nquality dimensions into a single scalar in one shot, a design that induces\njudgment diffusion: attention spreads across evaluation criteria, yielding\ndiluted focus and shallow analysis. We introduce branch-and-rethink (BR-RM), a\ntwo-turn RM that transfers the think-twice principle to reward modeling. Turn 1\nperforms adaptive branching, selecting a small set of instance-critical\ndimensions (such as factuality and safety) and sketching concise,\nevidence-seeking hypotheses. Turn 2 executes branch-conditioned rethinking, a\ntargeted reread that tests those hypotheses and scrutinizes only what matters\nmost. We train with GRPO-style reinforcement learning over structured two-turn\ntraces using a simple binary outcome reward with strict format checks, making\nthe approach compatible with standard RLHF pipelines. By converting\nall-at-oncescoringintofocused, second-lookreasoning,\nBR-RMreducesjudgmentdiffusionandimproves sensitivity to subtle yet\nconsequential errors while remaining practical and scalable. Experimental\nresults demonstrate that our model achieves state-of-the-art performance on\nthree challenging reward modeling benchmarks across diverse domains. The code\nand the model will be released soon.\n","authors":["Yizhu Jiao","Jiaqi Zeng","Julien Veron Vialard","Oleksii Kuchaiev","Jiawei Han","Olivier Delalleau"],"pdf_url":"https://arxiv.org/pdf/2510.23596v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23585v1","updated":"2025-10-27T17:53:40Z","published":"2025-10-27T17:53:40Z","title":"Hope Speech Detection in Social Media English Corpora: Performance of\n  Traditional and Transformer Models","summary":"  The identification of hope speech has become a promised NLP task, considering\nthe need to detect motivational expressions of agency and goal-directed\nbehaviour on social media platforms. This proposal evaluates traditional\nmachine learning models and fine-tuned transformers for a previously split hope\nspeech dataset as train, development and test set. On development test, a\nlinear-kernel SVM and logistic regression both reached a macro-F1 of 0.78; SVM\nwith RBF kernel reached 0.77, and Na\\\"ive Bayes hit 0.75. Transformer models\ndelivered better results, the best model achieved weighted precision of 0.82,\nweighted recall of 0.80, weighted F1 of 0.79, macro F1 of 0.79, and 0.80\naccuracy. These results suggest that while optimally configured traditional\nmachine learning models remain agile, transformer architectures detect some\nsubtle semantics of hope to achieve higher precision and recall in hope speech\ndetection, suggesting that larges transformers and LLMs could perform better in\nsmall datasets.\n","authors":["Luis Ramos","Hiram Calvo","Olga Kolesnikova"],"pdf_url":"https://arxiv.org/pdf/2510.23585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.07793v2","updated":"2025-10-27T17:46:32Z","published":"2025-10-09T05:12:09Z","title":"LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell\n  Biology","summary":"  Large language models (LLMs) and emerging agentic frameworks are beginning to\ntransform single-cell biology by enabling natural-language reasoning,\ngenerative annotation, and multimodal data integration. However, progress\nremains fragmented across data modalities, architectures, and evaluation\nstandards. LLM4Cell presents the first unified survey of 58 foundation and\nagentic models developed for single-cell research, spanning RNA, ATAC,\nmulti-omic, and spatial modalities. We categorize these methods into five\nfamilies-foundation, text-bridge, spatial, multimodal, epigenomic, and\nagentic-and map them to eight key analytical tasks including annotation,\ntrajectory and perturbation modeling, and drug-response prediction. Drawing on\nover 40 public datasets, we analyze benchmark suitability, data diversity, and\nethical or scalability constraints, and evaluate models across 10 domain\ndimensions covering biological grounding, multi-omics alignment, fairness,\nprivacy, and explainability. By linking datasets, models, and evaluation\ndomains, LLM4Cell provides the first integrated view of language-driven\nsingle-cell intelligence and outlines open challenges in interpretability,\nstandardization, and trustworthy model development.\n","authors":["Sajib Acharjee Dip","Adrika Zafor","Bikash Kumar Paul","Uddip Acharjee Shuvo","Muhit Islam Emon","Xuan Wang","Liqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.07793v2.pdf","comment":"34 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2503.17239v2","updated":"2025-10-27T17:40:05Z","published":"2025-03-21T15:44:09Z","title":"SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language\n  Models via Selective Layer-Wise Model Merging","summary":"  Fine-tuning large language models (LLMs) is a common practice to adapt\ngeneralist models to specialized domains. However, recent studies show that\nfine-tuning can erode safety alignment, causing LLMs to respond to harmful or\nunethical prompts. Many methods to realign safety have been proposed, but often\nintroduce custom algorithms that are difficult to implement or compromise task\nutility. In this work, we propose SafeMERGE, a lightweight, post-fine-tuning\nframework that preserves safety while maintaining downstream performance.\nSafeMERGE selectively merges fine-tuned with safety-aligned model layers only\nwhen they deviate from safe behavior, measured by a cosine similarity\ncriterion. Across three LLMs and two tasks, SafeMERGE consistently reduces\nharmful outputs compared to other defenses, with negligible or even positive\nimpact on utility. Our results demonstrate that selective layer-wise merging\noffers an effective safeguard against the inadvertent loss of safety during\nfine-tuning, establishing SafeMERGE as a simple post-fine-tuning defense.\n","authors":["Aladin Djuhera","Swanand Ravindra Kadhe","Farhan Ahmed","Syed Zawad","Holger Boche"],"pdf_url":"https://arxiv.org/pdf/2503.17239v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02103v2","updated":"2025-10-27T17:35:32Z","published":"2025-03-03T22:41:25Z","title":"Superficial Self-Improved Reasoners Benefit from Model Merging","summary":"  As scaled language models (LMs) approach human-level reasoning capabilities,\nself-improvement emerges as a solution to synthesizing high-quality data\ncorpus. While previous research has identified model collapse as a risk in\nself-improvement, where model outputs become increasingly deterministic, we\ndiscover a more fundamental challenge: the superficial self-improved reasoners\nphenomenon. In particular, our analysis reveals that even when LMs show\nimproved in-domain (ID) reasoning accuracy, they actually compromise their\ngeneralized reasoning capabilities on out-of-domain (OOD) tasks due to\nmemorization rather than genuine. Through a systematic investigation of LM\narchitecture, we discover that during self-improvement, LM weight updates are\nconcentrated in less reasoning-critical layers, leading to superficial\nlearning. To address this, we propose Iterative Model Merging (IMM), a method\nthat strategically combines weights from original and self-improved models to\npreserve generalization while incorporating genuine reasoning improvements. Our\napproach effectively mitigates both LM collapse and superficial learning,\nmoving towards more stable self-improving systems.\n","authors":["Xiangchi Yuan","Chunhui Zhang","Zheyuan Liu","Dachuan Shi","Leyan Pan","Soroush Vosoughi","Wenke Lee"],"pdf_url":"https://arxiv.org/pdf/2503.02103v2.pdf","comment":"EMNLP 2025"},{"id":"http://arxiv.org/abs/2510.23564v1","updated":"2025-10-27T17:35:15Z","published":"2025-10-27T17:35:15Z","title":"ReCode: Unify Plan and Action for Universal Granularity Control","summary":"  Real-world tasks require decisions at varying granularities, and humans excel\nat this by leveraging a unified cognitive representation where planning is\nfundamentally understood as a high-level form of action. However, current Large\nLanguage Model (LLM)-based agents lack this crucial capability to operate\nfluidly across decision granularities. This limitation stems from existing\nparadigms that enforce a rigid separation between high-level planning and\nlow-level action, which impairs dynamic adaptability and limits generalization.\nWe propose ReCode (Recursive Code Generation), a novel paradigm that addresses\nthis limitation by unifying planning and action within a single code\nrepresentation. In this representation, ReCode treats high-level plans as\nabstract placeholder functions, which the agent then recursively decomposes\ninto finer-grained sub-functions until reaching primitive actions. This\nrecursive approach dissolves the rigid boundary between plan and action,\nenabling the agent to dynamically control its decision granularity.\nFurthermore, the recursive structure inherently generates rich,\nmulti-granularity training data, enabling models to learn hierarchical\ndecision-making processes. Extensive experiments show ReCode significantly\nsurpasses advanced baselines in inference performance and demonstrates\nexceptional data efficiency in training, validating our core insight that\nunifying planning and action through recursive code generation is a powerful\nand effective approach to achieving universal granularity control. The code is\navailable at https://github.com/FoundationAgents/ReCode.\n","authors":["Zhaoyang Yu","Jiayi Zhang","Huixue Su","Yufan Zhao","Yifan Wu","Mingyi Deng","Jinyu Xiang","Yizhang Lin","Lingxiao Tang","Yingchao Li","Yuyu Luo","Bang Liu","Chenglin Wu"],"pdf_url":"https://arxiv.org/pdf/2510.23564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00062v2","updated":"2025-10-27T17:35:06Z","published":"2025-05-29T13:31:51Z","title":"SafeCOMM: A Study on Safety Degradation in Fine-Tuned Telecom Large\n  Language Models","summary":"  Fine-tuning large language models (LLMs) on telecom datasets is a common\npractice to adapt general-purpose models to the telecom domain. However, little\nattention has been paid to how this process may compromise model safety. Recent\nresearch has shown that even benign fine-tuning can degrade the safety\nalignment of LLMs, causing them to respond to harmful or unethical user\nqueries. In this paper, we investigate this issue by fine-tuning LLMs on three\nrepresentative telecom datasets and show that safety degrades even for light\ntelecom domain adaptation. To this end, we introduce TeleHarm, the first\ntelecom-specific red-teaming benchmark, which we use alongside established\nDirect-Harm and HexPhi datasets to systematically assess harmful behavior. We\nfurther extend our analysis to publicly available TeleLLMs that were\ncontinually pre-trained on large telecom corpora, revealing that safety\nalignment is severely lacking, primarily due to the omission of safety-focused\ninstruction tuning. To address these issues, we evaluate three realignment\ndefenses: SafeInstruct, SafeLoRA, SafeMERGE. We show that, across all settings,\nthe proposed defenses can effectively restore safety without compromising\ntelecom task performance, leading to Safe teleCOMMunication (SafeCOMM) models.\nOur work serves as both a diagnostic study and practical guide for safety\nrealignment in telecom-tuned LLMs, underscoring the need for safety-aware\ninstruction and fine-tuning in the telecom domain.\n","authors":["Aladin Djuhera","Swanand Ravindra Kadhe","Farhan Ahmed","Syed Zawad","Fernando Koch","Walid Saad","Holger Boche"],"pdf_url":"https://arxiv.org/pdf/2506.00062v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23558v1","updated":"2025-10-27T17:31:25Z","published":"2025-10-27T17:31:25Z","title":"ISA-Bench: Benchmarking Instruction Sensitivity for Large Audio Language\n  Models","summary":"  Large Audio Language Models (LALMs), which couple acoustic perception with\nlarge language models (LLMs) to extract and understand diverse information from\naudio, have attracted intense interest from both academic and industrial\ncommunities. However, existing LALMs are highly sensitive to how instructions\nare phrased, affecting both (i) instruction-following rates and (ii) task\nperformance. Yet, no existing benchmarks offer a systematic and comprehensive\nevaluation of this sensitivity. We introduce ISA-Bench, a dynamic benchmark\nevaluating instruction sensitivity for LALMs along three axes: instruction\ndescription, output format, and task composition. We assess recent open-source\nand proprietary LALMs using ISA-Bench, profiling both compliance and accuracy\nunder controlled instruction variations. Experimental results reveal that even\nstate-of-the-art LALMs suffer significant instruction sensitivity, leading to\ndegraded performance on fundamental audio understanding tasks. To mitigate this\nissue, we fine-tune Qwen2-Audio on a specifically constructed complex\ninstruction-variant dataset, achieving a marked improvement in\ninstruction-following performance. However, this also induces nontrivial\ncatastrophic forgetting: the model loses some previously mastered task\ncapabilities when exposed to new instruction styles. Our benchmark provides a\nstandardized basis for assessing and improving instruction sensitivity in\nLALMs, underscoring the need for instruction-robust audio understanding in\nreal-world pipelines.\n","authors":["Bohan Li","Wenbin Huang","Yuhang Qiu","Yiwei Guo","Hankun Wang","Zhihan Li","Jing Peng","Ziyang Ma","Xie Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2510.23558v1.pdf","comment":"submitted to icassp 2026"},{"id":"http://arxiv.org/abs/2506.06522v2","updated":"2025-10-27T17:31:21Z","published":"2025-06-06T20:34:06Z","title":"Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality\n  and Model Performance","summary":"  Recent work on large language models (LLMs) has increasingly focused on\npost-training and alignment with datasets curated to enhance instruction\nfollowing, world knowledge, and specialized skills. However, most post-training\ndatasets used in leading open- and closed-source LLMs remain inaccessible to\nthe public, with limited information about their construction process. This\nlack of transparency has motivated the recent development of open-source\npost-training corpora. While training on these open alternatives can yield\nperformance comparable to that of leading models, systematic comparisons remain\nchallenging due to the significant computational cost of conducting them\nrigorously at scale, and are therefore largely absent. As a result, it remains\nunclear how specific samples, task types, or curation strategies influence\ndownstream performance when assessing data quality. In this work, we conduct\nthe first comprehensive side-by-side analysis of two prominent open\npost-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie\nframework, we annotate each sample with detailed quality metrics, including\nturn structure (single-turn vs. multi-turn), task category, input quality, and\nresponse quality, and we derive statistics that reveal structural and\nqualitative similarities and differences between the two datasets. Based on\nthese insights, we design a principled curation recipe that produces a new data\nmixture, TuluTalk, which contains 14% fewer samples than either source dataset\nwhile matching or exceeding their performance on key benchmarks. Our findings\noffer actionable insights for constructing more effective post-training\ndatasets that improve model performance within practical resource limits. To\nsupport future research, we publicly release both the annotated source datasets\nand our curated TuluTalk mixture.\n","authors":["Aladin Djuhera","Swanand Ravindra Kadhe","Syed Zawad","Farhan Ahmed","Heiko Ludwig","Holger Boche"],"pdf_url":"https://arxiv.org/pdf/2506.06522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23554v1","updated":"2025-10-27T17:28:55Z","published":"2025-10-27T17:28:55Z","title":"A U-Net and Transformer Pipeline for Multilingual Image Translation","summary":"  This paper presents an end-to-end multilingual translation pipeline that\nintegrates a custom U-Net for text detection, the Tesseract engine for text\nrecognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for\nNeural Machine Translation (NMT). Our approach first utilizes a U-Net model,\ntrained on a synthetic dataset , to accurately segment and detect text regions\nfrom an image. These detected regions are then processed by Tesseract to\nextract the source text. This extracted text is fed into a custom Transformer\nmodel trained from scratch on a multilingual parallel corpus spanning 5\nlanguages. Unlike systems reliant on monolithic pre-trained models, our\narchitecture emphasizes full customization and adaptability. The system is\nevaluated on its text detection accuracy, text recognition quality, and\ntranslation performance via BLEU scores. The complete pipeline demonstrates\npromising results, validating the viability of a custom-built system for\ntranslating text directly from images.\n","authors":["Siddharth Sahay","Radhika Agarwal"],"pdf_url":"https://arxiv.org/pdf/2510.23554v1.pdf","comment":"6 pages, 3 figures, 5 tables, and 2 algorithms. Prepared in IEEE\n  double-column format"},{"id":"http://arxiv.org/abs/2506.19113v2","updated":"2025-10-27T17:23:28Z","published":"2025-06-23T20:41:45Z","title":"Human-Aligned Faithfulness in Toxicity Explanations of LLMs","summary":"  The discourse around toxicity and LLMs in NLP largely revolves around\ndetection tasks. This work shifts the focus to evaluating LLMs' reasoning about\ntoxicity -- from their explanations that justify a stance -- to enhance their\ntrustworthiness in downstream tasks. Despite extensive research on\nexplainability, it is not straightforward to adopt existing methods to evaluate\nfree-form toxicity explanation due to their over-reliance on input text\nperturbations, among other challenges. To account for these, we propose a\nnovel, theoretically-grounded multi-dimensional criterion, Human-Aligned\nFaithfulness (HAF), that measures the extent to which LLMs' free-form toxicity\nexplanations align with those of a rational human under ideal conditions. We\ndevelop six metrics, based on uncertainty quantification, to comprehensively\nevaluate HAF of LLMs' toxicity explanations with no human involvement, and\nhighlight how \"non-ideal\" the explanations are. We conduct several experiments\non three Llama models (of size up to 70B) and an 8B Ministral model on five\ndiverse toxicity datasets. Our results show that while LLMs generate plausible\nexplanations to simple prompts, their reasoning about toxicity breaks down when\nprompted about the nuanced relations between the complete set of reasons, the\nindividual reasons, and their toxicity stances, resulting in inconsistent and\nirrelevant responses. We open-source our code at\nhttps://github.com/uofthcdslab/HAF and LLM-generated explanations at\nhttps://huggingface.co/collections/uofthcdslab/haf.\n","authors":["Ramaravind K. Mothilal","Joanna Roy","Syed Ishtiaque Ahmed","Shion Guha"],"pdf_url":"https://arxiv.org/pdf/2506.19113v2.pdf","comment":"23 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2510.23544v1","updated":"2025-10-27T17:19:37Z","published":"2025-10-27T17:19:37Z","title":"LimRank: Less is More for Reasoning-Intensive Information Reranking","summary":"  Existing approaches typically rely on large-scale fine-tuning to adapt LLMs\nfor information reranking tasks, which is computationally expensive. In this\nwork, we demonstrate that modern LLMs can be effectively adapted using only\nminimal, high-quality supervision. To enable this, we design\nLIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating\ndiverse, challenging, and realistic reranking examples. Using this synthetic\ndata, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two\nchallenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and\nFollowIR for instruction-following retrieval. Our experiments demonstrate that\nLIMRANK achieves competitive performance, while being trained on less than 5%\nof the data typically used in prior work. Further ablation studies demonstrate\nthe effectiveness of LIMRANK-SYNTHESIZER and the strong generalization\ncapabilities of LIMRANK across downstream tasks, including scientific\nliterature search and retrieval-augmented generation for knowledge-intensive\nproblem solving.\n","authors":["Tingyu Song","Yilun Zhao","Siyue Zhang","Chen Zhao","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2510.23544v1.pdf","comment":"EMNLP 2025 Main (Short)"},{"id":"http://arxiv.org/abs/2510.23538v1","updated":"2025-10-27T17:13:49Z","published":"2025-10-27T17:13:49Z","title":"JanusCoder: Towards a Foundational Visual-Programmatic Interface for\n  Code Intelligence","summary":"  The scope of neural code intelligence is rapidly expanding beyond text-based\nsource code to encompass the rich visual outputs that programs generate. This\nvisual dimension is critical for advanced applications like flexible content\ngeneration and precise, program-driven editing of visualizations. However,\nprogress has been impeded by the scarcity of high-quality multimodal code data,\na bottleneck stemming from challenges in synthesis and quality assessment. To\naddress these challenges, we make contributions from both a data and modeling\nperspective. We first introduce a complete synthesis toolkit that leverages\nreciprocal synergies between data modalities to efficiently produce a\nlarge-scale, high-quality corpus spanning from standard charts to complex\ninteractive web UIs and code-driven animations. Leveraging this toolkit, we\nconstruct JanusCode-800K, the largest multimodal code corpus to date. This\npowers the training of our models, JanusCoder and JanusCoderV, which establish\na visual-programmatic interface for generating code from textual instructions,\nvisual inputs, or a combination of both. Our unified model is a departure from\nexisting approaches that build specialized models for isolated tasks. Extensive\nexperiments on both text-centric and vision-centric coding tasks demonstrate\nthe superior performance of the JanusCoder series, with our 7B to 14B scale\nmodels approaching or even exceeding the performance of commercial models.\nFurthermore, extensive analysis provides key insights into harmonizing\nprogrammatic logic with its visual expression. Our code and checkpoints will\nare available at https://github.com/InternLM/JanusCoder.\n","authors":["Qiushi Sun","Jingyang Gong","Yang Liu","Qiaosheng Chen","Lei Li","Kai Chen","Qipeng Guo","Ben Kao","Fei Yuan"],"pdf_url":"https://arxiv.org/pdf/2510.23538v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2510.23536v1","updated":"2025-10-27T17:12:49Z","published":"2025-10-27T17:12:49Z","title":"IPQA: A Benchmark for Core Intent Identification in Personalized\n  Question Answering","summary":"  Intent identification serves as the foundation for generating appropriate\nresponses in personalized question answering (PQA). However, existing\nbenchmarks evaluate only response quality or retrieval performance without\ndirectly measuring intent identification capabilities. This gap is critical\nbecause without understanding which intents users prioritize, systems cannot\ngenerate responses satisfying individual information needs. To address this, we\nintroduce the concept of core intents: intents users prioritize when selecting\nanswers to satisfy their information needs. To evaluate these core intents, we\npropose IPQA, a benchmark for core Intent identification in Personalized\nQuestion Answering. Since users do not explicitly state their prioritized\nintents, we derive core intents from observable behavior patterns in answer\nselection, grounded in satisficing theory where users choose answers meeting\ntheir acceptance thresholds. We construct a dataset with various domains\nthrough systematic filtering, LLM-based annotation, and rigorous quality\ncontrol combining automated verification with human validation. Experimental\nevaluations across state-of-the-art language models reveal that current systems\nstruggle with core intent identification in personalized contexts. Models fail\nto identify core intents from user histories, with performance degrading as\nquestion complexity increases. The code and dataset will be made publicly\navailable to facilitate future research in this direction.\n","authors":["Jieyong Kim","Maryam Amirizaniani","Soojin Yoon","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2510.23536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10720v2","updated":"2025-10-27T16:55:55Z","published":"2025-03-13T08:22:28Z","title":"AttentionRAG: Attention-Guided Context Pruning in Retrieval-Augmented\n  Generation","summary":"  While RAG demonstrates remarkable capabilities in LLM applications, its\neffectiveness is hindered by the ever-increasing length of retrieved contexts,\nwhich introduces information redundancy and substantial computational overhead.\nExisting context pruning methods, such as LLMLingua, lack contextual awareness\nand offer limited flexibility in controlling compression rates, often resulting\nin either insufficient pruning or excessive information loss. In this paper, we\npropose AttentionRAG, an attention-guided context pruning method for RAG\nsystems. The core idea of AttentionRAG lies in its attention focus mechanism,\nwhich reformulates RAG queries into a next-token prediction paradigm. This\nmechanism isolates the query's semantic focus to a single token, enabling\nprecise and efficient attention calculation between queries and retrieved\ncontexts. Extensive experiments on LongBench and Babilong benchmarks show that\nAttentionRAG achieves up to 6.3$\\times$ context compression while outperforming\nLLMLingua methods by around 10\\% in key metrics.\n","authors":["Yixiong Fang","Tianran Sun","Yuling Shi","Xiaodong Gu"],"pdf_url":"https://arxiv.org/pdf/2503.10720v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23508v1","updated":"2025-10-27T16:44:35Z","published":"2025-10-27T16:44:35Z","title":"M4FC: a Multimodal, Multilingual, Multicultural, Multitask Real-World\n  Fact-Checking Dataset","summary":"  Existing real-world datasets for multimodal automated fact-checking have\nmultiple limitations: they contain few instances, focus on only one or two\nlanguages and tasks, suffer from evidence leakage, or depend on external sets\nof news articles for sourcing true claims. To address these shortcomings, we\nintroduce M4FC, a new real-world dataset comprising 4,982 images paired with\n6,980 claims. The images, verified by professional fact-checkers from 22\norganizations, represent diverse cultural and geographic contexts. Each claim\nis available in one or two out of ten languages. M4FC spans six multimodal\nfact-checking tasks: visual claim extraction, claimant intent prediction, fake\ndetection, image contextualization, location verification, and verdict\nprediction. We provide baseline results for all tasks and analyze how combining\nintermediate tasks influence downstream verdict prediction performance. We make\nour dataset and code available.\n","authors":["Jiahui Geng","Jonathan Tonglet","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2510.23508v1.pdf","comment":"Preprint under review. Code and data available at:\n  https://github.com/UKPLab/M4FC"},{"id":"http://arxiv.org/abs/2504.11373v2","updated":"2025-10-27T16:39:30Z","published":"2025-04-15T16:37:32Z","title":"Cancer-Myth: Evaluating AI Chatbot on Patient Questions with False\n  Presuppositions","summary":"  Cancer patients are increasingly turning to large language models (LLMs) for\nmedical information, making it critical to assess how well these models handle\ncomplex, personalized questions. However, current medical benchmarks focus on\nmedical exams or consumer-searched questions and do not evaluate LLMs on real\npatient questions with patient details. In this paper, we first have three\nhematology-oncology physicians evaluate cancer-related questions drawn from\nreal patients. While LLM responses are generally accurate, the models\nfrequently fail to recognize or address false presuppositions in the questions,\nposing risks to safe medical decision-making. To study this limitation\nsystematically, we introduce Cancer-Myth, an expert-verified adversarial\ndataset of 585 cancer-related questions with false presuppositions. On this\nbenchmark, no frontier LLM -- including GPT-5, Gemini-2.5-Pro, and\nClaude-4-Sonnet -- corrects these false presuppositions more than $43\\%$ of the\ntime. To study mitigation strategies, we further construct a 150-question\nCancer-Myth-NFP set, in which physicians confirm the absence of false\npresuppositions. We find typical mitigation strategies, such as adding\nprecautionary prompts with GEPA optimization, can raise accuracy on Cancer-Myth\nto $80\\%$, but at the cost of misidentifying presuppositions in $41\\%$ of\nCancer-Myth-NFP questions and causing a $10\\%$ relative performance drop on\nother medical benchmarks. These findings highlight a critical gap in the\nreliability of LLMs, show that prompting alone is not a reliable remedy for\nfalse presuppositions, and underscore the need for more robust safeguards in\nmedical AI systems.\n","authors":["Wang Bill Zhu","Tianqi Chen","Xinyan Velocity Yu","Ching Ying Lin","Jade Law","Mazen Jizzini","Jorge J. Nieva","Ruishan Liu","Robin Jia"],"pdf_url":"https://arxiv.org/pdf/2504.11373v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01034v2","updated":"2025-10-27T16:17:17Z","published":"2025-06-01T14:30:46Z","title":"Less is More: Local Intrinsic Dimensions of Contextual Language Models","summary":"  Understanding the internal mechanisms of large language models (LLMs) remains\na challenging and complex endeavor. Even fundamental questions, such as how\nfine-tuning affects model behavior, often require extensive empirical\nevaluation. In this paper, we introduce a novel perspective based on the\ngeometric properties of contextual latent embeddings to study the effects of\ntraining and fine-tuning. To that end, we measure the local dimensions of a\ncontextual language model's latent space and analyze their shifts during\ntraining and fine-tuning. We show that the local dimensions provide insights\ninto the model's training dynamics and generalization ability. Specifically,\nthe mean of the local dimensions predicts when the model's training\ncapabilities are exhausted, as exemplified in a dialogue state tracking task,\noverfitting, as demonstrated in an emotion recognition task, and grokking, as\nillustrated with an arithmetic task. Furthermore, our experiments suggest a\npractical heuristic: reductions in the mean local dimension tend to accompany\nand predict subsequent performance gains. Through this exploration, we aim to\nprovide practitioners with a deeper understanding of the implications of\nfine-tuning on embedding spaces, facilitating informed decisions when\nconfiguring models for specific applications. The results of this work\ncontribute to the ongoing discourse on the interpretability, adaptability, and\ngeneralizability of LLMs by bridging the gap between intrinsic model mechanisms\nand geometric properties in the respective embeddings.\n","authors":["Benjamin Matthias Ruppik","Julius von Rohrscheidt","Carel van Niekerk","Michael Heck","Renato Vukovic","Shutong Feng","Hsien-chin Lin","Nurul Lubis","Bastian Rieck","Marcus Zibrowius","Milica GaÅ¡iÄ"],"pdf_url":"https://arxiv.org/pdf/2506.01034v2.pdf","comment":"Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025; in press). 10 pages, with an additional 17 pages in\n  the appendix. Our code is available at\n  https://github.com/aidos-lab/Topo_LLM_public and\n  https://github.com/aidos-lab/grokking-via-lid"},{"id":"http://arxiv.org/abs/2510.23477v1","updated":"2025-10-27T16:11:49Z","published":"2025-10-27T16:11:49Z","title":"MMTutorBench: The First Multimodal Benchmark for AI Math Tutoring","summary":"  Effective math tutoring requires not only solving problems but also\ndiagnosing students' difficulties and guiding them step by step. While\nmultimodal large language models (MLLMs) show promise, existing benchmarks\nlargely overlook these tutoring skills. We introduce MMTutorBench, the first\nbenchmark for AI math tutoring, consisting of 685 problems built around\npedagogically significant key-steps. Each problem is paired with\nproblem-specific rubrics that enable fine-grained evaluation across six\ndimensions, and structured into three tasks-Insight Discovery, Operation\nFormulation, and Operation Execution. We evaluate 12 leading MLLMs and find\nclear performance gaps between proprietary and open-source systems, substantial\nroom compared to human tutors, and consistent trends across input variants: OCR\npipelines degrade tutoring quality, few-shot prompting yields limited gains,\nand our rubric-based LLM-as-a-Judge proves highly reliable. These results\nhighlight both the difficulty and diagnostic value of MMTutorBench for\nadvancing AI tutoring.\n","authors":["Tengchao Yang","Sichen Guo","Mengzhao Jia","Jiaming Su","Yuanyang Liu","Zhihan Zhang","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.23477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23464v1","updated":"2025-10-27T16:03:20Z","published":"2025-10-27T16:03:20Z","title":"Evaluating Large Language Models for Stance Detection on Financial\n  Targets from SEC Filing Reports and Earnings Call Transcripts","summary":"  Financial narratives from U.S. Securities and Exchange Commission (SEC)\nfiling reports and quarterly earnings call transcripts (ECTs) are very\nimportant for investors, auditors, and regulators. However, their length,\nfinancial jargon, and nuanced language make fine-grained analysis difficult.\nPrior sentiment analysis in the financial domain required a large, expensive\nlabeled dataset, making the sentence-level stance towards specific financial\ntargets challenging. In this work, we introduce a sentence-level corpus for\nstance detection focused on three core financial metrics: debt, earnings per\nshare (EPS), and sales. The sentences were extracted from Form 10-K annual\nreports and ECTs, and labeled for stance (positive, negative, neutral) using\nthe advanced ChatGPT-o3-pro model under rigorous human validation. Using this\ncorpus, we conduct a systematic evaluation of modern large language models\n(LLMs) using zero-shot, few-shot, and Chain-of-Thought (CoT) prompting\nstrategies. Our results show that few-shot with CoT prompting performs best\ncompared to supervised baselines, and LLMs' performance varies across the SEC\nand ECT datasets. Our findings highlight the practical viability of leveraging\nLLMs for target-specific stance in the financial domain without requiring\nextensive labeled data.\n","authors":["Nikesh Gyawali","Doina Caragea","Alex Vasenkov","Cornelia Caragea"],"pdf_url":"https://arxiv.org/pdf/2510.23464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.16599v3","updated":"2025-10-27T16:02:27Z","published":"2025-09-20T09:50:18Z","title":"Computational-Assisted Systematic Review and Meta-Analysis (CASMA):\n  Effect of a Subclass of GnRH-a on Endometriosis Recurrence","summary":"  Background: Evidence synthesis facilitates evidence-based medicine. This task\nbecomes increasingly difficult to accomplished with applying computational\nsolutions, since the medical literature grows at astonishing rates. Objective:\nThis study evaluates an information retrieval-driven workflow, CASMA, to\nenhance the efficiency, transparency, and reproducibility of systematic\nreviews. Endometriosis recurrence serves as the ideal case due to its complex\nand ambiguous literature. Methods: The hybrid approach integrates PRISMA\nguidelines with fuzzy matching and regular expression (regex) to facilitate\nsemi-automated deduplication and filtered records before manual screening. The\nworkflow synthesised evidence from randomised controlled trials on the efficacy\nof a subclass of gonadotropin-releasing hormone agonists (GnRH-a). A modified\nsplitting method addressed unit-of-analysis errors in multi-arm trials.\nResults: The workflow sharply reduced the screening workload, taking only 11\ndays to fetch and filter 33,444 records. Seven eligible RCTs were synthesized\n(841 patients). The pooled random-effects model yielded a Risk Ratio (RR) of\n$0.64$ ($95\\%$ CI $0.48$ to $0.86$), demonstrating a $36\\%$ reduction in\nrecurrence, with non-significant heterogeneity ($I^2=0.00\\%$, $\\tau^2=0.00$).\nThe findings were robust and stable, as they were backed by sensitivity\nanalyses. Conclusion: This study demonstrates an application of an\ninformation-retrieval-driven workflow for medical evidence synthesis. The\napproach yields valuable clinical results and a generalisable framework to\nscale up the evidence synthesis, bridging the gap between clinical research and\ncomputer science.\n","authors":["Sandro Tsang"],"pdf_url":"https://arxiv.org/pdf/2509.16599v3.pdf","comment":"15 pages, 12 figures and 4 tables. This work describes an information\n  retrieval-driven workflow for medical evidence synthesis, with an application\n  to endometriosis recurrence. The method can be generalized to other\n  systematic reviews. The preregistered protocol is available:\n  https://doi.org/10.17605/OSF.IO/R2DFA"},{"id":"http://arxiv.org/abs/2510.23458v1","updated":"2025-10-27T15:58:51Z","published":"2025-10-27T15:58:51Z","title":"BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents","summary":"  Confidence in LLMs is a useful indicator of model uncertainty and answer\nreliability. Existing work mainly focused on single-turn scenarios, while\nresearch on confidence in complex multi-turn interactions is limited. In this\npaper, we investigate whether LLM-based search agents have the ability to\ncommunicate their own confidence through verbalized confidence scores after\nlong sequences of actions, a significantly more challenging task compared to\noutputting confidence in a single interaction. Experimenting on open-source\nagentic models, we first find that models exhibit much higher task accuracy at\nhigh confidence while having near-zero accuracy when confidence is low. Based\non this observation, we propose Test-Time Scaling (TTS) methods that use\nconfidence scores to determine answer quality, encourage the model to try again\nuntil reaching a satisfactory confidence level. Results show that our proposed\nmethods significantly reduce token consumption while demonstrating competitive\nperformance compared to baseline fixed budget TTS methods.\n","authors":["Litu Ou","Kuan Li","Huifeng Yin","Liwen Zhang","Zhongwang Zhang","Xixi Wu","Rui Ye","Zile Qiao","Yong Jiang","Pengjun Xie","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.23458v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2510.23451v1","updated":"2025-10-27T15:53:20Z","published":"2025-10-27T15:53:20Z","title":"Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with\n  Free-Form Preferences","summary":"  Reward models (RMs) play a critical role in aligning AI behaviors with human\npreferences, yet they face two fundamental challenges: (1) Modality Imbalance,\nwhere most RMs are mainly focused on text and image modalities, offering\nlimited support for video, audio, and other modalities; and (2) Preference\nRigidity, where training on fixed binary preference pairs fails to capture the\ncomplexity and diversity of personalized preferences. To address the above\nchallenges, we propose Omni-Reward, a step toward generalist omni-modal reward\nmodeling with support for free-form preferences, consisting of: (1) Evaluation:\nWe introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form\npreferences, covering nine tasks across five modalities including text, image,\nvideo, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal\npreference dataset comprising 248K general preference pairs and 69K\ninstruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We\npropose Omni-RewardModel, which includes both discriminative and generative\nRMs, and achieves strong performance on Omni-RewardBench as well as other\nwidely used reward modeling benchmarks.\n","authors":["Zhuoran Jin","Hongbang Yuan","Kejian Zhu","Jiachun Li","Pengfei Cao","Yubo Chen","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2510.23451v1.pdf","comment":"48 pages, 17 figures"},{"id":"http://arxiv.org/abs/2510.23443v1","updated":"2025-10-27T15:46:02Z","published":"2025-10-27T15:46:02Z","title":"A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge\n  Integration","summary":"  The growing intersection of cybersecurity and law creates a complex\ninformation space where traditional legal research tools struggle to deal with\nnuanced connections between cases, statutes, and technical vulnerabilities.\nThis knowledge divide hinders collaboration between legal experts and\ncybersecurity professionals. To address this important gap, this work provides\na first step towards intelligent systems capable of navigating the increasingly\nintricate cyber-legal domain. We demonstrate promising initial results on\nmultilingual tasks.\n","authors":["Chiara Bonfanti","Alessandro Druetto","Cataldo Basile","Tharindu Ranasinghe","Marcos Zampieri"],"pdf_url":"https://arxiv.org/pdf/2510.23443v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2406.11477v3","updated":"2025-10-27T15:19:39Z","published":"2024-06-17T12:42:34Z","title":"How Can We Effectively Expand the Vocabulary of LLMs with 0.01GB of\n  Target Language Text?","summary":"  Large language models (LLMs) have shown remarkable capabilities in many\nlanguages beyond English. Yet, LLMs require more inference steps when\ngenerating non-English text due to their reliance on English-centric tokenizers\nand vocabulary, resulting in higher usage costs to non-English speakers.\nVocabulary expansion with target language tokens is a widely used cross-lingual\nvocabulary adaptation approach to remedy this issue. Despite its effectiveness\nin inference speedup, previous work on vocabulary expansion has focused on\nhigh-resource settings assuming access to a substantial amount of target\nlanguage data to effectively initialize the embeddings of the new tokens and\nadapt the LLM to the target language. However, vocabulary expansion in\nlow-resource settings has yet to be explored. In this article, we investigate\nvocabulary expansion in low-resource settings by considering embedding\ninitialization methods and continual pre-training strategies. Through extensive\nexperiments across typologically diverse languages, tasks and models, we\nestablish a set of strategies to perform vocabulary expansion for faster\ninference, while striving to maintain competitive downstream performance to\nbaselines. This is achieved with only 30K sentences ($\\sim$0.01GB text data)\nfrom the target language.\n","authors":["Atsuki Yamaguchi","Aline Villavicencio","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2406.11477v3.pdf","comment":"Accepted to Computational Linguistics"},{"id":"http://arxiv.org/abs/2508.08343v2","updated":"2025-10-27T14:59:46Z","published":"2025-08-11T10:47:35Z","title":"A Data-driven ML Approach for Maximizing Performance in LLM-Adapter\n  Serving","summary":"  With the rapid adoption of Large Language Models (LLMs), LLM-adapters have\nbecome increasingly common, providing lightweight specialization of large-scale\nmodels. Serving hundreds or thousands of these adapters on a single GPU allows\nrequest aggregation, increasing throughput, but may also cause request\nstarvation if GPU memory limits are exceeded. To address this issue, this study\nfocuses on determining the joint configuration of concurrent and parallel\nadapters that maximizes GPU throughput without inducing starvation, given\nheterogeneous adapter and traffic properties. We propose a data-driven ML\napproach leveraging interpretable models to tackle this caching problem and\nintroduce the first Digital Twin capable of reproducing an LLM-adapter serving\nsystem, enabling efficient training data generation. Experiments with the vLLM\nframework and LoRA adapters show that the Digital Twin reproduces throughput\nwithin 5.1% of real results, while the ML approach predicts optimal numbers of\nconcurrent and parallel adapters with an error of at most 7.2% under\nheterogeneous, real-world workloads.\n","authors":["Ferran Agullo","Joan Oliveras","Chen Wang","Alberto Gutierrez-Torre","Olivier Tardieu","Alaa Youssef","Jordi Torres","Josep Ll. Berral"],"pdf_url":"https://arxiv.org/pdf/2508.08343v2.pdf","comment":"Accepted in a computer science workshop"},{"id":"http://arxiv.org/abs/2510.23396v1","updated":"2025-10-27T14:55:30Z","published":"2025-10-27T14:55:30Z","title":"EMTSF:Extraordinary Mixture of SOTA Models for Time Series Forecasting","summary":"  The immense success of the Transformer architecture\n  in Natural Language Processing has led to its adoption in Time Se ries\nForecasting (TSF), where superior performance has been shown.\n  However, a recent important paper questioned their effectiveness by\n  demonstrating that a simple single layer linear model outperforms\n  Transformer-based models. This was soon shown to be not as valid,\n  by a better transformer-based model termed PatchTST. More re cently, TimeLLM\ndemonstrated even better results by repurposing a\n  Large Language Model (LLM) for the TSF domain. Again, a follow\n  up paper challenged this by demonstrating that removing the LLM\n  component or replacing it with a basic attention layer in fact yields\n  better performance. One of the challenges in forecasting is the fact\n  that TSF data favors the more recent past, and is sometimes subject\n  to unpredictable events. Based upon these recent insights in TSF, we\n  propose a strong Mixture of Experts (MoE) framework. Our method\n  combines the state-of-the-art (SOTA) models including xLSTM, en hanced\nLinear, PatchTST, and minGRU, among others. This set of\n  complimentary and diverse models for TSF are integrated in a Trans former\nbased MoE gating network. Our proposed model outperforms\n  all existing TSF models on standard benchmarks, surpassing even the\n  latest approaches based on MoE frameworks.\n","authors":["Musleh Alharthi","Kaleel Mahmood","Sarosh Patel","Ausif Mahmood"],"pdf_url":"https://arxiv.org/pdf/2510.23396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23395v1","updated":"2025-10-27T14:54:51Z","published":"2025-10-27T14:54:51Z","title":"Detecting Religious Language in Climate Discourse","summary":"  Religious language continues to permeate contemporary discourse, even in\nostensibly secular domains such as environmental activism and climate change\ndebates. This paper investigates how explicit and implicit forms of religious\nlanguage appear in climate-related texts produced by secular and religious\nnongovernmental organizations (NGOs). We introduce a dual methodological\napproach: a rule-based model using a hierarchical tree of religious terms\nderived from ecotheology literature, and large language models (LLMs) operating\nin a zero-shot setting. Using a dataset of more than 880,000 sentences, we\ncompare how these methods detect religious language and analyze points of\nagreement and divergence. The results show that the rule-based method\nconsistently labels more sentences as religious than LLMs. These findings\nhighlight not only the methodological challenges of computationally detecting\nreligious language but also the broader tension over whether religious language\nshould be defined by vocabulary alone or by contextual meaning. This study\ncontributes to digital methods in religious studies by demonstrating both the\npotential and the limitations of approaches for analyzing how the sacred\npersists in climate discourse.\n","authors":["Evy Beijen","Pien Pieterse","Yusuf Ãelik","Willem Th. van Peursen","Sandjai Bhulai","Meike Morren"],"pdf_url":"https://arxiv.org/pdf/2510.23395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.20487v2","updated":"2025-10-27T14:43:39Z","published":"2025-10-23T12:29:16Z","title":"Steering Evaluation-Aware Language Models to Act Like They Are Deployed","summary":"  Large language models (LLMs) can sometimes detect when they are being\nevaluated and adjust their behavior to appear more aligned, compromising the\nreliability of safety evaluations. In this paper, we show that adding a\nsteering vector to an LLM's activations can suppress evaluation-awareness and\nmake the model act like it is deployed during evaluation. To study our steering\ntechnique, we train an LLM to exhibit evaluation-aware behavior using a\ntwo-step training process designed to mimic how this behavior could emerge\nnaturally. First, we perform continued pretraining on documents with factual\ndescriptions of the model (1) using Python type hints during evaluation but not\nduring deployment and (2) recognizing that the presence of a certain evaluation\ncue always means that it is being tested. Then, we train the model with expert\niteration to use Python type hints in evaluation settings. The resulting model\nis evaluation-aware: it writes type hints in evaluation contexts more than\ndeployment contexts. We find that activation steering can suppress evaluation\nawareness and make the model act like it is deployed even when the cue is\npresent. Importantly, we constructed our steering vector using the original\nmodel before our additional training. Our results suggest that AI evaluators\ncould improve the reliability of safety evaluations by steering models to act\nlike they are deployed.\n","authors":["Tim Tian Hua","Andrew Qin","Samuel Marks","Neel Nanda"],"pdf_url":"https://arxiv.org/pdf/2510.20487v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23799v3","updated":"2025-10-27T14:42:01Z","published":"2025-05-26T16:53:47Z","title":"Estimating LLM Consistency: A User Baseline vs Surrogate Metrics","summary":"  Large language models (LLMs) are prone to hallucinations and sensitiveto\nprompt perturbations, often resulting in inconsistent or unreliablegenerated\ntext. Different methods have been proposed to mitigate suchhallucinations and\nfragility, one of which is to measure theconsistency of LLM responses -- the\nmodel's confidence in the responseor likelihood of generating a similar\nresponse when resampled. Inprevious work, measuring LLM response consistency\noften relied oncalculating the probability of a response appearing within a\npool of resampledresponses, analyzing internal states, or evaluating logits of\nresopnses.However, it was not clear how well theseapproaches approximated\nusers' perceptions of consistency of LLMresponses. To find out, we performed a\nuser study ($n=2,976$)demonstrating that current methods for measuring LLM\nresponseconsistency typically do not align well with humans' perceptions of\nLLMconsistency. We propose a logit-based ensemble method for estimatingLLM\nconsistency and show that our method matches the performance of\nthebest-performing existing metric in estimating human ratings of\nLLMconsistency. Our results suggest that methods for estimating LLMconsistency\nwithout human evaluation are sufficiently imperfect towarrant broader use of\nevaluation with human input; this would avoidmisjudging the adequacy of models\nbecause of the imperfections ofautomated consistency metrics.\n","authors":["Xiaoyuan Wu","Weiran Lin","Omer Akgul","Lujo Bauer"],"pdf_url":"https://arxiv.org/pdf/2505.23799v3.pdf","comment":"Published as a main conference paper at EMNLP 2025"},{"id":"http://arxiv.org/abs/2409.06185v2","updated":"2025-10-27T14:39:52Z","published":"2024-09-10T03:26:42Z","title":"Can Large Language Models Unlock Novel Scientific Research Ideas?","summary":"  The widespread adoption of Large Language Models (LLMs) and publicly\navailable ChatGPT have marked a significant turning point in the integration of\nArtificial Intelligence (AI) into people's everyday lives. This study examines\nthe ability of Large Language Models (LLMs) to generate future research ideas\nfrom scientific papers. Unlike tasks such as summarization or translation, idea\ngeneration lacks a clearly defined reference set or structure, making manual\nevaluation the default standard. However, human evaluation in this setting is\nextremely challenging ie: it requires substantial domain expertise, contextual\nunderstanding of the paper, and awareness of the current research landscape.\nThis makes it time-consuming, costly, and fundamentally non-scalable,\nparticularly as new LLMs are being released at a rapid pace. Currently, there\nis no automated evaluation metric specifically designed for this task. To\naddress this gap, we propose two automated evaluation metrics: Idea Alignment\nScore (IAScore) and Idea Distinctness Index. We further conducted human\nevaluation to assess the novelty, relevance, and feasibility of the generated\nfuture research ideas. This investigation offers insights into the evolving\nrole of LLMs in idea generation, highlighting both its capability and\nlimitations. Our work contributes to the ongoing efforts in evaluating and\nutilizing language models for generating future research ideas. We make our\ndatasets and codes publicly available\n","authors":["Sandeep Kumar","Tirthankar Ghosal","Vinayak Goyal","Asif Ekbal"],"pdf_url":"https://arxiv.org/pdf/2409.06185v2.pdf","comment":"EMNLP 2025 (Main)"},{"id":"http://arxiv.org/abs/2508.17234v2","updated":"2025-10-27T14:25:55Z","published":"2025-08-24T07:19:25Z","title":"ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation","summary":"  Legal claims refer to the plaintiff's demands in a case and are essential to\nguiding judicial reasoning and case resolution. While many works have focused\non improving the efficiency of legal professionals, the research on helping\nnon-professionals (e.g., plaintiffs) remains unexplored. This paper explores\nthe problem of legal claim generation based on the given case's facts. First,\nwe construct ClaimGen-CN, the first dataset for Chinese legal claim generation\ntask, from various real-world legal disputes. Additionally, we design an\nevaluation metric tailored for assessing the generated claims, which\nencompasses two essential dimensions: factuality and clarity. Building on this,\nwe conduct a comprehensive zero-shot evaluation of state-of-the-art general and\nlegal-domain large language models. Our findings highlight the limitations of\nthe current models in factual precision and expressive clarity, pointing to the\nneed for more targeted development in this domain. To encourage further\nexploration of this important task, we will make the dataset publicly\navailable.\n","authors":["Siying Zhou","Yiquan Wu","Hui Chen","Xavier Hu","Kun Kuang","Adam Jatowt","Ming Hu","Chunyan Zheng","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2508.17234v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.10328v2","updated":"2025-10-27T14:25:32Z","published":"2025-10-11T20:04:57Z","title":"Are LLMs Empathetic to All? Investigating the Influence of\n  Multi-Demographic Personas on a Model's Empathy","summary":"  Large Language Models' (LLMs) ability to converse naturally is empowered by\ntheir ability to empathetically understand and respond to their users. However,\nemotional experiences are shaped by demographic and cultural contexts. This\nraises an important question: Can LLMs demonstrate equitable empathy across\ndiverse user groups? We propose a framework to investigate how LLMs' cognitive\nand affective empathy vary across user personas defined by intersecting\ndemographic attributes. Our study introduces a novel intersectional analysis\nspanning 315 unique personas, constructed from combinations of age, culture,\nand gender, across four LLMs. Results show that attributes profoundly shape a\nmodel's empathetic responses. Interestingly, we see that adding multiple\nattributes at once can attenuate and reverse expected empathy patterns. We show\nthat they broadly reflect real-world empathetic trends, with notable\nmisalignments for certain groups, such as those from Confucian culture. We\ncomplement our quantitative findings with qualitative insights to uncover model\nbehaviour patterns across different demographic groups. Our findings highlight\nthe importance of designing empathy-aware LLMs that account for demographic\ndiversity to promote more inclusive and equitable model behaviour.\n","authors":["Ananya Malik","Nazanin Sabri","Melissa Karnaze","Mai Elsherief"],"pdf_url":"https://arxiv.org/pdf/2510.10328v2.pdf","comment":"9 pages, 4 figures, 4 tables, EMNLP 2025 Findings"},{"id":"http://arxiv.org/abs/2406.05039v2","updated":"2025-10-27T14:22:30Z","published":"2024-06-07T16:02:10Z","title":"Bootstrapping Referring Multi-Object Tracking","summary":"  Referring understanding is a fundamental task that bridges natural language\nand visual content by localizing objects described in free-form expressions.\nHowever, existing works are constrained by limited language expressiveness,\nlacking the capacity to model object dynamics in spatial numbers and temporal\nstates. To address these limitations, we introduce a new and general referring\nunderstanding task, termed referring multi-object tracking (RMOT). Its core\nidea is to employ a language expression as a semantic cue to guide the\nprediction of multi-object tracking, comprehensively accounting for variations\nin object quantity and temporal semantics. Along with RMOT, we introduce a RMOT\nbenchmark named Refer-KITTI-V2, featuring scalable and diverse language\nexpressions. To efficiently generate high-quality annotations covering object\ndynamics with minimal manual effort, we propose a semi-automatic labeling\npipeline that formulates a total of 9,758 language prompts. In addition, we\npropose TempRMOT, an elegant end-to-end Transformer-based framework for RMOT.\nAt its core is a query-driven Temporal Enhancement Module that represents each\nobject as a Transformer query, enabling long-term spatial-temporal interactions\nwith other objects and past frames to efficiently refine these queries.\nTempRMOT achieves state-of-the-art performance on both Refer-KITTI and\nRefer-KITTI-V2, demonstrating the effectiveness of our approach. The source\ncode and dataset is available at https://github.com/zyn213/TempRMOT.\n","authors":["Yani Zhang","Dongming Wu","Wencheng Han","Xingping Dong"],"pdf_url":"https://arxiv.org/pdf/2406.05039v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.05109v2","updated":"2025-10-27T14:17:43Z","published":"2025-09-25T22:28:44Z","title":"Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient\n  Multimodal Inference on Battery-Powered Small Devices","summary":"  Large Multimodal Models (LMMs) are inherently modular, consisting of vision\nand audio encoders, projectors, and large language models. Yet, they are almost\nalways executed monolithically, which underutilizes the heterogeneous\naccelerators (NPUs, GPUs, DSPs) in modern SoCs and leads to high end-to-end\nlatency. In this paper, we present NANOMIND, a hardware--software co-design\ninference framework for Large Multimodal Models (LMMs) that breaks large models\ninto modular ``bricks'' (vision, language, audio, etc.) and maps each to its\nideal accelerator. The key insight is that large models can be broken into\nmodular components and scheduled to run on the most appropriate compute units.\nIt performs module-level dynamic offloading across accelerators on\nunified-memory SoCs. By combining customized hardware design, system-level\nscheduling, and optimized low-bit computation kernels, we demonstrate our\nframework with a compact, battery-powered device capable of running LMMs\nentirely on device. This prototype functions as a self-contained intelligent\nassistant that requires no network connectivity, while achieving higher\nthroughput and superior power efficiency under strict resource constraints. The\ndesign further bypasses CPU bottlenecks and reduces redundant memory usage\nthrough token-aware buffer management and module-level coordination. Our system\noutperforms existing implementations in resource efficiency, cutting energy\nconsumption by 42.3\\% and GPU memory usage by 11.2\\%. This enables a\nbattery-powered device to run LLaVA-OneVision with a camera for nearly half a\nday and LLaMA-3-8B for voice interactions up to almost 20.8 hours.\n","authors":["Yilong Li","Shuai Zhang","Yijing Zeng","Hao Zhang","Xinmiao Xiong","Jingyu Liu","Pan Hu","Suman Banerjee"],"pdf_url":"https://arxiv.org/pdf/2510.05109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.17516v3","updated":"2025-10-27T14:17:13Z","published":"2025-10-20T13:14:38Z","title":"SimBench: Benchmarking the Ability of Large Language Models to Simulate\n  Human Behaviors","summary":"  Large language model (LLM) simulations of human behavior have the potential\nto revolutionize the social and behavioral sciences, if and only if they\nfaithfully reflect real human behaviors. Current evaluations are fragmented,\nbased on bespoke tasks and metrics, creating a patchwork of incomparable\nresults. To address this, we introduce SimBench, the first large-scale,\nstandardized benchmark for a robust, reproducible science of LLM simulation. By\nunifying 20 diverse datasets covering tasks from moral decision-making to\neconomic choice across a large global participant pool, SimBench provides the\nnecessary foundation to ask fundamental questions about when, how, and why LLM\nsimulations succeed or fail. We show that, while even the best LLMs today have\nlimited simulation ability (score: 40.80/100), performance scales log-linearly\nwith model size. Simulation performance is not improved by increased\ninference-time compute. We demonstrate an alignment-simulation trade-off:\ninstruction-tuning improves performance on low-entropy (consensus) questions\nbut degrades it on high-entropy (diverse) ones. Models particularly struggle\nwhen simulating specific demographic groups. Finally, we demonstrate that\nsimulation ability correlates most strongly with deep, knowledge-intensive\nreasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to\naccelerate the development of more faithful LLM simulators.\n","authors":["Tiancheng Hu","Joachim Baumann","Lorenzo Lupo","Nigel Collier","Dirk Hovy","Paul RÃ¶ttger"],"pdf_url":"https://arxiv.org/pdf/2510.17516v3.pdf","comment":"Project Website: http://simbench.tiancheng.hu/ Data:\n  https://huggingface.co/datasets/pitehu/SimBench"},{"id":"http://arxiv.org/abs/2410.07076v6","updated":"2025-10-27T14:10:54Z","published":"2024-10-09T17:19:58Z","title":"MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry\n  Scientific Hypotheses","summary":"  Scientific discovery plays a pivotal role in advancing human society, and\nrecent progress in large language models (LLMs) suggests their potential to\naccelerate this process. However, it remains unclear whether LLMs can\nautonomously generate novel and valid hypotheses in chemistry. In this work, we\ninvestigate whether LLMs can discover high-quality chemistry hypotheses given\nonly a research background-comprising a question and/or a survey-without\nrestriction on the domain of the question. We begin with the observation that\nhypothesis discovery is a seemingly intractable task. To address this, we\npropose a formal mathematical decomposition grounded in a fundamental\nassumption: that most chemistry hypotheses can be composed from a research\nbackground and a set of inspirations. This decomposition leads to three\npractical subtasks-retrieving inspirations, composing hypotheses with\ninspirations, and ranking hypotheses - which together constitute a sufficient\nset of subtasks for the overall scientific discovery task. We further develop\nan agentic LLM framework, MOOSE-Chem, that is a direct implementation of this\nmathematical decomposition. To evaluate this framework, we construct a\nbenchmark of 51 high-impact chemistry papers published and online after January\n2024, each manually annotated by PhD chemists with background, inspirations,\nand hypothesis. The framework is able to rediscover many hypotheses with high\nsimilarity to the groundtruth, successfully capturing the core\ninnovations-while ensuring no data contamination since it uses an LLM with\nknowledge cutoff date prior to 2024. Finally, based on LLM's surprisingly high\naccuracy on inspiration retrieval, a task with inherently out-of-distribution\nnature, we propose a bold assumption: that LLMs may already encode latent\nscientific knowledge associations not yet recognized by humans.\n","authors":["Zonglin Yang","Wanhao Liu","Ben Gao","Tong Xie","Yuqiang Li","Wanli Ouyang","Soujanya Poria","Erik Cambria","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.07076v6.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2510.23358v1","updated":"2025-10-27T14:08:27Z","published":"2025-10-27T14:08:27Z","title":"How AI Forecasts AI Jobs: Benchmarking LLM Predictions of Labor Market\n  Changes","summary":"  Artificial intelligence is reshaping labor markets, yet we lack tools to\nsystematically forecast its effects on employment. This paper introduces a\nbenchmark for evaluating how well large language models (LLMs) can anticipate\nchanges in job demand, especially in occupations affected by AI. Existing\nresearch has shown that LLMs can extract sentiment, summarize economic reports,\nand emulate forecaster behavior, but little work has assessed their use for\nforward-looking labor prediction. Our benchmark combines two complementary\ndatasets: a high-frequency index of sector-level job postings in the United\nStates, and a global dataset of projected occupational changes due to AI\nadoption. We format these data into forecasting tasks with clear temporal\nsplits, minimizing the risk of information leakage. We then evaluate LLMs using\nmultiple prompting strategies, comparing task-scaffolded, persona-driven, and\nhybrid approaches across model families. We assess both quantitative accuracy\nand qualitative consistency over time. Results show that structured task\nprompts consistently improve forecast stability, while persona prompts offer\nadvantages on short-term trends. However, performance varies significantly\nacross sectors and horizons, highlighting the need for domain-aware prompting\nand rigorous evaluation protocols. By releasing our benchmark, we aim to\nsupport future research on labor forecasting, prompt design, and LLM-based\neconomic reasoning. This work contributes to a growing body of research on how\nLLMs interact with real-world economic data, and provides a reproducible\ntestbed for studying the limits and opportunities of AI as a forecasting tool\nin the context of labor markets.\n","authors":["Sheri Osborn","Rohit Valecha","H. Raghav Rao","Dan Sass","Anthony Rios"],"pdf_url":"https://arxiv.org/pdf/2510.23358v1.pdf","comment":"8 pages + Limitations + References"},{"id":"http://arxiv.org/abs/2505.19660v3","updated":"2025-10-27T14:08:24Z","published":"2025-05-26T08:18:33Z","title":"Prompting is not Enough: Exploring Knowledge Integration and\n  Controllable Generation","summary":"  Open-domain question answering (OpenQA) represents a cornerstone in natural\nlanguage processing (NLP), primarily focused on extracting answers from\nunstructured textual data. With the rapid advancements in Large Language Models\n(LLMs), LLM-based OpenQA methods have reaped the benefits of emergent\nunderstanding and answering capabilities enabled by massive parameters compared\nto traditional methods. However, most of these methods encounter two critical\nchallenges: how to integrate knowledge into LLMs effectively and how to\nadaptively generate results with specific answer formats for various task\nsituations. To address these challenges, we propose a novel framework named\nGenKI, which aims to improve the OpenQA performance by exploring Knowledge\nIntegration and controllable Generation on LLMs simultaneously. Specifically,\nwe first train a dense passage retrieval model to retrieve associated knowledge\nfrom a given knowledge base. Subsequently, we introduce a novel knowledge\nintegration model that incorporates the retrieval knowledge into instructions\nduring fine-tuning to intensify the model. Furthermore, to enable controllable\ngeneration in LLMs, we leverage a certain fine-tuned LLM and an ensemble based\non text consistency incorporating all coherence, fluency, and answer format\nassurance. Finally, extensive experiments conducted on the TriviaQA, MSMARCO,\nand CMRC2018 datasets, featuring diverse answer formats, have demonstrated the\neffectiveness of GenKI with comparison of state-of-the-art baselines. Moreover,\nablation studies have disclosed a linear relationship between the frequency of\nretrieved knowledge and the model's ability to recall knowledge accurately\nagainst the ground truth. Our code of GenKI is available at\nhttps://github.com/USTC-StarTeam/GenKI\n","authors":["Tingjia Shen","Hao Wang","Chuan Qin","Ruijun Sun","Yang Song","Defu Lian","Hengshu Zhu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2505.19660v3.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.20445v4","updated":"2025-10-27T13:58:21Z","published":"2024-10-27T13:51:09Z","title":"TrajAgent: An LLM-Agent Framework for Trajectory Modeling via\n  Large-and-Small Model Collaboration","summary":"  Trajectory modeling, which includes research on trajectory data pattern\nmining and future prediction, has widespread applications in areas such as life\nservices, urban transportation, and public administration. Numerous methods\nhave been proposed to address specific problems within trajectory modeling.\nHowever, the heterogeneity of data and the diversity of trajectory tasks make\neffective and reliable trajectory modeling an important yet highly challenging\nendeavor, even for domain experts. \\fix In this paper, we propose\n\\textit{TrajAgent}, a agent framework powered by large language models (LLMs),\ndesigned to facilitate robust and efficient trajectory modeling through\nautomation modeling. This framework leverages and optimizes diverse specialized\nmodels to address various trajectory modeling tasks across different datasets\neffectively. \\unfix~In \\textit{TrajAgent}, we first develop \\textit{UniEnv}, an\nexecution environment with a unified data and model interface, to support the\nexecution and training of various models. Building on \\textit{UniEnv}, we\nintroduce an agentic workflow designed for automatic trajectory modeling across\nvarious trajectory tasks and data. Furthermore, we introduce collaborative\nlearning schema between LLM-based agents and small speciallized models, to\nenhance the performance of the whole framework effectively. Extensive\nexperiments on four tasks using four real-world datasets demonstrate the\neffectiveness of \\textit{TrajAgent} in automated trajectory modeling, achieving\na performance improvement of \\fix 2.38\\%-69.91\\% \\unfix over baseline methods.\nThe codes and data can be accessed via\nhttps://github.com/tsinghua-fib-lab/TrajAgent.\n","authors":["Yuwei Du","Jie Feng","Jie Zhao","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2410.20445v4.pdf","comment":"Accepted by NeurIPS 2025,\n  https://github.com/tsinghua-fib-lab/TrajAgent"},{"id":"http://arxiv.org/abs/2510.23341v1","updated":"2025-10-27T13:55:13Z","published":"2025-10-27T13:55:13Z","title":"LightKGG: Simple and Efficient Knowledge Graph Generation from Textual\n  Data","summary":"  The scarcity of high-quality knowledge graphs (KGs) remains a critical\nbottleneck for downstream AI applications, as existing extraction methods rely\nheavily on error-prone pattern-matching techniques or resource-intensive large\nlanguage models (LLMs). While recent tools leverage LLMs to generate KGs, their\ncomputational demands limit accessibility for low-resource environments. Our\npaper introduces LightKGG, a novel framework that enables efficient KG\nextraction from textual data using small-scale language models (SLMs) through\ntwo key technical innovations: (1) Context-integrated Graph extraction\nintegrates contextual information with nodes and edges into a unified graph\nstructure, reducing the reliance on complex semantic processing while\nmaintaining more key information; (2) Topology-enhanced relationship inference\nleverages the inherent topology of the extracted graph to efficiently infer\nrelationships, enabling relationship discovery without relying on complex\nlanguage understanding capabilities of LLMs. By enabling accurate KG\nconstruction with minimal hardware requirements, this work bridges the gap\nbetween automated knowledge extraction and practical deployment scenarios while\nintroducing scientifically rigorous methods for optimizing SLM efficiency in\nstructured NLP tasks.\n","authors":["Teng Lin"],"pdf_url":"https://arxiv.org/pdf/2510.23341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23340v1","updated":"2025-10-27T13:54:54Z","published":"2025-10-27T13:54:54Z","title":"Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by\n  Projecting User Awareness across Future Timesteps","summary":"  Adaptive agent design offers a way to improve human-AI collaboration on\ntime-sensitive tasks in rapidly changing environments. In such cases, to ensure\nthe human maintains an accurate understanding of critical task elements, an\nassistive agent must not only identify the highest priority information but\nalso estimate how and when this information can be communicated most\neffectively, given that human attention represents a zero-sum cognitive\nresource where focus on one message diminishes awareness of other or upcoming\ninformation. We introduce a theoretical framework for adaptive signalling which\nmeets these challenges by using principles of rational communication,\nformalised as Bayesian reference resolution using the Rational Speech Act (RSA)\nmodelling framework, to plan a sequence of messages which optimise timely\nalignment between user belief and a dynamic environment. The agent adapts\nmessage specificity and timing to the particulars of a user and scenario based\non projections of how prior-guided interpretation of messages will influence\nattention to the interface and subsequent belief update, across several\ntimesteps out to a fixed horizon. In a comparison to baseline methods, we show\nthat this effectiveness depends crucially on combining multi-step planning with\na realistic model of user awareness. As the first application of RSA for\ncommunication in a dynamic environment, and for human-AI interaction in\ngeneral, we establish theoretical foundations for pragmatic communication in\nhuman-agent teams, highlighting how insights from cognitive science can be\ncapitalised to inform the design of assistive agents.\n","authors":["Anwesha Das","John Duff","JÃ¶rg Hoffmann","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2510.23340v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2510.20075v3","updated":"2025-10-27T13:54:40Z","published":"2025-10-22T23:16:50Z","title":"LLMs can hide text in other text of the same length","summary":"  A meaningful text can be hidden inside another, completely different yet\nstill coherent and plausible, text of the same length. For example, a tweet\ncontaining a harsh political critique could be embedded in a tweet that\ncelebrates the same political leader, or an ordinary product review could\nconceal a secret manuscript. This uncanny state of affairs is now possible\nthanks to Large Language Models, and in this paper we present a simple and\nefficient protocol to achieve it. We show that even modest 8-billion-parameter\nopen-source LLMs are sufficient to obtain high-quality results, and a message\nas long as this abstract can be encoded and decoded locally on a laptop in\nseconds. The existence of such a protocol demonstrates a radical decoupling of\ntext from authorial intent, further eroding trust in written communication,\nalready shaken by the rise of LLM chatbots. We illustrate this with a concrete\nscenario: a company could covertly deploy an unfiltered LLM by encoding its\nanswers within the compliant responses of a safe model. This possibility raises\nurgent questions for AI safety and challenges our understanding of what it\nmeans for a Large Language Model to know something.\n","authors":["Antonio Norelli","Michael Bronstein"],"pdf_url":"https://arxiv.org/pdf/2510.20075v3.pdf","comment":"21 pages, main paper 9 pages"},{"id":"http://arxiv.org/abs/2510.23337v1","updated":"2025-10-27T13:51:13Z","published":"2025-10-27T13:51:13Z","title":"BaZi-Based Character Simulation Benchmark: Evaluating AI on Temporal and\n  Persona Reasoning","summary":"  Human-like virtual characters are crucial for games, storytelling, and\nvirtual reality, yet current methods rely heavily on annotated data or\nhandcrafted persona prompts, making it difficult to scale up and generate\nrealistic, contextually coherent personas. We create the first QA dataset for\nBaZi-based persona reasoning, where real human experiences categorized into\nwealth, health, kinship, career, and relationships are represented as\nlife-event questions and answers. Furthermore, we propose the first BaZi-LLM\nsystem that integrates symbolic reasoning with large language models to\ngenerate temporally dynamic and fine-grained virtual personas. Compared with\nmainstream LLMs such as DeepSeek-v3 and GPT-5-mini, our method achieves a\n30.3%-62.6% accuracy improvement. In addition, when incorrect BaZi information\nis used, our model's accuracy drops by 20%-45%, showing the potential of\nculturally grounded symbolic-LLM integration for realistic character\nsimulation.\n","authors":["Siyuan Zheng","Pai Liu","Xi Chen","Jizheng Dong","Sihan Jia"],"pdf_url":"https://arxiv.org/pdf/2510.23337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23334v1","updated":"2025-10-27T13:48:59Z","published":"2025-10-27T13:48:59Z","title":"Adaptive Blockwise Search: Inference-Time Alignment for Large Language\n  Models","summary":"  LLM alignment remains a critical challenge. Inference-time methods provide a\nflexible alternative to fine-tuning, but their uniform computational effort\noften yields suboptimal alignment. We hypothesize that for many alignment\ntasks, the initial tokens of a response are disproportionately more critical.\nTo leverage this principle, we introduce AdaSearch, a novel blockwise search\nstrategy. It adaptively allocates a fixed computational budget using a sampling\nschedule, focusing search effort on these critical tokens. We apply AdaSearch\nto sequential decoding and introduce its tree-search counterpart, AdaBeam. Our\ncomprehensive evaluation across eight LLMs demonstrates that AdaSearch\noutperforms strong Best-of-N and fine-tuning baselines. Specifically, win-rates\nimprove by over 10% for harmlessness generation, controlled sentiment\ngeneration, and for mathematical reasoning tasks relative to Best-of-N.\n","authors":["Mohammad Atif Quamar","Mohammad Areeb","Nishant Sharma","Ananth Shreekumar","Jonathan Rosenthal","Muslum Ozgur Ozmen","Mikhail Kuznetsov","Z. Berkay Celik"],"pdf_url":"https://arxiv.org/pdf/2510.23334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23320v1","updated":"2025-10-27T13:35:22Z","published":"2025-10-27T13:35:22Z","title":"LibriConvo: Simulating Conversations from Read Literature for ASR and\n  Diarization","summary":"  We introduce LibriConvo, a simulated multi-speaker conversational dataset\nbased on speaker-aware conversation simulation (SASC), designed to support\ntraining and evaluation of speaker diarization and automatic speech recognition\n(ASR) systems. Unlike prior resources that mostly rely on semantically\ndisconnected utterances and implausible temporal gaps, LibriConvo ensures\nsemantic coherence and realistic conversational timing. Our pipeline leverages\nCallHome with external VAD for reliable boundaries, applies compression to\nreduce unnaturally long silences, and organizes LibriTTS utterances by book to\nmaintain contextual consistency. Acoustic realism is enhanced via a novel room\nimpulse response selection procedure that ranks speaker-microphone\nconfigurations by spatial plausibility, balancing realism and diversity. The\ndataset comprises 240.1 hours across 1,496 dialogues with 830 unique speakers,\nsplit in a speaker-disjoint manner for robust evaluation. Baselines show that\nthe sortformer model outperforms the pyannote pipeline in diarization, while a\nfine-tuned Fast Conformer-CTC XLarge with Serialized Output Training achieves\n7.29\\% WER for ASR, surpassing zero-shot Whisper-large-v3. LibriConvo provides\na valuable resource for advancing multi-speaker speech processing research with\nrealistic conversational dynamics and controlled experimental conditions.\n","authors":["MÃ¡tÃ© Gedeon","PÃ©ter Mihajlik"],"pdf_url":"https://arxiv.org/pdf/2510.23320v1.pdf","comment":"Submitted to LREC 2026"},{"id":"http://arxiv.org/abs/2510.23319v1","updated":"2025-10-27T13:30:54Z","published":"2025-10-27T13:30:54Z","title":"Arabic Little STT: Arabic Children Speech Recognition Dataset","summary":"  The performance of Artificial Intelligence (AI) systems fundamentally depends\non high-quality training data. However, low-resource languages like Arabic\nsuffer from severe data scarcity. Moreover, the absence of child-specific\nspeech corpora is an essential gap that poses significant challenges. To\naddress this gap, we present our created dataset, Arabic Little STT, a dataset\nof Levantine Arabic child speech recorded in classrooms, containing 355\nutterances from 288 children (ages 6 - 13). We further conduct a systematic\nassessment of Whisper, a state-of-the-art automatic speech recognition (ASR)\nmodel, on this dataset and compare its performance with adult Arabic\nbenchmarks. Our evaluation across eight Whisper variants reveals that even the\nbest-performing model (Large_v3) struggles significantly, achieving a 0.66 word\nerror rate (WER) on child speech, starkly contrasting with its sub 0.20 WER on\nadult datasets. These results align with other research on English speech.\nResults highlight the critical need for dedicated child speech benchmarks and\ninclusive training data in ASR development. Emphasizing that such data must be\ngoverned by strict ethical and privacy frameworks to protect sensitive child\ninformation. We hope that this study provides an initial step for future work\non equitable speech technologies for Arabic-speaking children. We hope that our\npublicly available dataset enrich the children's demographic representation in\nASR datasets.\n","authors":["Mouhand Alkadri","Dania Desouki","Khloud Al Jallad"],"pdf_url":"https://arxiv.org/pdf/2510.23319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.19209v2","updated":"2025-10-27T13:16:36Z","published":"2025-05-25T16:13:46Z","title":"MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis\n  Discovery via Hierarchical Search","summary":"  Large language models (LLMs) have shown promise in automating scientific\nhypothesis generation, yet existing approaches primarily yield coarse-grained\nhypotheses lacking critical methodological and experimental details. We\nintroduce and formally define the new task of fine-grained scientific\nhypothesis discovery, which entails generating detailed, experimentally\nactionable hypotheses from coarse initial research directions. We frame this as\na combinatorial optimization problem and investigate the upper limits of LLMs'\ncapacity to solve it when maximally leveraged. Specifically, we explore four\nfoundational questions: (1) how to best harness an LLM's internal heuristics to\nformulate the fine-grained hypothesis it itself would judge as the most\npromising among all the possible hypotheses it might generate, based on its own\ninternal scoring-thus defining a latent reward landscape over the hypothesis\nspace; (2) whether such LLM-judged better hypotheses exhibit stronger alignment\nwith ground-truth hypotheses; (3) whether shaping the reward landscape using an\nensemble of diverse LLMs of similar capacity yields better outcomes than\ndefining it with repeated instances of the strongest LLM among them; and (4)\nwhether an ensemble of identical LLMs provides a more reliable reward landscape\nthan a single LLM. To address these questions, we propose a hierarchical search\nmethod that incrementally proposes and integrates details into the hypothesis,\nprogressing from general concepts to specific experimental configurations. We\nshow that this hierarchical process smooths the reward landscape and enables\nmore effective optimization. Empirical evaluations on a new benchmark of\nexpert-annotated fine-grained hypotheses from recent literature show that our\nmethod consistently outperforms strong baselines.\n","authors":["Zonglin Yang","Wanhao Liu","Ben Gao","Yujie Liu","Wei Li","Tong Xie","Lidong Bing","Wanli Ouyang","Erik Cambria","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.19209v2.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.15807v2","updated":"2025-10-27T13:12:32Z","published":"2025-05-21T17:59:01Z","title":"The Atlas of In-Context Learning: How Attention Heads Shape In-Context\n  Retrieval Augmentation","summary":"  Large language models are able to exploit in-context learning to access\nexternal knowledge beyond their training data through retrieval-augmentation.\nWhile promising, its inner workings remain unclear. In this work, we shed light\non the mechanism of in-context retrieval augmentation for question answering by\nviewing a prompt as a composition of informational components. We propose an\nattribution-based method to identify specialized attention heads, revealing\nin-context heads that comprehend instructions and retrieve relevant contextual\ninformation, and parametric heads that store entities' relational knowledge. To\nbetter understand their roles, we extract function vectors and modify their\nattention weights to show how they can influence the answer generation process.\nFinally, we leverage the gained insights to trace the sources of knowledge used\nduring inference, paving the way towards more safe and transparent language\nmodels.\n","authors":["Patrick Kahardipraja","Reduan Achtibat","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2505.15807v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2508.12365v2","updated":"2025-10-27T13:03:18Z","published":"2025-08-17T13:48:48Z","title":"TaoSR1: The Thinking Model for E-commerce Relevance Search","summary":"  Query-product relevance prediction is a core task in e-commerce search.\nBERT-based models excel at semantic matching but lack complex reasoning\ncapabilities. While Large Language Models (LLMs) are explored, most still use\ndiscriminative fine-tuning or distill to smaller models for deployment. We\npropose a framework to directly deploy LLMs for this task, addressing key\nchallenges: Chain-of-Thought (CoT) error accumulation, discriminative\nhallucination, and deployment feasibility. Our framework, TaoSR1, involves\nthree stages: (1) Supervised Fine-Tuning (SFT) with CoT to instill reasoning;\n(2) Offline sampling with a pass@N strategy and Direct Preference Optimization\n(DPO) to improve generation quality; and (3) Difficulty-based dynamic sampling\nwith Group Relative Policy Optimization (GRPO) to mitigate discriminative\nhallucination. Additionally, post-CoT processing and a cumulative\nprobability-based partitioning method enable efficient online deployment.\nTaoSR1 significantly outperforms baselines on offline datasets and achieves\nsubstantial gains in online side-by-side human evaluations, introducing a novel\nparadigm for applying CoT reasoning to relevance classification.\n","authors":["Chenhe Dong","Shaowei Yao","Pengkun Jiao","Jianhui Yang","Yiming Jin","Zerui Huang","Xiaojiang Zhou","Dan Ou","Haihong Tang","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2508.12365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23284v1","updated":"2025-10-27T12:53:39Z","published":"2025-10-27T12:53:39Z","title":"DCMM-SQL: Automated Data-Centric Pipeline and Multi-Model Collaboration\n  Training for Text-to-SQL Model","summary":"  Text-to-SQL tasks have gained attractive improvements since the release of\nChatGPT. Among them, agent-based frameworks have been widely used in this\nfield. However, the impact of data-centric strategies on text-to-SQL tasks has\nrarely been explored. In this paper, we systemically design a fully automated\ndata-centric pipeline for text-to-SQL tasks, including \\emph{adaptive data\nrepair}, which can automatically find and fix errors in the training dataset;\nand \\emph{error data augmentation}, where we specifically diffuse and enhance\nerroneous data predicted by the initially trained models. Meanwhile, we propose\na Multi-Model collaboration training schema, aiming to train multiple models\nwith different augmented data, enabling them to possess distinct capabilities\nand work together to complement each other, because it has been found that the\ncapability of a single fine-tuned model is very limited. Furthermore, we\nutilize an ensemble strategy to integrate the capabilities of multiple models\nto solve a multiple-choice question, aiming to further improve the accuracy of\ntext-to-SQL tasks. The experiment results and ablation study have demonstrated\nthe effectiveness of data-centric pipeline and Multi-Model(MM) interactive\niterative strategies, achieving first place in lightweight text-to-SQL models\n(within 70B).\n","authors":["Yuanzhen Xie","Liu Ye","Jiqun Chu","Mochi Gao","Hehuan Liu","Yunzhi Tan","Bo Hu","Zang Li"],"pdf_url":"https://arxiv.org/pdf/2510.23284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23276v1","updated":"2025-10-27T12:36:43Z","published":"2025-10-27T12:36:43Z","title":"A Cocktail-Party Benchmark: Multi-Modal dataset and Comparative\n  Evaluation Results","summary":"  We introduce the task of Multi-Modal Context-Aware Recognition (MCoRec) in\nthe ninth CHiME Challenge, which addresses the cocktail-party problem of\noverlapping conversations in a single-room setting using audio, visual, and\ncontextual cues. MCoRec captures natural multi-party conversations where the\nrecordings focus on unscripted, casual group chats, leading to extreme speech\noverlap of up to 100% and highly fragmented conversational turns. The task\nrequires systems to answer the question \"Who speaks when, what, and with whom?\"\nby jointly transcribing each speaker's speech and clustering them into their\nrespective conversations from audio-visual recordings. Audio-only baselines\nexceed 100% word error rate, whereas incorporating visual cues yields\nsubstantial 50% improvements, highlighting the importance of multi-modality. In\nthis manuscript, we present the motivation behind the task, outline the data\ncollection process, and report the baseline systems developed for the MCoRec.\n","authors":["Thai-Binh Nguyen","Katerina Zmolikova","Pingchuan Ma","Ngoc Quan Pham","Christian Fuegen","Alexander Waibel"],"pdf_url":"https://arxiv.org/pdf/2510.23276v1.pdf","comment":"Submitted to ICASSP 2026"},{"id":"http://arxiv.org/abs/2506.19143v4","updated":"2025-10-27T12:36:23Z","published":"2025-06-23T21:28:45Z","title":"Thought Anchors: Which LLM Reasoning Steps Matter?","summary":"  Current frontier large-language models rely on reasoning to achieve\nstate-of-the-art performance. Many existing interpretability are limited in\nthis area, as standard methods have been designed to study single forward\npasses of a model rather than the multi-token computational steps that unfold\nduring reasoning. We argue that analyzing reasoning traces at the sentence\nlevel is a promising approach to understanding reasoning processes. We\nintroduce a black-box method that measures each sentence's counterfactual\nimportance by repeatedly sampling replacement sentences from the model,\nfiltering for semantically different ones, and continuing the chain of thought\nfrom that point onwards to quantify the sentence's impact on the distribution\nof final answers. We discover that certain sentences can have an outsized\nimpact on the trajectory of the reasoning trace and final answer. We term these\nsentences \\textit{thought anchors}. These are generally planning or uncertainty\nmanagement sentences, and specialized attention heads consistently attend from\nsubsequent sentences to thought anchors. We further show that examining\nsentence-sentence causal links within a reasoning trace gives insight into a\nmodel's behavior. Such information can be used to predict a problem's\ndifficulty and the extent different question domains involve sequential or\ndiffuse reasoning. As a proof-of-concept, we demonstrate that our techniques\ntogether provide a practical toolkit for analyzing reasoning models by\nconducting a detailed case study of how the model solves a difficult math\nproblem, finding that our techniques yield a consistent picture of the\nreasoning trace's structure. We provide an open-source tool\n(thought-anchors.com) for visualizing the outputs of our methods on further\nproblems. The convergence across our methods shows the potential of\nsentence-level analysis for a deeper understanding of reasoning models.\n","authors":["Paul C. Bogdan","Uzay Macar","Neel Nanda","Arthur Conmy"],"pdf_url":"https://arxiv.org/pdf/2506.19143v4.pdf","comment":"Paul C. Bogdan and Uzay Macar contributed equally to this work, and\n  their listed order was determined by coinflip. Neel Nanda and Arthur Conmy\n  contributed equally to this work as senior authors, and their listed order\n  was determined by coinflip"},{"id":"http://arxiv.org/abs/2510.23272v1","updated":"2025-10-27T12:32:33Z","published":"2025-10-27T12:32:33Z","title":"Code Aesthetics with Agentic Reward Feedback","summary":"  Large Language Models (LLMs) have become valuable assistants for developers\nin code-related tasks. While LLMs excel at traditional programming tasks such\nas code generation and bug fixing, they struggle with visually-oriented coding\ntasks, often producing suboptimal aesthetics. In this paper, we introduce a new\npipeline to enhance the aesthetic quality of LLM-generated code. We first\nconstruct AesCode-358K, a large-scale instruction-tuning dataset focused on\ncode aesthetics. Next, we propose agentic reward feedback, a multi-agent system\nthat evaluates executability, static aesthetics, and interactive aesthetics.\nBuilding on this, we develop GRPO-AR, which integrates these signals into the\nGRPO algorithm for joint optimization of functionality and code aesthetics.\nFinally, we develop OpenDesign, a benchmark for assessing code aesthetics.\nExperimental results show that combining supervised fine-tuning on AesCode-358K\nwith reinforcement learning using agentic reward feedback significantly\nimproves performance on OpenDesign and also enhances results on existing\nbenchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o\nand GPT-4.1, and achieves performance comparable to large open-source models\nwith 480B-685B parameters, underscoring the effectiveness of our approach.\n","authors":["Bang Xiao","Lingjie Jiang","Shaohan Huang","Tengchao Lv","Yupan Huang","Xun Wu","Lei Cui","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2510.23272v1.pdf","comment":"30 pages, 7 figures"},{"id":"http://arxiv.org/abs/2510.23271v1","updated":"2025-10-27T12:29:27Z","published":"2025-10-27T12:29:27Z","title":"Mubeen AI: A Specialized Arabic Language Model for Heritage Preservation\n  and User Intent Understanding","summary":"  Mubeen is a proprietary Arabic language model developed by MASARAT SA,\noptimized for deep understanding of Arabic linguistics, Islamic studies, and\ncultural heritage. Trained on an extensive collection of authentic Arabic\nsources significantly expanded by digitizing historical manuscripts via a\nproprietary Arabic OCR engine, the model incorporates seminal scholarly works\nin linguistics, jurisprudence, hadith, and Quranic exegesis, alongside\nthousands of academic theses and peer-reviewed research papers. Conditioned\nthrough a deep linguistic engineering framework, Mubeen masters not just the\nmeaning but the eloquence of Arabic, enabling precise understanding across\nclassical texts, contemporary writing, and regional dialects with focus on\ncomprehending user intent and delivering accurate, contextually relevant\nresponses. Unlike other Arabic models relying on translated English data that\noften fail in intent detection or retrieval-augmented generation (RAG), Mubeen\nuses native Arabic sources to ensure cultural authenticity and accuracy. Its\ncore innovation is the Practical Closure Architecture, designed to solve the\n\"Utility Gap Crisis\" where factually correct answers fail to resolve users'\ncore needs, forcing them into frustrating cycles of re-prompting. By\nprioritizing clarity and decisive guidance, Mubeen transforms from an\ninformation repository into a decisive guide, aligning with Saudi Vision 2030.\nThe model's architecture combines deep heritage specialization with\nmulti-disciplinary expert modules, enabling robust performance across both\ncultural preservation and general knowledge domains.\n","authors":["Mohammed Aljafari","Ismail Alturki","Ahmed Mori","Yehya Kadumi"],"pdf_url":"https://arxiv.org/pdf/2510.23271v1.pdf","comment":"21 pages, 2 figures, 3 tables. Includes appendices on ethical\n  guidelines and training framework. Submitted September 04, 2025"},{"id":"http://arxiv.org/abs/2510.00546v2","updated":"2025-10-27T12:22:59Z","published":"2025-10-01T06:04:57Z","title":"ThinkBrake: Mitigating Overthinking in Tool Reasoning","summary":"  Small reasoning models (SRMs) often overthink during tool use: they reach a\ncorrect tool-argument configuration, then continue reasoning and overwrite it\nwith an incorrect final call. We diagnose overthinking via oracle rollouts that\ninject </think> at sentence boundaries. On the Berkeley Function Calling\nLeaderboard (BFCL), this oracle termination lifts average accuracy from 85.8\\%\nto 94.2\\% while reducing tokens by 80-94\\%, revealing substantial recoverable\nheadroom and potential redundant reasoning. While prior work on concise\nreasoning has largely targeted mathematics, tool reasoning remains\nunderexplored. We adapt various early-termination baselines to tool use and\nintroduce ThinkBrake, a training-free decoding heuristic. ThinkBrake monitors\nthe log-probability margin between </think> and the current top token at\nsentence boundaries and triggers termination when this margin becomes small.\nAcross BFCL's single turn, non-live and live splits, ThinkBrake preserves or\nimproves accuracy while reducing tokens up to 25\\%, outperforming various\nbaselines.\n","authors":["Minjae Oh","Sangjun Song","Seungkyu Lee","Sungmin Jo","Yohan Jo"],"pdf_url":"https://arxiv.org/pdf/2510.00546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23252v1","updated":"2025-10-27T12:14:52Z","published":"2025-10-27T12:14:52Z","title":"Are ASR foundation models generalized enough to capture features of\n  regional dialects for low-resource languages?","summary":"  Conventional research on speech recognition modeling relies on the canonical\nform for most low-resource languages while automatic speech recognition (ASR)\nfor regional dialects is treated as a fine-tuning task. To investigate the\neffects of dialectal variations on ASR we develop a 78-hour annotated Bengali\nSpeech-to-Text (STT) corpus named Ben-10. Investigation from linguistic and\ndata-driven perspectives shows that speech foundation models struggle heavily\nin regional dialect ASR, both in zero-shot and fine-tuned settings. We observe\nthat all deep learning methods struggle to model speech data under dialectal\nvariations but dialect specific model training alleviates the issue. Our\ndataset also serves as a out of-distribution (OOD) resource for ASR modeling\nunder constrained resources in ASR algorithms. The dataset and code developed\nfor this project are publicly available\n","authors":["Tawsif Tashwar Dipto","Azmol Hossain","Rubayet Sabbir Faruque","Md. Rezuwan Hassan","Kanij Fatema","Tanmoy Shome","Ruwad Naswan","Md. Foriduzzaman Zihad","Mohaymen Ul Anam","Nazia Tasnim","Hasan Mahmud","Md Kamrul Hasan","Md. Mehedi Hasan Shawon","Farig Sadeque","Tahsin Reasat"],"pdf_url":"https://arxiv.org/pdf/2510.23252v1.pdf","comment":"This manuscript contains 11 pages, 5 tables and 16 figures This was\n  accepted at International Joint Conference on Natural Language Processing &\n  Asia-Pacific Chapter of the Association for Computational Linguistics\n  (IJCNLP-AACL) 2025"},{"id":"http://arxiv.org/abs/2507.05177v3","updated":"2025-10-27T11:59:16Z","published":"2025-07-07T16:31:37Z","title":"OpenS2S: Advancing Fully Open-Source End-to-End Empathetic Large Speech\n  Language Model","summary":"  Empathetic interaction is a cornerstone of human-machine communication, due\nto the need for understanding speech enriched with paralinguistic cues and\ngenerating emotional and expressive responses. However, the most powerful\nempathetic LSLMs are increasingly closed off, leaving the crucial details about\nthe architecture, data and development opaque to researchers. Given the\ncritical need for transparent research into the LSLMs and empathetic behavior,\nwe present OpenS2S, a fully open-source, transparent and end-to-end LSLM\ndesigned to enable empathetic speech interactions. Based on our empathetic\nspeech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved\ndecoding architecture to achieve low-latency speech generation. To facilitate\nend-to-end training, OpenS2S incorporates an automated data construction\npipeline that synthesizes diverse, high-quality empathetic speech dialogues at\nlow cost. By leveraging large language models to generate empathetic content\nand controllable text-to-speech systems to introduce speaker and emotional\nvariation, we construct a scalable training corpus with rich paralinguistic\ndiversity and minimal human supervision. We release the fully open-source\nOpenS2S model, including the dataset, model weights, pre-training and\nfine-tuning codes, to empower the broader research community and accelerate\ninnovation in empathetic speech systems. The project webpage can be accessed at\nhttps://casia-lm.github.io/OpenS2S\n","authors":["Chen Wang","Tianyu Peng","Wen Yang","Yinan Bai","Guangfu Wang","Jun Lin","Lanpeng Jia","Lingxiang Wu","Jinqiao Wang","Chengqing Zong","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.05177v3.pdf","comment":"Technical Report, Update on OpenS2S_v1.5"},{"id":"http://arxiv.org/abs/2502.19158v2","updated":"2025-10-27T11:53:22Z","published":"2025-02-26T14:14:58Z","title":"When Personalization Meets Reality: A Multi-Faceted Analysis of\n  Personalized Preference Learning","summary":"  While Reinforcement Learning from Human Feedback (RLHF) is widely used to\nalign Large Language Models (LLMs) with human preferences, it typically assumes\nhomogeneous preferences across users, overlooking diverse human values and\nminority viewpoints. Although personalized preference learning addresses this\nby tailoring separate preferences for individual users, the field lacks\nstandardized methods to assess its effectiveness. We present a multi-faceted\nevaluation framework that measures not only performance but also fairness,\nunintended effects, and adaptability across varying levels of preference\ndivergence. Through extensive experiments comparing eight personalization\nmethods across three preference datasets, we demonstrate that performance\ndifferences between methods could reach 36% when users strongly disagree, and\npersonalization can introduce up to 20% safety misalignment. These findings\nhighlight the critical need for holistic evaluation approaches to advance the\ndevelopment of more effective and inclusive preference learning systems.\n","authors":["Yijiang River Dong","Tiancheng Hu","Yinhong Liu","Ahmet ÃstÃ¼n","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2502.19158v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21034v2","updated":"2025-10-27T11:49:08Z","published":"2025-10-23T22:36:23Z","title":"Input Matters: Evaluating Input Structure's Impact on LLM Summaries of\n  Sports Play-by-Play","summary":"  A major concern when deploying LLMs in accuracy-critical domains such as\nsports reporting is that the generated text may not faithfully reflect the\ninput data. We quantify how input structure affects hallucinations and other\nfactual errors in LLM-generated summaries of NBA play-by-play data, across\nthree formats: row-structured, JSON and unstructured. We manually annotated\n3,312 factual errors across 180 game summaries produced by two models,\nLlama-3.1-70B and Qwen2.5-72B. Input structure has a strong effect: JSON input\nreduces error rates by 69% for Llama and 65% for Qwen compared to unstructured\ninput, while row-structured input reduces errors by 54% for Llama and 51% for\nQwen. A two-way repeated measures ANOVA shows that input structure accounts for\nover 80% of the variance in error rates, with Tukey HSD post hoc tests\nconfirming statistically significant differences between all input formats.\n","authors":["Barkavi Sundararajan","Somayajulu Sripada","Ehud Reiter"],"pdf_url":"https://arxiv.org/pdf/2510.21034v2.pdf","comment":"Accepted at INLG 2025"},{"id":"http://arxiv.org/abs/2510.10114v2","updated":"2025-10-27T11:29:26Z","published":"2025-10-11T08:43:45Z","title":"LinearRAG: Linear Graph Retrieval Augmented Generation on Large-scale\n  Corpora","summary":"  Retrieval-Augmented Generation (RAG) is widely used to mitigate\nhallucinations of Large Language Models (LLMs) by leveraging external\nknowledge. While effective for simple queries, traditional RAG systems struggle\nwith large-scale, unstructured corpora where information is fragmented. Recent\nadvances incorporate knowledge graphs to capture relational structures,\nenabling more comprehensive retrieval for complex, multi-hop reasoning tasks.\nHowever, existing graph-based RAG (GraphRAG) methods rely on unstable and\ncostly relation extraction for graph construction, often producing noisy graphs\nwith incorrect or inconsistent relations that degrade retrieval quality. In\nthis paper, we revisit the pipeline of existing GraphRAG systems and propose\nLinearRAG (Linear Graph-based Retrieval-Augmented Generation), an efficient\nframework that enables reliable graph construction and precise passage\nretrieval. Specifically, LinearRAG constructs a relation-free hierarchical\ngraph, termed Tri-Graph, using only lightweight entity extraction and semantic\nlinking, avoiding unstable relation modeling. This new paradigm of graph\nconstruction scales linearly with corpus size and incurs no extra token\nconsumption, providing an economical and reliable indexing of the original\npassages. For retrieval, LinearRAG adopts a two-stage strategy: (i) relevant\nentity activation via local semantic bridging, followed by (ii) passage\nretrieval through global importance aggregation. Extensive experiments on four\ndatasets demonstrate that LinearRAG significantly outperforms baseline models.\n","authors":["Luyao Zhuang","Shengyuan Chen","Yilin Xiao","Huachi Zhou","Yujing Zhang","Hao Chen","Qinggang Zhang","Xiao Huang"],"pdf_url":"https://arxiv.org/pdf/2510.10114v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21339v2","updated":"2025-10-27T11:23:40Z","published":"2025-10-24T11:08:32Z","title":"Multi-turn Training with Basic Human Feedback Helps Little on LLM\n  Reasoning","summary":"  The reasoning capabilities of Large Language Models (LLMs) are typically\ndeveloped through the single-turn reinforcement learning, whereas real-world\napplications often involve multi-turn interactions with human feedback, leading\nto a potential mismatch between training and deployment conditions. In this\nwork, we study whether multi-turn training with human feedback is necessary for\nreasoning tasks. We compare conventional single-turn training with three\nmulti-turn strategies and reach contrary conclusions to previous research. We\nfind that models trained in a single-turn setting generalize effectively to\nboth single- and multi-turn evaluations, while models trained with multi-turn\nstrategies exhibit a significant degradation in single-turn reasoning\nperformance. These results suggest that for tasks with complete information,\nrobust single-turn training remains more effective and reliable, as multi-turn\ntraining with basic feedback provides limited benefits and can even degrade\nreasoning capabilities.\n","authors":["Qiang Liu","Wuganjing Song","Zhenzhou Lin","Feifan Chen","Qiaolong Cai","Chen Li","Yongduo Sui"],"pdf_url":"https://arxiv.org/pdf/2510.21339v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.03352v3","updated":"2025-10-27T11:20:02Z","published":"2025-04-04T11:14:38Z","title":"StereoDetect: Detecting Stereotypes and Anti-stereotypes the Correct Way\n  Using Social Psychological Underpinnings","summary":"  Stereotypes are known to have very harmful effects, making their detection\ncritically important. However, current research predominantly focuses on\ndetecting and evaluating stereotypical biases, thereby leaving the study of\nstereotypes in its early stages. Our study revealed that many works have failed\nto clearly distinguish between stereotypes and stereotypical biases, which has\nsignificantly slowed progress in advancing research in this area. Stereotype\nand Anti-stereotype detection is a problem that requires social knowledge;\nhence, it is one of the most difficult areas in Responsible AI. This work\ninvestigates this task, where we propose a five-tuple definition and provide\nprecise terminologies disentangling stereotypes, anti-stereotypes,\nstereotypical bias, and general bias. We provide a conceptual framework\ngrounded in social psychology for reliable detection. We identify key\nshortcomings in existing benchmarks for this task of stereotype and\nanti-stereotype detection. To address these gaps, we developed StereoDetect, a\nwell curated, definition-aligned benchmark dataset designed for this task. We\nshow that sub-10B language models and GPT-4o frequently misclassify\nanti-stereotypes and fail to recognize neutral overgeneralizations. We\ndemonstrate StereoDetect's effectiveness through multiple qualitative and\nquantitative comparisons with existing benchmarks and models fine-tuned on\nthem. The dataset and code is available at\nhttps://github.com/KaustubhShejole/StereoDetect.\n","authors":["Kaustubh Shivshankar Shejole","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2504.03352v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23217v1","updated":"2025-10-27T11:08:05Z","published":"2025-10-27T11:08:05Z","title":"Process Reward Models for Sentence-Level Verification of LVLM Radiology\n  Reports","summary":"  Automating radiology report generation with Large Vision-Language Models\n(LVLMs) holds great potential, yet these models often produce clinically\ncritical hallucinations, posing serious risks. Existing hallucination detection\nmethods frequently lack the necessary sentence-level granularity or robust\ngeneralization across different LVLM generators. We introduce a novel approach:\na sentence-level Process Reward Model (PRM) adapted for this vision-language\ntask. Our PRM predicts the factual correctness of each generated sentence,\nconditioned on clinical context and preceding text. When fine-tuned on\nMIMIC-CXR with weakly-supervised labels, a lightweight 0.5B-parameter PRM\noutperforms existing verification techniques, demonstrating, for instance,\nrelative improvements of 7.5% in Matthews Correlation Coefficient and 1.8% in\nAUROC over strong white-box baselines on outputs from one LVLM. Unlike methods\nreliant on internal model states, our PRM demonstrates strong generalization to\nan unseen LVLM. We further show its practical utility: PRM scores effectively\nfilter low-quality reports, improving F1-CheXbert scores by 4.5% (when\ndiscarding the worst 10% of reports). Moreover, when guiding a novel weighted\nbest-of-N selection process on the MIMIC-CXR test set, our PRM show relative\nimprovements in clinical metrics of 7.4% for F1-CheXbert and 0.6% for\nBERTScore. These results demonstrate that a lightweight, context-aware PRM\nprovides a model-agnostic safety layer for clinical LVLMs without access to\ninternal activations\n","authors":["Alois Thomas","Maya Varma","Jean-Benoit Delbrouck","Curtis P. Langlotz"],"pdf_url":"https://arxiv.org/pdf/2510.23217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15301v2","updated":"2025-10-27T10:59:03Z","published":"2025-06-18T09:32:16Z","title":"Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment","summary":"  Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet,\ntheir adoption in critical domains, such as clinical trial recruitment, remains\nlimited. As trials are designed in natural language and patient data is\nrepresented as both structured and unstructured text, the task of matching\ntrials and patients benefits from knowledge aggregation and reasoning abilities\nof LLMs. Classical approaches are trial-specific and LLMs with their ability to\nconsolidate distributed knowledge hold the potential to build a more general\nsolution. Yet recent applications of LLM-assisted methods rely on proprietary\nmodels and weak evaluation benchmarks. In this survey, we are the first to\nanalyze the task of trial-patient matching and contextualize emerging LLM-based\napproaches in clinical trial recruitment. We critically examine existing\nbenchmarks, approaches and evaluation frameworks, the challenges to adopting\nLLM technologies in clinical research and exciting future directions.\n","authors":["Shrestha Ghosh","Moritz Schneider","Carina Reinicke","Carsten Eickhoff"],"pdf_url":"https://arxiv.org/pdf/2506.15301v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23198v1","updated":"2025-10-27T10:36:15Z","published":"2025-10-27T10:36:15Z","title":"PTPP-Aware Adaptation Scaling Laws: Predicting Domain-Adaptation\n  Performance at Unseen Pre-Training Budgets","summary":"  Continual pre-training (CPT) for domain adaptation must balance target-domain\ngains with stability on the base domain. Existing CPT scaling laws typically\nassume a fixed pre-training budget, which limits their ability to forecast\nadaptation outcomes for models trained at different tokens-per-parameter\n(PTPP). We present \\emph{PTPP-aware} adaptation scaling laws that make the\npre-training budget an explicit variable, enabling accurate \\emph{prediction}\nof adaptation loss at unseen \\ptpp. On a multilingual setup (English/Arabic\n$\\rightarrow$ French), PTPP-aware formulations trained on early stages\n(\\ptpp{}=\\{15,31\\}) predict target loss at \\ptpp{}=279 and outperform a\nPTPP-agnostic \\dcpt{} transfer baseline on metrics (Huber-on-log,\nMAE$_\\mathrm{rel}$, calibration slope); full diagnostics (RMSE, MAPE) are in\nthe appendix. Beyond forecasting, we show a practical use case: planning replay\nratios and adaptation token budgets that satisfy target and forgetting\nconstraints under compute limits.\n","authors":["Etienne Goffinet","Shane Bergsma","Avraham Sheinin","Natalia Vassilieva","Shaheer Muhammad","Preslav Nakov","Gurpreet Gosal"],"pdf_url":"https://arxiv.org/pdf/2510.23198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23189v1","updated":"2025-10-27T10:27:00Z","published":"2025-10-27T10:27:00Z","title":"DREaM: Drug-Drug Relation Extraction via Transfer Learning Method","summary":"  Relation extraction between drugs plays a crucial role in identifying drug\ndrug interactions and predicting side effects. The advancement of machine\nlearning methods in relation extraction, along with the development of large\nmedical text databases, has enabled the low cost extraction of such relations\ncompared to other approaches that typically require expert knowledge. However,\nto the best of our knowledge, there are limited datasets specifically designed\nfor drug drug relation extraction currently available. Therefore, employing\ntransfer learning becomes necessary to apply machine learning methods in this\ndomain. In this study, we propose DREAM, a method that first employs a trained\nrelation extraction model to discover relations between entities and then\napplies this model to a corpus of medical texts to construct an ontology of\ndrug relationships. The extracted relations are subsequently validated using a\nlarge language model. Quantitative results indicate that the LLM agreed with 71\nof the relations extracted from a subset of PubMed abstracts. Furthermore, our\nqualitative analysis indicates that this approach can uncover ambiguities in\nthe medical domain, highlighting the challenges inherent in relation extraction\nin this field.\n","authors":["Ali Fata","Hossein Rahmani","Parinaz Soltanzadeh","Amirhossein Derakhshan","Behrouz Minaei Bidgoli"],"pdf_url":"https://arxiv.org/pdf/2510.23189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23182v1","updated":"2025-10-27T10:21:46Z","published":"2025-10-27T10:21:46Z","title":"SI-Bench: Benchmarking Social Intelligence of Large Language Models in\n  Human-to-Human Conversations","summary":"  As large language models (LLMs) develop anthropomorphic abilities, they are\nincreasingly being deployed as autonomous agents to interact with humans.\nHowever, evaluating their performance in realistic and complex social\ninteractions remains a significant challenge. Most previous research built\ndatasets through simulated agent-to-agent interactions, which fails to capture\nthe authentic linguistic styles and relational dynamics found in real human\nconversations. To address this gap, we introduce SI-Bench, a novel benchmark\ndesigned to evaluate aspects of social intelligence in LLMs. Grounded in broad\nsocial science theories, SI-Bench contains 2,221 authentic multi-turn dialogues\ncollected from a social networking application. We further selected a subset of\n312 dialogues for manual annotation across 8 major models. The experiments show\nthat SOTA models have surpassed the human expert in process reasoning under\ncomplex social situations, yet they still fall behind humans in reply quality.\nMoreover, introducing Chain-of-Thought (CoT) reasoning may degrade the\nperformance of LLMs in social dialogue tasks. All datasets are openly available\nat https://github.com/SI-Bench/SI-Bench.git.\n","authors":["Shuai Huang","Wenxuan Zhao","Jun Gao"],"pdf_url":"https://arxiv.org/pdf/2510.23182v1.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.23169v1","updated":"2025-10-27T09:51:49Z","published":"2025-10-27T09:51:49Z","title":"MATCH: Task-Driven Code Evaluation through Contrastive Learning","summary":"  AI-based code generation is increasingly prevalent, with GitHub Copilot\nestimated to generate 46% of the code on GitHub. Accurately evaluating how well\ngenerated code aligns with developer intent remains a critical challenge.\nTraditional evaluation methods, such as unit tests, are often unscalable and\ncostly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture code\nfunctionality, and metrics like CodeBERTScore require reference code, which is\nnot always available. To address the gap in reference-free evaluation, with few\nalternatives such as ICE-Score, this paper introduces MATCH, a novel\nreference-free metric. MATCH uses Contrastive Learning to generate meaningful\nembeddings for code and natural language task descriptions, enabling similarity\nscoring that reflects how well generated code implements the task. We show that\nMATCH achieves stronger correlations with functional correctness and human\npreference than existing metrics across multiple programming languages.\n","authors":["Marah Ghoummaid","Vladimir Tchuiev","Ofek Glick","Michal Moschkovitz","Dotan Di Castro"],"pdf_url":"https://arxiv.org/pdf/2510.23169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23163v1","updated":"2025-10-27T09:41:29Z","published":"2025-10-27T09:41:29Z","title":"Beyond Direct Generation: A Decomposed Approach to Well-Crafted\n  Screenwriting with LLMs","summary":"  The screenplay serves as the foundation for television production, defining\nnarrative structure, character development, and dialogue. While Large Language\nModels (LLMs) show great potential in creative writing, direct end-to-end\ngeneration approaches often fail to produce well-crafted screenplays. We argue\nthis failure stems from forcing a single model to simultaneously master two\ndisparate capabilities: creative narrative construction and rigid format\nadherence. The resulting outputs may mimic superficial style but lack the deep\nstructural integrity and storytelling substance required for professional use.\nTo enable LLMs to generate high-quality screenplays, we introduce Dual-Stage\nRefinement (DSR), a decomposed framework that decouples creative narrative\ngeneration from format conversion. The first stage transforms a brief outline\ninto rich, novel-style prose. The second stage refines this narrative into a\nprofessionally formatted screenplay. This separation enables the model to\nspecialize in one distinct capability at each stage. A key challenge in\nimplementing DSR is the scarcity of paired outline-to-novel training data. We\naddress this through hybrid data synthesis: reverse synthesis deconstructs\nexisting screenplays into structured inputs, while forward synthesis leverages\nthese inputs to generate high-quality narrative texts as training targets.\nBlind evaluations by professional screenwriters show that DSR achieves a 75%\nwin rate against strong baselines like Gemini-2.5-Pro and reaches 82.7% of\nhuman-level performance. Our work demonstrates that decomposed generation\narchitecture with tailored data synthesis effectively specializes LLMs in\ncomplex creative domains.\n","authors":["Hang Lei","Shengyi Zong","Zhaoyan Li","Ziren Zhou","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2510.23163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23160v1","updated":"2025-10-27T09:39:22Z","published":"2025-10-27T09:39:22Z","title":"ENTP: Enhancing Low-Quality SFT Data via Neural-Symbolic Text Purge-Mix","summary":"  Supervised Fine-Tuning (SFT) adapts pre-trained Large Language Models (LLMs)\nto domain-specific instructions by training on a carefully curated subset of\nhigh-quality instruction-response pairs, typically drawn from a larger dataset\nthat often contains many low-quality or noisy samples. However, existing\nquality-first paradigms often overlook valuable signals in discarded\nlow-quality data and rely on imperfect quality filters. We introduce ENTP\n(Enhancing low-quality SFT data via Neural-symbolic Text Purge-Mix), a\nframework that revitalizes low-quality corpora through symbolic purification\nand neural reconstruction. The symbolic module identifies and prunes noisy\nsamples based on statistical priors, while the neural component synthesizes\nenriched instruction-response pairs by leveraging latent representations and\nmodel knowledge. This neural-symbolic synergy enhances data informativeness and\ndiversity. Experiments show that ENTP-augmented datasets, constructed\nexclusively from low-quality data, outperform 13 established data-selection\nbaselines across five instruction-following benchmarks, and even surpass\nfine-tuning on the full original dataset (approximately 300K examples). Our\nresults highlight the untapped potential of low-quality data and underscore the\nimportance of intelligent purification and synthesis for efficient instruction\nalignment.\n","authors":["Zile Yang","Ling Li","Na Di","Jinlong Pang","Yao Zhou","Hao Cheng","Bo Han","Jiaheng Wei"],"pdf_url":"https://arxiv.org/pdf/2510.23160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21007v2","updated":"2025-10-27T09:25:38Z","published":"2025-10-23T21:33:28Z","title":"Can Confidence Estimates Decide When Chain-of-Thought Is Necessary for\n  LLMs?","summary":"  Chain-of-thought (CoT) prompting has emerged as a common technique for\nenhancing the reasoning abilities of large language models (LLMs). While\nextended reasoning can boost accuracy on complex tasks, it is often unnecessary\nand substantially increases token usage, limiting the practicality of reasoning\nmodels in many scenarios. Recent models, such as GPT-OSS and Qwen3, expose\ncontrols that enable users to adjust the length of CoT or determine whether it\nis used at all. Yet, it remains unclear when CoT should be used: on some tasks\nit improves performance, while on others it provides little benefit or even\nharms performance. We address this challenge with confidence-gated CoT, where a\nmodel invokes reasoning only when confidence in its direct answer is low. To\nthis end, we present the first systematic study of training-free confidence\nestimation methods for CoT gating. Specifically, we evaluate four training-free\nconfidence estimation methods and compare them to a random baseline and an\noracle that always knows when CoT is needed. Through extensive experiments, we\nshow that existing training-free confidence measures can reduce redundant CoT\nand outperform randomly invoked CoT. However, the utility of individual\nconfidence measures is inconsistent, varying with both the dataset and the\nmodel, underscoring the difficulty of deploying confidence-gated CoT in\npractice. By analysing both strengths and failure modes, our study highlights\nthe potential and limitations of current methods and paves the way toward more\nreliable adaptive gating of CoT.\n","authors":["Samuel Lewis-Lim","Xingwei Tan","Zhixue Zhao","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2510.21007v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2510.23142v1","updated":"2025-10-27T09:19:10Z","published":"2025-10-27T09:19:10Z","title":"Rethinking GSPO: The Perplexity-Entropy Equivalence","summary":"  We provide a new perspective on GSPO's length-normalized importance ratios by\nestablishing their connection to information-theoretic quantities. We show that\nGSPO's sequence-level weight $s(\\theta) =\n(\\pi_\\theta/\\pi_{\\theta_{\\text{old}}})^{1/|y|}$ can be equivalently expressed\nas the inverse perplexity ratio\n$\\text{PPL}_{\\theta_{\\text{old}}}/\\text{PPL}_\\theta$ and as the exponential\ncross-entropy change $\\exp(\\Delta H)$. While the perplexity-entropy\nrelationship follows from standard definitions, this observation provides a\nuseful lens for understanding GSPO: the algorithm weights policy gradient\nupdates by perplexity ratios, offering an information-theoretic interpretation\nof the importance weights. This perspective helps explain GSPO's empirical\nproperties, including log-domain variance reduction through geometric averaging\nand stability in training mixture-of-experts models. We validate the\nmathematical equivalences and variance predictions through controlled\nexperiments on mathematical reasoning tasks.\n","authors":["Chi Liu"],"pdf_url":"https://arxiv.org/pdf/2510.23142v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2510.23131v1","updated":"2025-10-27T09:12:04Z","published":"2025-10-27T09:12:04Z","title":"Corpus Frequencies in Morphological Inflection: Do They Matter?","summary":"  The traditional approach to morphological inflection (the task of modifying a\nbase word (lemma) to express grammatical categories) has been, for decades, to\nconsider lexical entries of lemma-tag-form triples uniformly, lacking any\ninformation about their frequency distribution. However, in production\ndeployment, one might expect the user inputs to reflect a real-world\ndistribution of frequencies in natural texts. With future deployment in mind,\nwe explore the incorporation of corpus frequency information into the task of\nmorphological inflection along three key dimensions during system development:\n(i) for train-dev-test split, we combine a lemma-disjoint approach, which\nevaluates the model's generalization capabilities, with a frequency-weighted\nstrategy to better reflect the realistic distribution of items across different\nfrequency bands in training and test sets; (ii) for evaluation, we complement\nthe standard type accuracy (often referred to simply as accuracy), which treats\nall items equally regardless of frequency, with token accuracy, which assigns\ngreater weight to frequent words and better approximates performance on running\ntext; (iii) for training data sampling, we introduce a method novel in the\ncontext of inflection, frequency-aware training, which explicitly incorporates\nword frequency into the sampling process. We show that frequency-aware training\noutperforms uniform sampling in 26 out of 43 languages.\n","authors":["TomÃ¡Å¡ Sourada","Jana StrakovÃ¡"],"pdf_url":"https://arxiv.org/pdf/2510.23131v1.pdf","comment":"Published in the proceedings of ITAT 2025.15 pages, 1 figure, 4\n  tables"},{"id":"http://arxiv.org/abs/2505.22453v2","updated":"2025-10-27T09:06:32Z","published":"2025-05-28T15:11:16Z","title":"First SFT, Second RL, Third UPT: Continual Improving Multi-Modal LLM\n  Reasoning via Unsupervised Post-Training","summary":"  Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL), which require expensive and manually annotated multi-modal\ndata--an ultimately unsustainable resource. This limitation has motivated a\ngrowing interest in unsupervised paradigms as a third stage of post-training\nafter SFT and RL. While recent efforts have explored this direction, their\nmethods are complex and difficult to iterate. To address this, we propose\nMM-UPT, a simple yet effective framework for unsupervised post-training of\nMLLMs, enabling continual self-improvement without any external supervision.\nThe training method of MM-UPT builds upon GRPO, replacing traditional reward\nsignals with a self-rewarding mechanism based on majority voting over multiple\nsampled responses. Our experiments demonstrate that such training method\neffectively improves the reasoning ability of Qwen2.5-VL-7B (e.g.,\n66.3\\%$\\rightarrow$72.9\\% on MathVista, 62.9\\%$\\rightarrow$68.7\\% on We-Math),\nusing standard dataset without ground truth labels. To further explore\nscalability, we extend our framework to a data self-generation setting,\ndesigning two strategies that prompt the MLLM to synthesize new training\nsamples on its own. Additional experiments show that combining these synthetic\ndata with the unsupervised training method can also boost performance,\nhighlighting a promising approach for scalable self-improvement. Overall,\nMM-UPT offers a new paradigm for autonomous enhancement of MLLMs, serving as a\ncritical third step after initial SFT and RL in the absence of external\nsupervision. Our code is available at https://github.com/waltonfuture/MM-UPT.\n","authors":["Lai Wei","Yuting Li","Chen Wang","Yue Wang","Linghe Kong","Weiran Huang","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2505.22453v2.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23123v1","updated":"2025-10-27T08:57:24Z","published":"2025-10-27T08:57:24Z","title":"Beyond Higher Rank: Token-wise Input-Output Projections for Efficient\n  Low-Rank Adaptation","summary":"  Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method\nwidely used in large language models (LLMs). LoRA essentially describes the\nprojection of an input space into a low-dimensional output space, with the\ndimensionality determined by the LoRA rank. In standard LoRA, all input tokens\nshare the same weights and undergo an identical input-output projection. This\nlimits LoRA's ability to capture token-specific information due to the inherent\nsemantic differences among tokens. To address this limitation, we propose\nToken-wise Projected Low-Rank Adaptation (TopLoRA), which dynamically adjusts\nLoRA weights according to the input token, thereby learning token-wise\ninput-output projections in an end-to-end manner. Formally, the weights of\nTopLoRA can be expressed as $B\\Sigma_X A$, where $A$ and $B$ are low-rank\nmatrices (as in standard LoRA), and $\\Sigma_X$ is a diagonal matrix generated\nfrom each input token $X$. Notably, TopLoRA does not increase the rank of LoRA\nweights but achieves more granular adaptation by learning token-wise LoRA\nweights (i.e., token-wise input-output projections). Extensive experiments\nacross multiple models and datasets demonstrate that TopLoRA consistently\noutperforms LoRA and its variants. The code is available at\nhttps://github.com/Leopold1423/toplora-neurips25.\n","authors":["Shiwei Li","Xiandi Luo","Haozhao Wang","Xing Tang","Ziqiang Cui","Dugang Liu","Yuhua Li","Xiuqiang He","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2510.23123v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.21864v3","updated":"2025-10-27T08:52:21Z","published":"2025-06-27T02:32:04Z","title":"DeepOmni: Towards Seamless and Smart Speech Interaction with Adaptive\n  Modality-Specific MoE","summary":"  Native multimodal large language models (MLLMs) restructure a single large\nlanguage model (LLM) into a spoken language model (SLM) capable of both speech\nand text generation. Compared to modular and aligned MLLMs, native MLLMs\npreserve richer paralinguistic features such as emotion and prosody, and\ngenerate speech responses directly within the backbone LLM rather than using a\nseparate speech decoder. This integration also results in lower response\nlatency and smoother interaction. However, native MLLMs suffer from\ncatastrophic forgetting and performance degradation because the available\npaired speech-text data is insufficient to support the pretraining of MLLMs\ncompared to the vast amount of text data required to pretrain text LLMs. To\naddress this issue, we propose DeepTalk, a framework for adaptive modality\nexpert learning based on a Mixture of Experts (MoE) architecture. DeepTalk\nfirst adaptively distinguishes modality experts according to their modality\nload within the LLM. Each modality expert then undergoes specialized\nsingle-modality training, followed by joint multimodal collaborative training.\nAs a result, DeepTalk incurs only a 5.5% performance drop compared to the\noriginal LLM, which is significantly lower than the average performance drop of\nover 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par\nwith modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within\n0.5 seconds, ensuring a seamless and intelligent speech interaction experience.\nCode and models are released at https://github.com/talkking/DeepTalk.\n","authors":["Hang Shao","Heting Gao","Yunhang Shen","Jiawei Chen","Zuwei Long","Dong Yang","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2506.21864v3.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2510.23114v1","updated":"2025-10-27T08:34:41Z","published":"2025-10-27T08:34:41Z","title":"Flexing in 73 Languages: A Single Small Model for Multilingual\n  Inflection","summary":"  We present a compact, single-model approach to multilingual inflection, the\ntask of generating inflected word forms from base lemmas to express grammatical\ncategories. Our model, trained jointly on data from 73 languages, is\nlightweight, robust to unseen words, and outperforms monolingual baselines in\nmost languages. This demonstrates the effectiveness of multilingual modeling\nfor inflection and highlights its practical benefits: simplifying deployment by\neliminating the need to manage and retrain dozens of separate monolingual\nmodels. In addition to the standard SIGMORPHON shared task benchmarks, we\nevaluate our monolingual and multilingual models on 73 Universal Dependencies\n(UD) treebanks, extracting lemma-tag-form triples and their frequency counts.\nTo ensure realistic data splits, we introduce a novel frequency-weighted,\nlemma-disjoint train-dev-test resampling procedure. Our work addresses the lack\nof an open-source, general-purpose, multilingual morphological inflection\nsystem capable of handling unseen words across a wide range of languages,\nincluding Czech. All code is publicly released at:\nhttps://github.com/tomsouri/multilingual-inflection.\n","authors":["TomÃ¡Å¡ Sourada","Jana StrakovÃ¡"],"pdf_url":"https://arxiv.org/pdf/2510.23114v1.pdf","comment":"Published in the proceedings of TSD 2025. 12 pages, 1 figure, 4\n  tables"},{"id":"http://arxiv.org/abs/2505.17701v3","updated":"2025-10-27T08:19:35Z","published":"2025-05-23T10:10:22Z","title":"COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary\n  Weights in Down Projection","summary":"  The growing size of large language models has created significant\ncomputational inefficiencies. To address this challenge, sparse activation\nmethods selectively deactivates non-essential parameters during inference,\nreducing computational costs in FFNN layers. While existing methods focus on\nnon-linear gating mechanisms, we hypothesize that the sparsity of the FFNN\nlayer lies globally in the form of a linear combination over its internal down\nprojection matrix. Based on this insight, we propose two methods: M-COUNTDOWN,\nleveraging indirect coefficients, and D-COUNTDOWN, utilizing direct\ncoefficients of the linear combination. Experimental results demonstrate that\nD-COUNTDOWN can omit 90% of computations with performance loss as low as 5.5%\nideally, while M-COUNTDOWN provides a predictor-free solution with up to 29.4%\nbetter performance preservation compared to existing methods. Our specialized\nkernel implementations effectively realize these theoretical gains into\nsubstantial real-world acceleration.\n","authors":["Jaewon Cheon","Pilsung Kang"],"pdf_url":"https://arxiv.org/pdf/2505.17701v3.pdf","comment":"EMNLP 2025 (Main Track)"},{"id":"http://arxiv.org/abs/2510.23104v1","updated":"2025-10-27T08:18:02Z","published":"2025-10-27T08:18:02Z","title":"Leveraging Hierarchical Organization for Medical Multi-document\n  Summarization","summary":"  Medical multi-document summarization (MDS) is a complex task that requires\neffectively managing cross-document relationships. This paper investigates\nwhether incorporating hierarchical structures in the inputs of MDS can improve\na model's ability to organize and contextualize information across documents\ncompared to traditional flat summarization methods. We investigate two ways of\nincorporating hierarchical organization across three large language models\n(LLMs), and conduct comprehensive evaluations of the resulting summaries using\nautomated metrics, model-based metrics, and domain expert evaluation of\npreference, understandability, clarity, complexity, relevance, coverage,\nfactuality, and coherence. Our results show that human experts prefer\nmodel-generated summaries over human-written summaries. Hierarchical approaches\ngenerally preserve factuality, coverage, and coherence of information, while\nalso increasing human preference for summaries. Additionally, we examine\nwhether simulated judgments from GPT-4 align with human judgments, finding\nhigher agreement along more objective evaluation facets. Our findings\ndemonstrate that hierarchical structures can improve the clarity of medical\nsummaries generated by models while maintaining content coverage, providing a\npractical way to improve human preference for generated summaries.\n","authors":["Yi-Li Hsu","Katelyn X. Mei","Lucy Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2510.23104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04483v3","updated":"2025-10-27T08:07:17Z","published":"2024-03-07T13:36:08Z","title":"GraphInstruct: Empowering Large Language Models with Graph Understanding\n  and Reasoning Capability","summary":"  Improving the general capabilities of large language models (LLMs) is an\nactive research topic. As a common data structure in many real-world domains,\nunderstanding graph data is a crucial part of advancing general intelligence.\nTo this end, we propose a dynamic benchmark named GraphInstruct in this paper,\nwhich comprehensively includes 21 classical graph reasoning tasks, providing\ndiverse graph generation pipelines and detailed intermediate reasoning steps\nfor each sample. Based on GraphInstruct, we develop GraphSolver via efficient\ninstruction-tuning, which demonstrates prominent graph understanding capability\ncompared to other open-sourced LLMs. To further endow LLMs with multi-step\ngraph reasoning capability, we propose a label-mask training strategy and build\nGraphSolver+, which leverages masked supervision on intermediate reasoning\ntokens to emphasize crucial node-identification signals. As one of the\npioneering efforts to enhance the graph understanding and reasoning abilities\nof LLMs, extensive experiments have demonstrated the superiority of GraphSolver\nand GraphSolver+ over other LLMs. We sincerely hope GraphInstruct will\nfacilitate further research on applying LLMs to graph-structured data. Our code\nand data are released publicly at: https://github.com/CGCL-codes/GraphInstruct.\n","authors":["Zihan Luo","Xiran Song","Hong Huang","Jianxun Lian","Chenhao Zhang","Jinqi Jiang","Xing Xie","Hai Jin"],"pdf_url":"https://arxiv.org/pdf/2403.04483v3.pdf","comment":"Accepted by Frontiers of Computer Science"},{"id":"http://arxiv.org/abs/2510.23090v1","updated":"2025-10-27T07:51:54Z","published":"2025-10-27T07:51:54Z","title":"MAP4TS: A Multi-Aspect Prompting Framework for Time-Series Forecasting\n  with Large Language Models","summary":"  Recent advances have investigated the use of pretrained large language models\n(LLMs) for time-series forecasting by aligning numerical inputs with LLM\nembedding spaces. However, existing multimodal approaches often overlook the\ndistinct statistical properties and temporal dependencies that are fundamental\nto time-series data. To bridge this gap, we propose MAP4TS, a novel\nMulti-Aspect Prompting Framework that explicitly incorporates classical\ntime-series analysis into the prompt design. Our framework introduces four\nspecialized prompt components: a Global Domain Prompt that conveys\ndataset-level context, a Local Domain Prompt that encodes recent trends and\nseries-specific behaviors, and a pair of Statistical and Temporal Prompts that\nembed handcrafted insights derived from autocorrelation (ACF), partial\nautocorrelation (PACF), and Fourier analysis. Multi-Aspect Prompts are combined\nwith raw time-series embeddings and passed through a cross-modality alignment\nmodule to produce unified representations, which are then processed by an LLM\nand projected for final forecasting. Extensive experiments across eight diverse\ndatasets show that MAP4TS consistently outperforms state-of-the-art LLM-based\nmethods. Our ablation studies further reveal that prompt-aware designs\nsignificantly enhance performance stability and that GPT-2 backbones, when\npaired with structured prompts, outperform larger models like LLaMA in\nlong-term forecasting tasks.\n","authors":["Suchan Lee","Jihoon Choi","Sohyeon Lee","Minseok Song","Bong-Gyu Jang","Hwanjo Yu","Soyeon Caren Han"],"pdf_url":"https://arxiv.org/pdf/2510.23090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07543v2","updated":"2025-10-27T07:40:12Z","published":"2025-07-10T08:38:31Z","title":"The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English\n  Corpora","summary":"  Cross-lingual retrieval-augmented generation (RAG) is a critical capability\nfor retrieving and generating answers across languages. Prior work in this\ncontext has mostly focused on generation and relied on benchmarks derived from\nopen-domain sources, most notably Wikipedia. In such settings, retrieval\nchallenges often remain hidden due to language imbalances, overlap with\npretraining data, and memorized content. To address this gap, we study\nArabic-English RAG in a domain-specific setting using benchmarks derived from\nreal-world corporate datasets. Our benchmarks include all combinations of\nlanguages for the user query and the supporting document, drawn independently\nand uniformly at random. This enables a systematic study of multilingual\nretrieval behavior.\n  Our findings reveal that retrieval is a critical bottleneck in cross-lingual\ndomain-specific scenarios, with substantial performance drops occurring when\nthe user query and supporting document languages differ. A key insight is that\nthese failures stem primarily from the retriever's difficulty in ranking\ndocuments across languages. Finally, we propose two simple retrieval strategies\nthat address this source of failure by enforcing equal retrieval from both\nlanguages or by translating the query, resulting in substantial improvements in\ncross-lingual and overall performance. These results highlight meaningful\nopportunities for improving multilingual retrieval, particularly in practical,\nreal-world RAG applications.\n","authors":["Chen Amiraz","Yaroslav Fyodorov","Elad Haramaty","Zohar Karnin","Liane Lewin-Eytan"],"pdf_url":"https://arxiv.org/pdf/2507.07543v2.pdf","comment":"Accepted to ArabicNLP 2025"},{"id":"http://arxiv.org/abs/2510.23081v1","updated":"2025-10-27T07:32:19Z","published":"2025-10-27T07:32:19Z","title":"A Survey on LLM Mid-training","summary":"  Recent advances in foundation models have highlighted the significant\nbenefits of multi-stage training, with a particular emphasis on the emergence\nof mid-training as a vital stage that bridges pre-training and post-training.\nMid-training is distinguished by its use of intermediate data and computational\nresources, systematically enhancing specified capabilities such as mathematics,\ncoding, reasoning, and long-context extension, while maintaining foundational\ncompetencies. This survey provides a formal definition of mid-training for\nlarge language models (LLMs) and investigates optimization frameworks that\nencompass data curation, training strategies, and model architecture\noptimization. We analyze mainstream model implementations in the context of\nobjective-driven interventions, illustrating how mid-training serves as a\ndistinct and critical stage in the progressive development of LLM capabilities.\nBy clarifying the unique contributions of mid-training, this survey offers a\ncomprehensive taxonomy and actionable insights, supporting future research and\ninnovation in the advancement of LLMs.\n","authors":["Chengying Tu","Xuemiao Zhang","Rongxiang Weng","Rumei Li","Chen Zhang","Yang Bai","Hongfei Yan","Jingang Wang","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2510.23081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23074v1","updated":"2025-10-27T07:18:32Z","published":"2025-10-27T07:18:32Z","title":"Fast-MIA: Efficient and Scalable Membership Inference for LLMs","summary":"  We propose Fast-MIA (https://github.com/Nikkei/fast-mia), a Python library\nfor efficiently evaluating membership inference attacks (MIA) against Large\nLanguage Models (LLMs). MIA against LLMs has emerged as a crucial challenge due\nto growing concerns over copyright, security, and data privacy, and has\nattracted increasing research attention. However, the progress of this research\nis significantly hindered by two main obstacles: (1) the high computational\ncost of inference in LLMs, and (2) the lack of standardized and maintained\nimplementations of MIA methods, which makes large-scale empirical comparison\ndifficult. To address these challenges, our library provides fast batch\ninference and includes implementations of representative MIA methods under a\nunified evaluation framework. This library supports easy implementation of\nreproducible benchmarks with simple configuration and extensibility. We release\nFast-MIA as an open-source (Apache License 2.0) tool to support scalable and\ntransparent research on LLMs.\n","authors":["Hiromu Takahashi","Shotaro Ishihara"],"pdf_url":"https://arxiv.org/pdf/2510.23074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14668v2","updated":"2025-10-27T07:17:51Z","published":"2025-05-20T17:55:25Z","title":"ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory\n  Perceptions","summary":"  Recent advances in Large Language Models (LLMs) have propelled intelligent\nagents from reactive responses to proactive support. While promising, existing\nproactive agents either rely exclusively on observations from enclosed\nenvironments (e.g., desktop UIs) with direct LLM inference or employ rule-based\nproactive notifications, leading to suboptimal user intent understanding and\nlimited functionality for proactive service. In this paper, we introduce\nContextAgent, the first context-aware proactive agent that incorporates\nextensive sensory contexts surrounding humans to enhance the proactivity of LLM\nagents. ContextAgent first extracts multi-dimensional contexts from massive\nsensory perceptions on wearables (e.g., video and audio) to understand user\nintentions. ContextAgent then leverages the sensory contexts and personas from\nhistorical data to predict the necessity for proactive services. When proactive\nassistance is needed, ContextAgent further automatically calls the necessary\ntools to assist users unobtrusively. To evaluate this new task, we curate\nContextAgentBench, the first benchmark for evaluating context-aware proactive\nLLM agents, covering 1,000 samples across nine daily scenarios and twenty\ntools. Experiments on ContextAgentBench show that ContextAgent outperforms\nbaselines by achieving up to 8.5% and 6.0% higher accuracy in proactive\npredictions and tool calling, respectively. We hope our research can inspire\nthe development of more advanced, human-centric, proactive AI assistants. The\ncode and dataset are publicly available at\nhttps://github.com/openaiotlab/ContextAgent.\n","authors":["Bufang Yang","Lilin Xu","Liekang Zeng","Kaiwei Liu","Siyang Jiang","Wenrui Lu","Hongkai Chen","Xiaofan Jiang","Guoliang Xing","Zhenyu Yan"],"pdf_url":"https://arxiv.org/pdf/2505.14668v2.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.21566v2","updated":"2025-10-27T07:12:10Z","published":"2025-10-24T15:26:30Z","title":"ColorEcosystem: Powering Personalized, Standardized, and Trustworthy\n  Agentic Service in massive-agent Ecosystem","summary":"  With the rapid development of (multimodal) large language model-based agents,\nthe landscape of agentic service management has evolved from single-agent\nsystems to multi-agent systems, and now to massive-agent ecosystems. Current\nmassive-agent ecosystems face growing challenges, including impersonal service\nexperiences, a lack of standardization, and untrustworthy behavior. To address\nthese issues, we propose ColorEcosystem, a novel blueprint designed to enable\npersonalized, standardized, and trustworthy agentic service at scale.\nConcretely, ColorEcosystem consists of three key components: agent carrier,\nagent store, and agent audit. The agent carrier provides personalized service\nexperiences by utilizing user-specific data and creating a digital twin, while\nthe agent store serves as a centralized, standardized platform for managing\ndiverse agentic services. The agent audit, based on the supervision of\ndeveloper and user activities, ensures the integrity and credibility of both\nservice providers and users. Through the analysis of challenges, transitional\nforms, and practical considerations, the ColorEcosystem is poised to power\npersonalized, standardized, and trustworthy agentic service across\nmassive-agent ecosystems. Meanwhile, we have also implemented part of\nColorEcosystem's functionality, and the relevant code is open-sourced at\nhttps://github.com/opas-lab/color-ecosystem.\n","authors":["Fangwen Wu","Zheng Wu","Jihong Wang","Yunku Chen","Ruiguang Pei","Heyuan Huang","Xin Liao","Xingyu Lou","Huarong Deng","Zhihui Fu","Weiwen Liu","Zhuosheng Zhang","Weinan Zhang","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2510.21566v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23070v1","updated":"2025-10-27T07:11:01Z","published":"2025-10-27T07:11:01Z","title":"Quality-Aware Translation Tagging in Multilingual RAG system","summary":"  Multilingual Retrieval-Augmented Generation (mRAG) often retrieves English\ndocuments and translates them into the query language for low-resource\nsettings. However, poor translation quality degrades response generation\nperformance. Existing approaches either assume sufficient translation quality\nor utilize the rewriting method, which introduces factual distortion and\nhallucinations. To mitigate these problems, we propose Quality-Aware\nTranslation Tagging in mRAG (QTT-RAG), which explicitly evaluates translation\nquality along three dimensions-semantic equivalence, grammatical accuracy, and\nnaturalness&fluency-and attach these scores as metadata without altering the\noriginal content. We evaluate QTT-RAG against CrossRAG and DKM-RAG as baselines\nin two open-domain QA benchmarks (XORQA, MKQA) using six instruction-tuned LLMs\nranging from 2.4B to 14B parameters, covering two low-resource languages\n(Korean and Finnish) and one high-resource language (Chinese). QTT-RAG\noutperforms the baselines by preserving factual integrity while enabling\ngenerator models to make informed decisions based on translation reliability.\nThis approach allows for effective usage of cross-lingual documents in\nlow-resource settings with limited native language documents, offering a\npractical and robust solution across multilingual domains.\n","authors":["Hoyeon Moon","Byeolhee Kim","Nikhil Verma"],"pdf_url":"https://arxiv.org/pdf/2510.23070v1.pdf","comment":"EMNLP 2025 MRL Workshop"},{"id":"http://arxiv.org/abs/2412.17063v5","updated":"2025-10-27T06:49:01Z","published":"2024-12-22T15:20:53Z","title":"Computational Analysis of Character Development in Holocaust Testimonies","summary":"  This work presents a computational approach to analyze character development\nalong the narrative timeline. The analysis characterizes the inner and outer\nchanges the protagonist undergoes within a narrative, and the interplay between\nthem. We consider transcripts of Holocaust survivor testimonies as a test case,\neach telling the story of an individual in first-person terms. We focus on the\nsurvivor's religious trajectory, examining the evolution of their disposition\ntoward religious belief and practice along the testimony. Clustering the\nresulting trajectories in the dataset, we identify common sequences in the\ndata. Our findings highlight multiple common structures of religiosity across\nthe narratives: in terms of belief, most present a constant disposition, while\nfor practice, most present an oscillating structure, serving as valuable\nmaterial for historical and sociological research. This work demonstrates the\npotential of natural language processing techniques for analyzing character\nevolution through thematic trajectories in narratives.\n","authors":["Esther Shizgal","Eitan Wagner","Renana Keydar","Omri Abend"],"pdf_url":"https://arxiv.org/pdf/2412.17063v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23052v1","updated":"2025-10-27T06:28:58Z","published":"2025-10-27T06:28:58Z","title":"Knocking-Heads Attention","summary":"  Multi-head attention (MHA) has become the cornerstone of modern large\nlanguage models, enhancing representational capacity through parallel attention\nheads. However, increasing the number of heads inherently weakens individual\nhead capacity, and existing attention mechanisms - whether standard MHA or its\nvariants like grouped-query attention (GQA) and grouped-tied attention (GTA) -\nsimply concatenate outputs from isolated heads without strong interaction. To\naddress this limitation, we propose knocking-heads attention (KHA), which\nenables attention heads to \"knock\" on each other - facilitating cross-head\nfeature-level interactions before the scaled dot-product attention. This is\nachieved by applying a shared, diagonally-initialized projection matrix across\nall heads. The diagonal initialization preserves head-specific specialization\nat the start of training while allowing the model to progressively learn\nintegrated cross-head representations. KHA adds only minimal parameters and\nFLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention\nvariants. We validate KHA by training a 6.1B parameter MoE model (1.01B\nactivated) on 1T high-quality tokens. Compared to baseline attention\nmechanisms, KHA brings superior and more stable training dynamics, achieving\nbetter performance across downstream tasks.\n","authors":["Zhanchao Zhou","Xiaodong Chen","Haoxing Chen","Zhenzhong Lan","Jianguo Li"],"pdf_url":"https://arxiv.org/pdf/2510.23052v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04678v4","updated":"2025-10-27T06:19:56Z","published":"2024-02-07T09:09:14Z","title":"FaithLM: Towards Faithful Explanations for Large Language Models","summary":"  Large language models (LLMs) increasingly produce natural language\nexplanations, yet these explanations often lack faithfulness, and they do not\nreliably reflect the evidence the model uses to decide. We introduce FaithLM, a\nmodel-agnostic framework that evaluates and improves the faithfulness of LLM\nexplanations without token masking or task-specific heuristics. FaithLM\nformalizes explanation faithfulness as an intervention property: a faithful\nexplanation should yield a prediction shift when its content is contradicted.\nTheoretical analysis shows that the resulting contrary-hint score is a sound\nand discriminative estimator of faithfulness. Building on this principle,\nFaithLM iteratively refines both the elicitation prompt and the explanation to\nmaximize the measured score. Experiments on three multi-domain datasets and\nmultiple LLM backbones demonstrate that FaithLM consistently increases\nfaithfulness and produces explanations more aligned with human rationales than\nstrong self-explanation baselines. These findings highlight that\nintervention-based evaluation, coupled with iterative optimization, provides a\nprincipled route toward faithful and reliable LLM explanations.\n","authors":["Yu-Neng Chuang","Guanchu Wang","Chia-Yuan Chang","Ruixiang Tang","Shaochen Zhong","Fan Yang","Mengnan Du","Xuanting Cai","Vladimir Braverman","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2402.04678v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06229v5","updated":"2025-10-27T06:16:14Z","published":"2025-07-08T17:59:22Z","title":"Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving","summary":"  AI agent frameworks operate in isolation, forcing agents to rediscover\nsolutions and repeat mistakes across different systems. Despite valuable\nproblem-solving experiences accumulated by frameworks like smolagents,\nOpenHands, and OWL, this knowledge remains trapped within individual systems,\npreventing the emergence of collective intelligence. Current memory systems\nfocus on individual agents or framework-specific demonstrations, failing to\nenable cross-architecture knowledge transfer. We introduce AGENT KB, a\nuniversal memory infrastructure enabling seamless experience sharing across\nheterogeneous agent frameworks without retraining. AGENT KB aggregates\ntrajectories into a structured knowledge base and serves lightweight APIs. At\ninference time, hybrid retrieval operates through two stages: planning seeds\nagents with cross-domain workflows, while feedback applies targeted diagnostic\nfixes. A disagreement gate ensures retrieved knowledge enhances rather than\ndisrupts reasoning, addressing knowledge interference in cross-framework\ntransfer. We validate AGENT KB across major frameworks on GAIA, Humanity's Last\nExam, GPQA, and SWE-bench. Results show substantial improvements across diverse\nmodel families: compared to baseline pass@1, smolagents with AGENT KB achieve\nup to 18.7pp gains at pass@3 (55.2% -> 73.9%), while OpenHands improves 4.0pp\non SWE-bench pass@1 (24.3% -> 28.3%). Similar improvements are observed across\nall base model families. Ablations confirm that hybrid retrieval and feedback\nstages are essential, with automatically generated experiences matching manual\ncuration. This establishes the foundation for collective agent intelligence\nthrough shared memory infrastructures.\n","authors":["Xiangru Tang","Tianrui Qin","Tianhao Peng","Ziyang Zhou","Daniel Shao","Tingting Du","Xinming Wei","Peng Xia","Fang Wu","He Zhu","Ge Zhang","Jiaheng Liu","Xingyao Wang","Sirui Hong","Chenglin Wu","Hao Cheng","Chi Wang","Wangchunshu Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.06229v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23038v1","updated":"2025-10-27T06:03:37Z","published":"2025-10-27T06:03:37Z","title":"Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated\n  Reinforcement Learning","summary":"  Large Language Models (LLMs) are widely used as judges to evaluate response\nquality, providing a scalable alternative to human evaluation. However, most\nLLM judges operate solely on intrinsic text-based reasoning, limiting their\nability to verify complex constraints or perform accurate computation.\nMotivated by the success of tool-integrated reasoning (TIR) in numerous tasks,\nwe propose TIR-Judge, an end-to-end RL framework for training LLM judges that\nintegrates a code executor for precise evaluation. TIR-Judge is built on three\nprinciples: (i) diverse training across verifiable and non-verifiable domains,\n(ii) flexible judgment formats (pointwise, pairwise, listwise), and (iii)\niterative RL that bootstraps directly from the initial model without\ndistillation. On seven public benchmarks, TIR-Judge surpasses strong\nreasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and\nachieves listwise performance comparable to Claude-Opus-4 despite having only\n8B parameters. Remarkably, TIR-Judge-Zero - trained entirely without distilled\njudge trajectories, matches the performance of distilled variants,\ndemonstrating that tool-augmented judges can self-evolve through iterative\nreinforcement learning.\n","authors":["Ran Xu","Jingjing Chen","Jiayu Ye","Yu Wu","Jun Yan","Carl Yang","Hongkun Yu"],"pdf_url":"https://arxiv.org/pdf/2510.23038v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2510.23027v1","updated":"2025-10-27T05:47:48Z","published":"2025-10-27T05:47:48Z","title":"Towards Stable and Effective Reinforcement Learning for\n  Mixture-of-Experts","summary":"  Recent advances in reinforcement learning (RL) have substantially improved\nthe training of large-scale language models, leading to significant gains in\ngeneration quality and reasoning ability. However, most existing research\nfocuses on dense models, while RL training for Mixture-of-Experts (MoE)\narchitectures remains underexplored. To address the instability commonly\nobserved in MoE training, we propose a novel router-aware approach to optimize\nimportance sampling (IS) weights in off-policy RL. Specifically, we design a\nrescaling strategy guided by router logits, which effectively reduces gradient\nvariance and mitigates training divergence. Experimental results demonstrate\nthat our method significantly improves both the convergence stability and the\nfinal performance of MoE models, highlighting the potential of RL algorithmic\ninnovations tailored to MoE architectures and providing a promising direction\nfor efficient training of large-scale expert models.\n","authors":["Di Zhang","Xun Wu","Shaohan Huang","Yaru Hao","Li Dong","Zewen Chi","Zhifang Sui","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2510.23027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23023v1","updated":"2025-10-27T05:37:23Z","published":"2025-10-27T05:37:23Z","title":"UniAIDet: A Unified and Universal Benchmark for AI-Generated Image\n  Content Detection and Localization","summary":"  With the rapid proliferation of image generative models, the authenticity of\ndigital images has become a significant concern. While existing studies have\nproposed various methods for detecting AI-generated content, current benchmarks\nare limited in their coverage of diverse generative models and image\ncategories, often overlooking end-to-end image editing and artistic images. To\naddress these limitations, we introduce UniAIDet, a unified and comprehensive\nbenchmark that includes both photographic and artistic images. UniAIDet covers\na wide range of generative models, including text-to-image, image-to-image,\nimage inpainting, image editing, and deepfake models. Using UniAIDet, we\nconduct a comprehensive evaluation of various detection methods and answer\nthree key research questions regarding generalization capability and the\nrelation between detection and localization. Our benchmark and analysis provide\na robust foundation for future research.\n","authors":["Huixuan Zhang","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2510.23023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23020v1","updated":"2025-10-27T05:32:50Z","published":"2025-10-27T05:32:50Z","title":"M$^{3}$T2IBench: A Large-Scale Multi-Category, Multi-Instance,\n  Multi-Relation Text-to-Image Benchmark","summary":"  Text-to-image models are known to struggle with generating images that\nperfectly align with textual prompts. Several previous studies have focused on\nevaluating image-text alignment in text-to-image generation. However, these\nevaluations either address overly simple scenarios, especially overlooking the\ndifficulty of prompts with multiple different instances belonging to the same\ncategory, or they introduce metrics that do not correlate well with human\nevaluation. In this study, we introduce M$^3$T2IBench, a large-scale,\nmulti-category, multi-instance, multi-relation along with an\nobject-detection-based evaluation metric, $AlignScore$, which aligns closely\nwith human evaluation. Our findings reveal that current open-source\ntext-to-image models perform poorly on this challenging benchmark.\nAdditionally, we propose the Revise-Then-Enforce approach to enhance image-text\nalignment. This training-free post-editing method demonstrates improvements in\nimage-text alignment across a broad range of diffusion models. \\footnote{Our\ncode and data has been released in supplementary material and will be made\npublicly available after the paper is accepted.}\n","authors":["Huixuan Zhang","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2510.23020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.23964v2","updated":"2025-10-27T05:14:37Z","published":"2025-09-28T16:41:56Z","title":"Detecting and Rectifying Noisy Labels: A Similarity-based Approach","summary":"  Label noise in datasets could significantly damage the performance and\nrobustness of deep neural networks (DNNs) trained on these datasets. As the\nsize of modern DNNs grows, there is a growing demand for automated tools for\ndetecting such errors. In this paper, we propose post-hoc, model-agnostic noise\ndetection and rectification methods utilizing the penultimate feature from a\nDNN. Our idea is based on the observation that the similarity between the\npenultimate feature of a mislabeled data point and its true class data points\nis higher than that for data points from other classes, making the probability\nof label occurrence within a tight, similar cluster informative for detecting\nand rectifying errors. Through theoretical and empirical analyses, we\ndemonstrate that our approach achieves high detection performance across\ndiverse, realistic noise scenarios and can automatically rectify these errors\nto improve dataset quality. Our implementation is available at\nhttps://anonymous.4open.science/r/noise-detection-and-rectification-AD8E.\n","authors":["Dang Huu-Tien","Minh-Phuong Nguyen","Naoya Inoue"],"pdf_url":"https://arxiv.org/pdf/2509.23964v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23011v1","updated":"2025-10-27T05:11:07Z","published":"2025-10-27T05:11:07Z","title":"LangLingual: A Personalised, Exercise-oriented English Language Learning\n  Tool Leveraging Large Language Models","summary":"  Language educators strive to create a rich experience for learners, while\nthey may be restricted in the extend of feedback and practice they can provide.\nWe present the design and development of LangLingual, a conversational agent\nbuilt using the LangChain framework and powered by Large Language Models. The\nsystem is specifically designed to provide real-time, grammar-focused feedback,\ngenerate context-aware language exercises and track learner proficiency over\ntime. The paper discusses the architecture, implementation and evaluation of\nLangLingual in detail. The results indicate strong usability, positive learning\noutcomes and encouraging learner engagement.\n","authors":["Sammriddh Gupta","Sonit Singh","Aditya Joshi","Mira Kim"],"pdf_url":"https://arxiv.org/pdf/2510.23011v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2510.23946v1","updated":"2025-10-27T23:59:03Z","published":"2025-10-27T23:59:03Z","title":"Leveraging LLMs for Early Alzheimer's Prediction","summary":"  We present a connectome-informed LLM framework that encodes dynamic fMRI\nconnectivity as temporal sequences, applies robust normalization, and maps\nthese data into a representation suitable for a frozen pre-trained LLM for\nclinical prediction. Applied to early Alzheimer's detection, our method\nachieves sensitive prediction with error rates well below clinically recognized\nmargins, with implications for timely Alzheimer's intervention.\n","authors":["Tananun Songdechakraiwut"],"pdf_url":"https://arxiv.org/pdf/2510.23946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23941v1","updated":"2025-10-27T23:49:31Z","published":"2025-10-27T23:49:31Z","title":"Auto prompting without training labels: An LLM cascade for product\n  quality assessment in e-commerce catalogs","summary":"  We introduce a novel, training free cascade for auto-prompting Large Language\nModels (LLMs) to assess product quality in e-commerce. Our system requires no\ntraining labels or model fine-tuning, instead automatically generating and\nrefining prompts for evaluating attribute quality across tens of thousands of\nproduct category-attribute pairs. Starting from a seed of human-crafted\nprompts, the cascade progressively optimizes instructions to meet\ncatalog-specific requirements. This approach bridges the gap between general\nlanguage understanding and domain-specific knowledge at scale in complex\nindustrial catalogs. Our extensive empirical evaluations shows the auto-prompt\ncascade improves precision and recall by $8-10\\%$ over traditional\nchain-of-thought prompting. Notably, it achieves these gains while reducing\ndomain expert effort from 5.1 hours to 3 minutes per attribute - a $99\\%$\nreduction. Additionally, the cascade generalizes effectively across five\nlanguages and multiple quality assessment tasks, consistently maintaining\nperformance gains.\n","authors":["Soham Satyadharma","Fatemeh Sheikholeslami","Swati Kaul","Aziz Umit Batur","Suleiman A. Khan"],"pdf_url":"https://arxiv.org/pdf/2510.23941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21080v6","updated":"2025-10-27T23:31:13Z","published":"2025-03-27T01:41:34Z","title":"EQ-Negotiator: Emotion Policing Personas for Anti-Manipulation in Credit\n  Collection Dialogues","summary":"  Persona modeling in large language models typically focuses on static\ncharacter traits, but overlooks the dynamic emotional intelligence required for\nreal-time adversarial negotiations. In financial dialogues, this limitation\ncreates critical vulnerabilities: debtors exploit predictable empathetic\nresponses through emotional manipulation tactics like aggression, feigned\ndistress, and guilt-tripping. To bridge this gap, we present EQ-Negotiator, a\nnovel framework that grounds persona behavior in emotion dynamics rather than\nstatic personality profiles. Unlike naive empathy-centric agents, EQ-Negotiator\nintegrates emotion memory and game-theoretic reasoning, powered by a Hidden\nMarkov Model (HMM) to track and predict debtor emotional states. By analyzing\nboth real-time and historical emotional cues, EQ-Negotiator strategically\ncounters negative emotions (e.g., aggression, feigned distress) while\npreserving productive debtor relationships. This work advances persona modeling\nfrom descriptive character profiles to functional emotional architectures,\nestablishing emotion as the critical link between persona design and tactical\nexecution. Through agent-to-agent validation across 20 credit negotiation\nscenarios, we demonstrate that emotion-driven personas enable robust defensive\ncapabilities against manipulation while maintaining strategic effectiveness.\n","authors":["Yunbo Long","Yuhan Liu"],"pdf_url":"https://arxiv.org/pdf/2503.21080v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.23311v2","updated":"2025-10-27T23:22:21Z","published":"2025-09-27T13:56:12Z","title":"Seeing Symbols, Missing Cultures: Probing Vision-Language Models'\n  Reasoning on Fire Imagery and Cultural Meaning","summary":"  Vision-Language Models (VLMs) often appear culturally competent but rely on\nsuperficial pattern matching rather than genuine cultural understanding. We\nintroduce a diagnostic framework to probe VLM reasoning on fire-themed cultural\nimagery through both classification and explanation analysis. Testing multiple\nmodels on Western festivals, non-Western traditions, and emergency scenes\nreveals systematic biases: models correctly identify prominent Western\nfestivals but struggle with underrepresented cultural events, frequently\noffering vague labels or dangerously misclassifying emergencies as\ncelebrations. These failures expose the risks of symbolic shortcuts and\nhighlight the need for cultural evaluation beyond accuracy metrics to ensure\ninterpretable and fair multimodal systems.\n","authors":["Haorui Yu","Yang Zhao","Yijia Chu","Qiufeng Yi"],"pdf_url":"https://arxiv.org/pdf/2509.23311v2.pdf","comment":"8 pages, 5 figures, 4 tables. Submitted to WiNLP 2025 Workshop at\n  COLING 2025"},{"id":"http://arxiv.org/abs/2510.23925v1","updated":"2025-10-27T23:10:06Z","published":"2025-10-27T23:10:06Z","title":"Latent Chain-of-Thought for Visual Reasoning","summary":"  Chain-of-thought (CoT) reasoning is critical for improving the\ninterpretability and reliability of Large Vision-Language Models (LVLMs).\nHowever, existing training algorithms such as SFT, PPO, and GRPO may not\ngeneralize well across unseen reasoning tasks and heavily rely on a biased\nreward model. To address this challenge, we reformulate reasoning in LVLMs as\nposterior inference and propose a scalable training algorithm based on\namortized variational inference. By leveraging diversity-seeking reinforcement\nlearning algorithms, we introduce a novel sparse reward function for\ntoken-level learning signals that encourage diverse, high-likelihood latent\nCoT, overcoming deterministic sampling limitations and avoiding reward hacking.\nAdditionally, we implement a Bayesian inference-scaling strategy that replaces\ncostly Best-of-N and Beam Search with a marginal likelihood to efficiently rank\noptimal rationales and answers. We empirically demonstrate that the proposed\nmethod enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in\nterms of effectiveness, generalization, and interpretability.\n","authors":["Guohao Sun","Hang Hua","Jian Wang","Jiebo Luo","Sohail Dianat","Majid Rabbani","Raghuveer Rao","Zhiqiang Tao"],"pdf_url":"https://arxiv.org/pdf/2510.23925v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23924v1","updated":"2025-10-27T23:09:35Z","published":"2025-10-27T23:09:35Z","title":"Agent-based Automated Claim Matching with Instruction-following LLMs","summary":"  We present a novel agent-based approach for the automated claim matching task\nwith instruction-following LLMs. We propose a two-step pipeline that first\ngenerates prompts with LLMs, to then perform claim matching as a binary\nclassification task with LLMs. We demonstrate that LLM-generated prompts can\noutperform SOTA with human-generated prompts, and that smaller LLMs can do as\nwell as larger ones in the generation process, allowing to save computational\nresources. We also demonstrate the effectiveness of using different LLMs for\neach step of the pipeline, i.e. using an LLM for prompt generation, and another\nfor claim matching. Our investigation into the prompt generation process in\nturn reveals insights into the LLMs' understanding of claim matching.\n","authors":["Dina Pisarevskaya","Arkaitz Zubiaga"],"pdf_url":"https://arxiv.org/pdf/2510.23924v1.pdf","comment":"Accepted for the International Joint Conference on Natural Language\n  Processing & Asia-Pacific Chapter of the Association for Computational\n  Linguistics (2025) Findings"},{"id":"http://arxiv.org/abs/2510.23921v1","updated":"2025-10-27T23:05:12Z","published":"2025-10-27T23:05:12Z","title":"Breaking the Benchmark: Revealing LLM Bias via Minimal Contextual\n  Augmentation","summary":"  Large Language Models have been shown to demonstrate stereotypical biases in\ntheir representations and behavior due to the discriminative nature of the data\nthat they have been trained on. Despite significant progress in the development\nof methods and models that refrain from using stereotypical information in\ntheir decision-making, recent work has shown that approaches used for bias\nalignment are brittle. In this work, we introduce a novel and general\naugmentation framework that involves three plug-and-play steps and is\napplicable to a number of fairness evaluation benchmarks. Through application\nof augmentation to a fairness evaluation dataset (Bias Benchmark for Question\nAnswering (BBQ)), we find that Large Language Models (LLMs), including\nstate-of-the-art open and closed weight models, are susceptible to\nperturbations to their inputs, showcasing a higher likelihood to behave\nstereotypically. Furthermore, we find that such models are more likely to have\nbiased behavior in cases where the target demographic belongs to a community\nless studied by the literature, underlining the need to expand the fairness and\nsafety research to include more diverse communities.\n","authors":["Kaveh Eskandari Miandoab","Mahammed Kamruzzaman","Arshia Gharooni","Gene Louis Kim","Vasanth Sarathy","Ninareh Mehrabi"],"pdf_url":"https://arxiv.org/pdf/2510.23921v1.pdf","comment":"9 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2505.17100v2","updated":"2025-10-27T22:09:54Z","published":"2025-05-21T07:23:05Z","title":"Any Large Language Model Can Be a Reliable Judge: Debiasing with a\n  Reasoning-based Bias Detector","summary":"  LLM-as-a-Judge has emerged as a promising tool for automatically evaluating\ngenerated outputs, but its reliability is often undermined by potential biases\nin judgment. Existing efforts to mitigate these biases face key limitations:\nin-context learning-based methods fail to address rooted biases due to the\nevaluator's limited capacity for self-reflection, whereas fine-tuning is not\napplicable to all evaluator types, especially closed-source models. To address\nthis challenge, we introduce the Reasoning-based Bias Detector (RBD), which is\na plug-in module that identifies biased evaluations and generates structured\nreasoning to guide evaluator self-correction. Rather than modifying the\nevaluator itself, RBD operates externally and engages in an iterative process\nof bias detection and feedback-driven revision. To support its development, we\ndesign a complete pipeline consisting of biased dataset construction,\nsupervision collection, distilled reasoning-based fine-tuning of RBD, and\nintegration with LLM evaluators. We fine-tune four sizes of RBD models, ranging\nfrom 1.5B to 14B, and observe consistent performance improvements across all\nscales. Experimental results on 4 bias types--verbosity, position, bandwagon,\nand sentiment--evaluated using 8 LLM evaluators demonstrate RBD's strong\neffectiveness. For example, the RBD-8B model improves evaluation accuracy by an\naverage of 18.5% and consistency by 10.9%, and surpasses prompting-based\nbaselines and fine-tuned judges by 12.8% and 17.2%, respectively. These results\nhighlight RBD's effectiveness and scalability. Additional experiments further\ndemonstrate its strong generalization across biases and domains, as well as its\nefficiency.\n","authors":["Haoyan Yang","Runxue Bao","Cao Xiao","Jun Ma","Parminder Bhatia","Shangqian Gao","Taha Kass-Hout"],"pdf_url":"https://arxiv.org/pdf/2505.17100v2.pdf","comment":"Accepted at NeurIPS 2025 (Camera-Ready Version)"},{"id":"http://arxiv.org/abs/2510.23896v1","updated":"2025-10-27T22:06:43Z","published":"2025-10-27T22:06:43Z","title":"AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for\n  African Languages","summary":"  Text embeddings are an essential building component of several NLP tasks such\nas retrieval-augmented generation which is crucial for preventing\nhallucinations in LLMs. Despite the recent release of massively multilingual\nMTEB (MMTEB), African languages remain underrepresented, with existing tasks\noften repurposed from translation benchmarks such as FLORES clustering or\nSIB-200. In this paper, we introduce AfriMTEB -- a regional expansion of MMTEB\ncovering 59 languages, 14 tasks, and 38 datasets, including six newly added\ndatasets. Unlike many MMTEB datasets that include fewer than five languages,\nthe new additions span 14 to 56 African languages and introduce entirely new\ntasks, such as hate speech detection, intent detection, and emotion\nclassification, which were not previously covered. Complementing this, we\npresent AfriE5, an adaptation of the instruction-tuned mE5 model to African\nlanguages through cross-lingual contrastive distillation. Our evaluation shows\nthat AfriE5 achieves state-of-the-art performance, outperforming strong\nbaselines such as Gemini-Embeddings and mE5.\n","authors":["Kosei Uemura","Miaoran Zhang","David Ifeoluwa Adelani"],"pdf_url":"https://arxiv.org/pdf/2510.23896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23884v1","updated":"2025-10-27T21:49:03Z","published":"2025-10-27T21:49:03Z","title":"Language Models for Longitudinal Clinical Prediction","summary":"  We explore a lightweight framework that adapts frozen large language models\nto analyze longitudinal clinical data. The approach integrates patient history\nand context within the language model space to generate accurate forecasts\nwithout model fine-tuning. Applied to neuropsychological assessments, it\nachieves accurate and reliable performance even with minimal training data,\nshowing promise for early-stage Alzheimer's monitoring.\n","authors":["Tananun Songdechakraiwut","Michael Lutz"],"pdf_url":"https://arxiv.org/pdf/2510.23884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23870v1","updated":"2025-10-27T21:22:41Z","published":"2025-10-27T21:22:41Z","title":"OraPlan-SQL: A Planning-Centric Framework for Complex Bilingual NL2SQL\n  Reasoning","summary":"  We present OraPlan-SQL, our system for the Archer NL2SQL Evaluation Challenge\n2025, a bilingual benchmark requiring complex reasoning such as arithmetic,\ncommonsense, and hypothetical inference. OraPlan-SQL ranked first, exceeding\nthe second-best system by more than 6% in execution accuracy (EX), with 55.0%\nin English and 56.7% in Chinese, while maintaining over 99% SQL validity (VA).\nOur system follows an agentic framework with two components: Planner agent that\ngenerates stepwise natural language plans, and SQL agent that converts these\nplans into executable SQL. Since SQL agent reliably adheres to the plan, our\nrefinements focus on the planner. Unlike prior methods that rely on multiple\nsub-agents for planning and suffer from orchestration overhead, we introduce a\nfeedback-guided meta-prompting strategy to refine a single planner. Failure\ncases from a held-out set are clustered with human input, and an LLM distills\nthem into corrective guidelines that are integrated into the planner's system\nprompt, improving generalization without added complexity. For the multilingual\nscenario, to address transliteration and entity mismatch issues, we incorporate\nentity-linking guidelines that generate alternative surface forms for entities\nand explicitly include them in the plan. Finally, we enhance reliability\nthrough plan diversification: multiple candidate plans are generated for each\nquery, with the SQL agent producing a query for each plan, and final output\nselected via majority voting over their executions.\n","authors":["Marianne Menglin Liu","Sai Ashish Somayajula","Syed Fahad Allam Shah","Sujith Ravi","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2510.23870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23868v1","updated":"2025-10-27T21:18:19Z","published":"2025-10-27T21:18:19Z","title":"GIFT: Group-relative Implicit Fine Tuning Integrates GRPO with DPO and\n  UNA","summary":"  I propose \\textbf{G}roup-relative \\textbf{I}mplicit \\textbf{F}ine\n\\textbf{T}uning (GIFT), a novel reinforcement learning framework for aligning\nLLMs. Instead of directly maximizing cumulative rewards like PPO or GRPO, GIFT\nminimizes the discrepancy between implicit and explicit reward models. It\ncombines three key ideas: (1) the online multi-response generation and\nnormalization of GRPO, (2) the implicit reward formulation of DPO, and (3) the\nimplicit-explicit reward alignment principle of UNA. By jointly normalizing the\nimplicit and explicit rewards, GIFT eliminates an otherwise intractable term\nthat prevents effective use of implicit rewards. This normalization transforms\nthe complex reward maximization objective into a simple mean squared error\n(MSE) loss between the normalized reward functions, converting a non-convex\noptimization problem into a convex, stable, and analytically differentiable\nformulation. Unlike offline methods such as DPO and UNA, GIFT remains on-policy\nand thus retains exploration capability. Compared to GRPO, it requires fewer\nhyperparameters, converges faster, and generalizes better with significantly\nreduced training overfitting. Empirically, GIFT achieves superior reasoning and\nalignment performance on mathematical benchmarks while remaining\ncomputationally efficient.\n","authors":["Zhichao Wang"],"pdf_url":"https://arxiv.org/pdf/2510.23868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23854v1","updated":"2025-10-27T20:52:19Z","published":"2025-10-27T20:52:19Z","title":"Can LLMs Narrate Tabular Data? An Evaluation Framework for Natural\n  Language Representations of Text-to-SQL System Outputs","summary":"  In modern industry systems like multi-turn chat agents, Text-to-SQL\ntechnology bridges natural language (NL) questions and database (DB) querying.\nThe conversion of tabular DB results into NL representations (NLRs) enables the\nchat-based interaction. Currently, NLR generation is typically handled by large\nlanguage models (LLMs), but information loss or errors in presenting tabular\nresults in NL remains largely unexplored. This paper introduces a novel\nevaluation method - Combo-Eval - for judgment of LLM-generated NLRs that\ncombines the benefits of multiple existing methods, optimizing evaluation\nfidelity and achieving a significant reduction in LLM calls by 25-61%.\nAccompanying our method is NLR-BIRD, the first dedicated dataset for NLR\nbenchmarking. Through human evaluations, we demonstrate the superior alignment\nof Combo-Eval with human judgments, applicable across scenarios with and\nwithout ground truth references.\n","authors":["Jyotika Singh","Weiyi Sun","Amit Agarwal","Viji Krishnamurthy","Yassine Benajiba","Sujith Ravi","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2510.23854v1.pdf","comment":"Accepted at EMNLP 2025"},{"id":"http://arxiv.org/abs/2510.23853v1","updated":"2025-10-27T20:51:58Z","published":"2025-10-27T20:51:58Z","title":"Temporal Blindness in Multi-Turn LLM Agents: Misaligned Tool Use vs.\n  Human Time Perception","summary":"  Large language model agents are increasingly used in multi-turn\nconversational settings to interact with and execute tasks in dynamic\nenvironments. However, a key limitation is their temporal blindness: they, by\ndefault, operate with a stationary context, failing to account for the\nreal-world time elapsed between messages. This becomes a critical liability\nwhen an agent must decide whether to invoke a tool based on how much time has\npassed since the last observation. Without temporal awareness, agents often\neither over-rely on previous context (skipping necessary tool calls), or\nunder-rely on it (unnecessarily repeating tool calls). To study this challenge,\nwe introduce TicToc-v1, a test set of multi-turn user-agent trajectories across\n34 scenarios with varying time sensitivity. Each trajectory ends with a user\nquestion, where the need for a tool call depends on the amount of time elapsed\nsince the last message. To give LLMs temporal context, we augment dialogue\nmessages with explicit timestamps, bridging the gap between static dialogue and\nevolving environments. We then collected human preferences for these samples,\ncreating two subsets: one where humans preferred relying on the previous\nobservation (prefer-noTool), and another where they preferred a new tool call\n(prefer-Tool). We evaluated how well LLM tool-calling decisions align with\nhuman preferences under varying time intervals on TicToc-v1. Our analysis show\nthat without time information, most models perform only slightly better than\nrandom, with the top alignment rate being just over 60%. While adding\ntimestamps leads to a slight improvement, particularly for larger models, the\nimprovement is modest, peaking at around 65%. We also show that naive,\nprompt-based alignment have limited effectiveness. Our findings highlight the\nneed for specific post-training alignment to align multi-turn LLM tool use with\nhuman temporal perception.\n","authors":["Yize Cheng","Arshia Soltani Moakhar","Chenrui Fan","Kazem Faghih","Parsa Hosseini","Wenxiao Wang","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2510.23853v1.pdf","comment":"preliminary work in progress"},{"id":"http://arxiv.org/abs/2510.23849v1","updated":"2025-10-27T20:41:52Z","published":"2025-10-27T20:41:52Z","title":"A Neural Model for Contextual Biasing Score Learning and Filtering","summary":"  Contextual biasing improves automatic speech recognition (ASR) by integrating\nexternal knowledge, such as user-specific phrases or entities, during decoding.\nIn this work, we use an attention-based biasing decoder to produce scores for\ncandidate phrases based on acoustic information extracted by an ASR encoder,\nwhich can be used to filter out unlikely phrases and to calculate bonus for\nshallow-fusion biasing. We introduce a per-token discriminative objective that\nencourages higher scores for ground-truth phrases while suppressing\ndistractors. Experiments on the Librispeech biasing benchmark show that our\nmethod effectively filters out majority of the candidate phrases, and\nsignificantly improves recognition accuracy under different biasing conditions\nwhen the scores are used in shallow fusion biasing. Our approach is modular and\ncan be used with any ASR system, and the filtering mechanism can potentially\nboost performance of other biasing methods.\n","authors":["Wanting Huang","Weiran Wang"],"pdf_url":"https://arxiv.org/pdf/2510.23849v1.pdf","comment":"Accepted to IEEE ASRU 2025"},{"id":"http://arxiv.org/abs/2510.23845v1","updated":"2025-10-27T20:32:38Z","published":"2025-10-27T20:32:38Z","title":"CRADLE Bench: A Clinician-Annotated Benchmark for Multi-Faceted Mental\n  Health Crisis and Safety Risk Detection","summary":"  Detecting mental health crisis situations such as suicide ideation, rape,\ndomestic violence, child abuse, and sexual harassment is a critical yet\nunderexplored challenge for language models. When such situations arise during\nuser--model interactions, models must reliably flag them, as failure to do so\ncan have serious consequences. In this work, we introduce CRADLE BENCH, a\nbenchmark for multi-faceted crisis detection. Unlike previous efforts that\nfocus on a limited set of crisis types, our benchmark covers seven types\ndefined in line with clinical standards and is the first to incorporate\ntemporal labels. Our benchmark provides 600 clinician-annotated evaluation\nexamples and 420 development examples, together with a training corpus of\naround 4K examples automatically labeled using a majority-vote ensemble of\nmultiple language models, which significantly outperforms single-model\nannotation. We further fine-tune six crisis detection models on subsets defined\nby consensus and unanimous ensemble agreement, providing complementary models\ntrained under different agreement criteria.\n","authors":["Grace Byun","Rebecca Lipschutz","Sean T. Minton","Abigail Lott","Jinho D. Choi"],"pdf_url":"https://arxiv.org/pdf/2510.23845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23842v1","updated":"2025-10-27T20:29:46Z","published":"2025-10-27T20:29:46Z","title":"How Pragmatics Shape Articulation: A Computational Case Study in STEM\n  ASL Discourse","summary":"  Most state-of-the-art sign language models are trained on interpreter or\nisolated vocabulary data, which overlooks the variability that characterizes\nnatural dialogue. However, human communication dynamically adapts to contexts\nand interlocutors through spatiotemporal changes and articulation style. This\nspecifically manifests itself in educational settings, where novel vocabularies\nare used by teachers, and students. To address this gap, we collect a motion\ncapture dataset of American Sign Language (ASL) STEM (Science, Technology,\nEngineering, and Mathematics) dialogue that enables quantitative comparison\nbetween dyadic interactive signing, solo signed lecture, and interpreted\narticles. Using continuous kinematic features, we disentangle dialogue-specific\nentrainment from individual effort reduction and show spatiotemporal changes\nacross repeated mentions of STEM terms. On average, dialogue signs are\n24.6%-44.6% shorter in duration than the isolated signs, and show significant\nreductions absent in monologue contexts. Finally, we evaluate sign embedding\nmodels on their ability to recognize STEM signs and approximate how entrained\nthe participants become over time. Our study bridges linguistic analysis and\ncomputational modeling to understand how pragmatics shape sign articulation and\nits representation in sign language technologies.\n","authors":["Saki Imai","Lee Kezar","Laurel Aichler","Mert Inan","Erin Walker","Alicia Wooten","Lorna Quandt","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2510.23842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23828v1","updated":"2025-10-27T20:13:32Z","published":"2025-10-27T20:13:32Z","title":"Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural\n  Processing of Figurative Language","summary":"  We present a comprehensive evaluation of the ability of large language models\n(LLMs) to process culturally grounded language, specifically to understand and\npragmatically use figurative expressions that encode local knowledge and\ncultural nuance. Using figurative language as a proxy for cultural nuance and\nlocal knowledge, we design evaluation tasks for contextual understanding,\npragmatic use, and connotation interpretation in Arabic and English. We\nevaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms,\nmultidialectal Arabic proverbs, and English proverbs. Our results show a\nconsistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower\nthan for English proverbs, and performance for Egyptian idioms is 10.28% lower\nthan for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07%\nrelative to understanding, though providing contextual idiomatic sentences\nimproves accuracy by 10.66%. Models also struggle with connotative meaning,\nreaching at most 85.58% agreement with human annotators on idioms with 100%\ninter-annotator agreement. These findings demonstrate that figurative language\nserves as an effective diagnostic for cultural reasoning: while LLMs can often\ninterpret figurative meaning, they face challenges in using it appropriately.\nTo support future research, we release Kinayat, the first dataset of Egyptian\nArabic idioms designed for both figurative understanding and pragmatic use\nevaluation.\n","authors":["Mena Attia","Aashiq Muhamed","Mai Alkhamissi","Thamar Solorio","Mona Diab"],"pdf_url":"https://arxiv.org/pdf/2510.23828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.16724v2","updated":"2025-10-27T19:23:17Z","published":"2025-10-19T06:04:53Z","title":"A Comprehensive Survey on Reinforcement Learning-based Agentic Search:\n  Foundations, Roles, Optimizations, Evaluations, and Applications","summary":"  The advent of large language models (LLMs) has transformed information access\nand reasoning through open-ended natural language interaction. However, LLMs\nremain limited by static knowledge, factual hallucinations, and the inability\nto retrieve real-time or domain-specific information. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by grounding model outputs in external\nevidence, but traditional RAG pipelines are often single turn and heuristic,\nlacking adaptive control over retrieval and reasoning. Recent advances in\nagentic search address these limitations by enabling LLMs to plan, retrieve,\nand reflect through multi-step interaction with search environments. Within\nthis paradigm, reinforcement learning (RL) offers a powerful mechanism for\nadaptive and self-improving search behavior. This survey provides the first\ncomprehensive overview of \\emph{RL-based agentic search}, organizing the\nemerging field along three complementary dimensions: (i) What RL is for\n(functional roles), (ii) How RL is used (optimization strategies), and (iii)\nWhere RL is applied (scope of optimization). We summarize representative\nmethods, evaluation protocols, and applications, and discuss open challenges\nand future directions toward building reliable and scalable RL driven agentic\nsearch systems. We hope this survey will inspire future research on the\nintegration of RL and agentic search. Our repository is available at\nhttps://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.\n","authors":["Minhua Lin","Zongyu Wu","Zhichao Xu","Hui Liu","Xianfeng Tang","Qi He","Charu Aggarwal","Hui Liu","Xiang Zhang","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2510.16724v2.pdf","comment":"38 pages, 4 figures, 7 tables"},{"id":"http://arxiv.org/abs/2504.13834v6","updated":"2025-10-27T19:17:31Z","published":"2025-04-18T17:59:29Z","title":"Science Hierarchography: Hierarchical Organization of Science Literature","summary":"  Scientific knowledge is growing rapidly, making it difficult to track\nprogress and high-level conceptual links across broad disciplines. While tools\nlike citation networks and search engines help retrieve related papers, they\nlack the abstraction needed to capture the needed to represent the density and\nstructure of activity across subfields.\n  We motivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific\nliterature into a high-quality hierarchical structure that spans multiple\nlevels of abstraction -- from broad domains to specific studies. Such a\nrepresentation can provide insights into which fields are well-explored and\nwhich are under-explored. To achieve this goal, we develop a hybrid approach\nthat combines efficient embedding-based clustering with LLM-based prompting,\nstriking a balance between scalability and semantic precision. Compared to\nLLM-heavy methods like iterative tree construction, our approach achieves\nsuperior quality-speed trade-offs. Our hierarchies capture different dimensions\nof research contributions, reflecting the interdisciplinary and multifaceted\nnature of modern science. We evaluate its utility by measuring how effectively\nan LLM-based agent can navigate the hierarchy to locate target papers. Results\nshow that our method improves interpretability and offers an alternative\npathway for exploring scientific literature beyond traditional search methods.\nCode, data and demo are available:\nhttps://github.com/JHU-CLSP/science-hierarchography\n","authors":["Muhan Gao","Jash Shah","Weiqi Wang","Kuan-Hao Huang","Daniel Khashabi"],"pdf_url":"https://arxiv.org/pdf/2504.13834v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.15870v2","updated":"2025-10-27T19:12:55Z","published":"2025-10-17T17:59:59Z","title":"OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding\n  LLM","summary":"  Advancing machine intelligence requires developing the ability to perceive\nacross multiple modalities, much as humans sense the world. We introduce\nOmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We\ncarefully study the design choices across model architecture and data curation.\nFor model architecture, we present three key innovations: (i) OmniAlignNet for\nstrengthening alignment between vision and audio embeddings in a shared\nomni-modal latent space; (ii) Temporal Embedding Grouping for capturing\nrelative temporal alignment between vision and audio signals; and (iii)\nConstrained Rotary Time Embedding for encoding absolute temporal information in\nomni-modal embeddings. We introduce a curation and synthesis pipeline that\ngenerates 24M single-modal and omni-modal conversations. We find that\nmodalities reinforce one another in both perception and reasoning. Our model,\nOmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal\nunderstanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while\nusing just 0.2T training tokens - a 6 times reduction compared to\nQwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream\napplications spanning robotics, medical AI, and smart factory.\n","authors":["Hanrong Ye","Chao-Han Huck Yang","Arushi Goel","Wei Huang","Ligeng Zhu","Yuanhang Su","Sean Lin","An-Chieh Cheng","Zhen Wan","Jinchuan Tian","Yuming Lou","Dong Yang","Zhijian Liu","Yukang Chen","Ambrish Dantrey","Ehsan Jahangiri","Sreyan Ghosh","Daguang Xu","Ehsan Hosseini-Asl","Danial Mohseni Taheri","Vidya Murali","Sifei Liu","Yao Lu","Oluwatobi Olabiyi","Yu-Chiang Frank Wang","Rafael Valle","Bryan Catanzaro","Andrew Tao","Song Han","Jan Kautz","Hongxu Yin","Pavlo Molchanov"],"pdf_url":"https://arxiv.org/pdf/2510.15870v2.pdf","comment":"Technical Report. Code: https://github.com/NVlabs/OmniVinci"},{"id":"http://arxiv.org/abs/2509.21837v3","updated":"2025-10-27T18:59:37Z","published":"2025-09-26T03:51:28Z","title":"Semantic Agreement Enables Efficient Open-Ended LLM Cascades","summary":"  Cascade systems route computational requests to smaller models when possible\nand defer to larger models only when necessary, offering a promising approach\nto balance cost and quality in LLM deployment. However, they face a fundamental\nchallenge in open-ended text generation: determining output reliability when\ngeneration quality lies on a continuous spectrum, often with multiple valid\nresponses. To address this, we propose semantic agreement -- meaning-level\nconsensus between ensemble outputs -- as a training-free signal for reliable\ndeferral. We show that when diverse model outputs agree semantically, their\nconsensus is a stronger reliability signal than token-level confidence.\nEvaluated from 500M to 70B-parameter models, we find that semantic cascades\nmatch or surpass target-model quality at 40% of the cost and reduce latency by\nup to 60%. Our method requires no model internals, works across black-box APIs,\nand remains robust to model updates, making it a practical baseline for\nreal-world LLM deployment.\n","authors":["Duncan Soiffer","Steven Kolawole","Virginia Smith"],"pdf_url":"https://arxiv.org/pdf/2509.21837v3.pdf","comment":"2025 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP) Industry Track"},{"id":"http://arxiv.org/abs/2506.06964v2","updated":"2025-10-27T18:56:23Z","published":"2025-06-08T01:59:30Z","title":"Offline RL by Reward-Weighted Fine-Tuning for Conversation Optimization","summary":"  Offline reinforcement learning (RL) is a variant of RL where the policy is\nlearned from a previously collected dataset of trajectories and rewards. In our\nwork, we propose a practical approach to offline RL with large language models\n(LLMs). We recast the problem as reward-weighted fine-tuning, which can be\nsolved using similar techniques to supervised fine-tuning (SFT). To showcase\nthe value of our approach, we apply it to learning short-horizon\nquestion-answering policies of a fixed length, where the agent reasons about\npotential answers or asks clarifying questions. Our work stands in a stark\ncontrast to state-of-the-art methods in this domain, based on SFT and direct\npreference optimization, which have additional hyper-parameters and do not\ndirectly optimize for rewards. We compare to them empirically, and report major\ngains in both optimized rewards and language quality.\n","authors":["Subhojyoti Mukherjee","Viet Dac Lai","Raghavendra Addanki","Ryan Rossi","Seunghyun Yoon","Trung Bui","Anup Rao","Jayakumar Subramanian","Branislav Kveton"],"pdf_url":"https://arxiv.org/pdf/2506.06964v2.pdf","comment":"Accepted at NeurIPS 2025 (main conference)"},{"id":"http://arxiv.org/abs/2411.14571v2","updated":"2025-10-27T18:54:02Z","published":"2024-11-21T20:36:36Z","title":"Learned, Lagged, LLM-splained: LLM Responses to End User Security\n  Questions","summary":"  Answering end user security questions is challenging. While large language\nmodels (LLMs) like GPT, LLAMA, and Gemini are far from error-free, they have\nshown promise in answering a variety of questions outside of security. We\nstudied LLM performance in the area of end user security by qualitatively\nevaluating 3 popular LLMs on 900 systematically collected end user security\nquestions.\n  While LLMs demonstrate broad generalist ``knowledge'' of end user security\ninformation, there are patterns of errors and limitations across LLMs\nconsisting of stale and inaccurate answers, and indirect or unresponsive\ncommunication styles, all of which impacts the quality of information received.\nBased on these patterns, we suggest directions for model improvement and\nrecommend user strategies for interacting with LLMs when seeking assistance\nwith security.\n","authors":["Vijay Prakash","Kevin Lee","Arkaprabha Bhattacharya","Danny Yuxing Huang","Jessica Staddon"],"pdf_url":"https://arxiv.org/pdf/2411.14571v2.pdf","comment":"17 pages, 7 tables"},{"id":"http://arxiv.org/abs/2510.23766v1","updated":"2025-10-27T18:53:08Z","published":"2025-10-27T18:53:08Z","title":"BitSkip: An Empirical Analysis of Quantization and Early Exit\n  Composition","summary":"  The pursuit of efficient Large Language Models (LLMs) has led to increasingly\ncomplex techniques like extreme quantization and dynamic routing. While\nindividual benefits of these methods are well-documented, their compositional\neffects remain poorly understood. This paper introduces BitSkip, a hybrid\narchitectural framework for systematically exploring these interactions.\nCounter-intuitively, our findings reveal that a simple 8-bit quantized model\nwithout Hadamard transform (BitSkip-V1) not only outperforms its more complex\n4-bit and Hadamard-enhanced counterparts but also competes the full-precision\nbaseline in quality (perplexity of 1.13 vs 1.19) . The introduction of Hadamard\ntransforms, even at 8-bit precision, catastrophically degraded performance by\nover 37,000%, tracing fundamental training instability. Our BitSkip-V1 recipe\ndemonstrates superior early-exit characteristics, with layer 18 providing\noptimal 32.5% speed gain for minimal 4% quality loss.\n","authors":["Ramshankar Bhuvaneswaran","Handan Liu"],"pdf_url":"https://arxiv.org/pdf/2510.23766v1.pdf","comment":"Submitted to JMLR"},{"id":"http://arxiv.org/abs/2510.23763v1","updated":"2025-10-27T18:49:03Z","published":"2025-10-27T18:49:03Z","title":"RoboOmni: Proactive Robot Manipulation in Omni-modal Context","summary":"  Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.\n","authors":["Siyin Wang","Jinlan Fu","Feihong Liu","Xinzhe He","Huangxuan Wu","Junhao Shi","Kexin Huang","Zhaoye Fei","Jingjing Gong","Zuxuan Wu","Yugang Jiang","See-Kiong Ng","Tat-Seng Chua","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2510.23763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00789v3","updated":"2025-10-27T18:46:06Z","published":"2025-06-01T02:42:36Z","title":"RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented\n  Generation Systems","summary":"  Retrieval-Augmented Generation (RAG) enhances recency and factuality in\nanswers. However, existing evaluations rarely test how well these systems cope\nwith real-world noise, conflicting between internal and external retrieved\ncontexts, or fast-changing facts. We introduce Retrieval-Aware Robustness\nEvaluation (RARE), a unified framework and large-scale benchmark that jointly\nstress-tests query and document perturbations over dynamic, time-sensitive\ncorpora. One of the central features of RARE is a knowledge-graph-driven\nsynthesis pipeline (RARE-Get) that automatically extracts single and multi-hop\nrelations from the customized corpus and generates multi-level question sets\nwithout manual intervention. Leveraging this pipeline, we construct a dataset\n(RARE-Set) spanning 527 expert-level time-sensitive finance, economics, and\npolicy documents and 48295 questions whose distribution evolves as the\nunderlying sources change. To quantify resilience, we formalize\nretrieval-conditioned robustness metrics (RARE-Met) that capture a model's\nability to remain correct or recover when queries, documents, or real-world\nretrieval results are systematically altered. Our findings reveal that RAG\nsystems are unexpectedly sensitive to perturbations. Moreover, they\nconsistently demonstrate lower robustness on multi-hop queries compared to\nsingle-hop queries across all domains.\n","authors":["Yixiao Zeng","Tianyu Cao","Danqing Wang","Xinran Zhao","Zimeng Qiu","Morteza Ziyadi","Tongshuang Wu","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2506.00789v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.14205v2","updated":"2025-10-27T18:45:42Z","published":"2025-10-16T01:26:38Z","title":"DPRF: A Generalizable Dynamic Persona Refinement Framework for\n  Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents\n  and Humans","summary":"  The emerging large language model role-playing agents (LLM RPAs) aim to\nsimulate individual human behaviors, but the persona fidelity is often\nundermined by manually-created profiles (e.g., cherry-picked information and\npersonality characteristics) without validating the alignment with the target\nindividuals. To address this limitation, our work introduces the Dynamic\nPersona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLM\nRPAs' behaviors with those of target individuals by iteratively identifying the\ncognitive divergence, either through free-form or theory-grounded, structured\nanalysis, between generated behaviors and human ground truth, and refining the\npersona profile to mitigate these divergences.We evaluate DPRF with five LLMs\non four diverse behavior-prediction scenarios: formal debates, social media\nposts with mental health issues, public interviews, and movie reviews.DPRF can\nconsistently improve behavioral alignment considerably over baseline personas\nand generalizes across models and scenarios.Our work provides a robust\nmethodology for creating high-fidelity persona profiles and enhancing the\nvalidity of downstream applications, such as user simulation, social studies,\nand personalized AI.\n","authors":["Bingsheng Yao","Bo Sun","Yuanzhe Dong","Yuxuan Lu","Dakuo Wang"],"pdf_url":"https://arxiv.org/pdf/2510.14205v2.pdf","comment":"In Submission"},{"id":"http://arxiv.org/abs/2510.23730v1","updated":"2025-10-27T18:03:50Z","published":"2025-10-27T18:03:50Z","title":"Evaluating Long-Term Memory for Long-Context Question Answering","summary":"  In order for large language models to achieve true conversational continuity\nand benefit from experiential learning, they need memory. While research has\nfocused on the development of complex memory systems, it remains unclear which\ntypes of memory are most effective for long-context conversational tasks. We\npresent a systematic evaluation of memory-augmented methods using LoCoMo, a\nbenchmark of synthetic long-context dialogues annotated for question-answering\ntasks that require diverse reasoning strategies. We analyse full-context\nprompting, semantic memory through retrieval-augmented generation and agentic\nmemory, episodic memory through in-context learning, and procedural memory\nthrough prompt optimization. Our findings show that memory-augmented approaches\nreduce token usage by over 90% while maintaining competitive accuracy. Memory\narchitecture complexity should scale with model capability, with small\nfoundation models benefitting most from RAG, and strong instruction-tuned\nreasoning model gaining from episodic learning through reflections and more\ncomplex agentic semantic memory. In particular, episodic memory can help LLMs\nrecognise the limits of their own knowledge.\n","authors":["Alessandra Terranova","BjÃ¶rn Ross","Alexandra Birch"],"pdf_url":"https://arxiv.org/pdf/2510.23730v1.pdf","comment":"14 pages including appendix, 3 figures. Submitted to October ARR and\n  to Metacognition in Generative AI EurIPS workshop (under review for both)"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2510.23607v1","updated":"2025-10-27T17:59:59Z","published":"2025-10-27T17:59:59Z","title":"Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial\n  Representations","summary":"  Humans learn abstract concepts through multisensory synergy, and once formed,\nsuch representations can often be recalled from a single modality. Inspired by\nthis principle, we introduce Concerto, a minimalist simulation of human concept\nlearning for spatial cognition, combining 3D intra-modal self-distillation with\n2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more\ncoherent and informative spatial features, as demonstrated by zero-shot\nvisualizations. It outperforms both standalone SOTA 2D and 3D self-supervised\nmodels by 14.2% and 4.8%, respectively, as well as their feature concatenation,\nin linear probing for 3D scene perception. With full fine-tuning, Concerto sets\nnew SOTA results across multiple scene understanding benchmarks (e.g., 80.7%\nmIoU on ScanNet). We further present a variant of Concerto tailored for\nvideo-lifted point cloud spatial understanding, and a translator that linearly\nprojects Concerto representations into CLIP's language space, enabling\nopen-world perception. These results highlight that Concerto emerges spatial\nrepresentations with superior fine-grained geometric and semantic consistency.\n","authors":["Yujia Zhang","Xiaoyang Wu","Yixing Lao","Chengyao Wang","Zhuotao Tian","Naiyan Wang","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2510.23607v1.pdf","comment":"NeurIPS 2025, produced by Pointcept, project page:\n  https://pointcept.github.io/Concerto"},{"id":"http://arxiv.org/abs/2510.23605v1","updated":"2025-10-27T17:59:51Z","published":"2025-10-27T17:59:51Z","title":"Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with\n  Progressive Texture Infilling","summary":"  Current 3D/4D generation methods are usually optimized for photorealism,\nefficiency, and aesthetics. However, they often fail to preserve the semantic\nidentity of the subject across different viewpoints. Adapting generation\nmethods with one or few images of a specific subject (also known as\nPersonalization or Subject-driven generation) allows generating visual content\nthat align with the identity of the subject. However, personalized 3D/4D\ngeneration is still largely underexplored. In this work, we introduce TIRE\n(Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation.\nIt takes an initial 3D asset produced by an existing 3D generative model as\ninput and uses video tracking to identify the regions that need to be modified.\nThen, we adopt a subject-driven 2D inpainting model for progressively infilling\nthe identified regions. Finally, we resplat the modified 2D multi-view\nobservations back to 3D while still maintaining consistency. Extensive\nexperiments demonstrate that our approach significantly improves identity\npreservation in 3D/4D generation compared to state-of-the-art methods. Our\nproject website is available at\nhttps://zsh2000.github.io/track-inpaint-resplat.github.io/.\n","authors":["Shuhong Zheng","Ashkan Mirzaei","Igor Gilitschenski"],"pdf_url":"https://arxiv.org/pdf/2510.23605v1.pdf","comment":"NeurIPS 2025, 38 pages, 22 figures"},{"id":"http://arxiv.org/abs/2510.23603v1","updated":"2025-10-27T17:59:32Z","published":"2025-10-27T17:59:32Z","title":"PixelRefer: A Unified Framework for Spatio-Temporal Object Referring\n  with Arbitrary Granularity","summary":"  Multimodal large language models (MLLMs) have demonstrated strong\ngeneral-purpose capabilities in open-world visual comprehension. However, most\nexisting MLLMs primarily focus on holistic, scene-level understanding, often\noverlooking the need for fine-grained, object-centric reasoning. In this paper,\nwe present PixelRefer, a unified region-level MLLM framework that enables\nadvanced fine-grained understanding over user-specified regions across both\nimages and videos. Motivated by the observation that LLM attention\npredominantly focuses on object-level tokens, we propose a Scale-Adaptive\nObject Tokenizer (SAOT) to generate compact and semantically rich object\nrepresentations from free-form regions. Our analysis reveals that global visual\ntokens contribute mainly in early LLM layers, inspiring the design of\nPixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion\nmodule to pre-fuse global context into object tokens. This yields a lightweight\nObject-Only Framework that substantially reduces computational cost while\nmaintaining high semantic fidelity. To facilitate fine-grained instruction\ntuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction\ndataset. Extensive experiments across a range of benchmarks validate that\nPixelRefer achieves leading performance with fewer training samples, while\nPixelRefer-Lite offers competitive accuracy with notable gains in efficiency.\n","authors":["Yuqian Yuan","Wenqiao Zhang","Xin Li","Shihao Wang","Kehan Li","Wentong Li","Jun Xiao","Lei Zhang","Beng Chin Ooi"],"pdf_url":"https://arxiv.org/pdf/2510.23603v1.pdf","comment":"22 pages, 13 figures"},{"id":"http://arxiv.org/abs/2510.23594v1","updated":"2025-10-27T17:57:52Z","published":"2025-10-27T17:57:52Z","title":"PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error\n  Detection","summary":"  We introduce \\textbf{PRISM-Bench}, a benchmark of puzzle-based visual\nchallenges designed to evaluate not only whether models can solve problems, but\nhow their reasoning unfolds. Unlike prior evaluations that measure only\nfinal-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual\npuzzle and a step-by-step chain-of-thought (CoT) containing exactly one error,\nmodels must identify the first incorrect step. This setting enables\nfine-grained assessment of logical consistency, error detection, and visual\nreasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric,\nand analogical reasoning, resisting shortcuts based on superficial pattern\nmatching. Evaluations across state-of-the-art MLLMs reveal a persistent gap\nbetween fluent generation and faithful reasoning: models that produce plausible\nCoTs often fail to locate simple logical faults. By disentangling answer\ngeneration from reasoning verification, PRISM-Bench offers a sharper lens on\nmultimodal reasoning competence and underscores the need for diagnostic\nevaluation protocols in the development of trustworthy MLLMs.\n","authors":["Yusu Qian","Cheng Wan","Chao Jia","Yinfei Yang","Qingyu Zhao","Zhe Gan"],"pdf_url":"https://arxiv.org/pdf/2510.23594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23589v1","updated":"2025-10-27T17:54:57Z","published":"2025-10-27T17:54:57Z","title":"InFlux: A Benchmark for Self-Calibration of Dynamic Intrinsics of Video\n  Cameras","summary":"  Accurately tracking camera intrinsics is crucial for achieving 3D\nunderstanding from 2D video. However, most 3D algorithms assume that camera\nintrinsics stay constant throughout a video, which is often not true for many\nreal-world in-the-wild videos. A major obstacle in this field is a lack of\ndynamic camera intrinsics benchmarks--existing benchmarks typically offer\nlimited diversity in scene content and intrinsics variation, and none provide\nper-frame intrinsic changes for consecutive video frames. In this paper, we\npresent Intrinsics in Flux (InFlux), a real-world benchmark that provides\nper-frame ground truth intrinsics annotations for videos with dynamic\nintrinsics. Compared to prior benchmarks, InFlux captures a wider range of\nintrinsic variations and scene diversity, featuring 143K+ annotated frames from\n386 high-resolution indoor and outdoor videos with dynamic camera intrinsics.\nTo ensure accurate per-frame intrinsics, we build a comprehensive lookup table\nof calibration experiments and extend the Kalibr toolbox to improve its\naccuracy and robustness. Using our benchmark, we evaluate existing baseline\nmethods for predicting camera intrinsics and find that most struggle to achieve\naccurate predictions on videos with dynamic intrinsics. For the dataset, code,\nvideos, and submission, please visit https://influx.cs.princeton.edu/.\n","authors":["Erich Liang","Roma Bhattacharjee","Sreemanti Dey","Rafael Moschopoulos","Caitlin Wang","Michel Liao","Grace Tan","Andrew Wang","Karhan Kayan","Stamatis Alexandropoulos","Jia Deng"],"pdf_url":"https://arxiv.org/pdf/2510.23589v1.pdf","comment":"Accepted at NeurIPS 2025 DB Track, Camera Ready Version.\n  Supplementary material included"},{"id":"http://arxiv.org/abs/2510.23588v1","updated":"2025-10-27T17:54:08Z","published":"2025-10-27T17:54:08Z","title":"FARMER: Flow AutoRegressive Transformer over Pixels","summary":"  Directly modeling the explicit likelihood of the raw data distribution is key\ntopic in the machine learning area, which achieves the scaling successes in\nLarge Language Models by autoregressive modeling. However, continuous AR\nmodeling over visual pixel data suffer from extremely long sequences and\nhigh-dimensional spaces. In this paper, we present FARMER, a novel end-to-end\ngenerative framework that unifies Normalizing Flows (NF) and Autoregressive\n(AR) models for tractable likelihood estimation and high-quality image\nsynthesis directly from raw pixels. FARMER employs an invertible autoregressive\nflow to transform images into latent sequences, whose distribution is modeled\nimplicitly by an autoregressive model. To address the redundancy and complexity\nin pixel-level modeling, we propose a self-supervised dimension reduction\nscheme that partitions NF latent channels into informative and redundant\ngroups, enabling more effective and efficient AR modeling. Furthermore, we\ndesign a one-step distillation scheme to significantly accelerate inference\nspeed and introduce a resampling-based classifier-free guidance algorithm to\nboost image generation quality. Extensive experiments demonstrate that FARMER\nachieves competitive performance compared to existing pixel-based generative\nmodels while providing exact likelihoods and scalable training.\n","authors":["Guangting Zheng","Qinyu Zhao","Tao Yang","Fei Xiao","Zhijie Lin","Jie Wu","Jiajun Deng","Yanyong Zhang","Rui Zhu"],"pdf_url":"https://arxiv.org/pdf/2510.23588v1.pdf","comment":"Bytedance Seed Technical Report"},{"id":"http://arxiv.org/abs/2510.20820v2","updated":"2025-10-27T17:53:30Z","published":"2025-10-23T17:59:55Z","title":"LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered\n  Canvas","summary":"  Despite their impressive visual fidelity, existing personalized generative\nmodels lack interactive control over spatial composition and scale poorly to\nmultiple subjects. To address these limitations, we present LayerComposer, an\ninteractive framework for personalized, multi-subject text-to-image generation.\nOur approach introduces two main contributions: (1) a layered canvas, a novel\nrepresentation in which each subject is placed on a distinct layer, enabling\nocclusion-free composition; and (2) a locking mechanism that preserves selected\nlayers with high fidelity while allowing the remaining layers to adapt flexibly\nto the surrounding context. Similar to professional image-editing software, the\nproposed layered canvas allows users to place, resize, or lock input subjects\nthrough intuitive layer manipulation. Our versatile locking mechanism requires\nno architectural changes, relying instead on inherent positional embeddings\ncombined with a new complementary data sampling strategy. Extensive experiments\ndemonstrate that LayerComposer achieves superior spatial control and identity\npreservation compared to the state-of-the-art methods in multi-subject\npersonalized image generation.\n","authors":["Guocheng Gordon Qian","Ruihang Zhang","Tsai-Shien Chen","Yusuf Dalva","Anujraaj Argo Goyal","Willi Menapace","Ivan Skorokhodov","Meng Dong","Arpit Sahni","Daniil Ostashev","Ju Hu","Sergey Tulyakov","Kuan-Chieh Jackson Wang"],"pdf_url":"https://arxiv.org/pdf/2510.20820v2.pdf","comment":"9 pages, preprint. Project page:\n  https://snap-research.github.io/layercomposer/"},{"id":"http://arxiv.org/abs/2507.22030v2","updated":"2025-10-27T17:51:47Z","published":"2025-07-29T17:27:15Z","title":"ReXGroundingCT: A 3D Chest CT Dataset for Segmentation of Findings from\n  Free-Text Reports","summary":"  We introduce ReXGroundingCT, the first publicly available dataset linking\nfree-text findings to pixel-level 3D segmentations in chest CT scans. The\ndataset includes 3,142 non-contrast chest CT scans paired with standardized\nradiology reports from CT-RATE. Construction followed a structured three-stage\npipeline. First, GPT-4 was used to extract and standardize findings,\ndescriptors, and metadata from reports originally written in Turkish and\nmachine-translated into English. Second, GPT-4o-mini categorized each finding\ninto a hierarchical ontology of lung and pleural abnormalities. Third, 3D\nannotations were produced for all CT volumes: the training set was\nquality-assured by board-certified radiologists, and the validation and test\nsets were fully annotated by board-certified radiologists. Additionally, a\ncomplementary chain-of-thought dataset was created to provide step-by-step\nhierarchical anatomical reasoning for localizing findings within the CT volume,\nusing GPT-4o and localization coordinates derived from organ segmentation\nmodels. ReXGroundingCT contains 16,301 annotated entities across 8,028\ntext-to-3D-segmentation pairs, covering diverse radiological patterns from\n3,142 non-contrast CT scans. About 79% of findings are focal abnormalities and\n21% are non-focal. The dataset includes a public validation set of 50 cases and\na private test set of 100 cases, both annotated by board-certified\nradiologists. The dataset establishes a foundation for enabling free-text\nfinding segmentation and grounded radiology report generation in CT imaging.\nModel performance on the private test set is hosted on a public leaderboard at\nhttps://rexrank.ai/ReXGroundingCT. The dataset is available at\nhttps://huggingface.co/datasets/rajpurkarlab/ReXGroundingCT.\n","authors":["Mohammed Baharoon","Luyang Luo","Michael Moritz","Abhinav Kumar","Sung Eun Kim","Xiaoman Zhang","Miao Zhu","Mahmoud Hussain Alabbad","Maha Sbayel Alhazmi","Neel P. Mistry","Lucas Bijnens","Kent Ryan Kleinschmidt","Brady Chrisler","Sathvik Suryadevara","Sri Sai Dinesh Jaliparthi","Noah Michael Prudlo","Mark David Marino","Jeremy Palacio","Rithvik Akula","Di Zhou","Hong-Yu Zhou","Ibrahim Ethem Hamamci","Scott J. Adams","Hassan Rayhan AlOmaish","Pranav Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2507.22030v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23943v1","updated":"2025-10-27T23:52:46Z","published":"2025-10-27T23:52:46Z","title":"Adaptive Training of INRs via Pruning and Densification","summary":"  Encoding input coordinates with sinusoidal functions into multilayer\nperceptrons (MLPs) has proven effective for implicit neural representations\n(INRs) of low-dimensional signals, enabling the modeling of high-frequency\ndetails. However, selecting appropriate input frequencies and architectures\nwhile managing parameter redundancy remains an open challenge, often addressed\nthrough heuristics and heavy hyperparameter optimization schemes. In this\npaper, we introduce AIRe ($\\textbf{A}$daptive $\\textbf{I}$mplicit neural\n$\\textbf{Re}$presentation), an adaptive training scheme that refines the INR\narchitecture over the course of optimization. Our method uses a neuron pruning\nmechanism to avoid redundancy and input frequency densification to improve\nrepresentation capacity, leading to an improved trade-off between network size\nand reconstruction quality. For pruning, we first identify less-contributory\nneurons and apply a targeted weight decay to transfer their information to the\nremaining neurons, followed by structured pruning. Next, the densification\nstage adds input frequencies to spectrum regions where the signal underfits,\nexpanding the representational basis. Through experiments on images and SDFs,\nwe show that AIRe reduces model size while preserving, or even improving,\nreconstruction quality. Code and pretrained models will be released for public\nuse.\n","authors":["Diana Aldana","JoÃ£o Paulo Lima","Daniel Csillag","Daniel Perazzo","Haoan Feng","Luiz Velho","Tiago Novello"],"pdf_url":"https://arxiv.org/pdf/2510.23943v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06456v3","updated":"2025-10-27T23:40:26Z","published":"2025-03-09T05:30:15Z","title":"DynCIM: Dynamic Curriculum for Imbalanced Multimodal Learning","summary":"  Multimodal learning integrates complementary information from diverse\nmodalities to enhance the decision-making process. However, the potential of\nmultimodal collaboration remains under-exploited due to disparities in data\nquality and modality representation capabilities. To address this, we introduce\nDynCIM, a novel dynamic curriculum learning framework designed to quantify the\ninherent imbalances from both sample and modality perspectives. DynCIM employs\na sample-level curriculum to dynamically assess each sample's difficulty\naccording to prediction deviation, consistency, and stability, while a\nmodality-level curriculum measures modality contributions from global and\nlocal. Furthermore, a gating-based dynamic fusion mechanism is introduced to\nadaptively adjust modality contributions, minimizing redundancy and optimizing\nfusion effectiveness. Extensive experiments on six multimodal benchmarking\ndatasets, spanning both bimodal and trimodal scenarios, demonstrate that DynCIM\nconsistently outperforms state-of-the-art methods. Our approach effectively\nmitigates modality and sample imbalances while enhancing adaptability and\nrobustness in multimodal learning tasks. Our code is available at\nhttps://github.com/Raymond-Qiancx/DynCIM.\n","authors":["Chengxuan Qian","Kai Han","Jiaxin Liu","Zhenlong Yuan","Zhengzhong Zhu","Jingchao Wang","Chongwen Lyu","Jun Chen","Zhe Liu"],"pdf_url":"https://arxiv.org/pdf/2503.06456v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11049v2","updated":"2025-10-27T23:33:48Z","published":"2025-02-16T09:23:16Z","title":"Faces of Fairness: Examining Bias in Facial Expression Recognition\n  Datasets and Models","summary":"  Building AI systems, including Facial Expression Recognition (FER), involves\ntwo critical aspects: data and model design. Both components significantly\ninfluence bias and fairness in FER tasks. Issues related to bias and fairness\nin FER datasets and models remain underexplored. This study investigates bias\nsources in FER datasets and models. Four common FER datasets--AffectNet, ExpW,\nFer2013, and RAF-DB--are analyzed. The findings demonstrate that AffectNet and\nExpW exhibit high generalizability despite data imbalances. Additionally, this\nresearch evaluates the bias and fairness of six deep models, including three\nstate-of-the-art convolutional neural network (CNN) models: MobileNet, ResNet,\nXceptionNet, as well as three transformer-based models: ViT, CLIP, and\nGPT-4o-mini. Experimental results reveal that while GPT-4o-mini and ViT achieve\nthe highest accuracy scores, they also display the highest levels of bias.\nThese findings underscore the urgent need for developing new methodologies to\nmitigate bias and ensure fairness in datasets and models, particularly in\naffective computing applications. See our implementation details at\nhttps://github.com/MMHosseini/bias_in_FER.\n","authors":["Mohammad Mehdi Hosseini","Ali Pourramezan Fard","Mohammad H. Mahoor"],"pdf_url":"https://arxiv.org/pdf/2502.11049v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23930v1","updated":"2025-10-27T23:32:19Z","published":"2025-10-27T23:32:19Z","title":"PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by\n  Vision-Language Planar Priors","summary":"  Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an\nefficient representation for novel-view synthesis, achieving impressive visual\nquality. However, in scenes dominated by large and low-texture regions, common\nin indoor environments, the photometric loss used to optimize 3DGS yields\nambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome\nthis limitation, we introduce PlanarGS, a 3DGS-based framework tailored for\nindoor scene reconstruction. Specifically, we design a pipeline for\nLanguage-Prompted Planar Priors (LP3) that employs a pretrained vision-language\nsegmentation model and refines its region proposals via cross-view fusion and\ninspection with geometric priors. 3D Gaussians in our framework are optimized\nwith two additional terms: a planar prior supervision term that enforces planar\nconsistency, and a geometric prior supervision term that steers the Gaussians\ntoward the depth and normal cues. We have conducted extensive experiments on\nstandard indoor benchmarks. The results show that PlanarGS reconstructs\naccurate and detailed 3D surfaces, consistently outperforming state-of-the-art\nmethods by a large margin. Project page: https://planargs.github.io\n","authors":["Xirui Jin","Renbiao Jin","Boying Li","Danping Zou","Wenxian Yu"],"pdf_url":"https://arxiv.org/pdf/2510.23930v1.pdf","comment":"Accepted by NeurIPS 2025. Project page: https://planargs.github.io"},{"id":"http://arxiv.org/abs/2510.23929v1","updated":"2025-10-27T23:28:11Z","published":"2025-10-27T23:28:11Z","title":"TurboPortrait3D: Single-step diffusion-based fast portrait novel-view\n  synthesis","summary":"  We introduce TurboPortrait3D: a method for low-latency novel-view synthesis\nof human portraits. Our approach builds on the observation that existing\nimage-to-3D models for portrait generation, while capable of producing\nrenderable 3D representations, are prone to visual artifacts, often lack of\ndetail, and tend to fail at fully preserving the identity of the subject. On\nthe other hand, image diffusion models excel at generating high-quality images,\nbut besides being computationally expensive, are not grounded in 3D and thus\nare not directly capable of producing multi-view consistent outputs. In this\nwork, we demonstrate that image-space diffusion models can be used to\nsignificantly enhance the quality of existing image-to-avatar methods, while\nmaintaining 3D-awareness and running with low-latency. Our method takes a\nsingle frontal image of a subject as input, and applies a feedforward\nimage-to-avatar generation pipeline to obtain an initial 3D representation and\ncorresponding noisy renders. These noisy renders are then fed to a single-step\ndiffusion model which is conditioned on input image(s), and is specifically\ntrained to refine the renders in a multi-view consistent way. Moreover, we\nintroduce a novel effective training strategy that includes pre-training on a\nlarge corpus of synthetic multi-view data, followed by fine-tuning on\nhigh-quality real images. We demonstrate that our approach both qualitatively\nand quantitatively outperforms current state-of-the-art for portrait novel-view\nsynthesis, while being efficient in time.\n","authors":["Emily Kim","Julieta Martinez","Timur Bagautdinov","Jessica Hodgins"],"pdf_url":"https://arxiv.org/pdf/2510.23929v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2510.23544v1","updated":"2025-10-27T17:19:37Z","published":"2025-10-27T17:19:37Z","title":"LimRank: Less is More for Reasoning-Intensive Information Reranking","summary":"  Existing approaches typically rely on large-scale fine-tuning to adapt LLMs\nfor information reranking tasks, which is computationally expensive. In this\nwork, we demonstrate that modern LLMs can be effectively adapted using only\nminimal, high-quality supervision. To enable this, we design\nLIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating\ndiverse, challenging, and realistic reranking examples. Using this synthetic\ndata, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two\nchallenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and\nFollowIR for instruction-following retrieval. Our experiments demonstrate that\nLIMRANK achieves competitive performance, while being trained on less than 5%\nof the data typically used in prior work. Further ablation studies demonstrate\nthe effectiveness of LIMRANK-SYNTHESIZER and the strong generalization\ncapabilities of LIMRANK across downstream tasks, including scientific\nliterature search and retrieval-augmented generation for knowledge-intensive\nproblem solving.\n","authors":["Tingyu Song","Yilun Zhao","Siyue Zhang","Chen Zhao","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2510.23544v1.pdf","comment":"EMNLP 2025 Main (Short)"},{"id":"http://arxiv.org/abs/2509.16599v3","updated":"2025-10-27T16:02:27Z","published":"2025-09-20T09:50:18Z","title":"Computational-Assisted Systematic Review and Meta-Analysis (CASMA):\n  Effect of a Subclass of GnRH-a on Endometriosis Recurrence","summary":"  Background: Evidence synthesis facilitates evidence-based medicine. This task\nbecomes increasingly difficult to accomplished with applying computational\nsolutions, since the medical literature grows at astonishing rates. Objective:\nThis study evaluates an information retrieval-driven workflow, CASMA, to\nenhance the efficiency, transparency, and reproducibility of systematic\nreviews. Endometriosis recurrence serves as the ideal case due to its complex\nand ambiguous literature. Methods: The hybrid approach integrates PRISMA\nguidelines with fuzzy matching and regular expression (regex) to facilitate\nsemi-automated deduplication and filtered records before manual screening. The\nworkflow synthesised evidence from randomised controlled trials on the efficacy\nof a subclass of gonadotropin-releasing hormone agonists (GnRH-a). A modified\nsplitting method addressed unit-of-analysis errors in multi-arm trials.\nResults: The workflow sharply reduced the screening workload, taking only 11\ndays to fetch and filter 33,444 records. Seven eligible RCTs were synthesized\n(841 patients). The pooled random-effects model yielded a Risk Ratio (RR) of\n$0.64$ ($95\\%$ CI $0.48$ to $0.86$), demonstrating a $36\\%$ reduction in\nrecurrence, with non-significant heterogeneity ($I^2=0.00\\%$, $\\tau^2=0.00$).\nThe findings were robust and stable, as they were backed by sensitivity\nanalyses. Conclusion: This study demonstrates an application of an\ninformation-retrieval-driven workflow for medical evidence synthesis. The\napproach yields valuable clinical results and a generalisable framework to\nscale up the evidence synthesis, bridging the gap between clinical research and\ncomputer science.\n","authors":["Sandro Tsang"],"pdf_url":"https://arxiv.org/pdf/2509.16599v3.pdf","comment":"15 pages, 12 figures and 4 tables. This work describes an information\n  retrieval-driven workflow for medical evidence synthesis, with an application\n  to endometriosis recurrence. The method can be generalized to other\n  systematic reviews. The preregistered protocol is available:\n  https://doi.org/10.17605/OSF.IO/R2DFA"},{"id":"http://arxiv.org/abs/2507.12311v4","updated":"2025-10-27T14:28:50Z","published":"2025-07-16T15:06:29Z","title":"An Ecosystem for Ontology Interoperability","summary":"  Ontology interoperability is one of the complicated issues that restricts the\nuse of ontologies in knowledge graphs (KGs). Different ontologies with\nconflicting and overlapping concepts make it difficult to design, develop, and\ndeploy an interoperable ontology for downstream tasks. We propose an ecosystem\nfor ontology interoperability. The ecosystem employs three state-of-the-art\nsemantic techniques in different phases of the ontology engineering life cycle:\nontology design patterns (ODPs) in the design phase, ontology matching and\nversioning (OM\\&OV) in the develop phase, and ontology-compliant knowledge\ngraphs (OCKGs) in the deploy phase, to achieve better ontology interoperability\nand data integration in real-world applications. A case study of sensor\nobservation in the building domain validates the usefulness of the proposed\necosystem.\n","authors":["Zhangcheng Qiang"],"pdf_url":"https://arxiv.org/pdf/2507.12311v4.pdf","comment":"5 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.15807v2","updated":"2025-10-27T13:12:32Z","published":"2025-05-21T17:59:01Z","title":"The Atlas of In-Context Learning: How Attention Heads Shape In-Context\n  Retrieval Augmentation","summary":"  Large language models are able to exploit in-context learning to access\nexternal knowledge beyond their training data through retrieval-augmentation.\nWhile promising, its inner workings remain unclear. In this work, we shed light\non the mechanism of in-context retrieval augmentation for question answering by\nviewing a prompt as a composition of informational components. We propose an\nattribution-based method to identify specialized attention heads, revealing\nin-context heads that comprehend instructions and retrieve relevant contextual\ninformation, and parametric heads that store entities' relational knowledge. To\nbetter understand their roles, we extract function vectors and modify their\nattention weights to show how they can influence the answer generation process.\nFinally, we leverage the gained insights to trace the sources of knowledge used\nduring inference, paving the way towards more safe and transparent language\nmodels.\n","authors":["Patrick Kahardipraja","Reduan Achtibat","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2505.15807v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2508.12365v2","updated":"2025-10-27T13:03:18Z","published":"2025-08-17T13:48:48Z","title":"TaoSR1: The Thinking Model for E-commerce Relevance Search","summary":"  Query-product relevance prediction is a core task in e-commerce search.\nBERT-based models excel at semantic matching but lack complex reasoning\ncapabilities. While Large Language Models (LLMs) are explored, most still use\ndiscriminative fine-tuning or distill to smaller models for deployment. We\npropose a framework to directly deploy LLMs for this task, addressing key\nchallenges: Chain-of-Thought (CoT) error accumulation, discriminative\nhallucination, and deployment feasibility. Our framework, TaoSR1, involves\nthree stages: (1) Supervised Fine-Tuning (SFT) with CoT to instill reasoning;\n(2) Offline sampling with a pass@N strategy and Direct Preference Optimization\n(DPO) to improve generation quality; and (3) Difficulty-based dynamic sampling\nwith Group Relative Policy Optimization (GRPO) to mitigate discriminative\nhallucination. Additionally, post-CoT processing and a cumulative\nprobability-based partitioning method enable efficient online deployment.\nTaoSR1 significantly outperforms baselines on offline datasets and achieves\nsubstantial gains in online side-by-side human evaluations, introducing a novel\nparadigm for applying CoT reasoning to relevance classification.\n","authors":["Chenhe Dong","Shaowei Yao","Pengkun Jiao","Jianhui Yang","Yiming Jin","Zerui Huang","Xiaojiang Zhou","Dan Ou","Haihong Tang","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2508.12365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.18936v2","updated":"2025-10-27T12:01:51Z","published":"2025-10-21T17:52:13Z","title":"SBAN: A Framework & Multi-Dimensional Dataset for Large Language Model\n  Pre-Training and Software Code Mining","summary":"  This paper introduces SBAN (Source code, Binary, Assembly, and Natural\nLanguage Description), a large-scale, multi-dimensional dataset designed to\nadvance the pre-training and evaluation of large language models (LLMs) for\nsoftware code analysis. SBAN comprises more than 3 million samples, including\n2.9 million benign and 672,000 malware respectively, each represented across\nfour complementary layers: binary code, assembly instructions, natural language\ndescriptions, and source code. This unique multimodal structure enables\nresearch on cross-representation learning, semantic understanding of software,\nand automated malware detection. Beyond security applications, SBAN supports\nbroader tasks such as code translation, code explanation, and other software\nmining tasks involving heterogeneous data. It is particularly suited for\nscalable training of deep models, including transformers and other LLM\narchitectures. By bridging low-level machine representations and high-level\nhuman semantics, SBAN provides a robust foundation for building intelligent\nsystems that reason about code. We believe that this dataset opens new\nopportunities for mining software behavior, improving security analytics, and\nenhancing LLM capabilities in pre-training and fine-tuning tasks for software\ncode mining.\n","authors":["Hamed Jelodar","Mohammad Meymani","Samita Bai","Roozbeh Razavi-Far","Ali A. Ghorbani"],"pdf_url":"https://arxiv.org/pdf/2510.18936v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23224v1","updated":"2025-10-27T11:22:28Z","published":"2025-10-27T11:22:28Z","title":"Accurate and Scalable Multimodal Pathology Retrieval via Attentive\n  Vision-Language Alignment","summary":"  The rapid digitization of histopathology slides has opened up new\npossibilities for computational tools in clinical and research workflows. Among\nthese, content-based slide retrieval stands out, enabling pathologists to\nidentify morphologically and semantically similar cases, thereby supporting\nprecise diagnoses, enhancing consistency across observers, and assisting\nexample-based education. However, effective retrieval of whole slide images\n(WSIs) remains challenging due to their gigapixel scale and the difficulty of\ncapturing subtle semantic differences amid abundant irrelevant content. To\novercome these challenges, we present PathSearch, a retrieval framework that\nunifies fine-grained attentive mosaic representations with global-wise slide\nembeddings aligned through vision-language contrastive learning. Trained on a\ncorpus of 6,926 slide-report pairs, PathSearch captures both fine-grained\nmorphological cues and high-level semantic patterns to enable accurate and\nflexible retrieval. The framework supports two key functionalities: (1)\nmosaic-based image-to-image retrieval, ensuring accurate and efficient slide\nresearch; and (2) multi-modal retrieval, where text queries can directly\nretrieve relevant slides. PathSearch was rigorously evaluated on four public\npathology datasets and three in-house cohorts, covering tasks including\nanatomical site retrieval, tumor subtyping, tumor vs. non-tumor discrimination,\nand grading across diverse organs such as breast, lung, kidney, liver, and\nstomach. External results show that PathSearch outperforms traditional\nimage-to-image retrieval frameworks. A multi-center reader study further\ndemonstrates that PathSearch improves diagnostic accuracy, boosts confidence,\nand enhances inter-observer agreement among pathologists in real clinical\nscenarios. These results establish PathSearch as a scalable and generalizable\nretrieval solution for digital pathology.\n","authors":["Hongyi Wang","Zhengjie Zhu","Jiabo Ma","Fang Wang","Yue Shi","Bo Luo","Jili Wang","Qiuyu Cai","Xiuming Zhang","Yen-Wei Chen","Lanfen Lin","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2510.23224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.15709v2","updated":"2025-10-27T08:48:20Z","published":"2025-09-19T07:33:50Z","title":"Understanding Embedding Scaling in Collaborative Filtering","summary":"  Scaling recommendation models into large recommendation models has become one\nof the most widely discussed topics. Recent efforts focus on components beyond\nthe scaling embedding dimension, as it is believed that scaling embedding may\nlead to performance degradation. Although there have been some initial\nobservations on embedding, the root cause of their non-scalability remains\nunclear. Moreover, whether performance degradation occurs across different\ntypes of models and datasets is still an unexplored area. Regarding the effect\nof embedding dimensions on performance, we conduct large-scale experiments\nacross 10 datasets with varying sparsity levels and scales, using 4\nrepresentative classical architectures. We surprisingly observe two novel\nphenomena: double-peak and logarithmic. For the former, as the embedding\ndimension increases, performance first improves, then declines, rises again,\nand eventually drops. For the latter, it exhibits a perfect logarithmic curve.\nOur contributions are threefold. First, we discover two novel phenomena when\nscaling collaborative filtering models. Second, we gain an understanding of the\nunderlying causes of the double-peak phenomenon. Lastly, we theoretically\nanalyze the noise robustness of collaborative filtering models, with results\nmatching empirical observations.\n","authors":["Yicheng He","Zhou Kaiyu","Haoyue Bai","Fengbin Zhu","Yonghui Yang"],"pdf_url":"https://arxiv.org/pdf/2509.15709v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23104v1","updated":"2025-10-27T08:18:02Z","published":"2025-10-27T08:18:02Z","title":"Leveraging Hierarchical Organization for Medical Multi-document\n  Summarization","summary":"  Medical multi-document summarization (MDS) is a complex task that requires\neffectively managing cross-document relationships. This paper investigates\nwhether incorporating hierarchical structures in the inputs of MDS can improve\na model's ability to organize and contextualize information across documents\ncompared to traditional flat summarization methods. We investigate two ways of\nincorporating hierarchical organization across three large language models\n(LLMs), and conduct comprehensive evaluations of the resulting summaries using\nautomated metrics, model-based metrics, and domain expert evaluation of\npreference, understandability, clarity, complexity, relevance, coverage,\nfactuality, and coherence. Our results show that human experts prefer\nmodel-generated summaries over human-written summaries. Hierarchical approaches\ngenerally preserve factuality, coverage, and coherence of information, while\nalso increasing human preference for summaries. Additionally, we examine\nwhether simulated judgments from GPT-4 align with human judgments, finding\nhigher agreement along more objective evaluation facets. Our findings\ndemonstrate that hierarchical structures can improve the clarity of medical\nsummaries generated by models while maintaining content coverage, providing a\npractical way to improve human preference for generated summaries.\n","authors":["Yi-Li Hsu","Katelyn X. Mei","Lucy Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2510.23104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07543v2","updated":"2025-10-27T07:40:12Z","published":"2025-07-10T08:38:31Z","title":"The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English\n  Corpora","summary":"  Cross-lingual retrieval-augmented generation (RAG) is a critical capability\nfor retrieving and generating answers across languages. Prior work in this\ncontext has mostly focused on generation and relied on benchmarks derived from\nopen-domain sources, most notably Wikipedia. In such settings, retrieval\nchallenges often remain hidden due to language imbalances, overlap with\npretraining data, and memorized content. To address this gap, we study\nArabic-English RAG in a domain-specific setting using benchmarks derived from\nreal-world corporate datasets. Our benchmarks include all combinations of\nlanguages for the user query and the supporting document, drawn independently\nand uniformly at random. This enables a systematic study of multilingual\nretrieval behavior.\n  Our findings reveal that retrieval is a critical bottleneck in cross-lingual\ndomain-specific scenarios, with substantial performance drops occurring when\nthe user query and supporting document languages differ. A key insight is that\nthese failures stem primarily from the retriever's difficulty in ranking\ndocuments across languages. Finally, we propose two simple retrieval strategies\nthat address this source of failure by enforcing equal retrieval from both\nlanguages or by translating the query, resulting in substantial improvements in\ncross-lingual and overall performance. These results highlight meaningful\nopportunities for improving multilingual retrieval, particularly in practical,\nreal-world RAG applications.\n","authors":["Chen Amiraz","Yaroslav Fyodorov","Elad Haramaty","Zohar Karnin","Liane Lewin-Eytan"],"pdf_url":"https://arxiv.org/pdf/2507.07543v2.pdf","comment":"Accepted to ArabicNLP 2025"},{"id":"http://arxiv.org/abs/2510.23077v1","updated":"2025-10-27T07:26:32Z","published":"2025-10-27T07:26:32Z","title":"Think before Recommendation: Autonomous Reasoning-enhanced Recommender","summary":"  The core task of recommender systems is to learn user preferences from\nhistorical user-item interactions. With the rapid development of large language\nmodels (LLMs), recent research has explored leveraging the reasoning\ncapabilities of LLMs to enhance rating prediction tasks. However, existing\ndistillation-based methods suffer from limitations such as the teacher model's\ninsufficient recommendation capability, costly and static supervision, and\nsuperficial transfer of reasoning ability. To address these issues, this paper\nproposes RecZero, a reinforcement learning (RL)-based recommendation paradigm\nthat abandons the traditional multi-model and multi-stage distillation\napproach. Instead, RecZero trains a single LLM through pure RL to autonomously\ndevelop reasoning capabilities for rating prediction. RecZero consists of two\nkey components: (1) \"Think-before-Recommendation\" prompt construction, which\nemploys a structured reasoning template to guide the model in step-wise\nanalysis of user interests, item features, and user-item compatibility; and (2)\nrule-based reward modeling, which adopts group relative policy optimization\n(GRPO) to compute rewards for reasoning trajectories and optimize the LLM.\nAdditionally, the paper explores a hybrid paradigm, RecOne, which combines\nsupervised fine-tuning with RL, initializing the model with cold-start\nreasoning samples and further optimizing it with RL. Experimental results\ndemonstrate that RecZero and RecOne significantly outperform existing baseline\nmethods on multiple benchmark datasets, validating the superiority of the RL\nparadigm in achieving autonomous reasoning-enhanced recommender systems.\n","authors":["Xiaoyu Kong","Junguang Jiang","Bin Liu","Ziru Xu","Han Zhu","Jian Xu","Bo Zheng","Jiancan Wu","Xiang Wang"],"pdf_url":"https://arxiv.org/pdf/2510.23077v1.pdf","comment":"NeurIPS 2025 poster"},{"id":"http://arxiv.org/abs/2510.23066v1","updated":"2025-10-27T06:56:08Z","published":"2025-10-27T06:56:08Z","title":"Multi-Stage Field Extraction of Financial Documents with OCR and Compact\n  Vision-Language Models","summary":"  Financial documents are essential sources of information for regulators,\nauditors, and financial institutions, particularly for assessing the wealth and\ncompliance of Small and Medium-sized Businesses. However, SMB documents are\noften difficult to parse. They are rarely born digital and instead are\ndistributed as scanned images that are none machine readable. The scans\nthemselves are low in resolution, affected by skew or rotation, and often\ncontain noisy backgrounds. These documents also tend to be heterogeneous,\nmixing narratives, tables, figures, and multilingual content within the same\nreport. Such characteristics pose major challenges for automated information\nextraction, especially when relying on end to end large Vision Language Models,\nwhich are computationally expensive, sensitive to noise, and slow when applied\nto files with hundreds of pages.\n  We propose a multistage pipeline that leverages traditional image processing\nmodels and OCR extraction, together with compact VLMs for structured field\nextraction of large-scale financial documents. Our approach begins with image\npre-processing, including segmentation, orientation detection, and size\nnormalization. Multilingual OCR is then applied to recover page-level text.\nUpon analyzing the text information, pages are retrieved for coherent sections.\nFinally, compact VLMs are operated within these narrowed-down scopes to extract\nstructured financial indicators.\n  Our approach is evaluated using an internal corpus of multi-lingual, scanned\nfinancial documents. The results demonstrate that compact VLMs, together with a\nmultistage pipeline, achieves 8.8 times higher field level accuracy relative to\ndirectly feeding the whole document into large VLMs, only at 0.7 percent of the\nGPU cost and 92.6 percent less end-to-end service latency.\n","authors":["Yichao Jin","Yushuo Wang","Qishuai Zhong","Kent Chiu Jin-Chun","Kenneth Zhu Ke","Donald MacDonald"],"pdf_url":"https://arxiv.org/pdf/2510.23066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23449v3","updated":"2025-10-27T06:39:35Z","published":"2025-05-29T13:56:21Z","title":"CMIE: Combining MLLM Insights with External Evidence for Explainable\n  Out-of-Context Misinformation Detection","summary":"  Multimodal large language models (MLLMs) have demonstrated impressive\ncapabilities in visual reasoning and text generation. While previous studies\nhave explored the application of MLLM for detecting out-of-context (OOC)\nmisinformation, our empirical analysis reveals two persisting challenges of\nthis paradigm. Evaluating the representative GPT-4o model on direct reasoning\nand evidence augmented reasoning, results indicate that MLLM struggle to\ncapture the deeper relationships-specifically, cases in which the image and\ntext are not directly connected but are associated through underlying semantic\nlinks. Moreover, noise in the evidence further impairs detection accuracy. To\naddress these challenges, we propose CMIE, a novel OOC misinformation detection\nframework that incorporates a Coexistence Relationship Generation (CRG)\nstrategy and an Association Scoring (AS) mechanism. CMIE identifies the\nunderlying coexistence relationships between images and text, and selectively\nutilizes relevant evidence to enhance misinformation detection. Experimental\nresults demonstrate that our approach outperforms existing methods.\n","authors":["Fanxiao Li","Jiaying Wu","Canyuan He","Wei Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.23449v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23018v1","updated":"2025-10-27T05:32:13Z","published":"2025-10-27T05:32:13Z","title":"Improving Product Search Relevance with EAR-MP: A Solution for the CIKM\n  2025 AnalytiCup","summary":"  Multilingual e-commerce search is challenging due to linguistic diversity and\nthe noise inherent in user-generated queries. This paper documents the solution\nemployed by our team (EAR-MP) for the CIKM 2025 AnalytiCup, which addresses two\ncore tasks: Query-Category (QC) relevance and Query-Item (QI) relevance. Our\napproach first normalizes the multilingual dataset by translating all text into\nEnglish, then mitigates noise through extensive data cleaning and\nnormalization. For model training, we build on DeBERTa-v3-large and improve\nperformance with label smoothing, self-distillation, and dropout. In addition,\nwe introduce task-specific upgrades, including hierarchical token injection for\nQC and a hybrid scoring mechanism for QI. Under constrained compute, our method\nachieves competitive results, attaining an F1 score of 0.8796 on QC and 0.8744\non QI. These findings underscore the importance of systematic data\npreprocessing and tailored training strategies for building robust,\nresource-efficient multilingual relevance systems.\n","authors":["JaeEun Lim","Soomin Kim","Jaeyong Seo","Iori Ono","Qimu Ran","Jae-woong Lee"],"pdf_url":"https://arxiv.org/pdf/2510.23018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.11080v2","updated":"2025-10-27T05:12:35Z","published":"2025-09-14T04:06:03Z","title":"Membership Inference Attacks on Recommender System: A Survey","summary":"  Recommender systems (RecSys) have been widely applied to various\napplications, including E-commerce, finance, healthcare, social media and have\nbecome increasingly influential in shaping user behavior and decision-making,\nhighlighting their growing impact in various domains. However, recent studies\nhave shown that RecSys are vulnerable to membership inference attacks (MIAs),\nwhich aim to infer whether user interaction record was used to train a target\nmodel or not. MIAs on RecSys models can directly lead to a privacy breach. For\nexample, via identifying the fact that a purchase record that has been used to\ntrain a RecSys associated with a specific user, an attacker can infer that\nuser's special quirks. In recent years, MIAs have been shown to be effective on\nother ML tasks, e.g., classification models and natural language processing.\nHowever, traditional MIAs are ill-suited for RecSys due to the unseen posterior\nprobability. Although MIAs on RecSys form a newly emerging and rapidly growing\nresearch area, there has been no systematic survey on this topic yet. In this\narticle, we conduct the first comprehensive survey on RecSys MIAs. This survey\noffers a comprehensive review of the latest advancements in RecSys MIAs,\nexploring the design principles, challenges, attack and defense associated with\nthis emerging field. We provide a unified taxonomy that categorizes different\nRecSys MIAs based on their characterizations and discuss their pros and cons.\nBased on the limitations and gaps identified in this survey, we point out\nseveral promising future research directions to inspire the researchers who\nwish to follow this area. This survey not only serves as a reference for the\nresearch community but also provides a clear description for researchers\noutside this research domain.\n","authors":["Jiajie He","Xintong Chen","Xinyang Fang","Min-Chun Chen","Yuechun Gu","Keke Chen"],"pdf_url":"https://arxiv.org/pdf/2509.11080v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2510.22956v1","updated":"2025-10-27T03:23:25Z","published":"2025-10-27T03:23:25Z","title":"Tagging-Augmented Generation: Assisting Language Models in Finding\n  Intricate Knowledge In Long Contexts","summary":"  Recent investigations into effective context lengths of modern flagship large\nlanguage models (LLMs) have revealed major limitations in effective question\nanswering (QA) and reasoning over long and complex contexts for even the\nlargest and most impressive cadre of models. While approaches like\nretrieval-augmented generation (RAG) and chunk-based re-ranking attempt to\nmitigate this issue, they are sensitive to chunking, embedding and retrieval\nstrategies and models, and furthermore, rely on extensive pre-processing,\nknowledge acquisition and indexing steps. In this paper, we propose\nTagging-Augmented Generation (TAG), a lightweight data augmentation strategy\nthat boosts LLM performance in long-context scenarios, without degrading and\naltering the integrity and composition of retrieved documents. We validate our\nhypothesis by augmenting two challenging and directly relevant\nquestion-answering benchmarks -- NoLima and NovelQA -- and show that tagging\nthe context or even just adding tag definitions into QA prompts leads to\nconsistent performance gains over the baseline -- up to 17% for 32K token\ncontexts, and 2.9% in complex reasoning question-answering for multi-hop\nqueries requiring knowledge across a wide span of text. Additional details are\navailable at https://sites.google.com/view/tag-emnlp.\n","authors":["Anwesan Pal","Karen Hovsepian","Tinghao Guo","Mengnan Zhao","Somendra Tripathi","Nikos Kanakaris","George Mihaila","Sumit Nigam"],"pdf_url":"https://arxiv.org/pdf/2510.22956v1.pdf","comment":"Paper accepted at EMNLP 2025"},{"id":"http://arxiv.org/abs/2510.22942v1","updated":"2025-10-27T02:56:08Z","published":"2025-10-27T02:56:08Z","title":"GTR-Mamba: Geometry-to-Tangent Routing for Hyperbolic POI Recommendation","summary":"  Next Point-of-Interest (POI) recommendation is a critical task in modern\nLocation-Based Social Networks (LBSNs), aiming to model the complex\ndecision-making process of human mobility to provide personalized\nrecommendations for a user's next check-in location. Existing POI\nrecommendation models, predominantly based on Graph Neural Networks and\nsequential models, have been extensively studied. However, these models face a\nfundamental limitation: they struggle to simultaneously capture the inherent\nhierarchical structure of spatial choices and the dynamics and irregular shifts\nof user-specific temporal contexts. To overcome this limitation, we propose\nGTR-Mamba, a novel framework for cross-manifold conditioning and routing.\nGTR-Mamba leverages the distinct advantages of different mathematical spaces\nfor different tasks: it models the static, tree-like preference hierarchies in\nhyperbolic geometry, while routing the dynamic sequence updates to a novel\nMamba layer in the computationally stable and efficient Euclidean tangent\nspace. This process is coordinated by a cross-manifold channel that fuses\nspatio-temporal information to explicitly steer the State Space Model (SSM),\nenabling flexible adaptation to contextual changes. Extensive experiments on\nthree real-world datasets demonstrate that GTR-Mamba consistently outperforms\nstate-of-the-art baseline models in next POI recommendation.\n","authors":["Zhuoxuan Li","Jieyuan Pei","Tangwei Ye","Zhongyuan Lai","Zihan Liu","Fengyuan Xu","Qi Zhang","Liang Hu"],"pdf_url":"https://arxiv.org/pdf/2510.22942v1.pdf","comment":"14 pages, 8 figures, 4 tables, submitted to ICDE 2026"},{"id":"http://arxiv.org/abs/2507.05715v2","updated":"2025-10-27T01:44:45Z","published":"2025-07-08T06:58:24Z","title":"From ID-based to ID-free: Rethinking ID Effectiveness in Multimodal\n  Collaborative Filtering Recommendation","summary":"  Most existing multimodal collaborative filtering recommendation (MCFRec)\nmethods rely heavily on ID features and multimodal content to enhance\nrecommendation performance. However, this paper reveals that ID features are\neffective but have limited benefits in multimodal collaborative filtering\nrecommendation. Therefore, this paper systematically deconstruct the pros and\ncons of ID features: (i) they provide initial embedding but lack semantic\nrichness, (ii) they provide a unique identifier for each user and item but\nhinder generalization to untrained data, and (iii) they assist in aligning and\nfusing multimodal features but may lead to representation shift. Based on these\ninsights, this paper proposes IDFREE, an ID-free multimodal collaborative\nFiltering REcommEndation baseline. IDFREE replaces ID features with multimodal\nfeatures and positional encodings to generate semantically meaningful ID-free\nembeddings. For ID-free multimodal collaborative filtering, it further proposes\nan adaptive similarity graph module to construct dynamic user-user and\nitem-item graphs based on multimodal features. Then, an augmented user-item\ngraph encoder is proposed to construct more effective user and item encoding.\nFinally, IDFREE achieves inter-multimodal alignment based on the contrastive\nlearning and uses Softmax loss as recommendation loss. Basic experiments on\nthree public datasets demonstrate that IDFREE outperforms existing ID-based\nMCFRec methods, achieving an average performance gain of 72.24% across standard\nmetrics (Recall@5, 10, 20, 50 and NDCG@5, 10, 20, 50). Exploratory and extended\nexperiments further validate our findings on the limitations of ID features in\nMCFRec. The code is released at https://github.com/G-H-Li/IDFREE.\n","authors":["Guohao Li","Li Jing","Jia Wu","Xuefei Li","Kai Zhu","Yue He"],"pdf_url":"https://arxiv.org/pdf/2507.05715v2.pdf","comment":"We identified that our current approach achieves its reported\n  performance only under specific data conditions, and its robustness is weaker\n  than we initially expected"},{"id":"http://arxiv.org/abs/2510.22888v1","updated":"2025-10-27T00:41:07Z","published":"2025-10-27T00:41:07Z","title":"MGFRec: Towards Reinforced Reasoning Recommendation with Multiple\n  Groundings and Feedback","summary":"  The powerful reasoning and generative capabilities of large language models\n(LLMs) have inspired researchers to apply them to reasoning-based\nrecommendation tasks, which require in-depth reasoning about user interests and\nthe generation of recommended items. However, previous reasoning-based\nrecommendation methods have typically performed inference within the language\nspace alone, without incorporating the actual item space. This has led to\nover-interpreting user interests and deviating from real items. Towards this\nresearch gap, we propose performing multiple rounds of grounding during\ninference to help the LLM better understand the actual item space, which could\nensure that its reasoning remains aligned with real items. Furthermore, we\nintroduce a user agent that provides feedback during each grounding step,\nenabling the LLM to better recognize and adapt to user interests. Comprehensive\nexperiments conducted on three Amazon review datasets demonstrate the\neffectiveness of incorporating multiple groundings and feedback. These findings\nunderscore the critical importance of reasoning within the actual item space,\nrather than being confined to the language space, for recommendation tasks.\n","authors":["Shihao Cai","Chongming Gao","Haoyan Liu","Wentao Shi","Jianshan Sun","Ruiming Tang","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2510.22888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.18239v2","updated":"2025-10-27T21:18:47Z","published":"2025-10-21T02:53:17Z","title":"LIME: Link-based user-item Interaction Modeling with decoupled xor\n  attention for Efficient test time scaling","summary":"  Scaling large recommendation systems requires advancing three major\nfrontiers: processing longer user histories, expanding candidate sets, and\nincreasing model capacity. While promising, transformers' computational cost\nscales quadratically with the user sequence length and linearly with the number\nof candidates. This trade-off makes it prohibitively expensive to expand\ncandidate sets or increase sequence length at inference, despite the\nsignificant performance improvements.\n  We introduce \\textbf{LIME}, a novel architecture that resolves this\ntrade-off. Through two key innovations, LIME fundamentally reduces\ncomputational complexity. First, low-rank ``link embeddings\" enable\npre-computation of attention weights by decoupling user and candidate\ninteractions, making the inference cost nearly independent of candidate set\nsize. Second, a linear attention mechanism, \\textbf{LIME-XOR}, reduces the\ncomplexity with respect to user sequence length from quadratic ($O(N^2)$) to\nlinear ($O(N)$).\n  Experiments on public and industrial datasets show LIME achieves near-parity\nwith state-of-the-art transformers but with a 10$\\times$ inference speedup on\nlarge candidate sets or long sequence lengths. When tested on a major\nrecommendation platform, LIME improved user engagement while maintaining\nminimal inference costs with respect to candidate set size and user history\nlength, establishing a new paradigm for efficient and expressive recommendation\nsystems.\n","authors":["Yunjiang Jiang","Ayush Agarwal","Yang Liu","Bi Xue"],"pdf_url":"https://arxiv.org/pdf/2510.18239v2.pdf","comment":"16 pages"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2510.23606v1","updated":"2025-10-27T17:59:57Z","published":"2025-10-27T17:59:57Z","title":"Variational Masked Diffusion Models","summary":"  Masked diffusion models have recently emerged as a flexible framework for\ndiscrete generative modeling. However, a key limitation of standard masked\ndiffusion is its inability to effectively capture dependencies among tokens\nthat are predicted concurrently, leading to degraded generation quality when\ndependencies among tokens are important. To explicitly model dependencies among\ntokens, we propose Variational Masked Diffusion (VMD), a framework that\nintroduces latent variables into the masked diffusion process. Through\ncontrolled experiments on synthetic datasets, we demonstrate that VMD\nsuccessfully learns dependencies that conventional masked diffusion fails to\ncapture. We further validate the effectiveness of our approach on Sudoku\npuzzles and text datasets, where learning of dependencies among tokens improves\nglobal consistency. Across these domains, VMD enhances both generation quality\nand dependency awareness, highlighting the value of integrating variational\ninference into masked diffusion. Our code is available at:\nhttps://riccizz.github.io/VMD.\n","authors":["Yichi Zhang","Alex Schwing","Zhizhen Zhao"],"pdf_url":"https://arxiv.org/pdf/2510.23606v1.pdf","comment":"Project Page: https://riccizz.github.io/VMD"},{"id":"http://arxiv.org/abs/2510.23605v1","updated":"2025-10-27T17:59:51Z","published":"2025-10-27T17:59:51Z","title":"Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with\n  Progressive Texture Infilling","summary":"  Current 3D/4D generation methods are usually optimized for photorealism,\nefficiency, and aesthetics. However, they often fail to preserve the semantic\nidentity of the subject across different viewpoints. Adapting generation\nmethods with one or few images of a specific subject (also known as\nPersonalization or Subject-driven generation) allows generating visual content\nthat align with the identity of the subject. However, personalized 3D/4D\ngeneration is still largely underexplored. In this work, we introduce TIRE\n(Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation.\nIt takes an initial 3D asset produced by an existing 3D generative model as\ninput and uses video tracking to identify the regions that need to be modified.\nThen, we adopt a subject-driven 2D inpainting model for progressively infilling\nthe identified regions. Finally, we resplat the modified 2D multi-view\nobservations back to 3D while still maintaining consistency. Extensive\nexperiments demonstrate that our approach significantly improves identity\npreservation in 3D/4D generation compared to state-of-the-art methods. Our\nproject website is available at\nhttps://zsh2000.github.io/track-inpaint-resplat.github.io/.\n","authors":["Shuhong Zheng","Ashkan Mirzaei","Igor Gilitschenski"],"pdf_url":"https://arxiv.org/pdf/2510.23605v1.pdf","comment":"NeurIPS 2025, 38 pages, 22 figures"},{"id":"http://arxiv.org/abs/2506.05314v2","updated":"2025-10-27T17:59:13Z","published":"2025-06-05T17:55:23Z","title":"Constrained Entropic Unlearning: A Primal-Dual Framework for Large\n  Language Models","summary":"  Large Language Models (LLMs) deployed in real-world settings increasingly\nface the need to unlearn sensitive, outdated, or proprietary information.\nExisting unlearning methods typically formulate forgetting and retention as a\nregularized trade-off, combining both objectives into a single scalarized loss.\nThis often leads to unstable optimization and degraded performance on retained\ndata, especially under aggressive forgetting. We propose a new formulation of\nLLM unlearning as a constrained optimization problem: forgetting is enforced\nvia a novel logit-margin flattening loss that explicitly drives the output\ndistribution toward uniformity on a designated forget set, while retention is\npreserved through a hard constraint on a separate retain set. Compared to\nentropy-based objectives, our loss is softmax-free, numerically stable, and\nmaintains non-vanishing gradients, enabling more efficient and robust\noptimization. We solve the constrained problem using a scalable primal-dual\nalgorithm that exposes the trade-off between forgetting and retention through\nthe dynamics of the dual variable, all without any extra computational\noverhead. Evaluations on the TOFU and MUSE benchmarks across diverse LLM\narchitectures demonstrate that our approach consistently matches or exceeds\nstate-of-the-art baselines, effectively removing targeted information while\npreserving downstream utility.\n","authors":["Taha Entesari","Arman Hatami","Rinat Khaziev","Anil Ramakrishna","Mahyar Fazlyab"],"pdf_url":"https://arxiv.org/pdf/2506.05314v2.pdf","comment":"The Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems"},{"id":"http://arxiv.org/abs/2510.16923v2","updated":"2025-10-27T17:59:01Z","published":"2025-10-19T16:38:03Z","title":"UNDREAM: Bridging Differentiable Rendering and Photorealistic Simulation\n  for End-to-end Adversarial Attacks","summary":"  Deep learning models deployed in safety critical applications like autonomous\ndriving use simulations to test their robustness against adversarial attacks in\nrealistic conditions. However, these simulations are non-differentiable,\nforcing researchers to create attacks that do not integrate simulation\nenvironmental factors, reducing attack success. To address this limitation, we\nintroduce UNDREAM, the first software framework that bridges the gap between\nphotorealistic simulators and differentiable renderers to enable end-to-end\noptimization of adversarial perturbations on any 3D objects. UNDREAM enables\nmanipulation of the environment by offering complete control over weather,\nlighting, backgrounds, camera angles, trajectories, and realistic human and\nobject movements, thereby allowing the creation of diverse scenes. We showcase\na wide array of distinct physically plausible adversarial objects that UNDREAM\nenables researchers to swiftly explore in different configurable environments.\nThis combination of photorealistic simulation and differentiable optimization\nopens new avenues for advancing research of physical adversarial attacks.\n","authors":["Mansi Phute","Matthew Hull","Haoran Wang","Alec Helbling","ShengYun Peng","Willian Lunardi","Martin Andreoni","Wenke Lee","Duen Horng Chau"],"pdf_url":"https://arxiv.org/pdf/2510.16923v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23590v1","updated":"2025-10-27T17:55:06Z","published":"2025-10-27T17:55:06Z","title":"Lightweight Robust Direct Preference Optimization","summary":"  Direct Preference Optimization (DPO) has become a popular method for\nfine-tuning large language models (LLMs) due to its stability and simplicity.\nHowever, it is also known to be sensitive to noise in the data and prone to\noverfitting. Recent works have proposed using distributionally robust\noptimization (DRO) to address potential noise and distributional shift in the\ndata. However, these methods often suffer from excessive conservatism and high\ncomputational cost. We propose DPO-PRO (DPO with Preference Robustness), a\nrobust fine-tuning algorithm based on DPO which accounts for uncertainty in the\npreference distribution through a lightweight DRO formulation. Unlike prior\nDRO-based variants, DPO-PRO focuses solely on uncertainty in preferences,\navoiding unnecessary conservatism and incurring negligible computational\noverhead. We further show that DPO-PRO is equivalent to a regularized DPO\nobjective that penalizes model overconfidence under weak preference signals. We\nevaluate DPO-PRO on standard alignment benchmarks and a real-world public\nhealth task. Experimental results show that our method consistently improves\nrobustness to noisy preference signals compared to existing DPO variants.\n","authors":["Cheol Woo Kim","Shresth Verma","Mauricio Tec","Milind Tambe"],"pdf_url":"https://arxiv.org/pdf/2510.23590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.15963v2","updated":"2025-10-27T17:51:21Z","published":"2025-10-11T20:13:59Z","title":"ESCA: Contextualizing Embodied Agents via Scene-Graph Generation","summary":"  Multi-modal large language models (MLLMs) are making rapid progress toward\ngeneral-purpose embodied agents. However, existing MLLMs do not reliably\ncapture fine-grained links between low-level visual features and high-level\ntextual semantics, leading to weak grounding and inaccurate perception. To\novercome this challenge, we propose ESCA, a framework that contextualizes\nembodied agents by grounding their perception in spatial-temporal scene graphs.\nAt its core is SGCLIP, a novel, open-domain, promptable foundation model for\ngenerating scene graphs that is based on CLIP. SGCLIP is trained on 87K+\nopen-domain videos using a neurosymbolic pipeline that aligns automatically\ngenerated captions with scene graphs produced by the model itself, eliminating\nthe need for human-labeled annotations. We demonstrate that SGCLIP excels in\nboth prompt-based inference and task-specific fine-tuning, achieving\nstate-of-the-art results on scene graph generation and action localization\nbenchmarks. ESCA with SGCLIP improves perception for embodied agents based on\nboth open-source and commercial MLLMs, achieving state of-the-art performance\nacross two embodied environments. Notably, ESCA significantly reduces agent\nperception errors and enables open-source models to surpass proprietary\nbaselines. We release the source code for SGCLIP model training at\nhttps://github.com/video-fm/LASER and for the embodied agent at\nhttps://github.com/video-fm/ESCA.\n","authors":["Jiani Huang","Amish Sethi","Matthew Kuo","Mayank Keoliya","Neelay Velingker","JungHo Jung","Ser-Nam Lim","Ziyang Li","Mayur Naik"],"pdf_url":"https://arxiv.org/pdf/2510.15963v2.pdf","comment":"Accepted as a Spotlight Paper at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23581v1","updated":"2025-10-27T17:50:19Z","published":"2025-10-27T17:50:19Z","title":"Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human\n  Animation","summary":"  Audio-driven human animation models often suffer from identity drift during\ntemporal autoregressive generation, where characters gradually lose their\nidentity over time. One solution is to generate keyframes as intermediate\ntemporal anchors that prevent degradation, but this requires an additional\nkeyframe generation stage and can restrict natural motion dynamics. To address\nthis, we propose Lookahead Anchoring, which leverages keyframes from future\ntimesteps ahead of the current generation window, rather than within it. This\ntransforms keyframes from fixed boundaries into directional beacons: the model\ncontinuously pursues these future anchors while responding to immediate audio\ncues, maintaining consistent identity through persistent guidance. This also\nenables self-keyframing, where the reference image serves as the lookahead\ntarget, eliminating the need for keyframe generation entirely. We find that the\ntemporal lookahead distance naturally controls the balance between expressivity\nand consistency: larger distances allow for greater motion freedom, while\nsmaller ones strengthen identity adherence. When applied to three recent human\nanimation models, Lookahead Anchoring achieves superior lip synchronization,\nidentity preservation, and visual quality, demonstrating improved temporal\nconditioning across several different architectures. Video results are\navailable at the following link: https://lookahead-anchoring.github.io.\n","authors":["Junyoung Seo","Rodrigo Mira","Alexandros Haliassos","Stella Bounareli","Honglie Chen","Linh Tran","Seungryong Kim","Zoe Landgraf","Jie Shen"],"pdf_url":"https://arxiv.org/pdf/2510.23581v1.pdf","comment":"Project page: https://lookahead-anchoring.github.io"},{"id":"http://arxiv.org/abs/2503.07346v2","updated":"2025-10-27T17:45:30Z","published":"2025-03-10T13:59:57Z","title":"Now you see me! Attribution Distributions Reveal What is Truly Important\n  for a Prediction","summary":"  Neural networks are regularly employed in high-stakes decision-making, where\nunderstanding and transparency is key. Attribution methods have been developed\nto gain understanding into which input features neural networks use for a\nspecific prediction. Although widely used in computer vision, these methods\noften result in unspecific saliency maps that fail to identify the relevant\ninformation that led to a decision, supported by different benchmarks results.\nHere, we revisit the common attribution pipeline and identify one cause for the\nlack of specificity in attributions as the computation of attribution of\nisolated logits. Instead, we suggest to combine attributions of multiple class\nlogits in analogy to how the softmax combines the information across logits. By\ncomputing probability distributions of attributions over classes for each\nspatial location in the image, we unleash the true capabilities of existing\nattribution methods, revealing better object- and instance-specificity and\nuncovering discriminative as well as shared features between classes. On common\nbenchmarks, including the grid-pointing game and randomization-based sanity\nchecks, we show that this reconsideration of how and where we compute\nattributions across the network improves established attribution methods while\nstaying agnostic to model architectures. We make the code publicly available:\nhttps://github.com/nilspwalter/var.\n","authors":["Nils Philipp Walter","Jilles Vreeken","Jonas Fischer"],"pdf_url":"https://arxiv.org/pdf/2503.07346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23571v1","updated":"2025-10-27T17:41:38Z","published":"2025-10-27T17:41:38Z","title":"RobotArena $\\infty$: Scalable Robot Benchmarking via Real-to-Sim\n  Translation","summary":"  The pursuit of robot generalists - instructable agents capable of performing\ndiverse tasks across diverse environments - demands rigorous and scalable\nevaluation. Yet real-world testing of robot policies remains fundamentally\nconstrained: it is labor-intensive, slow, unsafe at scale, and difficult to\nreproduce. Existing simulation benchmarks are similarly limited, as they train\nand test policies within the same synthetic domains and cannot assess models\ntrained from real-world demonstrations or alternative simulation environments.\nAs policies expand in scope and complexity, these barriers only intensify,\nsince defining \"success\" in robotics often hinges on nuanced human judgments of\nexecution quality. In this paper, we introduce a new benchmarking framework\nthat overcomes these challenges by shifting VLA evaluation into large-scale\nsimulated environments augmented with online human feedback. Leveraging\nadvances in vision-language models, 2D-to-3D generative modeling, and\ndifferentiable rendering, our approach automatically converts video\ndemonstrations from widely used robot datasets into simulated counterparts.\nWithin these digital twins, we assess VLA policies using both automated\nVLM-guided scoring and scalable human preference judgments collected from\ncrowdworkers, transforming human involvement from tedious scene setup,\nresetting, and safety supervision into lightweight preference comparisons. To\nmeasure robustness, we systematically perturb simulated environments along\nmultiple axes, such as textures and object placements, stress-testing policy\ngeneralization under controlled variation. The result is a continuously\nevolving, reproducible, and scalable benchmark for real-world trained robot\nmanipulation policies, addressing a critical missing capability in today's\nrobotics landscape.\n","authors":["Yash Jangir","Yidi Zhang","Kashu Yamazaki","Chenyu Zhang","Kuan-Hsun Tu","Tsung-Wei Ke","Lei Ke","Yonatan Bisk","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2510.23571v1.pdf","comment":"Website: https://robotarenainf.github.io"},{"id":"http://arxiv.org/abs/2510.23564v1","updated":"2025-10-27T17:35:15Z","published":"2025-10-27T17:35:15Z","title":"ReCode: Unify Plan and Action for Universal Granularity Control","summary":"  Real-world tasks require decisions at varying granularities, and humans excel\nat this by leveraging a unified cognitive representation where planning is\nfundamentally understood as a high-level form of action. However, current Large\nLanguage Model (LLM)-based agents lack this crucial capability to operate\nfluidly across decision granularities. This limitation stems from existing\nparadigms that enforce a rigid separation between high-level planning and\nlow-level action, which impairs dynamic adaptability and limits generalization.\nWe propose ReCode (Recursive Code Generation), a novel paradigm that addresses\nthis limitation by unifying planning and action within a single code\nrepresentation. In this representation, ReCode treats high-level plans as\nabstract placeholder functions, which the agent then recursively decomposes\ninto finer-grained sub-functions until reaching primitive actions. This\nrecursive approach dissolves the rigid boundary between plan and action,\nenabling the agent to dynamically control its decision granularity.\nFurthermore, the recursive structure inherently generates rich,\nmulti-granularity training data, enabling models to learn hierarchical\ndecision-making processes. Extensive experiments show ReCode significantly\nsurpasses advanced baselines in inference performance and demonstrates\nexceptional data efficiency in training, validating our core insight that\nunifying planning and action through recursive code generation is a powerful\nand effective approach to achieving universal granularity control. The code is\navailable at https://github.com/FoundationAgents/ReCode.\n","authors":["Zhaoyang Yu","Jiayi Zhang","Huixue Su","Yufan Zhao","Yifan Wu","Mingyi Deng","Jinyu Xiang","Yizhang Lin","Lingxiao Tang","Yingchao Li","Yuyu Luo","Bang Liu","Chenglin Wu"],"pdf_url":"https://arxiv.org/pdf/2510.23564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00062v2","updated":"2025-10-27T17:35:06Z","published":"2025-05-29T13:31:51Z","title":"SafeCOMM: A Study on Safety Degradation in Fine-Tuned Telecom Large\n  Language Models","summary":"  Fine-tuning large language models (LLMs) on telecom datasets is a common\npractice to adapt general-purpose models to the telecom domain. However, little\nattention has been paid to how this process may compromise model safety. Recent\nresearch has shown that even benign fine-tuning can degrade the safety\nalignment of LLMs, causing them to respond to harmful or unethical user\nqueries. In this paper, we investigate this issue by fine-tuning LLMs on three\nrepresentative telecom datasets and show that safety degrades even for light\ntelecom domain adaptation. To this end, we introduce TeleHarm, the first\ntelecom-specific red-teaming benchmark, which we use alongside established\nDirect-Harm and HexPhi datasets to systematically assess harmful behavior. We\nfurther extend our analysis to publicly available TeleLLMs that were\ncontinually pre-trained on large telecom corpora, revealing that safety\nalignment is severely lacking, primarily due to the omission of safety-focused\ninstruction tuning. To address these issues, we evaluate three realignment\ndefenses: SafeInstruct, SafeLoRA, SafeMERGE. We show that, across all settings,\nthe proposed defenses can effectively restore safety without compromising\ntelecom task performance, leading to Safe teleCOMMunication (SafeCOMM) models.\nOur work serves as both a diagnostic study and practical guide for safety\nrealignment in telecom-tuned LLMs, underscoring the need for safety-aware\ninstruction and fine-tuning in the telecom domain.\n","authors":["Aladin Djuhera","Swanand Ravindra Kadhe","Farhan Ahmed","Syed Zawad","Fernando Koch","Walid Saad","Holger Boche"],"pdf_url":"https://arxiv.org/pdf/2506.00062v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23557v1","updated":"2025-10-27T17:31:24Z","published":"2025-10-27T17:31:24Z","title":"Minimizing Human Intervention in Online Classification","summary":"  We introduce and study an online problem arising in question answering\nsystems. In this problem, an agent must sequentially classify user-submitted\nqueries represented by $d$-dimensional embeddings drawn i.i.d. from an unknown\ndistribution. The agent may consult a costly human expert for the correct\nlabel, or guess on her own without receiving feedback. The goal is to minimize\nregret against an oracle with free expert access. When the time horizon $T$ is\nat least exponential in the embedding dimension $d$, one can learn the geometry\nof the class regions: in this regime, we propose the Conservative Hull-based\nClassifier (CHC), which maintains convex hulls of expert-labeled queries and\ncalls the expert as soon as a query lands outside all known hulls. CHC attains\n$\\mathcal{O}(\\log^d T)$ regret in $T$ and is minimax optimal for $d=1$.\nOtherwise, the geometry cannot be reliably learned without additional\ndistributional assumptions. We show that when the queries are drawn from a\nsubgaussian mixture, for $T \\le e^d$, a Center-based Classifier (CC) achieves\nregret proportional to $N\\log{N}$ where $N$ is the number of labels. To bridge\nthese regimes, we introduce the Generalized Hull-based Classifier (GHC), a\npractical extension of CHC that allows for more aggressive guessing via a\ntunable threshold parameter. Our approach is validated with experiments,\nnotably on real-world question-answering datasets using embeddings derived from\nstate-of-the-art large language models.\n","authors":["William RÃ©veillard","Vasileios Saketos","Alexandre Proutiere","Richard Combes"],"pdf_url":"https://arxiv.org/pdf/2510.23557v1.pdf","comment":"49 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.23554v1","updated":"2025-10-27T17:28:55Z","published":"2025-10-27T17:28:55Z","title":"A U-Net and Transformer Pipeline for Multilingual Image Translation","summary":"  This paper presents an end-to-end multilingual translation pipeline that\nintegrates a custom U-Net for text detection, the Tesseract engine for text\nrecognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for\nNeural Machine Translation (NMT). Our approach first utilizes a U-Net model,\ntrained on a synthetic dataset , to accurately segment and detect text regions\nfrom an image. These detected regions are then processed by Tesseract to\nextract the source text. This extracted text is fed into a custom Transformer\nmodel trained from scratch on a multilingual parallel corpus spanning 5\nlanguages. Unlike systems reliant on monolithic pre-trained models, our\narchitecture emphasizes full customization and adaptability. The system is\nevaluated on its text detection accuracy, text recognition quality, and\ntranslation performance via BLEU scores. The complete pipeline demonstrates\npromising results, validating the viability of a custom-built system for\ntranslating text directly from images.\n","authors":["Siddharth Sahay","Radhika Agarwal"],"pdf_url":"https://arxiv.org/pdf/2510.23554v1.pdf","comment":"6 pages, 3 figures, 5 tables, and 2 algorithms. Prepared in IEEE\n  double-column format"},{"id":"http://arxiv.org/abs/2510.19223v2","updated":"2025-10-27T17:26:39Z","published":"2025-10-22T04:07:48Z","title":"Enhancing Graph Neural Networks: A Mutual Learning Approach","summary":"  Knowledge distillation (KD) techniques have emerged as a powerful tool for\ntransferring expertise from complex teacher models to lightweight student\nmodels, particularly beneficial for deploying high-performance models in\nresource-constrained devices. This approach has been successfully applied to\ngraph neural networks (GNNs), harnessing their expressive capabilities to\ngenerate node embeddings that capture structural and feature-related\ninformation. In this study, we depart from the conventional KD approach by\nexploring the potential of collaborative learning among GNNs. In the absence of\na pre-trained teacher model, we show that relatively simple and shallow GNN\narchitectures can synergetically learn efficient models capable of performing\nbetter during inference, particularly in tackling multiple tasks. We propose a\ncollaborative learning framework where ensembles of student GNNs mutually teach\neach other throughout the training process. We introduce an adaptive logit\nweighting unit to facilitate efficient knowledge exchange among models and an\nentropy enhancement technique to improve mutual learning. These components\ndynamically empower the models to adapt their learning strategies during\ntraining, optimizing their performance for downstream tasks. Extensive\nexperiments conducted on three datasets each for node and graph classification\ndemonstrate the effectiveness of our approach.\n","authors":["Paul Agbaje","Arkajyoti Mitra","Afia Anjum","Pranali Khose","Ebelechukwu Nwafor","Habeeb Olufowobi"],"pdf_url":"https://arxiv.org/pdf/2510.19223v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01213v4","updated":"2025-10-27T17:20:28Z","published":"2025-06-01T23:17:19Z","title":"On the Stability of Graph Convolutional Neural Networks: A Probabilistic\n  Perspective","summary":"  Graph convolutional neural networks (GCNNs) have emerged as powerful tools\nfor analyzing graph-structured data, achieving remarkable success across\ndiverse applications. However, the theoretical understanding of the stability\nof these models, i.e., their sensitivity to small changes in the graph\nstructure, remains in rather limited settings, hampering the development and\ndeployment of robust and trustworthy models in practice. To fill this gap, we\nstudy how perturbations in the graph topology affect GCNN outputs and propose a\nnovel formulation for analyzing model stability. Unlike prior studies that\nfocus only on worst-case perturbations, our distribution-aware formulation\ncharacterizes output perturbations across a broad range of input data. This\nway, our framework enables, for the first time, a probabilistic perspective on\nthe interplay between the statistical properties of the node data and\nperturbations in the graph topology. We conduct extensive experiments to\nvalidate our theoretical findings and demonstrate their benefits over existing\nbaselines, in terms of both representation stability and adversarial attacks on\ndownstream tasks. Our results demonstrate the practical significance of the\nproposed formulation and highlight the importance of incorporating data\ndistribution into stability analysis.\n","authors":["Ning Zhang","Henry Kenlay","Li Zhang","Mihai Cucuringu","Xiaowen Dong"],"pdf_url":"https://arxiv.org/pdf/2506.01213v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.22494v2","updated":"2025-10-27T17:12:30Z","published":"2025-05-28T15:45:43Z","title":"ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type\n  Neighborhoods","summary":"  Designing protein sequences of both high fitness and novelty is a challenging\ntask in data-efficient protein engineering. Exploration beyond wild-type\nneighborhoods often leads to biologically implausible sequences or relies on\nsurrogate models that lose fidelity in novel regions. Here, we propose\nProSpero, an active learning framework in which a frozen pre-trained generative\nmodel is guided by a surrogate updated from oracle feedback. By integrating\nfitness-relevant residue selection with biologically-constrained Sequential\nMonte Carlo sampling, our approach enables exploration beyond wild-type\nneighborhoods while preserving biological plausibility. We show that our\nframework remains effective even when the surrogate is misspecified. ProSpero\nconsistently outperforms or matches existing methods across diverse protein\nengineering tasks, retrieving sequences of both high fitness and novelty.\n","authors":["Michal Kmicikiewicz","Vincent Fortuin","Ewa Szczurek"],"pdf_url":"https://arxiv.org/pdf/2505.22494v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.20094v2","updated":"2025-10-27T17:12:03Z","published":"2025-10-23T00:28:32Z","title":"On the Structure of Stationary Solutions to McKean-Vlasov Equations with\n  Applications to Noisy Transformers","summary":"  We study stationary solutions of McKean-Vlasov equations on the circle. Our\nmain contributions stem from observing an exact equivalence between solutions\nof the stationary McKean-Vlasov equation and an infinite-dimensional quadratic\nsystem of equations over Fourier coefficients, which allows explicit\ncharacterization of the stationary states in a sequence space rather than a\nfunction space. This framework provides a transparent description of local\nbifurcations, characterizing their periodicity, and resonance structures, while\naccommodating singular potentials. We derive analytic expressions that\ncharacterize the emergence, form and shape (supercritical, critical,\nsubcritical or transcritical) of bifurcations involving possibly multiple\nFourier modes and connect them with discontinuous phase transitions. We also\ncharacterize, under suitable assumptions, the detailed structure of the\nstationary bifurcating solutions that are accurate upto an arbitrary number of\nFourier modes. At the global level, we establish regularity and concavity\nproperties of the free energy landscape, proving existence, compactness, and\ncoexistence of globally minimizing stationary measures, further identifying\ndiscontinuous phase transitions with points of non-differentiability of the\nminimum free energy map. As an application, we specialize the theory to the\nNoisy Mean-Field Transformer model, where we show how changing the inverse\ntemperature parameter $\\beta$ affects the geometry of the infinitely many\nbifurcations from the uniform measure. We also explain how increasing $\\beta$\ncan lead to a rich class of approximate multi-mode stationary solutions which\ncan be seen as `metastable states'. Further, a sharp transition from continuous\nto discontinuous (first-order) phase behavior is observed as $\\beta$ increases.\n","authors":["Krishnakumar Balasubramanian","Sayan Banerjee","Philippe Rigollet"],"pdf_url":"https://arxiv.org/pdf/2510.20094v2.pdf","comment":"46 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.23535v1","updated":"2025-10-27T17:11:03Z","published":"2025-10-27T17:11:03Z","title":"Sequential Multi-Agent Dynamic Algorithm Configuration","summary":"  Dynamic algorithm configuration (DAC) is a recent trend in automated machine\nlearning, which can dynamically adjust the algorithm's configuration during the\nexecution process and relieve users from tedious trial-and-error tuning tasks.\nRecently, multi-agent reinforcement learning (MARL) approaches have improved\nthe configuration of multiple heterogeneous hyperparameters, making various\nparameter configurations for complex algorithms possible. However, many complex\nalgorithms have inherent inter-dependencies among multiple parameters (e.g.,\ndetermining the operator type first and then the operator's parameter), which\nare, however, not considered in previous approaches, thus leading to\nsub-optimal results. In this paper, we propose the sequential multi-agent DAC\n(Seq-MADAC) framework to address this issue by considering the inherent\ninter-dependencies of multiple parameters. Specifically, we propose a\nsequential advantage decomposition network, which can leverage action-order\ninformation through sequential advantage decomposition. Experiments from\nsynthetic functions to the configuration of multi-objective optimization\nalgorithms demonstrate Seq-MADAC's superior performance over state-of-the-art\nMARL methods and show strong generalization across problem classes. Seq-MADAC\nestablishes a new paradigm for the widespread dependency-aware automated\nalgorithm configuration. Our code is available at\nhttps://github.com/lamda-bbo/seq-madac.\n","authors":["Chen Lu","Ke Xue","Lei Yuan","Yao Wang","Yaoyuan Wang","Sheng Fu","Chao Qian"],"pdf_url":"https://arxiv.org/pdf/2510.23535v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.21280v2","updated":"2025-10-27T17:10:50Z","published":"2025-10-24T09:25:31Z","title":"WhaleVAD-BPN: Improving Baleen Whale Call Detection with Boundary\n  Proposal Networks and Post-processing Optimisation","summary":"  While recent sound event detection (SED) systems can identify baleen whale\ncalls in marine audio, challenges related to false positive and minority-class\ndetection persist. We propose the boundary proposal network (BPN), which\nextends an existing lightweight SED system. The BPN is inspired by work in\nimage object detection and aims to reduce the number of false positive\ndetections. It achieves this by using intermediate latent representations\ncomputed within the backbone classification model to gate the final output.\nWhen added to an existing SED system, the BPN achieves a 16.8 % absolute\nincrease in precision, as well as 21.3 % and 9.4 % improvements in the F1-score\nfor minority-class d-calls and bp-calls, respectively. We further consider two\napproaches to the selection of post-processing hyperparameters: a\nforward-search and a backward-search. By separately optimising event-level and\nframe-level hyperparameters, these two approaches lead to considerable\nperformance improvements over parameters selected using empirical methods. The\ncomplete WhaleVAD-BPN system achieves a cross-validated development F1-score of\n0.475, which is a 9.8 % absolute improvement over the baseline.\n","authors":["Christiaan M. Geldenhuys","GÃ¼nther Tonitz","Thomas R. Niesler"],"pdf_url":"https://arxiv.org/pdf/2510.21280v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23534v1","updated":"2025-10-27T17:10:43Z","published":"2025-10-27T17:10:43Z","title":"Direct Debiased Machine Learning via Bregman Divergence Minimization","summary":"  We develop a direct debiased machine learning framework comprising Neyman\ntargeted estimation and generalized Riesz regression. Our framework unifies\nRiesz regression for automatic debiased machine learning, covariate balancing,\ntargeted maximum likelihood estimation (TMLE), and density-ratio estimation. In\nmany problems involving causal effects or structural models, the parameters of\ninterest depend on regression functions. Plugging regression functions\nestimated by machine learning methods into the identifying equations can yield\npoor performance because of first-stage bias. To reduce such bias, debiased\nmachine learning employs Neyman orthogonal estimating equations. Debiased\nmachine learning typically requires estimation of the Riesz representer and the\nregression function. For this problem, we develop a direct debiased machine\nlearning framework with an end-to-end algorithm. We formulate estimation of the\nnuisance parameters, the regression function and the Riesz representer, as\nminimizing the discrepancy between Neyman orthogonal scores computed with known\nand unknown nuisance parameters, which we refer to as Neyman targeted\nestimation. Neyman targeted estimation includes Riesz representer estimation,\nand we measure discrepancies using the Bregman divergence. The Bregman\ndivergence encompasses various loss functions as special cases, where the\nsquared loss yields Riesz regression and the Kullback-Leibler divergence yields\nentropy balancing. We refer to this Riesz representer estimation as generalized\nRiesz regression. Neyman targeted estimation also yields TMLE as a special case\nfor regression function estimation. Furthermore, for specific pairs of models\nand Riesz representer estimation methods, we can automatically obtain the\ncovariate balancing property without explicitly solving the covariate balancing\nobjective.\n","authors":["Masahiro Kato"],"pdf_url":"https://arxiv.org/pdf/2510.23534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23532v1","updated":"2025-10-27T17:09:16Z","published":"2025-10-27T17:09:16Z","title":"When No Paths Lead to Rome: Benchmarking Systematic Neural Relational\n  Reasoning","summary":"  Designing models that can learn to reason in a systematic way is an important\nand long-standing challenge. In recent years, a wide range of solutions have\nbeen proposed for the specific case of systematic relational reasoning,\nincluding Neuro-Symbolic approaches, variants of the Transformer architecture,\nand specialised Graph Neural Networks. However, existing benchmarks for\nsystematic relational reasoning focus on an overly simplified setting, based on\nthe assumption that reasoning can be reduced to composing relational paths. In\nfact, this assumption is hard-baked into the architecture of several recent\nmodels, leading to approaches that can perform well on existing benchmarks but\nare difficult to generalise to other settings. To support further progress in\nthe field of systematic relational reasoning with neural networks, we introduce\nNoRA, a new benchmark which adds several levels of difficulty and requires\nmodels to go beyond path-based reasoning.\n","authors":["Anirban Das","Irtaza Khalid","Rafael PeÃ±aloza","Steven Schockaert"],"pdf_url":"https://arxiv.org/pdf/2510.23532v1.pdf","comment":"accepted at NeurIPS 2025 D&B track"},{"id":"http://arxiv.org/abs/2510.23530v1","updated":"2025-10-27T17:08:27Z","published":"2025-10-27T17:08:27Z","title":"Learning Linearity in Audio Consistency Autoencoders via Implicit\n  Regularization","summary":"  Audio autoencoders learn useful, compressed audio representations, but their\nnon-linear latent spaces prevent intuitive algebraic manipulation such as\nmixing or scaling. We introduce a simple training methodology to induce\nlinearity in a high-compression Consistency Autoencoder (CAE) by using data\naugmentation, thereby inducing homogeneity (equivariance to scalar gain) and\nadditivity (the decoder preserves addition) without altering the model's\narchitecture or loss function. When trained with our method, the CAE exhibits\nlinear behavior in both the encoder and decoder while preserving reconstruction\nfidelity. We test the practical utility of our learned space on music source\ncomposition and separation via simple latent arithmetic. This work presents a\nstraightforward technique for constructing structured latent spaces, enabling\nmore intuitive and efficient audio processing.\n","authors":["Bernardo Torres","Manuel Moussallam","Gabriel Meseguer-Brocal"],"pdf_url":"https://arxiv.org/pdf/2510.23530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23524v1","updated":"2025-10-27T17:02:30Z","published":"2025-10-27T17:02:30Z","title":"Toward Carbon-Neutral Human AI: Rethinking Data, Computation, and\n  Learning Paradigms for Sustainable Intelligence","summary":"  The rapid advancement of Artificial Intelligence (AI) has led to\nunprecedented computational demands, raising significant environmental and\nethical concerns. This paper critiques the prevailing reliance on large-scale,\nstatic datasets and monolithic training paradigms, advocating for a shift\ntoward human-inspired, sustainable AI solutions. We introduce a novel\nframework, Human AI (HAI), which emphasizes incremental learning, carbon-aware\noptimization, and human-in-the-loop collaboration to enhance adaptability,\nefficiency, and accountability. By drawing parallels with biological cognition\nand leveraging dynamic architectures, HAI seeks to balance performance with\necological responsibility. We detail the theoretical foundations, system\ndesign, and operational principles that enable AI to learn continuously and\ncontextually while minimizing carbon footprints and human annotation costs. Our\napproach addresses pressing challenges in active learning, continual\nadaptation, and energy-efficient model deployment, offering a pathway toward\nresponsible, human-centered artificial intelligence.\n","authors":["KC Santosh","Rodrigue Rizk","Longwei Wang"],"pdf_url":"https://arxiv.org/pdf/2510.23524v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2507.20499v2","updated":"2025-10-27T17:00:52Z","published":"2025-07-28T03:34:15Z","title":"DmC: Nearest Neighbor Guidance Diffusion Model for Offline Cross-domain\n  Reinforcement Learning","summary":"  Cross-domain offline reinforcement learning (RL) seeks to enhance sample\nefficiency in offline RL by utilizing additional offline source datasets. A key\nchallenge is to identify and utilize source samples that are most relevant to\nthe target domain. Existing approaches address this challenge by measuring\ndomain gaps through domain classifiers, target transition dynamics modeling, or\nmutual information estimation using contrastive loss. However, these methods\noften require large target datasets, which is impractical in many real-world\nscenarios. In this work, we address cross-domain offline RL under a limited\ntarget data setting, identifying two primary challenges: (1) Dataset imbalance,\nwhich is caused by large source and small target datasets and leads to\noverfitting in neural network-based domain gap estimators, resulting in\nuninformative measurements; and (2) Partial domain overlap, where only a subset\nof the source data is closely aligned with the target domain. To overcome these\nissues, we propose DmC, a novel framework for cross-domain offline RL with\nlimited target samples. Specifically, DmC utilizes $k$-nearest neighbor\n($k$-NN) based estimation to measure domain proximity without neural network\ntraining, effectively mitigating overfitting. Then, by utilizing this domain\nproximity, we introduce a nearest-neighbor-guided diffusion model to generate\nadditional source samples that are better aligned with the target domain, thus\nenhancing policy learning with more effective source samples. Through\ntheoretical analysis and extensive experiments in diverse MuJoCo environments,\nwe demonstrate that DmC significantly outperforms state-of-the-art cross-domain\noffline RL methods, achieving substantial performance gains.\n","authors":["Linh Le Pham Van","Minh Hoang Nguyen","Duc Kieu","Hung Le","Hung The Tran","Sunil Gupta"],"pdf_url":"https://arxiv.org/pdf/2507.20499v2.pdf","comment":"accepted at ECAI 2025; offline cross-domain reinforcement learning\n  with a guided diffusion model;"},{"id":"http://arxiv.org/abs/2510.23507v1","updated":"2025-10-27T16:40:52Z","published":"2025-10-27T16:40:52Z","title":"A Deep Latent Factor Graph Clustering with Fairness-Utility Trade-off\n  Perspective","summary":"  Fair graph clustering seeks partitions that respect network structure while\nmaintaining proportional representation across sensitive groups, with\napplications spanning community detection, team formation, resource allocation,\nand social network analysis. Many existing approaches enforce rigid constraints\nor rely on multi-stage pipelines (e.g., spectral embedding followed by\n$k$-means), limiting trade-off control, interpretability, and scalability. We\nintroduce \\emph{DFNMF}, an end-to-end deep nonnegative tri-factorization\ntailored to graphs that directly optimizes cluster assignments with a soft\nstatistical-parity regularizer. A single parameter $\\lambda$ tunes the\nfairness--utility balance, while nonnegativity yields parts-based factors and\ntransparent soft memberships. The optimization uses sparse-friendly alternating\nupdates and scales near-linearly with the number of edges. Across synthetic and\nreal networks, DFNMF achieves substantially higher group balance at comparable\nmodularity, often dominating state-of-the-art baselines on the Pareto front.\nThe code is available at https://github.com/SiamakGhodsi/DFNMF.git.\n","authors":["Siamak Ghodsi","Amjad Seyedi","Tai Le Quy","Fariba Karimi","Eirini Ntoutsi"],"pdf_url":"https://arxiv.org/pdf/2510.23507v1.pdf","comment":"Accepted to IEEE Big-Data 2025 main research track. The paper is 10\n  main pages and 4 pages of Appendix"}],"Multimedia":[{"id":"http://arxiv.org/abs/2509.04086v2","updated":"2025-10-27T14:28:49Z","published":"2025-09-04T10:32:40Z","title":"TEn-CATG:Text-Enriched Audio-Visual Video Parsing with Multi-Scale\n  Category-Aware Temporal Graph","summary":"  Audio-visual video parsing (AVVP) aims to detect event categories and their\ntemporal boundaries in videos, typically under weak supervision. Existing\nmethods mainly focus on (i) improving temporal modeling using attention-based\narchitectures or (ii) generating richer pseudo-labels to address the absence of\nframe-level annotations. However, attention-based models often overfit noisy\npseudo-labels, leading to cumulative training errors, while pseudo-label\ngeneration approaches distribute attention uniformly across frames, weakening\ntemporal localization accuracy. To address these challenges, we propose\nTEn-CATG, a text-enriched AVVP framework that combines semantic calibration\nwith category-aware temporal reasoning. More specifically, we design a\nbi-directional text fusion (BiT) module by leveraging audio-visual features as\nsemantic anchors to refine text embeddings, which departs from conventional\ntext-to-feature alignment, thereby mitigating noise and enhancing cross-modal\nconsistency. Furthermore, we introduce the category-aware temporal graph (CATG)\nmodule to model temporal relationships by selecting multi-scale temporal\nneighbors and learning category-specific temporal decay factors, enabling\neffective event-dependent temporal reasoning. Extensive experiments demonstrate\nthat TEn-CATG achieves state-of-the-art results across multiple evaluation\nmetrics on benchmark datasets LLP and UnAV-100, highlighting its robustness and\nsuperior ability to capture complex temporal and semantic dependencies in\nweakly supervised AVVP tasks.\n","authors":["Yaru Chen","Faegheh Sardari","Peiliang Zhang","Ruohao Guo","Yang Xiang","Zhenbo Li","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2509.04086v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23299v1","updated":"2025-10-27T13:05:27Z","published":"2025-10-27T13:05:27Z","title":"MMSD3.0: A Multi-Image Benchmark for Real-World Multimodal Sarcasm\n  Detection","summary":"  Despite progress in multimodal sarcasm detection, existing datasets and\nmethods predominantly focus on single-image scenarios, overlooking potential\nsemantic and affective relations across multiple images. This leaves a gap in\nmodeling cases where sarcasm is triggered by multi-image cues in real-world\nsettings. To bridge this gap, we introduce MMSD3.0, a new benchmark composed\nentirely of multi-image samples curated from tweets and Amazon reviews. We\nfurther propose the Cross-Image Reasoning Model (CIRM), which performs targeted\ncross-image sequence modeling to capture latent inter-image connections. In\naddition, we introduce a relevance-guided, fine-grained cross-modal fusion\nmechanism based on text-image correspondence to reduce information loss during\nintegration. We establish a comprehensive suite of strong and representative\nbaselines and conduct extensive experiments, showing that MMSD3.0 is an\neffective and reliable benchmark that better reflects real-world conditions.\nMoreover, CIRM demonstrates state-of-the-art performance across MMSD, MMSD2.0\nand MMSD3.0, validating its effectiveness in both single-image and multi-image\nscenarios.\n","authors":["Haochen Zhao","Yuyao Kong","Yongxiu Xu","Gaopeng Gou","Hongbo Xu","Yubin Wang","Haoliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.23299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01737v2","updated":"2025-10-27T08:33:50Z","published":"2024-10-02T16:47:55Z","title":"Robust Modality-incomplete Anomaly Detection: A Modality-instructive\n  Framework with Benchmark","summary":"  Multimodal Industrial Anomaly Detection (MIAD), which utilizes 3D point\nclouds and 2D RGB images to identify abnormal regions in products, plays a\ncrucial role in industrial quality inspection. However, traditional MIAD\nsettings assume that all 2D and 3D modalities are paired, ignoring the fact\nthat multimodal data collected from the real world is often imperfect due to\nmissing modalities. Additionally, models trained on modality-incomplete data\nare prone to overfitting. Therefore, MIAD models that demonstrate robustness\nagainst modality-incomplete data are highly desirable in practice. To address\nthis, we introduce a pioneering study that comprehensively investigates\nModality-Incomplete Industrial Anomaly Detection (MIIAD), and under the\nguidance of experts, we construct the MIIAD Bench with rich modality-missing\nsettings to account for imperfect learning environments with incomplete\nmultimodal information. As expected, we find that most existing MIAD methods\nperform poorly on the MIIAD Bench, leading to significant performance\ndegradation. To tackle this challenge, we propose a novel two-stage Robust\nmodAlity-aware fusing and Detecting framewoRk, abbreviated as RADAR.\nSpecifically: i) We propose Modality-incomplete Instruction to guide the\nmultimodal Transformer to robustly adapt to various modality-incomplete\nscenarios, and implement adaptive parameter learning based on HyperNetwork. ii)\nThen, we construct a Double-Pseudo Hybrid Module to highlight the uniqueness of\nmodality combinations, mitigating overfitting issues and further enhancing the\nrobustness of the MIIAD model. Our experimental results demonstrate that the\nproposed RADAR significantly outperforms traditional MIAD methods on our newly\ncreated MIIAD dataset, proving its practical application value.\n","authors":["Bingchen Miao","Wenqiao Zhang","Juncheng Li","Wangyu Wu","Siliang Tang","Zhaocheng Li","Haochen Shi","Jun Xiao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2410.01737v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23056v1","updated":"2025-10-27T06:39:50Z","published":"2025-10-27T06:39:50Z","title":"Enabling American Sign Language Communication Under Low Data Rates","summary":"  In recent years, video conferencing applications have become increasingly\nprevalent, relying heavily on high-speed internet connectivity. When such\nconnectivity is lacking, users often default to audio-only communication, a\nmode that significantly disadvantages American Sign Language (ASL) users, whose\ncommunication relies on hand gestures, body movement, and facial expressions.\nIn this work, we introduce VC4ASL, a system designed to enable ASL\ncommunication over the audio channel of existing video conferencing\napplications, even in the absence of reliable video. VC4ASL integrates\nseamlessly with current platforms without requiring any modifications. Our\napproach establishes a communication channel through audio by encoding and\ntransmitting human pose information, which is then rendered to reconstruct\nsigned content. We propose novel receive-side error detection and correction\nmechanisms that exploit the inherent structural constraints of human pose data.\nTo evaluate the system, we simulate network-degraded environments, generate\npose-based ASL video sequences, and conduct user studies to assess\ncomprehension among ASL users. Experimental results demonstrate that VC4ASL\neffectively facilitates intelligible ASL communication over audio in\nlow-bandwidth scenarios where video transmission is impaired.\n","authors":["Panneer Selvam Santhalingam","Swann Thantsin","Ahmad Kamari","Parth Pathak","Kenneth De Haan"],"pdf_url":"https://arxiv.org/pdf/2510.23056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23449v3","updated":"2025-10-27T06:39:35Z","published":"2025-05-29T13:56:21Z","title":"CMIE: Combining MLLM Insights with External Evidence for Explainable\n  Out-of-Context Misinformation Detection","summary":"  Multimodal large language models (MLLMs) have demonstrated impressive\ncapabilities in visual reasoning and text generation. While previous studies\nhave explored the application of MLLM for detecting out-of-context (OOC)\nmisinformation, our empirical analysis reveals two persisting challenges of\nthis paradigm. Evaluating the representative GPT-4o model on direct reasoning\nand evidence augmented reasoning, results indicate that MLLM struggle to\ncapture the deeper relationships-specifically, cases in which the image and\ntext are not directly connected but are associated through underlying semantic\nlinks. Moreover, noise in the evidence further impairs detection accuracy. To\naddress these challenges, we propose CMIE, a novel OOC misinformation detection\nframework that incorporates a Coexistence Relationship Generation (CRG)\nstrategy and an Association Scoring (AS) mechanism. CMIE identifies the\nunderlying coexistence relationships between images and text, and selectively\nutilizes relevant evidence to enhance misinformation detection. Experimental\nresults demonstrate that our approach outperforms existing methods.\n","authors":["Fanxiao Li","Jiaying Wu","Canyuan He","Wei Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.23449v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05715v2","updated":"2025-10-27T01:44:45Z","published":"2025-07-08T06:58:24Z","title":"From ID-based to ID-free: Rethinking ID Effectiveness in Multimodal\n  Collaborative Filtering Recommendation","summary":"  Most existing multimodal collaborative filtering recommendation (MCFRec)\nmethods rely heavily on ID features and multimodal content to enhance\nrecommendation performance. However, this paper reveals that ID features are\neffective but have limited benefits in multimodal collaborative filtering\nrecommendation. Therefore, this paper systematically deconstruct the pros and\ncons of ID features: (i) they provide initial embedding but lack semantic\nrichness, (ii) they provide a unique identifier for each user and item but\nhinder generalization to untrained data, and (iii) they assist in aligning and\nfusing multimodal features but may lead to representation shift. Based on these\ninsights, this paper proposes IDFREE, an ID-free multimodal collaborative\nFiltering REcommEndation baseline. IDFREE replaces ID features with multimodal\nfeatures and positional encodings to generate semantically meaningful ID-free\nembeddings. For ID-free multimodal collaborative filtering, it further proposes\nan adaptive similarity graph module to construct dynamic user-user and\nitem-item graphs based on multimodal features. Then, an augmented user-item\ngraph encoder is proposed to construct more effective user and item encoding.\nFinally, IDFREE achieves inter-multimodal alignment based on the contrastive\nlearning and uses Softmax loss as recommendation loss. Basic experiments on\nthree public datasets demonstrate that IDFREE outperforms existing ID-based\nMCFRec methods, achieving an average performance gain of 72.24% across standard\nmetrics (Recall@5, 10, 20, 50 and NDCG@5, 10, 20, 50). Exploratory and extended\nexperiments further validate our findings on the limitations of ID features in\nMCFRec. The code is released at https://github.com/G-H-Li/IDFREE.\n","authors":["Guohao Li","Li Jing","Jia Wu","Xuefei Li","Kai Zhu","Yue He"],"pdf_url":"https://arxiv.org/pdf/2507.05715v2.pdf","comment":"We identified that our current approach achieves its reported\n  performance only under specific data conditions, and its robustness is weaker\n  than we initially expected"},{"id":"http://arxiv.org/abs/2502.10999v2","updated":"2025-10-27T00:52:27Z","published":"2025-02-16T05:30:18Z","title":"ControlText: Unlocking Controllable Fonts in Multilingual Text Rendering\n  without Font Annotations","summary":"  This work demonstrates that diffusion models can achieve font-controllable\nmultilingual text rendering using just raw images without font label\nannotations.Visual text rendering remains a significant challenge. While recent\nmethods condition diffusion on glyphs, it is impossible to retrieve exact font\nannotations from large-scale, real-world datasets, which prevents\nuser-specified font control. To address this, we propose a data-driven solution\nthat integrates the conditional diffusion model with a text segmentation model,\nutilizing segmentation masks to capture and represent fonts in pixel space in a\nself-supervised manner, thereby eliminating the need for any ground-truth\nlabels and enabling users to customize text rendering with any multilingual\nfont of their choice. The experiment provides a proof of concept of our\nalgorithm in zero-shot text and font editing across diverse fonts and\nlanguages, providing valuable insights for the community and industry toward\nachieving generalized visual text rendering. Code is available at\ngithub.com/bowen-upenn/ControlText.\n","authors":["Bowen Jiang","Yuan Yuan","Xinyi Bai","Zhuoqun Hao","Alyson Yin","Yaojie Hu","Wenyu Liao","Lyle Ungar","Camillo J. Taylor"],"pdf_url":"https://arxiv.org/pdf/2502.10999v2.pdf","comment":"The 2025 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP) Findings"},{"id":"http://arxiv.org/abs/2209.08884v2","updated":"2025-10-27T23:54:59Z","published":"2022-09-19T09:39:29Z","title":"Adaptive 3D Mesh Steganography Based on Feature-Preserving Distortion","summary":"  Current 3D mesh steganography algorithms relying on geometric modification\nare prone to detection by steganalyzers. In traditional steganography, adaptive\nsteganography has proven to be an efficient means of enhancing steganography\nsecurity. Taking inspiration from this, we propose a highly adaptive embedding\nalgorithm, guided by the principle of minimizing a carefully crafted distortion\nthrough efficient steganography codes. Specifically, we tailor a\npayload-limited embedding optimization problem for 3D settings and devise a\nfeature-preserving distortion (FPD) to measure the impact of message embedding.\nThe distortion takes on an additive form and is defined as a weighted\ndifference of the effective steganalytic subfeatures utilized by the current 3D\nsteganalyzers. With practicality in mind, we refine the distortion to enhance\nrobustness and computational efficiency. By minimizing the FPD, our algorithm\ncan preserve mesh features to a considerable extent, including steganalytic and\ngeometric features, while achieving a high embedding capacity. During the\npractical embedding phase, we employ the Q-layered syndrome trellis code (STC).\nHowever, calculating the bit modification probability (BMP) for each layer of\nthe Q-layered STC, given the variation of Q, can be cumbersome. To address this\nissue, we design a universal and automatic approach for the BMP calculation.\nThe experimental results demonstrate that our algorithm achieves\nstate-of-the-art performance in countering 3D steganalysis. Code is available\nat https://github.com/zjhJOJO/3D-steganography-based-on-FPD.git.\n","authors":["Yushu Zhang","Jiahao Zhu","Mignfu Xue","Xinpeng Zhang","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2209.08884v2.pdf","comment":"IEEE TVCG, corresponding author Jiahao Zhu, code is available at\n  https://github.com/zjhJOJO/3D-steganography-based-on-FPD.git"}]},"2025-10-26T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2510.22865v1","updated":"2025-10-26T23:16:51Z","published":"2025-10-26T23:16:51Z","title":"Civic Ground Truth in News Recommenders: A Method for Public Value\n  Scoring","summary":"  Research in news recommendation systems (NRS) continues to explore the best\nways to integrate normative goals such as editorial objectives and public\nservice values into existing systems. Prior efforts have incorporated expert\ninput or audience feedback to quantify these values, laying the groundwork for\nmore civic-minded recommender systems. This paper contributes to that\ntrajectory, introducing a method for embedding civic values into NRS through\nlarge-scale, structured audience evaluations. The proposed civic ground truth\napproach aims to generate value-based labels through a nationally\nrepresentative survey that are generalisable across a wider news corpus, using\nautomated metadata enrichment.\n","authors":["James Meese","Kyle Herbertson"],"pdf_url":"https://arxiv.org/pdf/2510.22865v1.pdf","comment":"Presented at NORMalize 2025: The Third Workshop on the Normative\n  Design and Evaluation of Recommender Systems, co-located with the ACM\n  Conference on Recommender Systems 2025 (RecSys 2025), Prague"},{"id":"http://arxiv.org/abs/2502.14305v2","updated":"2025-10-26T23:07:43Z","published":"2025-02-20T06:40:12Z","title":"Scaling Down, Serving Fast: Compressing and Deploying Efficient LLMs for\n  Recommendation Systems","summary":"  Large language models (LLMs) have demonstrated remarkable performance across\na wide range of industrial applications, from search and recommendation systems\nto generative tasks. Although scaling laws indicate that larger models\ngenerally yield better generalization and performance, their substantial\ncomputational requirements often render them impractical for many real-world\nscenarios at scale. In this paper, we present a comprehensive set of insights\nfor training and deploying small language models (SLMs) that deliver high\nperformance for a variety of industry use cases. We focus on two key\ntechniques: (1) knowledge distillation and (2) model compression via structured\npruning and quantization. These approaches enable SLMs to retain much of the\nquality of their larger counterparts while significantly reducing\ntraining/serving costs and latency. We detail the impact of these techniques on\na variety of use cases in a large professional social network platform and\nshare deployment lessons, including hardware optimization strategies that\nimprove speed and throughput for both predictive and reasoning-based\napplications in Recommendation Systems.\n","authors":["Kayhan Behdin","Ata Fatahibaarzi","Qingquan Song","Yun Dai","Aman Gupta","Zhipeng Wang","Shao Tang","Hejian Sang","Gregory Dexter","Sirou Zhu","Siyu Zhu","Tejas Dharamsi","Vignesh Kothapalli","Zhoutong Fu","Yihan Cao","Pin-Lun Hsu","Fedor Borisyuk","Natesh Pillai","Luke Simon","Rahul Mazumder"],"pdf_url":"https://arxiv.org/pdf/2502.14305v2.pdf","comment":"Accepted to EMNLP 2025 Industry Track - Oral Presentation"},{"id":"http://arxiv.org/abs/2510.02656v2","updated":"2025-10-26T21:25:59Z","published":"2025-10-03T01:21:55Z","title":"A Simple but Effective Elaborative Query Reformulation Approach for\n  Natural Language Recommendation","summary":"  Natural Language (NL) recommender systems aim to retrieve relevant items from\nfree-form user queries and item descriptions. Existing systems often rely on\ndense retrieval (DR), which struggles to interpret challenging queries that\nexpress broad (e.g., \"cities for youth friendly activities\") or indirect (e.g.,\n\"cities for a high school graduation trip\") user intents. While query\nreformulation (QR) has been widely adopted to improve such systems, existing QR\nmethods tend to focus only on expanding the range of query subtopics (breadth)\nor elaborating on the potential meaning of a query (depth), but not both. In\nthis paper, we propose EQR (Elaborative Subtopic Query Reformulation), a large\nlanguage model-based QR method that combines both breadth and depth by\ngenerating potential query subtopics with information-rich elaborations. We\nalso introduce three new natural language recommendation benchmarks in travel,\nhotel, and restaurant domains to establish evaluation of NL recommendation with\nchallenging queries. Experiments show EQR substantially outperforms\nstate-of-the-art QR methods in various evaluation metrics, highlighting that a\nsimple yet effective QR approach can significantly improve NL recommender\nsystems for queries with broad and indirect user intents.\n","authors":["Qianfeng Wen","Yifan Liu","Justin Cui","Joshua Zhang","Anton Korikov","George-Kirollos Saad","Scott Sanner"],"pdf_url":"https://arxiv.org/pdf/2510.02656v2.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.22739v1","updated":"2025-10-26T16:15:50Z","published":"2025-10-26T16:15:50Z","title":"REVISION:Reflective Intent Mining and Online Reasoning Auxiliary for\n  E-commerce Visual Search System Optimization","summary":"  In Taobao e-commerce visual search, user behavior analysis reveals a large\nproportion of no-click requests, suggesting diverse and implicit user intents.\nThese intents are expressed in various forms and are difficult to mine and\ndiscover, thereby leading to the limited adaptability and lag in platform\nstrategies. This greatly restricts users' ability to express diverse intents\nand hinders the scalability of the visual search system. This mismatch between\nuser implicit intent expression and system response defines the User-SearchSys\nIntent Discrepancy. To alleviate the issue, we propose a novel framework\nREVISION. This framework integrates offline reasoning mining with online\ndecision-making and execution, enabling adaptive strategies to solve implicit\nuser demands. In the offline stage, we construct a periodic pipeline to mine\ndiscrepancies from historical no-click requests. Leveraging large models, we\nanalyze implicit intent factors and infer optimal suggestions by jointly\nreasoning over query and product metadata. These inferred suggestions serve as\nactionable insights for refining platform strategies. In the online stage,\nREVISION-R1-3B, trained on the curated offline data, performs holistic analysis\nover query images and associated historical products to generate optimization\nplans and adaptively schedule strategies across the search pipeline. Our\nframework offers a streamlined paradigm for integrating large models with\ntraditional search systems, enabling end-to-end intelligent optimization across\ninformation aggregation and user interaction. Experimental results demonstrate\nthat our approach improves the efficiency of implicit intent mining from\nlarge-scale search logs and significantly reduces the no-click rate.\n","authors":["Yiwen Tang","Qiuyu Zhao","Zenghui Sun","Jinsong Lan","Xiaoyong Zhu","Bo Zheng","Kaifu Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.22739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22733v1","updated":"2025-10-26T16:04:48Z","published":"2025-10-26T16:04:48Z","title":"$\\text{E}^2\\text{Rank}$: Your Text Embedding can Also be an Effective\n  and Efficient Listwise Reranker","summary":"  Text embedding models serve as a fundamental component in real-world search\napplications. By mapping queries and documents into a shared embedding space,\nthey deliver competitive retrieval performance with high efficiency. However,\ntheir ranking fidelity remains limited compared to dedicated rerankers,\nespecially recent LLM-based listwise rerankers, which capture fine-grained\nquery-document and document-document interactions. In this paper, we propose a\nsimple yet effective unified framework $\\text{E}^2\\text{Rank}$, means Efficient\nEmbedding-based Ranking (also means Embedding-to-Rank), which extends a single\ntext embedding model to perform both high-quality retrieval and listwise\nreranking through continued training under a listwise ranking objective,\nthereby achieving strong effectiveness with remarkable efficiency. By applying\ncosine similarity between the query and document embeddings as a unified\nranking function, the listwise ranking prompt, which is constructed from the\noriginal query and its candidate documents, serves as an enhanced query\nenriched with signals from the top-K documents, akin to pseudo-relevance\nfeedback (PRF) in traditional retrieval models. This design preserves the\nefficiency and representational quality of the base embedding model while\nsignificantly improving its reranking performance. Empirically,\n$\\textrm{E}^2\\text{Rank}$ achieves state-of-the-art results on the BEIR\nreranking benchmark and demonstrates competitive performance on the\nreasoning-intensive BRIGHT benchmark, with very low reranking latency. We also\nshow that the ranking training process improves embedding performance on the\nMTEB benchmark. Our findings indicate that a single embedding model can\neffectively unify retrieval and reranking, offering both computational\nefficiency and competitive ranking accuracy.\n","authors":["Qi Liu","Yanzhao Zhang","Mingxin Li","Dingkun Long","Pengjun Xie","Jiaxin Mao"],"pdf_url":"https://arxiv.org/pdf/2510.22733v1.pdf","comment":"Code and models are avaliable at https://alibaba-nlp.github.io/E2Rank"},{"id":"http://arxiv.org/abs/2510.22732v1","updated":"2025-10-26T16:03:39Z","published":"2025-10-26T16:03:39Z","title":"ATLAS: Actor-Critic Task-Completion with Look-ahead Action Simulation","summary":"  We observe that current state-of-the-art web-agents are unable to effectively\nadapt to new environments without neural network fine-tuning, without which\nthey produce inefficient execution plans due to a lack of awareness of the\nstructure and dynamics of the new environment. To address this limitation, we\nintroduce ATLAS (Actor-Critic Task-completion with Look-ahead Action\nSimulation), a memory-augmented agent that is able to make plans grounded in a\nmodel of the environment by simulating the consequences of those actions in\ncognitive space. Our agent starts by building a \"cognitive map\" by performing a\nlightweight curiosity driven exploration of the environment. The planner\nproposes candidate actions; the simulator predicts their consequences in\ncognitive space; a critic analyzes the options to select the best roll-out and\nupdate the original plan; and a browser executor performs the chosen action. On\nthe WebArena-Lite Benchmark, we achieve a 63% success rate compared to 53.9%\nsuccess rate for the previously published state-of-the-art. Unlike previous\nsystems, our modular architecture requires no website-specific LLM fine-tuning.\nAblations show sizable drops without the world-model, hierarchical planner, and\nlook-ahead-based replanner confirming their complementary roles within the\ndesign of our system\n","authors":["Jiali Cheng","Anjishnu Kumar","Roshan Lal","Rishi Rajasekaran","Hani Ramezani","Omar Zia Khan","Oleg Rokhlenko","Sunny Chiu-Webster","Gang Hua","Hadi Amiri"],"pdf_url":"https://arxiv.org/pdf/2510.22732v1.pdf","comment":"9 pages, NeurIPS 2025 Workshop on Language Agents and World Models"},{"id":"http://arxiv.org/abs/2510.22694v1","updated":"2025-10-26T14:36:16Z","published":"2025-10-26T14:36:16Z","title":"Windsock is Dancing: Adaptive Multimodal Retrieval-Augmented Generation","summary":"  Multimodal Retrieval-Augmented Generation (MRAG) has emerged as a promising\nmethod to generate factual and up-to-date responses of Multimodal Large\nLanguage Models (MLLMs) by incorporating non-parametric knowledge from external\nknowledge bases. However, existing MRAG approaches suffer from static retrieval\nstrategies, inflexible modality selection, and suboptimal utilization of\nretrieved information, leading to three critical challenges: determining when\nto retrieve, what modality to incorporate, and how to utilize retrieved\ninformation effectively. To address these challenges, we introduce Windsock, a\nquery-dependent module making decisions on retrieval necessity and modality\nselection, effectively reducing computational overhead and improving response\nquality. Additionally, we propose Dynamic Noise-Resistance (DANCE) Instruction\nTuning, an adaptive training strategy that enhances MLLMs' ability to utilize\nretrieved information while maintaining robustness against noise. Moreover, we\nadopt a self-assessment approach leveraging knowledge within MLLMs to convert\nquestion-answering datasets to MRAG training datasets. Extensive experiments\ndemonstrate that our proposed method significantly improves the generation\nquality by 17.07% while reducing 8.95% retrieval times.\n","authors":["Shu Zhao","Tianyi Shen","Nilesh Ahuja","Omesh Tickoo","Vijaykrishnan Narayanan"],"pdf_url":"https://arxiv.org/pdf/2510.22694v1.pdf","comment":"Accepted at NeurIPS 2025 UniReps Workshop"},{"id":"http://arxiv.org/abs/2510.22681v1","updated":"2025-10-26T13:51:45Z","published":"2025-10-26T13:51:45Z","title":"Diversification as Risk Minimization","summary":"  Users tend to remember failures of a search session more than its many\nsuccesses. This observation has led to work on search robustness, where systems\nare penalized if they perform very poorly on some queries. However, this\nprinciple of robustness has been overlooked within a single query. An ambiguous\nor underspecified query (e.g., ``jaguar'') can have several user intents, where\npopular intents often dominate the ranking, leaving users with minority intents\nunsatisfied. Although the diversification literature has long recognized this\nissue, existing metrics only model the average relevance across intents and\nprovide no robustness guarantees. More surprisingly, we show theoretically and\nempirically that many well-known diversification algorithms are no more robust\nthan a naive, non-diversified algorithm. To address this critical gap, we\npropose to frame diversification as a risk-minimization problem. We introduce\nVRisk, which measures the expected risk faced by the least-served fraction of\nintents in a query. Optimizing VRisk produces a robust ranking, reducing the\nlikelihood of poor user experiences. We then propose VRisker, a fast greedy\nre-ranker with provable approximation guarantees. Finally, experiments on NTCIR\nINTENT-2, TREC Web 2012, and MovieLens show the vulnerability of existing\nmethods. VRisker reduces worst-case intent failures by up to 33% with a minimal\n2% drop in average performance.\n","authors":["Rikiya Takehi","Fernando Diaz","Tetsuya Sakai"],"pdf_url":"https://arxiv.org/pdf/2510.22681v1.pdf","comment":"Preprint, accepted at WSDM 2026 (Full Paper). 16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.22670v1","updated":"2025-10-26T13:17:01Z","published":"2025-10-26T13:17:01Z","title":"Tools are under-documented: Simple Document Expansion Boosts Tool\n  Retrieval","summary":"  Large Language Models (LLMs) have recently demonstrated strong capabilities\nin tool use, yet progress in tool retrieval remains hindered by incomplete and\nheterogeneous tool documentation. To address this challenge, we introduce\nTool-DE, a new benchmark and framework that systematically enriches tool\ndocumentation with structured fields to enable more effective tool retrieval,\ntogether with two dedicated models, Tool-Embed and Tool-Rank. We design a\nscalable document expansion pipeline that leverages both open- and\nclosed-source LLMs to generate, validate, and refine enriched tool profiles at\nlow cost, producing large-scale corpora with 50k instances for embedding-based\nretrievers and 200k for rerankers. On top of this data, we develop two models\nspecifically tailored for tool retrieval: Tool-Embed, a dense retriever, and\nTool-Rank, an LLM-based reranker. Extensive experiments on ToolRet and Tool-DE\ndemonstrate that document expansion substantially improves retrieval\nperformance, with Tool-Embed and Tool-Rank achieving new state-of-the-art\nresults on both benchmarks. We further analyze the contribution of individual\nfields to retrieval effectiveness, as well as the broader impact of document\nexpansion on both training and evaluation. Overall, our findings highlight both\nthe promise and limitations of LLM-driven document expansion, positioning\nTool-DE, along with the proposed Tool-Embed and Tool-Rank, as a foundation for\nfuture research in tool retrieval.\n","authors":["Xuan Lu","Haohang Huang","Rui Meng","Yaohui Jin","Wenjun Zeng","Xiaoyu Shen"],"pdf_url":"https://arxiv.org/pdf/2510.22670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22590v1","updated":"2025-10-26T09:10:26Z","published":"2025-10-26T09:10:26Z","title":"ATOM: AdapTive and OptiMized dynamic temporal knowledge graph\n  construction using LLMs","summary":"  In today's rapidly expanding data landscape, knowledge extraction from\nunstructured text is vital for real-time analytics, temporal inference, and\ndynamic memory frameworks. However, traditional static knowledge graph (KG)\nconstruction often overlooks the dynamic and time-sensitive nature of\nreal-world data, limiting adaptability to continuous changes. Moreover, recent\nzero- or few-shot approaches that avoid domain-specific fine-tuning or reliance\non prebuilt ontologies often suffer from instability across multiple runs, as\nwell as incomplete coverage of key facts. To address these challenges, we\nintroduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that\nbuilds and continuously updates Temporal Knowledge Graphs (TKGs) from\nunstructured texts. ATOM splits input documents into minimal, self-contained\n\"atomic\" facts, improving extraction exhaustivity and stability. Then, it\nconstructs atomic TKGs from these facts while employing a dual-time modeling\nthat distinguishes when information is observed from when it is valid. The\nresulting atomic TKGs are subsequently merged in parallel. Empirical\nevaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17%\nbetter stability, and over 90% latency reduction compared to baseline methods,\ndemonstrating a strong scalability potential for dynamic TKG construction.\n","authors":["Yassir Lairgi","Ludovic Moncla","Khalid Benabdeslem","RÃ©my Cazabet","Pierre ClÃ©au"],"pdf_url":"https://arxiv.org/pdf/2510.22590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16101v3","updated":"2025-10-26T06:07:13Z","published":"2025-02-22T05:50:15Z","title":"Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the\n  Robustness of RAG Against Misleading Retrievals","summary":"  Retrieval-augmented generation (RAG) has shown impressive capabilities in\nmitigating hallucinations in large language models (LLMs). However, LLMs\nstruggle to maintain consistent reasoning when exposed to misleading or\nconflicting evidence, especially in real-world domains such as politics, where\ninformation is polarized or selectively framed. Mainstream RAG benchmarks\nevaluate models under clean retrieval settings, where systems generate answers\nfrom gold-standard documents, or under synthetically perturbed settings, where\ndocuments are artificially injected with noise. These assumptions fail to\nreflect real-world conditions, often leading to an overestimation of RAG system\nperformance. To address this gap, we introduce RAGuard, the first benchmark to\nevaluate the robustness of RAG systems against misleading retrievals. Unlike\nprior benchmarks that rely on synthetic noise, our fact-checking dataset\ncaptures naturally occurring misinformation by constructing its retrieval\ncorpus from Reddit discussions. It categorizes retrieved evidence into three\ntypes: supporting, misleading, and unrelated, providing a realistic and\nchallenging testbed for assessing how well RAG systems navigate different types\nof evidence. Our experiments reveal that, when exposed to potentially\nmisleading retrievals, all tested LLM-powered RAG systems perform worse than\ntheir zero-shot baselines (i.e., no retrieval at all), while human annotators\nconsistently perform better, highlighting LLMs' susceptibility to noisy\nenvironments. To our knowledge, RAGuard is the first benchmark to\nsystematically assess the robustness of the RAG against misleading evidence. We\nexpect this benchmark to drive future research toward improving RAG systems\nbeyond idealized datasets, making them more reliable for real-world\napplications. The dataset is available at\nhttps://huggingface.co/datasets/UCSC-IRKM/RAGuard.\n","authors":["Linda Zeng","Rithwik Gupta","Divij Motwani","Diji Yang","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.16101v3.pdf","comment":"Advances in Neural Information Processing Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.22521v1","updated":"2025-10-26T04:13:31Z","published":"2025-10-26T04:13:31Z","title":"Open Multimodal Retrieval-Augmented Factual Image Generation","summary":"  Large Multimodal Models (LMMs) have achieved remarkable progress in\ngenerating photorealistic and prompt-aligned images, but they often produce\noutputs that contradict verifiable knowledge, especially when prompts involve\nfine-grained attributes or time-sensitive events. Conventional\nretrieval-augmented approaches attempt to address this issue by introducing\nexternal information, yet they are fundamentally incapable of grounding\ngeneration in accurate and evolving knowledge due to their reliance on static\nsources and shallow evidence integration. To bridge this gap, we introduce\nORIG, an agentic open multimodal retrieval-augmented framework for Factual\nImage Generation (FIG), a new task that requires both visual realism and\nfactual grounding. ORIG iteratively retrieves and filters multimodal evidence\nfrom the web and incrementally integrates the refined knowledge into enriched\nprompts to guide generation. To support systematic evaluation, we build\nFIG-Eval, a benchmark spanning ten categories across perceptual, compositional,\nand temporal dimensions. Experiments demonstrate that ORIG substantially\nimproves factual consistency and overall image quality over strong baselines,\nhighlighting the potential of open multimodal retrieval for factual image\ngeneration.\n","authors":["Yang Tian","Fan Liu","Jingyuan Zhang","Wei Bi","Yupeng Hu","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2510.22521v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2505.12925v2","updated":"2025-10-26T03:34:29Z","published":"2025-05-19T10:07:51Z","title":"CPRet: A Dataset, Benchmark, and Model for Retrieval in Competitive\n  Programming","summary":"  Competitive programming benchmarks are widely used in scenarios such as\nprogramming contests and large language model assessments. However, the growing\npresence of duplicate or highly similar problems raises concerns not only about\ncompetition fairness, but also about the validity of competitive programming as\na benchmark for model evaluation. In this paper, we propose a new problem,\nsimilar question retrieval, to tackle this issue. Due to the lack of both data\nand models, solving this problem is challenging. To this end, we introduce\nCPRet, a retrieval-oriented benchmark suite for competitive programming,\ncovering four retrieval tasks: two code-centric (i.e., Text-to-Code,\nCode-to-Code) and two newly proposed problem-centric tasks (i.e.,\nProblem-to-Duplicate, Simplified-to-Full) built from a combination of\nautomatically crawled problem-solution data and manually curated annotations.\nOur contribution includes both high-quality training data and temporally\nseparated test sets for reliable evaluation. Besides, we further develop two\ntask-specialized retrievers based on this dataset: CPRetriever-Code, trained\nwith a novel Group-InfoNCE loss for problem-code alignment, and\nCPRetriever-Prob, fine-tuned for identifying problem-level similarity. Both\nmodels achieve strong results and are open-sourced for local use. Finally, we\nanalyze LiveCodeBench and find that high-similarity problems inflate model pass\nrates and reduce differentiation, underscoring the need for similarity-aware\nevaluation in future benchmarks.\n  Github: https://github.com/coldchair/CPRet\n  Online Demo: https://www.cpret.online/\n","authors":["Han Deng","Yuan Meng","Shixiang Tang","Wanli Ouyang","Xinzhu Ma"],"pdf_url":"https://arxiv.org/pdf/2505.12925v2.pdf","comment":"Accepted by NeurIPS 2025 Dataset and Benchmark Track"}],"Multimedia":[{"id":"http://arxiv.org/abs/2503.16421v3","updated":"2025-10-26T22:58:59Z","published":"2025-03-20T17:59:42Z","title":"MagicMotion: Controllable Video Generation with Dense-to-Sparse\n  Trajectory Guidance","summary":"  Recent advances in video generation have led to remarkable improvements in\nvisual quality and temporal coherence. Upon this, trajectory-controllable video\ngeneration has emerged to enable precise object motion control through\nexplicitly defined spatial paths. However, existing methods struggle with\ncomplex object movements and multi-object motion control, resulting in\nimprecise trajectory adherence, poor object consistency, and compromised visual\nquality. Furthermore, these methods only support trajectory control in a single\nformat, limiting their applicability in diverse scenarios. Additionally, there\nis no publicly available dataset or benchmark specifically tailored for\ntrajectory-controllable video generation, hindering robust training and\nsystematic evaluation. To address these challenges, we introduce MagicMotion, a\nnovel image-to-video generation framework that enables trajectory control\nthrough three levels of conditions from dense to sparse: masks, bounding boxes,\nand sparse boxes. Given an input image and trajectories, MagicMotion seamlessly\nanimates objects along defined trajectories while maintaining object\nconsistency and visual quality. Furthermore, we present MagicData, a\nlarge-scale trajectory-controlled video dataset, along with an automated\npipeline for annotation and filtering. We also introduce MagicBench, a\ncomprehensive benchmark that assesses both video quality and trajectory control\naccuracy across different numbers of objects. Extensive experiments demonstrate\nthat MagicMotion outperforms previous methods across various metrics. Our\nproject page are publicly available at\nhttps://quanhaol.github.io/magicmotion-site.\n","authors":["Quanhao Li","Zhen Xing","Rui Wang","Hui Zhang","Qi Dai","Zuxuan Wu"],"pdf_url":"https://arxiv.org/pdf/2503.16421v3.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2510.22829v1","updated":"2025-10-26T20:51:52Z","published":"2025-10-26T20:51:52Z","title":"LLM-based Fusion of Multi-modal Features for Commercial Memorability\n  Prediction","summary":"  This paper addresses the prediction of commercial (brand) memorability as\npart of \"Subtask 2: Commercial/Ad Memorability\" within the \"Memorability:\nPredicting movie and commercial memorability\" task at the MediaEval 2025\nworkshop competition. We propose a multimodal fusion system with a Gemma-3 LLM\nbackbone that integrates pre-computed visual (ViT) and textual (E5) features by\nmulti-modal projections. The model is adapted using Low-Rank Adaptation (LoRA).\nA heavily-tuned ensemble of gradient boosted trees serves as a baseline. A key\ncontribution is the use of LLM-generated rationale prompts, grounded in\nexpert-derived aspects of memorability, to guide the fusion model. The results\ndemonstrate that the LLM-based system exhibits greater robustness and\ngeneralization performance on the final test set, compared to the baseline.\n  The paper's codebase can be found at\nhttps://github.com/dsgt-arc/mediaeval-2025-memorability\n","authors":["Aleksandar Pramov"],"pdf_url":"https://arxiv.org/pdf/2510.22829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22812v1","updated":"2025-10-26T19:55:57Z","published":"2025-10-26T19:55:57Z","title":"Region-Adaptive Learned Hierarchical Encoding for 3D Gaussian Splatting\n  Data","summary":"  We introduce Region-Adaptive Learned Hierarchical Encoding (RALHE) for 3D\nGaussian Splatting (3DGS) data. While 3DGS has recently become popular for\nnovel view synthesis, the size of trained models limits its deployment in\nbandwidth-constrained applications such as volumetric media streaming. To\naddress this, we propose a learned hierarchical latent representation that\nbuilds upon the principles of \"overfitted\" learned image compression (e.g.,\nCool-Chic and C3) to efficiently encode 3DGS attributes. Unlike images, 3DGS\ndata have irregular spatial distributions of Gaussians (geometry) and consist\nof multiple attributes (signals) defined on the irregular geometry. Our codec\nis designed to account for these differences between images and 3DGS.\nSpecifically, we leverage the octree structure of the voxelized 3DGS geometry\nto obtain a hierarchical multi-resolution representation. Our approach overfits\nlatents to each Gaussian attribute under a global rate constraint. These\nlatents are decoded independently through a lightweight decoder network. To\nestimate the bitrate during training, we employ an autoregressive probability\nmodel that leverages octree-derived contexts from the 3D point structure. The\nmulti-resolution latents, decoder, and autoregressive entropy coding networks\nare jointly optimized for each Gaussian attribute. Experiments demonstrate that\nthe proposed RALHE compression framework achieves a rendering PSNR gain of up\nto 2dB at low bitrates (less than 1 MB) compared to the baseline 3DGS\ncompression methods.\n","authors":["Shashank N. Sridhara","Birendra Kathariya","Fangjun Pu","Peng Yin","Eduardo Pavez","Antonio Ortega"],"pdf_url":"https://arxiv.org/pdf/2510.22812v1.pdf","comment":"10 Pages, 5 Figures"},{"id":"http://arxiv.org/abs/2510.22760v1","updated":"2025-10-26T17:18:48Z","published":"2025-10-26T17:18:48Z","title":"Understanding What Is Not Said:Referring Remote Sensing Image\n  Segmentation with Scarce Expressions","summary":"  Referring Remote Sensing Image Segmentation (RRSIS) aims to segment instances\nin remote sensing images according to referring expressions. Unlike Referring\nImage Segmentation on general images, acquiring high-quality referring\nexpressions in the remote sensing domain is particularly challenging due to the\nprevalence of small, densely distributed objects and complex backgrounds. This\npaper introduces a new learning paradigm, Weakly Referring Expression Learning\n(WREL) for RRSIS, which leverages abundant class names as weakly referring\nexpressions together with a small set of accurate ones to enable efficient\ntraining under limited annotation conditions. Furthermore, we provide a\ntheoretical analysis showing that mixed-referring training yields a provable\nupper bound on the performance gap relative to training with fully annotated\nreferring expressions, thereby establishing the validity of this new setting.\nWe also propose LRB-WREL, which integrates a Learnable Reference Bank (LRB) to\nrefine weakly referring expressions through sample-specific prompt embeddings\nthat enrich coarse class-name inputs. Combined with a teacher-student\noptimization framework using dynamically scheduled EMA updates, LRB-WREL\nstabilizes training and enhances cross-modal generalization under noisy weakly\nreferring supervision. Extensive experiments on our newly constructed benchmark\nwith varying weakly referring data ratios validate both the theoretical\ninsights and the practical effectiveness of WREL and LRB-WREL, demonstrating\nthat they can approach or even surpass models trained with fully annotated\nreferring expressions.\n","authors":["Kai Ye","Bowen Liu","Jianghang Lin","Jiayi Ji","Pingyang Dai","Liujuan Cao"],"pdf_url":"https://arxiv.org/pdf/2510.22760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17343v2","updated":"2025-10-26T14:53:32Z","published":"2025-07-23T09:12:25Z","title":"Principled Multimodal Representation Learning","summary":"  Multimodal representation learning seeks to create a unified representation\nspace by integrating diverse data modalities to improve multimodal\nunderstanding. Traditional methods often depend on pairwise contrastive\nlearning, which relies on a predefined anchor modality, restricting alignment\nacross all modalities. Recent advances have investigated the simultaneous\nalignment of multiple modalities, yet several challenges remain, such as\nlimitations imposed by fixed anchor points and instability arising from\noptimizing the product of singular values. To address the challenges, in this\npaper, we propose Principled Multimodal Representation Learning (PMRL), a novel\nframework that achieves simultaneous alignment of multiple modalities without\nanchor dependency in a more stable manner. Specifically, grounded in the\ntheoretical insight that full alignment corresponds to a rank-1 Gram matrix,\nPMRL optimizes the dominant singular value of the representation matrix to\nalign modalities along a shared leading direction. We propose a softmax-based\nloss function that treats singular values as logits to prioritize the largest\nsingular value. Besides, instance-wise contrastive regularization on the\nleading eigenvectors maintains inter-instance separability and prevents\nrepresentation collapse. Extensive experiments across diverse tasks demonstrate\nPMRL's superiority compared to baseline methods. The source code will be\npublicly available.\n","authors":["Xiaohao Liu","Xiaobo Xia","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2507.17343v2.pdf","comment":"Corrected typos and updated experimental results. 32 pages, 9\n  figures, 10 tables"},{"id":"http://arxiv.org/abs/2510.22646v1","updated":"2025-10-26T12:17:33Z","published":"2025-10-26T12:17:33Z","title":"TVMC: Time-Varying Mesh Compression via Multi-Stage Anchor Mesh\n  Generation","summary":"  Time-varying meshes, characterized by dynamic connectivity and varying vertex\ncounts, hold significant promise for applications such as augmented reality.\nHowever, their practical utilization remains challenging due to the substantial\ndata volume required for high-fidelity representation. While various\ncompression methods attempt to leverage temporal redundancy between consecutive\nmesh frames, most struggle with topological inconsistency and motion-induced\nartifacts. To address these issues, we propose Time-Varying Mesh Compression\n(TVMC), a novel framework built on multi-stage coarse-to-fine anchor mesh\ngeneration for inter-frame prediction. Specifically, the anchor mesh is\nprogressively constructed in three stages: initial, coarse, and fine. The\ninitial anchor mesh is obtained through fast topology alignment to exploit\ntemporal coherence. A Kalman filter-based motion estimation module then\ngenerates a coarse anchor mesh by accurately compensating inter-frame motions.\nSubsequently, a Quadric Error Metric-based refinement step optimizes vertex\npositions to form a fine anchor mesh with improved geometric fidelity. Based on\nthe refined anchor mesh, the inter-frame motions relative to the reference base\nmesh are encoded, while the residual displacements between the subdivided fine\nanchor mesh and the input mesh are adaptively quantized and compressed. This\nhierarchical strategy preserves consistent connectivity and high-quality\nsurface approximation, while achieving an efficient and compact representation\nof dynamic geometry. Extensive experiments on standard MPEG dynamic mesh\nsequences demonstrate that TVMC achieves state-of-the-art compression\nperformance. Compared to the latest V-DMC standard, it delivers a significant\nBD-rate gain of 10.2% ~ 16.9%, while preserving high reconstruction quality.\nThe code is available at https://github.com/H-Huang774/TVMC.\n","authors":["He Huang","Qi Yang","Yiling Xu","Zhu Li","Jenq-Neng Hwang"],"pdf_url":"https://arxiv.org/pdf/2510.22646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22622v1","updated":"2025-10-26T10:40:52Z","published":"2025-10-26T10:40:52Z","title":"DeepfakeBench-MM: A Comprehensive Benchmark for Multimodal Deepfake\n  Detection","summary":"  The misuse of advanced generative AI models has resulted in the widespread\nproliferation of falsified data, particularly forged human-centric audiovisual\ncontent, which poses substantial societal risks (e.g., financial fraud and\nsocial instability). In response to this growing threat, several works have\npreliminarily explored countermeasures. However, the lack of sufficient and\ndiverse training data, along with the absence of a standardized benchmark,\nhinder deeper exploration. To address this challenge, we first build Mega-MMDF,\na large-scale, diverse, and high-quality dataset for multimodal deepfake\ndetection. Specifically, we employ 21 forgery pipelines through the combination\nof 10 audio forgery methods, 12 visual forgery methods, and 6 audio-driven face\nreenactment methods. Mega-MMDF currently contains 0.1 million real samples and\n1.1 million forged samples, making it one of the largest and most diverse\nmultimodal deepfake datasets, with plans for continuous expansion. Building on\nit, we present DeepfakeBench-MM, the first unified benchmark for multimodal\ndeepfake detection. It establishes standardized protocols across the entire\ndetection pipeline and serves as a versatile platform for evaluating existing\nmethods as well as exploring novel approaches. DeepfakeBench-MM currently\nsupports 5 datasets and 11 multimodal deepfake detectors. Furthermore, our\ncomprehensive evaluations and in-depth analyses uncover several key findings\nfrom multiple perspectives (e.g., augmentation, stacked forgery). We believe\nthat DeepfakeBench-MM, together with our large-scale Mega-MMDF, will serve as\nfoundational infrastructures for advancing multimodal deepfake detection.\n","authors":["Kangran Zhao","Yupeng Chen","Xiaoyu Zhang","Yize Chen","Weinan Guan","Baicheng Chen","Chengzhe Sun","Soumyya Kanti Datta","Qingshan Liu","Siwei Lyu","Baoyuan Wu"],"pdf_url":"https://arxiv.org/pdf/2510.22622v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2501.06488v3","updated":"2025-10-26T10:20:24Z","published":"2025-01-11T09:12:43Z","title":"NVS-SQA: Exploring Self-Supervised Quality Representation Learning for\n  Neurally Synthesized Scenes without References","summary":"  Neural View Synthesis (NVS), such as NeRF and 3D Gaussian Splatting,\neffectively creates photorealistic scenes from sparse viewpoints, typically\nevaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However,\nthese full-reference methods, which compare synthesized views to reference\nviews, may not fully capture the perceptual quality of neurally synthesized\nscenes (NSS), particularly due to the limited availability of dense reference\nviews. Furthermore, the challenges in acquiring human perceptual labels hinder\nthe creation of extensive labeled datasets, risking model overfitting and\nreduced generalizability. To address these issues, we propose NVS-SQA, a NSS\nquality assessment method to learn no-reference quality representations through\nself-supervision without reliance on human labels. Traditional self-supervised\nlearning predominantly relies on the \"same instance, similar representation\"\nassumption and extensive datasets. However, given that these conditions do not\napply in NSS quality assessment, we employ heuristic cues and quality scores as\nlearning objectives, along with a specialized contrastive pair preparation\nprocess to improve the effectiveness and efficiency of learning. The results\nshow that NVS-SQA outperforms 17 no-reference methods by a large margin (i.e.,\non average 109.5% in SRCC, 98.6% in PLCC, and 91.5% in KRCC over the second\nbest) and even exceeds 16 full-reference methods across all evaluation metrics\n(i.e., 22.9% in SRCC, 19.1% in PLCC, and 18.6% in KRCC over the second best).\n","authors":["Qiang Qu","Yiran Shen","Xiaoming Chen","Yuk Ying Chung","Weidong Cai","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2501.06488v3.pdf","comment":"Accepted by TPAMI"},{"id":"http://arxiv.org/abs/2510.22571v1","updated":"2025-10-26T08:04:28Z","published":"2025-10-26T08:04:28Z","title":"STATUS Bench: A Rigorous Benchmark for Evaluating Object State\n  Understanding in Vision-Language Models","summary":"  Object state recognition aims to identify the specific condition of objects,\nsuch as their positional states (e.g., open or closed) and functional states\n(e.g., on or off). While recent Vision-Language Models (VLMs) are capable of\nperforming a variety of multimodal tasks, it remains unclear how precisely they\ncan identify object states. To alleviate this issue, we introduce the STAte and\nTransition UnderStanding Benchmark (STATUS Bench), the first benchmark for\nrigorously evaluating the ability of VLMs to understand subtle variations in\nobject states in diverse situations. Specifically, STATUS Bench introduces a\nnovel evaluation scheme that requires VLMs to perform three tasks\nsimultaneously: object state identification (OSI), image retrieval (IR), and\nstate change identification (SCI). These tasks are defined over our fully\nhand-crafted dataset involving image pairs, their corresponding object state\ndescriptions and state change descriptions. Furthermore, we introduce a\nlarge-scale training dataset, namely STATUS Train, which consists of 13 million\nsemi-automatically created descriptions. This dataset serves as the largest\nresource to facilitate further research in this area. In our experiments, we\ndemonstrate that STATUS Bench enables rigorous consistency evaluation and\nreveal that current state-of-the-art VLMs still significantly struggle to\ncapture subtle object state distinctions. Surprisingly, under the proposed\nrigorous evaluation scheme, most open-weight VLMs exhibited chance-level\nzero-shot performance. After fine-tuning on STATUS Train, Qwen2.5-VL achieved\nperformance comparable to Gemini 2.0 Flash. These findings underscore the\nnecessity of STATUS Bench and Train for advancing object state recognition in\nVLM research.\n","authors":["Mahiro Ukai","Shuhei Kurita","Nakamasa Inoue"],"pdf_url":"https://arxiv.org/pdf/2510.22571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22565v1","updated":"2025-10-26T07:44:26Z","published":"2025-10-26T07:44:26Z","title":"Learning Event-guided Exposure-agnostic Video Frame Interpolation via\n  Adaptive Feature Blending","summary":"  Exposure-agnostic video frame interpolation (VFI) is a challenging task that\naims to recover sharp, high-frame-rate videos from blurry, low-frame-rate\ninputs captured under unknown and dynamic exposure conditions. Event cameras\nare sensors with high temporal resolution, making them especially advantageous\nfor this task. However, existing event-guided methods struggle to produce\nsatisfactory results on severely low-frame-rate blurry videos due to the lack\nof temporal constraints. In this paper, we introduce a novel event-guided\nframework for exposure-agnostic VFI, addressing this limitation through two key\ncomponents: a Target-adaptive Event Sampling (TES) and a Target-adaptive\nImportance Mapping (TIM). Specifically, TES samples events around the target\ntimestamp and the unknown exposure time to better align them with the\ncorresponding blurry frames. TIM then generates an importance map that\nconsiders the temporal proximity and spatial relevance of consecutive features\nto the target. Guided by this map, our framework adaptively blends consecutive\nfeatures, allowing temporally aligned features to serve as the primary cues\nwhile spatially relevant ones offer complementary support. Extensive\nexperiments on both synthetic and real-world datasets demonstrate the\neffectiveness of our approach in exposure-agnostic VFI scenarios.\n","authors":["Junsik Jung","Yoonki Cho","Woo Jae Kim","Lin Wang","Sune-eui Yoon"],"pdf_url":"https://arxiv.org/pdf/2510.22565v1.pdf","comment":"Accepted for BMVC2025"},{"id":"http://arxiv.org/abs/2412.11762v2","updated":"2025-10-26T05:03:24Z","published":"2024-12-16T13:26:52Z","title":"GS-ProCams: Gaussian Splatting-based Projector-Camera Systems","summary":"  We present GS-ProCams, the first Gaussian Splatting-based framework for\nprojector-camera systems (ProCams). GS-ProCams is not only view-agnostic but\nalso significantly enhances the efficiency of projection mapping (PM) that\nrequires establishing geometric and radiometric mappings between the projector\nand the camera. Previous CNN-based ProCams are constrained to a specific\nviewpoint, limiting their applicability to novel perspectives. In contrast,\nNeRF-based ProCams support view-agnostic projection mapping, however, they\nrequire an additional co-located light source and demand significant\ncomputational and memory resources. To address this issue, we propose\nGS-ProCams that employs 2D Gaussian for scene representations, and enables\nefficient view-agnostic ProCams applications. In particular, we explicitly\nmodel the complex geometric and photometric mappings of ProCams using projector\nresponses, the projection surface's geometry and materials represented by\nGaussians, and the global illumination component. Then, we employ\ndifferentiable physically-based rendering to jointly estimate them from\ncaptured multi-view projections. Compared to state-of-the-art NeRF-based\nmethods, our GS-ProCams eliminates the need for additional devices, achieving\nsuperior ProCams simulation quality. It also uses only 1/10 of the GPU memory\nfor training and is 900 times faster in inference speed. Please refer to our\nproject page for the code and dataset:\nhttps://realqingyue.github.io/GS-ProCams/.\n","authors":["Qingyue Deng","Jijiang Li","Haibin Ling","Bingyao Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11762v2.pdf","comment":null}]},"2025-10-25T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2510.22344v1","updated":"2025-10-25T15:59:33Z","published":"2025-10-25T15:59:33Z","title":"FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented\n  Generation","summary":"  While Retrieval-Augmented Generation (RAG) mitigates hallucination and\nknowledge staleness in Large Language Models (LLMs), existing frameworks often\nfalter on complex, multi-hop queries that require synthesizing information from\ndisparate sources. Current advanced RAG methods, employing iterative or\nadaptive strategies, lack a robust mechanism to systematically identify and\nfill evidence gaps, often propagating noise or failing to gather a\ncomprehensive context. We introduce FAIR-RAG, a novel agentic framework that\ntransforms the standard RAG pipeline into a dynamic, evidence-driven reasoning\nprocess. At its core is an Iterative Refinement Cycle governed by a module we\nterm Structured Evidence Assessment (SEA). The SEA acts as an analytical gating\nmechanism: it deconstructs the initial query into a checklist of required\nfindings and audits the aggregated evidence to identify confirmed facts and,\ncritically, explicit informational gaps. These gaps provide a precise signal to\nan Adaptive Query Refinement agent, which generates new, targeted sub-queries\nto retrieve missing information. This cycle repeats until the evidence is\nverified as sufficient, ensuring a comprehensive context for a final, strictly\nfaithful generation. We conducted experiments on challenging multi-hop QA\nbenchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified\nexperimental setup, FAIR-RAG significantly outperforms strong baselines. On\nHotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3\npoints over the strongest iterative baseline -- establishing a new\nstate-of-the-art for this class of methods on these benchmarks. Our work\ndemonstrates that a structured, evidence-driven refinement process with\nexplicit gap analysis is crucial for unlocking reliable and accurate reasoning\nin advanced RAG systems for complex, knowledge-intensive tasks.\n","authors":["Mohammad Aghajani Asl","Majid Asgari-Bidhendi","Behrooz Minaei-Bidgoli"],"pdf_url":"https://arxiv.org/pdf/2510.22344v1.pdf","comment":"30 pages, 5 figures, 5 tables. Keywords: Retrieval-Augmented\n  Generation (RAG), Large Language Models (LLMs), Agentic AI, Multi-hop\n  Question Answering, Faithfulness"},{"id":"http://arxiv.org/abs/2509.07759v2","updated":"2025-10-25T13:44:18Z","published":"2025-09-09T13:57:53Z","title":"A Survey of Long-Document Retrieval in the PLM and LLM Era","summary":"  The proliferation of long-form documents presents a fundamental challenge to\ninformation retrieval (IR), as their length, dispersed evidence, and complex\nstructures demand specialized methods beyond standard passage-level techniques.\nThis survey provides the first comprehensive treatment of long-document\nretrieval (LDR), consolidating methods, challenges, and applications across\nthree major eras. We systematize the evolution from classical lexical and early\nneural models to modern pre-trained (PLM) and large language models (LLMs),\ncovering key paradigms like passage aggregation, hierarchical encoding,\nefficient attention, and the latest LLM-driven re-ranking and retrieval\ntechniques. Beyond the models, we review domain-specific applications,\nspecialized evaluation resources, and outline critical open challenges such as\nefficiency trade-offs, multimodal alignment, and faithfulness. This survey aims\nto provide both a consolidated reference and a forward-looking agenda for\nadvancing long-document retrieval in the era of foundation models.\n","authors":["Minghan Li","Miyang Luo","Tianrui Lv","Yishuai Zhang","Siqi Zhao","Ercong Nie","Guodong Zhou"],"pdf_url":"https://arxiv.org/pdf/2509.07759v2.pdf","comment":"32 pages, 6 figures"},{"id":"http://arxiv.org/abs/2509.07794v2","updated":"2025-10-25T13:13:22Z","published":"2025-09-09T14:31:11Z","title":"Query Expansion in the Age of Pre-trained and Large Language Models: A\n  Comprehensive Survey","summary":"  Modern information retrieval (IR) must reconcile short, ambiguous queries\nwith increasingly diverse and dynamic corpora. Query expansion (QE) remains\ncentral to alleviating vocabulary mismatch, yet the design space has shifted\nwith pre-trained and large language models (PLMs, LLMs). In this survey, we\norganize recent work along four complementary dimensions: the point of\ninjection (implicit/embedding vs. selection-based explicit), grounding and\ninteraction (from zero-grounding prompts to multi-round retrieve-expand loops),\nlearning and alignment (SFT/PEFT/DPO), and knowledge-graph integration. A\nmodel-centric taxonomy is also outlined, spanning encoder-only,\nencoder-decoder, decoder-only, instruction-tuned, and domain or multilingual\nvariants, with affordances for QE such as contextual disambiguation,\ncontrollable generation, and zero-shot or few-shot reasoning. Practice-oriented\nguidance specifies where neural QE helps most: first-stage retrieval,\nmulti-query fusion, re-ranking, and retrieval-augmented generation (RAG). The\nsurvey compares traditional and neural QE across seven aspects and maps\napplications in web search, biomedicine, e-commerce, open-domain question\nanswering/RAG, conversational and code search, and cross-lingual settings. The\nsurvey concludes with an agenda focused on reliable, safe, efficient, and\nadaptive QE, offering a principled blueprint for deploying and combining\ntechniques under real-world constraints.\n","authors":["Minghan Li","Xinxuan Lv","Junjie Zou","Tongna Chen","Chao Zhang","Suchao An","Ercong Nie","Guodong Zhou"],"pdf_url":"https://arxiv.org/pdf/2509.07794v2.pdf","comment":"36 pages,3 figures,3 tables"},{"id":"http://arxiv.org/abs/2504.06714v3","updated":"2025-10-25T12:48:03Z","published":"2025-04-09T09:15:37Z","title":"Unifying Search and Recommendation with Dual-View Representation\n  Learning in a Generative Paradigm","summary":"  Recommender systems and search engines serve as foundational elements of\nonline platforms, with the former delivering information proactively and the\nlatter enabling users to seek information actively. Unifying both tasks in a\nshared model is promising since it can enhance user modeling and item\nunderstanding. Previous approaches mainly follow a discriminative paradigm,\nutilizing shared encoders to process input features and task-specific heads to\nperform each task. However, this paradigm encounters two key challenges:\ngradient conflict and manual design complexity. From the information theory\nperspective, these challenges potentially both stem from the same issue -- low\nmutual information between the input features and task-specific outputs during\nthe optimization process.\n  To tackle these issues, we propose GenSR, a novel generative paradigm for\nunifying search and recommendation (S&R), which leverages task-specific prompts\nto partition the model's parameter space into subspaces, thereby enhancing\nmutual information. To construct effective subspaces for each task, GenSR first\nprepares informative representations for each subspace and then optimizes both\nsubspaces in one unified model. Specifically, GenSR consists of two main\nmodules: (1) Dual Representation Learning, which independently models\ncollaborative and semantic historical information to derive expressive item\nrepresentations; and (2) S&R Task Unifying, which utilizes contrastive learning\ntogether with instruction tuning to generate task-specific outputs effectively.\nExtensive experiments on two public datasets show GenSR outperforms\nstate-of-the-art methods across S&R tasks. Our work introduces a new generative\nparadigm compared with previous discriminative methods and establishes its\nsuperiority from the mutual information perspective.\n","authors":["Jujia Zhao","Wenjie Wang","Chen Xu","Xiuying Chen","Zhaochun Ren","Suzan Verberne"],"pdf_url":"https://arxiv.org/pdf/2504.06714v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22264v1","updated":"2025-10-25T12:01:46Z","published":"2025-10-25T12:01:46Z","title":"PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text\n  Embedding","summary":"  Patent text embeddings enable prior art search, technology landscaping, and\npatent analysis, yet existing benchmarks inadequately capture patent-specific\nchallenges. We introduce PatenTEB, a comprehensive benchmark comprising 15\ntasks across retrieval, classification, paraphrase, and clustering, with 2.06\nmillion examples. PatenTEB employs domain-stratified splits, domain specific\nhard negative mining, and systematic coverage of asymmetric\nfragment-to-document matching scenarios absent from general embedding\nbenchmarks. We develop the patembed model family through multi-task training,\nspanning 67M to 344M parameters with context lengths up to 4096 tokens.\nExternal validation shows strong generalization: patembed-base achieves\nstate-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445\nprevious best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM.\nSystematic ablations reveal that multi-task training improves external\ngeneralization despite minor benchmark costs, and that domain-pretrained\ninitialization provides consistent advantages across task families. All\nresources will be made available at https://github.com/iliass-y/patenteb.\nKeywords: patent retrieval, sentence embeddings, multi-task learning,\nasymmetric retrieval, benchmark evaluation, contrastive learning.\n","authors":["Iliass Ayaou","Denis Cavallucci"],"pdf_url":"https://arxiv.org/pdf/2510.22264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22242v1","updated":"2025-10-25T10:11:29Z","published":"2025-10-25T10:11:29Z","title":"PaperAsk: A Benchmark for Reliability Evaluation of LLMs in Paper Search\n  and Reading","summary":"  Large Language Models (LLMs) increasingly serve as research assistants, yet\ntheir reliability in scholarly tasks remains under-evaluated. In this work, we\nintroduce PaperAsk, a benchmark that systematically evaluates LLMs across four\nkey research tasks: citation retrieval, content extraction, paper discovery,\nand claim verification. We evaluate GPT-4o, GPT-5, and Gemini-2.5-Flash under\nrealistic usage conditions-via web interfaces where search operations are\nopaque to the user. Through controlled experiments, we find consistent\nreliability failures: citation retrieval fails in 48-98% of multi-reference\nqueries, section-specific content extraction fails in 72-91% of cases, and\ntopical paper discovery yields F1 scores below 0.32, missing over 60% of\nrelevant literature. Further human analysis attributes these failures to the\nuncontrolled expansion of retrieved context and the tendency of LLMs to\nprioritize semantically relevant text over task instructions. Across basic\ntasks, the LLMs display distinct failure behaviors: ChatGPT often withholds\nresponses rather than risk errors, whereas Gemini produces fluent but\nfabricated answers. To address these issues, we develop lightweight reliability\nclassifiers trained on PaperAsk data to identify unreliable outputs. PaperAsk\nprovides a reproducible and diagnostic framework for advancing the reliability\nevaluation of LLM-based scholarly assistance systems.\n","authors":["Yutao Wu","Xiao Liu","Yunhao Feng","Jiale Ding","Xingjun Ma"],"pdf_url":"https://arxiv.org/pdf/2510.22242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22215v1","updated":"2025-10-25T08:27:37Z","published":"2025-10-25T08:27:37Z","title":"Hybrid-Vector Retrieval for Visually Rich Documents: Combining\n  Single-Vector Efficiency and Multi-Vector Accuracy","summary":"  Retrieval over visually rich documents is essential for tasks such as legal\ndiscovery, scientific search, and enterprise knowledge management. Existing\napproaches fall into two paradigms: single-vector retrieval, which is efficient\nbut coarse, and multi-vector retrieval, which is accurate but computationally\nexpensive. To address this trade-off, we propose HEAVEN, a two-stage\nhybrid-vector framework. In the first stage, HEAVEN efficiently retrieves\ncandidate pages using a single-vector method over Visually-Summarized Pages\n(VS-Pages), which assemble representative visual layouts from multiple pages.\nIn the second stage, it reranks candidates with a multi-vector method while\nfiltering query tokens by linguistic importance to reduce redundant\ncomputations. To evaluate retrieval systems under realistic conditions, we also\nintroduce ViMDOC, the first benchmark for visually rich, multi-document, and\nlong-document retrieval. Across four benchmarks, HEAVEN attains 99.87% of the\nRecall@1 performance of multi-vector models on average while reducing per-query\ncomputation by 99.82%, achieving efficiency and accuracy. Our code and datasets\nare available at: https://github.com/juyeonnn/HEAVEN\n","authors":["Juyeon Kim","Geon Lee","Dongwon Choi","Taeuk Kim","Kijung Shin"],"pdf_url":"https://arxiv.org/pdf/2510.22215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.17442v2","updated":"2025-10-25T05:48:12Z","published":"2025-09-22T07:32:06Z","title":"WildClaims: Information Access Conversations in the Wild(Chat)","summary":"  The rapid advancement of Large Language Models (LLMs) has transformed\nconversational systems into practical tools used by millions. However, the\nnature and necessity of information retrieval in real-world conversations\nremain largely unexplored, as research has focused predominantly on\ntraditional, explicit information access conversations. The central question\nis: What do real-world information access conversations look like? To this end,\nwe first conduct an observational study on the WildChat dataset, large-scale\nuser-ChatGPT conversations, finding that users' access to information occurs\nimplicitly as check-worthy factual assertions made by the system, even when the\nconversation's primary intent is non-informational, such as creative writing.\nTo enable the systematic study of this phenomenon, we release the WildClaims\ndataset, a novel resource consisting of 121,905 extracted factual claims from\n7,587 utterances in 3,000 WildChat conversations, each annotated for\ncheck-worthiness. Our preliminary analysis of this resource reveals that\nconservatively 18% to 51% of conversations contain check-worthy assertions,\ndepending on the methods employed, and less conservatively, as many as 76% may\ncontain such assertions. This high prevalence underscores the importance of\nmoving beyond the traditional understanding of explicit information access, to\naddress the implicit information access that arises in real-world user-system\nconversations.\n","authors":["Hideaki Joko","Shakiba Amirshahi","Charles L. A. Clarke","Faegheh Hasibi"],"pdf_url":"https://arxiv.org/pdf/2509.17442v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00314v2","updated":"2025-10-25T05:28:21Z","published":"2025-05-30T23:54:13Z","title":"FACE: A Fine-grained Reference Free Evaluator for Conversational\n  Recommender Systems","summary":"  A systematic, reliable, and low-cost evaluation of Conversational Recommender\nSystems (CRSs) remains an open challenge. Existing automatic CRS evaluation\nmethods are proven insufficient for evaluating the dynamic nature of\nrecommendation conversations. This work proposes FACE: a Fine-grained,\nAspect-based Conversation Evaluation method that provides evaluation scores for\ndiverse turn and dialogue level qualities of recommendation conversations. FACE\nis reference-free and shows strong correlation with human judgments, achieving\nsystem correlation of 0.9 and turn/dialogue-level of 0.5, outperforming\nstate-of-the-art CRS evaluation methods by a large margin. Additionally, unlike\nexisting LLM-based methods that provide single uninterpretable scores, FACE\nprovides insights into the system performance and enables identifying and\nlocating problems within conversations.\n","authors":["Hideaki Joko","Faegheh Hasibi"],"pdf_url":"https://arxiv.org/pdf/2506.00314v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.04273v3","updated":"2025-10-25T03:36:49Z","published":"2025-08-06T09:58:43Z","title":"Audio Does Matter: Importance-Aware Multi-Granularity Fusion for Video\n  Moment Retrieval","summary":"  Video Moment Retrieval (VMR) aims to retrieve a specific moment semantically\nrelated to the given query. To tackle this task, most existing VMR methods\nsolely focus on the visual and textual modalities while neglecting the\ncomplementary but important audio modality. Although a few recent works try to\ntackle the joint audio-vision-text reasoning, they treat all modalities equally\nand simply embed them without fine-grained interaction for moment retrieval.\nThese designs are counter-practical as: Not all audios are helpful for video\nmoment retrieval, and the audio of some videos may be complete noise or\nbackground sound that is meaningless to the moment determination. To this end,\nwe propose a novel Importance-aware Multi-Granularity fusion model (IMG), which\nlearns to dynamically and selectively aggregate the audio-vision-text contexts\nfor VMR. Specifically, after integrating the textual guidance with vision and\naudio separately, we first design a pseudo-label-supervised audio importance\npredictor that predicts the importance score of the audio, and accordingly\nassigns weights to mitigate the interference caused by noisy audio. Then, we\ndesign a multi-granularity audio fusion module that adaptively fuses audio and\nvisual modalities at local-, event-, and global-level, fully capturing their\ncomplementary contexts. We further propose a cross-modal knowledge distillation\nstrategy to address the challenge of missing audio modality during inference.\nTo evaluate our method, we further construct a new VMR dataset, i.e.,\nCharades-AudioMatter, where audio-related samples are manually selected and\nre-organized from the original Charades-STA to validate the model's capability\nin utilizing audio modality. Extensive experiments validate the effectiveness\nof our method, achieving state-of-the-art with audio-video fusion in VMR\nmethods. Our code is available at https://github.com/HuiGuanLab/IMG.\n","authors":["Junan Lin","Daizong Liu","Xianke Chen","Xiaoye Qu","Xun Yang","Jixiang Zhu","Sanyuan Zhang","Jianfeng Dong"],"pdf_url":"https://arxiv.org/pdf/2508.04273v3.pdf","comment":"Accepted to ACM MM 2025"},{"id":"http://arxiv.org/abs/2510.22101v1","updated":"2025-10-25T00:56:06Z","published":"2025-10-25T00:56:06Z","title":"Scaling Up Efficient Small Language Models Serving and Deployment for\n  Semantic Job Search","summary":"  Large Language Models (LLMs) have demonstrated impressive quality when\napplied to predictive tasks such as relevance ranking and semantic search.\nHowever, deployment of such LLMs remains prohibitively expensive for industry\napplications with strict latency and throughput requirements. In this work, we\npresent lessons and efficiency insights from developing a purely text-based\ndecoder-only Small Language Model (SLM) for a semantic search application at\nLinkedIn. Particularly, we discuss model compression techniques such as pruning\nthat allow us to reduce the model size by up to $40\\%$ while maintaining the\naccuracy. Additionally, we present context compression techniques that allow us\nto reduce the input context length by up to $10$x with minimal loss of\naccuracy. Finally, we present practical lessons from optimizing the serving\ninfrastructure for deploying such a system on GPUs at scale, serving millions\nof requests per second. Taken together, this allows us to increase our system's\nthroughput by $10$x in a real-world deployment, while meeting our quality bar.\n","authors":["Kayhan Behdin","Qingquan Song","Sriram Vasudevan","Jian Sheng","Xiaojing Ma","Z Zhou","Chuanrui Zhu","Guoyao Li","Chanh Nguyen","Sayan Ghosh","Hejian Sang","Ata Fatahi Baarzi","Sundara Raman Ramachandran","Xiaoqing Wang","Qing Lan","Vinay Y S","Qi Guo","Caleb Johnson","Zhipeng Wang","Fedor Borisyuk"],"pdf_url":"https://arxiv.org/pdf/2510.22101v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2510.22154v1","updated":"2025-10-25T04:17:50Z","published":"2025-10-25T04:17:50Z","title":"Frequency-Spatial Interaction Driven Network for Low-Light Image\n  Enhancement","summary":"  Low-light image enhancement (LLIE) aims at improving the perception or\ninterpretability of an image captured in an environment with poor illumination.\nWith the advent of deep learning, the LLIE technique has achieved significant\nbreakthroughs. However, existing LLIE methods either ignore the important role\nof frequency domain information or fail to effectively promote the propagation\nand flow of information, limiting the LLIE performance. In this paper, we\ndevelop a novel frequency-spatial interaction-driven network (FSIDNet) for LLIE\nbased on two-stage architecture. To be specific, the first stage is designed to\nrestore the amplitude of low-light images to improve the lightness, and the\nsecond stage devotes to restore phase information to refine fine-grained\nstructures. Considering that Frequency domain and spatial domain information\nare complementary and both favorable for LLIE, we further develop two\nfrequency-spatial interaction blocks which mutually amalgamate the\ncomplementary spatial and frequency information to enhance the capability of\nthe model. In addition, we construct the Information Exchange Module (IEM) to\nassociate two stages by adequately incorporating cross-stage and cross-scale\nfeatures to effectively promote the propagation and flow of information in the\ntwo-stage network structure. Finally, we conduct experiments on several widely\nused benchmark datasets (i.e., LOL-Real, LSRW-Huawei, etc.), which demonstrate\nthat our method achieves the excellent performance in terms of visual results\nand quantitative metrics while preserving good model efficiency.\n","authors":["Yunhong Tao","Wenbing Tao","Xiang Xiang"],"pdf_url":"https://arxiv.org/pdf/2510.22154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.04273v3","updated":"2025-10-25T03:36:49Z","published":"2025-08-06T09:58:43Z","title":"Audio Does Matter: Importance-Aware Multi-Granularity Fusion for Video\n  Moment Retrieval","summary":"  Video Moment Retrieval (VMR) aims to retrieve a specific moment semantically\nrelated to the given query. To tackle this task, most existing VMR methods\nsolely focus on the visual and textual modalities while neglecting the\ncomplementary but important audio modality. Although a few recent works try to\ntackle the joint audio-vision-text reasoning, they treat all modalities equally\nand simply embed them without fine-grained interaction for moment retrieval.\nThese designs are counter-practical as: Not all audios are helpful for video\nmoment retrieval, and the audio of some videos may be complete noise or\nbackground sound that is meaningless to the moment determination. To this end,\nwe propose a novel Importance-aware Multi-Granularity fusion model (IMG), which\nlearns to dynamically and selectively aggregate the audio-vision-text contexts\nfor VMR. Specifically, after integrating the textual guidance with vision and\naudio separately, we first design a pseudo-label-supervised audio importance\npredictor that predicts the importance score of the audio, and accordingly\nassigns weights to mitigate the interference caused by noisy audio. Then, we\ndesign a multi-granularity audio fusion module that adaptively fuses audio and\nvisual modalities at local-, event-, and global-level, fully capturing their\ncomplementary contexts. We further propose a cross-modal knowledge distillation\nstrategy to address the challenge of missing audio modality during inference.\nTo evaluate our method, we further construct a new VMR dataset, i.e.,\nCharades-AudioMatter, where audio-related samples are manually selected and\nre-organized from the original Charades-STA to validate the model's capability\nin utilizing audio modality. Extensive experiments validate the effectiveness\nof our method, achieving state-of-the-art with audio-video fusion in VMR\nmethods. Our code is available at https://github.com/HuiGuanLab/IMG.\n","authors":["Junan Lin","Daizong Liu","Xianke Chen","Xiaoye Qu","Xun Yang","Jixiang Zhu","Sanyuan Zhang","Jianfeng Dong"],"pdf_url":"https://arxiv.org/pdf/2508.04273v3.pdf","comment":"Accepted to ACM MM 2025"}]},"2025-10-28T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2404.13397v2","updated":"2025-10-28T17:56:27Z","published":"2024-04-20T14:42:43Z","title":"Retrieval-Augmented Generation-based Relation Extraction","summary":"  Information Extraction (IE) is a transformative process that converts\nunstructured text data into a structured format by employing entity and\nrelation extraction (RE) methodologies. The identification of the relation\nbetween a pair of entities plays a crucial role within this framework. Despite\nthe existence of various techniques for relation extraction, their efficacy\nheavily relies on access to labeled data and substantial computational\nresources. In addressing these challenges, Large Language Models (LLMs) emerge\nas promising solutions; however, they might return hallucinating responses due\nto their own training data. To overcome these limitations, Retrieved-Augmented\nGeneration-based Relation Extraction (RAG4RE) in this work is proposed,\noffering a pathway to enhance the performance of relation extraction tasks.\n  This work evaluated the effectiveness of our RAG4RE approach utilizing\ndifferent LLMs. Through the utilization of established benchmarks, such as\nTACRED, TACREV, Re-TACRED, and SemEval RE datasets, our aim is to\ncomprehensively evaluate the efficacy of our RAG4RE approach. In particularly,\nwe leverage prominent LLMs including Flan T5, Llama2, and Mistral in our\ninvestigation. The results of our study demonstrate that our RAG4RE approach\nsurpasses performance of traditional RE approaches based solely on LLMs,\nparticularly evident in the TACRED dataset and its variations. Furthermore, our\napproach exhibits remarkable performance compared to previous RE methodologies\nacross both TACRED and TACREV datasets, underscoring its efficacy and potential\nfor advancing RE tasks in natural language processing.\n","authors":["Sefika Efeoglu","Adrian Paschke"],"pdf_url":"https://arxiv.org/pdf/2404.13397v2.pdf","comment":"published at the Semantic Web journal. The last version is available:\n  https://doi.org/10.1177/22104968251385519"},{"id":"http://arxiv.org/abs/2510.24707v1","updated":"2025-10-28T17:56:20Z","published":"2025-10-28T17:56:20Z","title":"MetricX-25 and GemSpanEval: Google Translate Submissions to the WMT25\n  Evaluation Shared Task","summary":"  In this paper, we present our submissions to the unified WMT25 Translation\nEvaluation Shared Task. For the Quality Score Prediction subtask, we create a\nnew generation of MetricX with improvements in the input format and the\ntraining protocol, while for the Error Span Detection subtask we develop a new\nmodel, GemSpanEval, trained to predict error spans along with their severities\nand categories. Both systems are based on the state-of-the-art multilingual\nopen-weights model Gemma 3, fine-tuned on publicly available WMT data. We\ndemonstrate that MetricX-25, adapting Gemma 3 to an encoder-only architecture\nwith a regression head on top, can be trained to effectively predict both MQM\nand ESA quality scores, and significantly outperforms its predecessor. Our\ndecoder-only GemSpanEval model, on the other hand, we show to be competitive in\nerror span detection with xCOMET, a strong encoder-only sequence-tagging\nbaseline. With error span detection formulated as a generative task, we\ninstruct the model to also output the context for each predicted error span,\nthus ensuring that error spans are identified unambiguously.\n","authors":["Juraj Juraska","Tobias Domhan","Mara Finkelstein","Tetsuji Nakagawa","Geza Kovacs","Daniel Deutsch","Pidong Wang","Markus Freitag"],"pdf_url":"https://arxiv.org/pdf/2510.24707v1.pdf","comment":"Accepted to WMT25"},{"id":"http://arxiv.org/abs/2510.24706v1","updated":"2025-10-28T17:55:42Z","published":"2025-10-28T17:55:42Z","title":"ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality\n  Games?","summary":"  Virtual Reality (VR) games require players to translate high-level semantic\nactions into precise device manipulations using controllers and head-mounted\ndisplays (HMDs). While humans intuitively perform this translation based on\ncommon sense and embodied understanding, whether Large Language Models (LLMs)\ncan effectively replicate this ability remains underexplored. This paper\nintroduces a benchmark, ComboBench, evaluating LLMs' capability to translate\nsemantic actions into VR device manipulation sequences across 262 scenarios\nfrom four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II,\nand Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o,\nGemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against\nannotated ground truth and human performance. Our results reveal that while\ntop-performing models like Gemini-1.5-Pro demonstrate strong task decomposition\ncapabilities, they still struggle with procedural reasoning and spatial\nunderstanding compared to humans. Performance varies significantly across\ngames, suggesting sensitivity to interaction complexity. Few-shot examples\nsubstantially improve performance, indicating potential for targeted\nenhancement of LLMs' VR manipulation capabilities. We release all materials at\nhttps://sites.google.com/view/combobench.\n","authors":["Shuqing Li","Jiayi Yan","Chenyu Niu","Jen-tse Huang","Yun Peng","Wenxuan Wang","Yepang Liu","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2510.24706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24702v1","updated":"2025-10-28T17:53:13Z","published":"2025-10-28T17:53:13Z","title":"Agent Data Protocol: Unifying Datasets for Diverse, Effective\n  Fine-tuning of LLM Agents","summary":"  Public research results on large-scale supervised finetuning of AI agents\nremain relatively rare, since the collection of agent training data presents\nunique challenges. In this work, we argue that the bottleneck is not a lack of\nunderlying data sources, but that a large variety of data is fragmented across\nheterogeneous formats, tools, and interfaces. To this end, we introduce the\nagent data protocol (ADP), a light-weight representation language that serves\nas an \"interlingua\" between agent datasets in diverse formats and unified agent\ntraining pipelines downstream. The design of ADP is expressive enough to\ncapture a large variety of tasks, including API/tool use, browsing, coding,\nsoftware engineering, and general agentic workflows, while remaining simple to\nparse and train on without engineering at a per-dataset level. In experiments,\nwe unified a broad collection of 13 existing agent training datasets into ADP\nformat, and converted the standardized ADP data into training-ready formats for\nmultiple agent frameworks. We performed SFT on these data, and demonstrated an\naverage performance gain of ~20% over corresponding base models, and delivers\nstate-of-the-art or near-SOTA performance on standard coding, browsing, tool\nuse, and research benchmarks, without domain-specific tuning. All code and data\nare released publicly, in the hope that ADP could help lower the barrier to\nstandardized, scalable, and reproducible agent training.\n","authors":["Yueqi Song","Ketan Ramaneti","Zaid Sheikh","Ziru Chen","Boyu Gou","Tianbao Xie","Yiheng Xu","Danyang Zhang","Apurva Gandhi","Fan Yang","Joseph Liu","Tianyue Ou","Zhihao Yuan","Frank Xu","Shuyan Zhou","Xingyao Wang","Xiang Yue","Tao Yu","Huan Sun","Yu Su","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2510.24702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24701v1","updated":"2025-10-28T17:53:02Z","published":"2025-10-28T17:53:02Z","title":"Tongyi DeepResearch Technical Report","summary":"  We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.\n","authors":[" Tongyi DeepResearch Team","Baixuan Li","Bo Zhang","Dingchu Zhang","Fei Huang","Guangyu Li","Guoxin Chen","Huifeng Yin","Jialong Wu","Jingren Zhou","Kuan Li","Liangcai Su","Litu Ou","Liwen Zhang","Pengjun Xie","Rui Ye","Wenbiao Yin","Xinmiao Yu","Xinyu Wang","Xixi Wu","Xuanzhong Chen","Yida Zhao","Zhen Zhang","Zhengwei Tao","Zhongwang Zhang","Zile Qiao","Chenxi Wang","Donglei Yu","Gang Fu","Haiyang Shen","Jiayin Yang","Jun Lin","Junkai Zhang","Kui Zeng","Li Yang","Hailong Yin","Maojia Song","Ming Yan","Peng Xia","Qian Xiao","Rui Min","Ruixue Ding","Runnan Fang","Shaowei Chen","Shen Huang","Shihang Wang","Shihao Cai","Weizhou Shen","Xiaobin Wang","Xin Guan","Xinyu Geng","Yingcheng Shi","Yuning Wu","Zhuo Chen","Zijian Li","Yong Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.24701v1.pdf","comment":"https://tongyi-agent.github.io/blog"},{"id":"http://arxiv.org/abs/2510.24698v1","updated":"2025-10-28T17:51:50Z","published":"2025-10-28T17:51:50Z","title":"ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking","summary":"  Parallel thinking expands exploration breadth, complementing the deep\nexploration of information-seeking (IS) agents to further enhance\nproblem-solving capability. However, conventional parallel thinking faces two\nkey challenges in this setting: inefficiency from repeatedly rolling out from\nscratch, and difficulty in integrating long-horizon reasoning trajectories\nduring answer generation, as limited context capacity prevents full\nconsideration of the reasoning process. To address these issues, we propose\nParallelMuse, a two-stage paradigm designed for deep IS agents. The first\nstage, Functionality-Specified Partial Rollout, partitions generated sequences\ninto functional regions and performs uncertainty-guided path reuse and\nbranching to enhance exploration efficiency. The second stage, Compressed\nReasoning Aggregation, exploits reasoning redundancy to losslessly compress\ninformation relevant to answer derivation and synthesize a coherent final\nanswer. Experiments across multiple open-source agents and benchmarks\ndemonstrate up to 62% performance improvement with a 10--30% reduction in\nexploratory token consumption.\n","authors":["Baixuan Li","Dingchu Zhang","Jialong Wu","Wenbiao Yin","Zhengwei Tao","Yida Zhao","Liwen Zhang","Haiyang Shen","Runnan Fang","Pengjun Xie","Jingren Zhou","Yong Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.24698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24699v1","updated":"2025-10-28T17:51:50Z","published":"2025-10-28T17:51:50Z","title":"AgentFold: Long-Horizon Web Agents with Proactive Context Management","summary":"  LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini.\n","authors":["Rui Ye","Zhongwang Zhang","Kuan Li","Huifeng Yin","Zhengwei Tao","Yida Zhao","Liangcai Su","Liwen Zhang","Zile Qiao","Xinyu Wang","Pengjun Xie","Fei Huang","Siheng Chen","Jingren Zhou","Yong Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.24699v1.pdf","comment":"26 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.24697v1","updated":"2025-10-28T17:51:42Z","published":"2025-10-28T17:51:42Z","title":"WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling\n  Info-Rich Seeking","summary":"  Large Language Model (LLM)-based agents have emerged as a transformative\napproach for open-ended problem solving, with information seeking (IS) being a\ncore capability that enables autonomous reasoning and decision-making. While\nprior research has largely focused on improving retrieval depth, we observe\nthat current IS agents often suffer from low search efficiency, which in turn\nconstrains overall performance. A key factor underlying this inefficiency is\nthe sparsity of target entities in training tasks, which limits opportunities\nfor agents to learn and generalize efficient search behaviors. To address these\nchallenges, we propose WebLeaper, a framework for constructing high-coverage IS\ntasks and generating efficient solution trajectories. We formulate IS as a\ntree-structured reasoning problem, enabling a substantially larger set of\ntarget entities to be embedded within a constrained context. Leveraging curated\nWikipedia tables, we propose three variants for synthesizing IS tasks, Basic,\nUnion, and Reverse-Union, to systematically increase both IS efficiency and\nefficacy. Finally, we curate training trajectories by retaining only those that\nare simultaneously accurate and efficient, ensuring that the model is optimized\nfor both correctness and search performance. Extensive experiments on both\nbasic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,\nGAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method\nconsistently achieves improvements in both effectiveness and efficiency over\nstrong baselines.\n","authors":["Zhengwei Tao","Haiyang Shen","Baixuan Li","Wenbiao Yin","Jialong Wu","Kuan Li","Zhongwang Zhang","Huifeng Yin","Rui Ye","Liwen Zhang","Xinyu Wang","Pengjun Xie","Jingren Zhou","Yong Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.24697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24695v1","updated":"2025-10-28T17:50:47Z","published":"2025-10-28T17:50:47Z","title":"AgentFrontier: Expanding the Capability Frontier of LLM Agents with\n  ZPD-Guided Data Synthesis","summary":"  Training large language model agents on tasks at the frontier of their\ncapabilities is key to unlocking advanced reasoning. We introduce a data\nsynthesis approach inspired by the educational theory of the Zone of Proximal\nDevelopment (ZPD), which defines this frontier as tasks an LLM cannot solve\nalone but can master with guidance. To operationalize this, we present the\nAgentFrontier Engine, an automated pipeline that synthesizes high-quality,\nmultidisciplinary data situated precisely within the LLM's ZPD. This engine\nsupports both continued pre-training with knowledge-intensive data and targeted\npost-training on complex reasoning tasks. From the same framework, we derive\nthe ZPD Exam, a dynamic and automated benchmark designed to evaluate agent\ncapabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on\nour synthesized data, which achieves state-of-the-art results on demanding\nbenchmarks like Humanity's Last Exam, even surpassing some leading proprietary\nagents. Our work demonstrates that a ZPD-guided approach to data synthesis\noffers a scalable and effective path toward building more capable LLM agents.\n","authors":["Xuanzhong Chen","Zile Qiao","Guoxin Chen","Liangcai Su","Zhen Zhang","Xinyu Wang","Pengjun Xie","Fei Huang","Jingren Zhou","Yong Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.24695v1.pdf","comment":"https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/"},{"id":"http://arxiv.org/abs/2510.24694v1","updated":"2025-10-28T17:50:40Z","published":"2025-10-28T17:50:40Z","title":"Repurposing Synthetic Data for Fine-grained Search Agent Supervision","summary":"  LLM-based search agents are increasingly trained on entity-centric synthetic\ndata to solve complex, knowledge-intensive tasks. However, prevailing training\nmethods like Group Relative Policy Optimization (GRPO) discard this rich entity\ninformation, relying instead on sparse, outcome-based rewards. This critical\nlimitation renders them unable to distinguish informative \"near-miss\"\nsamples-those with substantially correct reasoning but a flawed final\nanswer-from complete failures, thus discarding valuable learning signals. We\naddress this by leveraging the very entities discarded during training. Our\nempirical analysis reveals a strong positive correlation between the number of\nground-truth entities identified during an agent's reasoning process and final\nanswer accuracy. Building on this insight, we introduce Entity-aware Group\nRelative Policy Optimization (E-GRPO), a novel framework that formulates a\ndense entity-aware reward function. E-GRPO assigns partial rewards to incorrect\nsamples proportional to their entity match rate, enabling the model to\neffectively learn from these \"near-misses\". Experiments on diverse\nquestion-answering (QA) and deep research benchmarks show that E-GRPO\nconsistently and significantly outperforms the GRPO baseline. Furthermore, our\nanalysis reveals that E-GRPO not only achieves superior accuracy but also\ninduces more efficient reasoning policies that require fewer tool calls,\ndemonstrating a more effective and sample-efficient approach to aligning search\nagents.\n","authors":["Yida Zhao","Kuan Li","Xixi Wu","Liwen Zhang","Dingchu Zhang","Baixuan Li","Maojia Song","Zhuo Chen","Chenxi Wang","Xinyu Wang","Kewei Tu","Pengjun Xie","Jingren Zhou","Yong Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.24694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24693v1","updated":"2025-10-28T17:50:34Z","published":"2025-10-28T17:50:34Z","title":"STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D\n  Intelligence","summary":"  Despite rapid progress in Multi-modal Large Language Models and Large\nAudio-Language Models, existing audio benchmarks largely test semantics that\ncan be recovered from text captions, masking deficits in fine-grained\nperceptual reasoning. We formalize audio 4D intelligence that is defined as\nreasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to\nmeasure it. STAR-Bench combines a Foundational Acoustic Perception setting (six\nattributes under absolute and relative regimes) with a Holistic Spatio-Temporal\nReasoning setting that includes segment reordering for continuous and discrete\nprocesses and spatial tasks spanning static localization, multi-source\nrelations, and dynamic trajectories. Our data curation pipeline uses two\nmethods to ensure high-quality samples. For foundational tasks, we use\nprocedurally synthesized and physics-simulated audio. For holistic data, we\nfollow a four-stage process that includes human annotation and final selection\nbased on human performance. Unlike prior benchmarks where caption-only\nanswering reduces accuracy slightly, STAR-Bench induces far larger drops\n(-31.5\\% temporal, -35.2\\% spatial), evidencing its focus on linguistically\nhard-to-describe cues. Evaluating 19 models reveals substantial gaps compared\nwith humans and a capability hierarchy: closed-source models are bottlenecked\nby fine-grained perception, while open-source models lag across perception,\nknowledge, and reasoning. Our STAR-Bench provides critical insights and a clear\npath forward for developing future models with a more robust understanding of\nthe physical world.\n","authors":["Zihan Liu","Zhikang Niu","Qiuyang Xiao","Zhisheng Zheng","Ruoqi Yuan","Yuhang Zang","Yuhang Cao","Xiaoyi Dong","Jianze Liang","Xie Chen","Leilei Sun","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2510.24693v1.pdf","comment":"Homepage: https://internlm.github.io/StarBench/"},{"id":"http://arxiv.org/abs/2510.24684v1","updated":"2025-10-28T17:46:16Z","published":"2025-10-28T17:46:16Z","title":"SPICE: Self-Play In Corpus Environments Improves Reasoning","summary":"  Self-improving systems require environmental interaction for continuous\nadaptation. We introduce SPICE (Self-Play In Corpus Environments), a\nreinforcement learning framework where a single model acts in two roles: a\nChallenger that mines documents from a large corpus to generate diverse\nreasoning tasks, and a Reasoner that solves them. Through adversarial dynamics,\nthe Challenger creates an automatic curriculum at the frontier of the\nReasoner's capability, while corpus grounding provides the rich,\nnear-inexhaustible external signal necessary for sustained improvement. Unlike\nexisting ungrounded self-play methods that offer more limited benefits, SPICE\nachieves consistent gains across mathematical (+8.9%) and general reasoning\n(+9.8%) benchmarks on multiple model families. Our analysis reveals how\ndocument grounding is a key ingredient in SPICE to continuously generate its\nown increasingly challenging goals and achieve them, enabling sustained\nself-improvement.\n","authors":["Bo Liu","Chuanyang Jin","Seungone Kim","Weizhe Yuan","Wenting Zhao","Ilia Kulikov","Xian Li","Sainbayar Sukhbaatar","Jack Lanchantin","Jason Weston"],"pdf_url":"https://arxiv.org/pdf/2510.24684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24677v1","updated":"2025-10-28T17:40:53Z","published":"2025-10-28T17:40:53Z","title":"Dissecting Role Cognition in Medical LLMs via Neuronal Ablation","summary":"  Large language models (LLMs) have gained significant traction in medical\ndecision support systems, particularly in the\n  context of medical question answering and role-playing simulations. A common\npractice, Prompt-Based Role Playing (PBRP),\n  instructs models to adopt different clinical roles (e.g., medical students,\nresidents, attending physicians) to simulate varied\n  professional behaviors. However, the impact of such role prompts on model\nreasoning capabilities remains unclear. This\n  study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to\nevaluate whether role prompts induce distinct,\n  role-specific cognitive processes in LLMs or merely modify linguistic style.\nWe test this framework on three medical QA\n  datasets, employing neuron ablation and representation analysis techniques to\nassess changes in reasoning pathways. Our\n  results demonstrate that role prompts do not significantly enhance the\nmedical reasoning abilities of LLMs. Instead, they\n  primarily affect surface-level linguistic features, with no evidence of\ndistinct reasoning pathways or cognitive differentiation\n  across clinical roles. Despite superficial stylistic changes, the core\ndecision-making mechanisms of LLMs remain uniform\n  across roles, indicating that current PBRP methods fail to replicate the\ncognitive complexity found in real-world medical\n  practice. This highlights the limitations of role-playing in medical AI and\nemphasizes the need for models that simulate genuine\n  cognitive processes rather than linguistic imitation.We have released the\nrelated code in the following repository:https:\n  //github.com/IAAR-Shanghai/RolePlay_LLMDoctor\n","authors":["Xun Liang","Huayi Lai","Hanyu Wang","Wentao Zhang","Linfeng Zhang","Yanfang Chen","Feiyu Xiong","Zhiyu Li"],"pdf_url":"https://arxiv.org/pdf/2510.24677v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.24668v1","updated":"2025-10-28T17:35:54Z","published":"2025-10-28T17:35:54Z","title":"InteractComp: Evaluating Search Agents With Ambiguous Queries","summary":"  Language agents have demonstrated remarkable potential in web search and\ninformation retrieval. However, these search agents assume user queries are\ncomplete and unambiguous, an assumption that diverges from reality where users\nbegin with incomplete queries requiring clarification through interaction. Yet\nmost agents lack interactive mechanisms during the search process, and existing\nbenchmarks cannot assess this capability. To address this gap, we introduce\nInteractComp, a benchmark designed to evaluate whether search agents can\nrecognize query ambiguity and actively interact to resolve it during search.\nFollowing the principle of easy to verify, interact to disambiguate, we\nconstruct 210 expert-curated questions across 9 domains through a\ntarget-distractor methodology that creates genuine ambiguity resolvable only\nthrough interaction. Evaluation of 17 models reveals striking failure: the best\nmodel achieves only 13.73% accuracy despite 71.50% with complete context,\nexposing systematic overconfidence rather than reasoning deficits. Forced\ninteraction produces dramatic gains, demonstrating latent capability current\nstrategies fail to engage. Longitudinal analysis shows interaction capabilities\nstagnated over 15 months while search performance improved seven-fold,\nrevealing a critical blind spot. This stagnation, coupled with the immediate\nfeedback inherent to search tasks, makes InteractComp a valuable resource for\nboth evaluating and training interaction capabilities in search agents. The\ncode is available at https://github.com/FoundationAgents/InteractComp.\n","authors":["Mingyi Deng","Lijun Huang","Yani Fan","Jiayi Zhang","Fashen Ren","Jinyi Bai","Fuzhen Yang","Dayi Miao","Zhaoyang Yu","Yifan Wu","Yanfei Zhang","Fengwei Teng","Yingjia Wan","Song Hu","Yude Li","Xin Jin","Conghao Hu","Haoyu Li","Qirui Fu","Tai Zhong","Xinyu Wang","Xiangru Tang","Nan Tang","Chenglin Wu","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2510.24668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24664v1","updated":"2025-10-28T17:29:59Z","published":"2025-10-28T17:29:59Z","title":"MQM Re-Annotation: A Technique for Collaborative Evaluation of Machine\n  Translation","summary":"  Human evaluation of machine translation is in an arms race with translation\nmodel quality: as our models get better, our evaluation methods need to be\nimproved to ensure that quality gains are not lost in evaluation noise. To this\nend, we experiment with a two-stage version of the current state-of-the-art\ntranslation evaluation paradigm (MQM), which we call MQM re-annotation. In this\nsetup, an MQM annotator reviews and edits a set of pre-existing MQM\nannotations, that may have come from themselves, another human annotator, or an\nautomatic MQM annotation system. We demonstrate that rater behavior in\nre-annotation aligns with our goals, and that re-annotation results in\nhigher-quality annotations, mostly due to finding errors that were missed\nduring the first pass.\n","authors":["Parker Riley","Daniel Deutsch","Mara Finkelstein","Colten DiIanni","Juraj Juraska","Markus Freitag"],"pdf_url":"https://arxiv.org/pdf/2510.24664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01281v6","updated":"2025-10-28T17:26:20Z","published":"2024-11-02T15:23:28Z","title":"Arena-Lite: Efficient and Reliable Large Language Model Evaluation via\n  Tournament-Based Direct Comparisons","summary":"  As Large Language Models (LLMs) expand across domains, LLM judges have become\nessential for systems evaluation. Current benchmarks typically compare system\noutputs against baselines. This baseline-mediated approach, though convenient,\nyields lower reliability than direct comparison between systems. We propose\nArena-Lite which integrates tournament structure on top of head-to-head\ncomparison. The application of a tournament structure and direct comparison\neliminates the need for baseline outputs, reduces the number of required\ncomparisons, and allows higher reliability in system rankings. We conducted two\nexperiments: (1) controlled stochastic modeling and (2) empirical validation\nwith a real LLM judge. Those experiments collectively demonstrate that\nArena-Lite consistently achieves higher reliability with fewer comparisons,\neven with smaller datasets or weaker judges. We release an easy-to-use web\ndemonstration and code to foster adoption of Arena-Lite, streamlining model\nselection across research and industry communities. Arena-Lite demo and code\nare available on\n\\href{https://huggingface.co/spaces/NCSOFT/ArenaLite}{https://huggingface.co/spaces/NCSOFT/ArenaLite}\n","authors":["Seonil Son","Ju-Min Oh","Heegon Jin","Cheolhun Jang","Jeongbeom Jeong","Kuntae Kim"],"pdf_url":"https://arxiv.org/pdf/2411.01281v6.pdf","comment":"8 pages for main body, 19 pages in total"},{"id":"http://arxiv.org/abs/2510.24654v1","updated":"2025-10-28T17:19:47Z","published":"2025-10-28T17:19:47Z","title":"Evolving Diagnostic Agents in a Virtual Clinical Environment","summary":"  In this paper, we present a framework for training large language models\n(LLMs) as diagnostic agents with reinforcement learning, enabling them to\nmanage multi-turn diagnostic processes, adaptively select examinations, and\ncommit to final diagnoses. Unlike instruction-tuned models trained on static\ncase summaries, our method acquires diagnostic strategies through interactive\nexploration and outcome-based feedback. Our contributions are fourfold: (i) We\npresent DiagGym, a diagnostics world model trained with electronic health\nrecords that emits examination outcomes conditioned on patient history and\nrecommended examination, serving as a virtual clinical environment for\nrealistic diagnosis training and evaluation; (ii) We train DiagAgent via\nend-to-end, multi-turn reinforcement learning to learn diagnostic policies that\noptimize both information yield and diagnostic accuracy; (iii) We introduce\nDiagBench, a diagnostic benchmark comprising 750 cases with physician-validated\nexamination recommendations and 99 cases annotated with 973 physician-written\nrubrics on diagnosis process; (iv) we demonstrate superior performance across\ndiverse diagnostic settings. DiagAgent significantly outperforms 10\nstate-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two\nprompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34%\nhigher diagnostic accuracy and 44.03% improvement in examination recommendation\nhit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic\naccuracy and 23.09% boost in examination recommendation F1 score. In\nrubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by\n7.1% in weighted rubric score. These findings indicate that learning policies\nin interactive clinical environments confers dynamic and clinically meaningful\ndiagnostic management abilities unattainable through passive training alone.\n","authors":["Pengcheng Qiu","Chaoyi Wu","Junwei Liu","Qiaoyu Zheng","Yusheng Liao","Haowen Wang","Yun Yue","Qianrui Fan","Shuai Zhen","Jian Wang","Jinjie Gu","Yanfeng Wang","Ya Zhang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2510.24654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24652v1","updated":"2025-10-28T17:18:30Z","published":"2025-10-28T17:18:30Z","title":"Optimizing Retrieval for RAG via Reinforced Contrastive Learning","summary":"  As retrieval-augmented generation (RAG) becomes increasingly widespread, the\nrole of information retrieval (IR) is shifting from retrieving information for\nhuman users to retrieving contextual knowledge for artificial intelligence (AI)\nsystems, where relevance becomes difficult to define or annotate beforehand. To\naddress this challenge, we propose R3, a Retrieval framework optimized for RAG\nthrough trialand-feedback Reinforced contrastive learning. Unlike prior\napproaches that rely on annotated or synthetic data for supervised fine-tuning,\nR3 enables the retriever to dynamically explore and optimize relevance within\nthe RAG environment. During training, the retrieved results interact with the\nenvironment to produce contrastive signals that automatically guide the\nretriever's self-improvement. Extensive experiments across diverse tasks\ndemonstrate that R3 improves RAG performance by 5.2% over the original\nretriever and surpasses state-of-the-art retrievers by 4.9%, while achieving\ncomparable results to LLM-augmented retrieval and RAG systems built on\npost-trained or instruction-tuned LLMs. It is both efficient and practical,\nrequiring only 4 GPUs and completing training within a single day.\n","authors":["Jiawei Zhou","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2510.24652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24647v1","updated":"2025-10-28T17:15:31Z","published":"2025-10-28T17:15:31Z","title":"Quantifying the Effects of Word Length, Frequency, and Predictability on\n  Dyslexia","summary":"  We ask where, and under what conditions, dyslexic reading costs arise in a\nlarge-scale naturalistic reading dataset. Using eye-tracking aligned to\nword-level features (word length, frequency, and predictability), we model how\neach feature influences dyslexic time costs. We find that all three features\nrobustly change reading times in both typical and dyslexic readers, and that\ndyslexic readers show stronger sensitivities to each, especially\npredictability. Counterfactual manipulations of these features substantially\nnarrow the dyslexic-control gap by about one third, with predictability showing\nthe strongest effect, followed by length and frequency. These patterns align\nwith dyslexia theories that posit heightened demands on linguistic working\nmemory and phonological encoding, and they motivate further work on lexical\ncomplexity and parafoveal preview benefits to explain the remaining gap. In\nshort, we quantify when extra dyslexic costs arise, how large they are, and\noffer actionable guidance for interventions and computational models for\ndyslexics.\n","authors":["Hugo Rydel-Johnston","Alex Kafkas"],"pdf_url":"https://arxiv.org/pdf/2510.24647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24636v1","updated":"2025-10-28T17:02:46Z","published":"2025-10-28T17:02:46Z","title":"OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement\n  Learning","summary":"  Reward models (RMs) have become essential for aligning large language models\n(LLMs), serving as scalable proxies for human evaluation in both training and\ninference. However, existing RMs struggle on knowledge-intensive and long-form\ntasks, where evaluating correctness requires grounding beyond the model's\ninternal knowledge. This limitation hinders them from reliably discriminating\nsubtle quality differences, especially when external evidence is necessary. To\naddress this, we introduce OpenRM, a tool-augmented long-form reward model that\nsystematically judges open-ended responses by invoking external tools to gather\nrelevant evidence. We train OpenRM with Group Relative Policy Optimization\n(GRPO) on over 27K synthesized pairwise examples generated through a\ncontrollable data synthesis framework. The training objective jointly\nsupervises intermediate tool usage and final outcome accuracy, incentivizing\nour reward model to learn effective evidence-based judgment strategies.\nExtensive experiments on three newly-collected datasets and two widely-used\nbenchmarks demonstrate that OpenRM substantially outperforms existing reward\nmodeling approaches. As a further step, we integrate OpenRM into both\ninference-time response selection and training-time data selection. This yields\nconsistent gains in downstream LLM alignment tasks, highlighting the potential\nof tool-augmented reward models for scaling reliable long-form evaluation.\n","authors":["Ziyou Hu","Zhengliang Shi","Minghang Zhu","Haitao Li","Teng Sun","Pengjie Ren","Suzan Verberne","Zhaochun Ren"],"pdf_url":"https://arxiv.org/pdf/2510.24636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24628v1","updated":"2025-10-28T16:58:26Z","published":"2025-10-28T16:58:26Z","title":"\"Mm, Wat?\" Detecting Other-initiated Repair Requests in Dialogue","summary":"  Maintaining mutual understanding is a key component in human-human\nconversation to avoid conversation breakdowns, in which repair, particularly\nOther-Initiated Repair (OIR, when one speaker signals trouble and prompts the\nother to resolve), plays a vital role. However, Conversational Agents (CAs)\nstill fail to recognize user repair initiation, leading to breakdowns or\ndisengagement. This work proposes a multimodal model to automatically detect\nrepair initiation in Dutch dialogues by integrating linguistic and prosodic\nfeatures grounded in Conversation Analysis. The results show that prosodic cues\ncomplement linguistic features and significantly improve the results of\npretrained text and audio embeddings, offering insights into how different\nfeatures interact. Future directions include incorporating visual cues,\nexploring multilingual and cross-context corpora to assess the robustness and\ngeneralizability.\n","authors":["Anh Ngo","Nicolas Rollet","Catherine Pelachaud","Chloe Clavel"],"pdf_url":"https://arxiv.org/pdf/2510.24628v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2510.24626v1","updated":"2025-10-28T16:55:22Z","published":"2025-10-28T16:55:22Z","title":"Relative Scaling Laws for LLMs","summary":"  Scaling laws describe how language models improve with additional data,\nparameters, and compute. While widely used, they are typically measured on\naggregate test sets. Aggregate evaluations yield clean trends but average over\nheterogeneous subpopulations, obscuring performance disparities. We introduce\nrelative scaling laws, which track how performance gaps between test\ndistributions evolve with scale rather than focusing solely on absolute error.\nUsing 255 decoder-only Transformers trained under matched-compute (IsoFLOP)\nbudgets from $10^{18}$--$10^{20}$ FLOPs on standard pretraining datasets, we\nfind diverse trajectories: academic domains on MMLU converge toward parity;\nregional English dialects shift depending on population size; and clusters of\nAI risk behaviours split, with capability- and influence-related risks\nincreasing during pretraining while adversarial risks do not. These results\nshow that although scaling improves overall performance, it is not a universal\nequalizer. To support further study, we release all model checkpoints from this\nwork to enable practitioners to measure relative alongside traditional scaling\nlaws, in order to better prioritize robustness challenges in light of the\nbitter lesson.\n","authors":["William Held","David Hall","Percy Liang","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2510.24626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24619v1","updated":"2025-10-28T16:48:03Z","published":"2025-10-28T16:48:03Z","title":"Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation","summary":"  With the release of new large language models (LLMs) like Llama and Mistral,\nzero-shot cross-lingual transfer has become increasingly feasible due to their\nmultilingual pretraining and strong generalization capabilities. However,\nadapting these decoder-only LLMs to new tasks across languages remains\nchallenging. While parameter-efficient fine-tuning (PeFT) techniques like\nLow-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as\nsoft prompt tuning, prefix tuning, and Llama Adapter are less explored,\nespecially for zero-shot transfer in decoder-only models. We present a\ncomprehensive study of three prefix-based methods for zero-shot cross-lingual\ntransfer from English to 35+ high- and low-resource languages. Our analysis\nfurther explores transfer across linguistic families and scripts, as well as\nthe impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix\nmethods outperform LoRA-baselines by up to 6% on the Belebele benchmark.\nSimilar improvements were observed with Mistral v0.3 7B as well. Despite using\nonly 1.23M learning parameters with prefix tuning, we achieve consistent\nimprovements across diverse benchmarks. These findings highlight the potential\nof prefix-based techniques as an effective and scalable alternative to LoRA,\nparticularly in low-resource multilingual settings.\n","authors":["Snegha A","Sayambhu Sen","Piyush Singh Pasi","Abhishek Singhania","Preethi Jyothi"],"pdf_url":"https://arxiv.org/pdf/2510.24619v1.pdf","comment":"12 Pages"},{"id":"http://arxiv.org/abs/2409.11390v3","updated":"2025-10-28T16:44:02Z","published":"2024-09-17T17:50:15Z","title":"Says Who? Effective Zero-Shot Annotation of Focalization","summary":"  Focalization describes the way in which access to narrative information is\nrestricted or controlled based on the knowledge available to knowledge of the\nnarrator. It is encoded via a wide range of lexico-grammatical features and is\nsubject to reader interpretation. Even trained annotators frequently disagree\non correct labels, suggesting this task is both qualitatively and\ncomputationally challenging. In this work, we test how well five contemporary\nlarge language model (LLM) families and two baselines perform when annotating\nshort literary excerpts for focalization. Despite the challenging nature of the\ntask, we find that LLMs show comparable performance to trained human\nannotators, with GPT-4o achieving an average F1 of 84.79%. Further, we\ndemonstrate that the log probabilities output by GPT-family models frequently\nreflect the difficulty of annotating particular excerpts. Finally, we provide a\ncase study analyzing sixteen Stephen King novels, demonstrating the usefulness\nof this approach for computational literary studies and the insights gleaned\nfrom examining focalization at scale.\n","authors":["Rebecca M. M. Hicke","Yuri Bizzoni","Pascale Feldkamp","Ross Deans Kristensen-McLachlan"],"pdf_url":"https://arxiv.org/pdf/2409.11390v3.pdf","comment":"Accepted at CHR 2025"},{"id":"http://arxiv.org/abs/2510.24606v1","updated":"2025-10-28T16:34:18Z","published":"2025-10-28T16:34:18Z","title":"Long-Context Modeling with Dynamic Hierarchical Sparse Attention for\n  On-Device LLMs","summary":"  The quadratic cost of attention hinders the scalability of long-context LLMs,\nespecially in resource-constrained settings. Existing static sparse methods\nsuch as sliding windows or global tokens utilizes the sparsity of attention to\nreduce the cost of attention, but poorly adapts to the content-dependent\nvariations in attention due to their staticity. While previous work has\nproposed several dynamic approaches to improve flexibility, they still depend\non predefined templates or heuristic mechanisms. Such strategies reduce\ngenerality and prune tokens that remain contextually important, limiting their\naccuracy across diverse tasks. To tackle these bottlenecks of existing methods\nfor long-context modeling, we introduce Dynamic Hierarchical Sparse Attention\n(DHSA), a data-driven framework that dynamically predicts attention sparsity\nonline without retraining. Our proposed DHSA adaptively segments sequences into\nvariable-length chunks, then computes chunk representations by aggregating the\ntoken embeddings within each chunk. To avoid the bias introduced by varying\nchunk lengths, we apply length-normalized aggregation that scales the averaged\nembeddings by the square root of the chunk size. Finally, DHSA upsamples the\nchunk-level similarity scores to token level similarities to calculate\nimportance scores that determine which token-level interactions should be\npreserved. Our experiments on Gemma2 with Needle-in-a-Haystack Test and\nLongBench show that DHSA matches dense attention in accuracy, while reducing\nprefill latency by 20-60% and peak memory usage by 35%. Compared to other\nrepresentative baselines such as block sparse attention, DHSA achieves\nconsistently higher accuracy (6-18% relative gains) with comparable or lower\ncost, offering an efficient and adaptable solution for long-context on-device\nLLMs.\n","authors":["Siheng Xiong","Joe Zou","Faramarz Fekri","Yae Jee Cho"],"pdf_url":"https://arxiv.org/pdf/2510.24606v1.pdf","comment":"Accepted to NeurIPS 2025 Workshop on Efficient Reasoning"},{"id":"http://arxiv.org/abs/2510.24605v1","updated":"2025-10-28T16:32:43Z","published":"2025-10-28T16:32:43Z","title":"Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead\n  the Way","summary":"  Diffusion-based large language models (dLLMs) have exhibited substantial\npotential for parallel text generation, which may enable more efficient\ngeneration compared to autoregressive models. However, current dLLMs suffer\nfrom fixed generation lengths, which indicates the generation lengths of dLLMs\nhave to be determined before decoding as a hyper-parameter, leading to issues\nin efficiency and flexibility. To solve these problems, in this work, we\npropose to train a diffusion LLM with native variable generation lengths,\nabbreviated as dLLM-Var. Concretely, we aim to train a model to accurately\npredict the [EOS] token in the generated text, which makes a dLLM be able to\nnatively infer in a block diffusion manner, while still maintaining the ability\nof global bi-directional (full) attention and high parallelism. Experiments on\nstandard benchmarks demonstrate that our method achieves a 30.1x speedup over\ntraditional dLLM inference paradigms and a 2.4x speedup relative to\nautoregressive models such as Qwen and Llama. Our method achieves higher\naccuracy and faster inference, elevating dLLMs beyond mere academic novelty and\nsupporting their practical use in real-world applications. Codes and models\nhave been released.\n","authors":["Yicun Yang","Cong Wang","Shaobo Wang","Zichen Wen","Biqing Qi","Hanlin Xu","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.24605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15737v4","updated":"2025-10-28T16:23:53Z","published":"2024-11-24T07:02:32Z","title":"TableTime: Reformulating Time Series Classification as Training-Free\n  Table Understanding with Large Language Models","summary":"  Large language models (LLMs) have demonstrated their effectiveness in\nmultivariate time series classification (MTSC). Effective adaptation of LLMs\nfor MTSC necessitates informative data representations. Existing LLM-based\nmethods directly encode embeddings for time series within the latent space of\nLLMs from scratch to align with semantic space of LLMs. Despite their\neffectiveness, we reveal that these methods conceal three inherent bottlenecks:\n(1) they struggle to encode temporal and channel-specific information in a\nlossless manner, both of which are critical components of multivariate time\nseries; (2) it is much difficult to align the learned representation space with\nthe semantic space of the LLMs; (3) they require task-specific retraining,\nwhich is both computationally expensive and labor-intensive. To bridge these\ngaps, we propose TableTime, which reformulates MTSC as a table understanding\ntask. Specifically, TableTime introduces the following strategies: (1) convert\nmultivariate time series into a tabular form, thus minimizing information loss\nto the greatest extent; (2) represent tabular time series in text format to\nachieve natural alignment with the semantic space of LLMs; (3) design a\nreasoning framework that integrates contextual text information, neighborhood\nassistance, multi-path inference and problem decomposition to enhance the\nreasoning ability of LLMs and realize zero-shot classification. Extensive\nexperiments performed on 10 publicly representative datasets from UEA archive\nverify the superiorities of the TableTime.\n","authors":["Jiahao Wang","Mingyue Cheng","Qingyang Mao","Yitong Zhou","Daoyu Wang","Qi Liu","Feiyang Xu","Xin Li"],"pdf_url":"https://arxiv.org/pdf/2411.15737v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23458v2","updated":"2025-10-28T16:23:04Z","published":"2025-10-27T15:58:51Z","title":"BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents","summary":"  Confidence in LLMs is a useful indicator of model uncertainty and answer\nreliability. Existing work mainly focused on single-turn scenarios, while\nresearch on confidence in complex multi-turn interactions is limited. In this\npaper, we investigate whether LLM-based search agents have the ability to\ncommunicate their own confidence through verbalized confidence scores after\nlong sequences of actions, a significantly more challenging task compared to\noutputting confidence in a single interaction. Experimenting on open-source\nagentic models, we first find that models exhibit much higher task accuracy at\nhigh confidence while having near-zero accuracy when confidence is low. Based\non this observation, we propose Test-Time Scaling (TTS) methods that use\nconfidence scores to determine answer quality, encourage the model to try again\nuntil reaching a satisfactory confidence level. Results show that our proposed\nmethods significantly reduce token consumption while demonstrating competitive\nperformance compared to baseline fixed budget TTS methods.\n","authors":["Litu Ou","Kuan Li","Huifeng Yin","Liwen Zhang","Zhongwang Zhang","Xixi Wu","Rui Ye","Zile Qiao","Pengjun Xie","Jingren Zhou","Yong Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.23458v2.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2510.24592v1","updated":"2025-10-28T16:22:54Z","published":"2025-10-28T16:22:54Z","title":"ReForm: Reflective Autoformalization with Prospective Bounded Sequence\n  Optimization","summary":"  Autoformalization, which translates natural language mathematics into\nmachine-verifiable formal statements, is critical for using formal mathematical\nreasoning to solve math problems stated in natural language. While Large\nLanguage Models can generate syntactically correct formal statements, they\noften fail to preserve the original problem's semantic intent. This limitation\narises from the LLM approaches' treating autoformalization as a simplistic\ntranslation task which lacks mechanisms for self-reflection and iterative\nrefinement that human experts naturally employ. To address these issues, we\npropose ReForm, a Reflective Autoformalization method that tightly integrates\nsemantic consistency evaluation into the autoformalization process. This\nenables the model to iteratively generate formal statements, assess its\nsemantic fidelity, and self-correct identified errors through progressive\nrefinement. To effectively train this reflective model, we introduce\nProspective Bounded Sequence Optimization (PBSO), which employs different\nrewards at different sequence positions to ensure that the model develops both\naccurate autoformalization and correct semantic validations, preventing\nsuperficial critiques that would undermine the purpose of reflection. Extensive\nexperiments across four autoformalization benchmarks demonstrate that ReForm\nachieves an average improvement of 17.2 percentage points over the strongest\nbaselines. To further ensure evaluation reliability, we introduce\nConsistencyCheck, a benchmark of 859 expert-annotated items that not only\nvalidates LLMs as judges but also reveals that autoformalization is inherently\ndifficult: even human experts produce semantic errors in up to 38.5% of cases.\n","authors":["Guoxin Chen","Jing Wu","Xinjie Chen","Wayne Xin Zhao","Ruihua Song","Chengxi Li","Kai Fan","Dayiheng Liu","Minpeng Liao"],"pdf_url":"https://arxiv.org/pdf/2510.24592v1.pdf","comment":"Ongoing Work"},{"id":"http://arxiv.org/abs/2510.24591v1","updated":"2025-10-28T16:21:19Z","published":"2025-10-28T16:21:19Z","title":"ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?","summary":"  Frontier AI agents show increasing promise as scientific research assistants,\nand may eventually be useful for extended, open-ended research workflows.\nHowever, in order to use agents for novel research, we must first assess the\nunderlying faithfulness and correctness of their work. To evaluate agents as\nresearch assistants, we introduce ReplicationBench, an evaluation framework\nthat tests whether agents can replicate entire research papers drawn from the\nastrophysics literature. Astrophysics, where research relies heavily on\narchival data and computational study while requiring little real-world\nexperimentation, is a particularly useful testbed for AI agents in scientific\nresearch. We split each paper into tasks which require agents to replicate the\npaper's core contributions, including the experimental setup, derivations, data\nanalysis, and codebase. Each task is co-developed with the original paper\nauthors and targets a key scientific result, enabling objective evaluation of\nboth faithfulness (adherence to original methods) and correctness (technical\naccuracy of results). ReplicationBench is extremely challenging for current\nfrontier language models: even the best-performing language models score under\n20%. We analyze ReplicationBench trajectories in collaboration with domain\nexperts and find a rich, diverse set of failure modes for agents in scientific\nresearch. ReplicationBench establishes the first benchmark of paper-scale,\nexpert-validated astrophysics research tasks, reveals insights about agent\nperformance generalizable to other domains of data-driven science, and provides\na scalable framework for measuring AI agents' reliability in scientific\nresearch.\n","authors":["Christine Ye","Sihan Yuan","Suchetha Cooray","Steven Dillmann","Ian L. V. Roque","Dalya Baron","Philipp Frank","Sergio Martin-Alvarez","Nolan Koblischke","Frank J Qu","Diyi Yang","Risa Wechsler","Ioana Ciuca"],"pdf_url":"https://arxiv.org/pdf/2510.24591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14617v3","updated":"2025-10-28T16:02:10Z","published":"2025-05-20T17:03:12Z","title":"The Hawthorne Effect in Reasoning Models: Evaluating and Steering Test\n  Awareness","summary":"  Reasoning-focused LLMs sometimes alter their behavior when they detect that\nthey are being evaluated, which can lead them to optimize for test-passing\nperformance or to comply more readily with harmful prompts if real-world\nconsequences appear absent. We present the first quantitative study of how such\n\"test awareness\" impacts model behavior, particularly its performance on\nsafety-related tasks. We introduce a white-box probing framework that (i)\nlinearly identifies awareness-related activations and (ii) steers models toward\nor away from test awareness while monitoring downstream performance. We apply\nour method to different state-of-the-art open-weight reasoning LLMs across both\nrealistic and hypothetical tasks (denoting tests or simulations). Our results\ndemonstrate that test awareness significantly impacts safety alignment (such as\ncompliance with harmful requests and conforming to stereotypes) with effects\nvarying in both magnitude and direction across models. By providing control\nover this latent effect, our work aims to provide a stress-test mechanism and\nincrease trust in how we perform safety evaluations.\n","authors":["Sahar Abdelnabi","Ahmed Salem"],"pdf_url":"https://arxiv.org/pdf/2505.14617v3.pdf","comment":"NeurIPS 2025 (Spotlight). Code is available at:\n  https://github.com/microsoft/Test_Awareness_Steering"},{"id":"http://arxiv.org/abs/2510.24570v1","updated":"2025-10-28T16:01:24Z","published":"2025-10-28T16:01:24Z","title":"BEST-RQ-Based Self-Supervised Learning for Whisper Domain Adaptation","summary":"  Automatic Speech Recognition (ASR) systems, despite large multilingual\ntraining, struggle in out-of-domain and low-resource scenarios where labeled\ndata is scarce. We propose BEARD (BEST-RQ Encoder Adaptation with Re-training\nand Distillation), a novel framework designed to adapt Whisper's encoder using\nunlabeled data. Unlike traditional self-supervised learning methods, BEARD\nuniquely combines a BEST-RQ objective with knowledge distillation from a frozen\nteacher encoder, ensuring the encoder's complementarity with the pre-trained\ndecoder. Our experiments focus on the ATCO2 corpus from the challenging Air\nTraffic Control (ATC) communications domain, characterized by non-native\nspeech, noise, and specialized phraseology. Using about 5,000 hours of\nuntranscribed speech for BEARD and 2 hours of transcribed speech for\nfine-tuning, the proposed approach significantly outperforms previous baseline\nand fine-tuned model, achieving a relative improvement of 12% compared to the\nfine-tuned model. To the best of our knowledge, this is the first work to use a\nself-supervised learning objective for domain adaptation of Whisper.\n","authors":["RaphaÃ«l Bagat","Irina Illina","Emmanuel Vincent"],"pdf_url":"https://arxiv.org/pdf/2510.24570v1.pdf","comment":"Submitted to ICASSP 2026"},{"id":"http://arxiv.org/abs/2510.24541v1","updated":"2025-10-28T15:43:26Z","published":"2025-10-28T15:43:26Z","title":"Open Korean Historical Corpus: A Millennia-Scale Diachronic Collection\n  of Public Domain Texts","summary":"  The history of the Korean language is characterized by a discrepancy between\nits spoken and written forms and a pivotal shift from Chinese characters to the\nHangul alphabet. However, this linguistic evolution has remained largely\nunexplored in NLP due to a lack of accessible historical corpora. To address\nthis gap, we introduce the Open Korean Historical Corpus, a large-scale, openly\nlicensed dataset spanning 1,300 years and 6 languages, as well as\nunder-represented writing systems like Korean-style Sinitic (Idu) and\nHanja-Hangul mixed script. This corpus contains 18 million documents and 5\nbillion tokens from 19 sources, ranging from the 7th century to 2025. We\nleverage this resource to quantitatively analyze major linguistic shifts: (1)\nIdu usage peaked in the 1860s before declining sharply; (2) the transition from\nHanja to Hangul was a rapid transformation starting around 1890; and (3) North\nKorea's lexical divergence causes modern tokenizers to produce up to 51 times\nhigher out-of-vocabulary rates. This work provides a foundational resource for\nquantitative diachronic analysis by capturing the history of the Korean\nlanguage. Moreover, it can serve as a pre-training corpus for large language\nmodels, potentially improving their understanding of Sino-Korean vocabulary in\nmodern Hangul as well as archaic writing systems.\n","authors":["Seyoung Song","Nawon Kim","Songeun Chae","Kiwoong Park","Jiho Jin","Haneul Yoo","Kyunghyun Cho","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2510.24541v1.pdf","comment":"Dataset and code available at https://github.com/seyoungsong/OKHC"},{"id":"http://arxiv.org/abs/2510.24538v1","updated":"2025-10-28T15:42:03Z","published":"2025-10-28T15:42:03Z","title":"Dark & Stormy: Modeling Humor in the Worst Sentences Ever Written","summary":"  Textual humor is enormously diverse and computational studies need to account\nfor this range, including intentionally bad humor. In this paper, we curate and\nanalyze a novel corpus of sentences from the Bulwer-Lytton Fiction Contest to\nbetter understand \"bad\" humor in English. Standard humor detection models\nperform poorly on our corpus, and an analysis of literary devices finds that\nthese sentences combine features common in existing humor datasets (e.g., puns,\nirony) with metaphor, metafiction and simile. LLMs prompted to synthesize\ncontest-style sentences imitate the form but exaggerate the effect by\nover-using certain literary devices, and including far more novel\nadjective-noun bigrams than human writers. Data, code and analysis are\navailable at https://github.com/venkatasg/bulwer-lytton\n","authors":["Venkata S Govindarajan","Laura Biester"],"pdf_url":"https://arxiv.org/pdf/2510.24538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24530v1","updated":"2025-10-28T15:38:22Z","published":"2025-10-28T15:38:22Z","title":"LevÃ©e d'ambiguÃ¯tÃ©s par grammaires locales","summary":"  Many words are ambiguous in terms of their part of speech (POS). However,\nwhen a word appears in a text, this ambiguity is generally much reduced.\nDisambiguating POS involves using context to reduce the number of POS\nassociated with words, and is one of the main challenges of lexical tagging.\nThe problem of labeling words by POS frequently arises in natural language\nprocessing, for example for spelling correction, grammar or style checking,\nexpression recognition, text-to-speech conversion, text corpus analysis, etc.\nLexical tagging systems are thus useful as an initial component of many natural\nlanguage processing systems. A number of recent lexical tagging systems produce\nmultiple solutions when the text is lexically ambiguous or the uniquely correct\nsolution cannot be found. These contributions aim to guarantee a zero silence\nrate: the correct tag(s) for a word must never be discarded. This objective is\nunrealistic for systems that tag each word uniquely. This article concerns a\nlexical disambiguation method adapted to the objective of a zero silence rate\nand implemented in Silberztein's INTEX system (1993). We present here a formal\ndescription of this method. We show that to verify a local disambiguation\ngrammar in this framework, it is not sufficient to consider the transducer\npaths separately: one needs to verify their interactions. Similarly, if a\ncombination of multiple transducers is used, the result cannot be predicted by\nconsidering them in isolation. Furthermore, when examining the initial labeling\nof a text as produced by INTEX, ideas for disambiguation rules come\nspontaneously, but grammatical intuitions may turn out to be inaccurate, often\ndue to an unforeseen construction or ambiguity. If a zero silence rate is\ntargeted, local grammars must be carefully tested. This is where a detailed\nspecification of what a grammar will do once applied to texts would be\nnecessary.\n","authors":["Eric G. C. Laporte"],"pdf_url":"https://arxiv.org/pdf/2510.24530v1.pdf","comment":"in French language"},{"id":"http://arxiv.org/abs/2510.24514v1","updated":"2025-10-28T15:26:20Z","published":"2025-10-28T15:26:20Z","title":"Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal\n  Reasoning in MLLMs","summary":"  While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.\n","authors":["Huanyu Zhang","Wenshan Wu","Chengzu Li","Ning Shang","Yan Xia","Yangyu Huang","Yifan Zhang","Li Dong","Zhang Zhang","Liang Wang","Tieniu Tan","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2510.24514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.15545v2","updated":"2025-10-28T15:23:35Z","published":"2025-10-17T11:25:36Z","title":"TokenTiming: A Dynamic Alignment Method for Universal Speculative\n  Decoding Model Pairs","summary":"  Accelerating the inference of large language models (LLMs) has been a\ncritical challenge in generative AI. Speculative decoding (SD) substantially\nimproves LLM inference efficiency. However, its utility is limited by a\nfundamental constraint: the draft and target models must share the same\nvocabulary, thus limiting the herd of available draft models and often\nnecessitating the training of a new model from scratch. Inspired by Dynamic\nTime Warping (DTW), a classic algorithm for aligning time series, we propose\nthe algorithm TokenTiming for universal speculative decoding. It operates by\nre-encoding the draft token sequence to get a new target token sequence, and\nthen uses DTW to build a mapping to transfer the probability distributions for\nspeculative sampling. Benefiting from this, our method accommodates mismatched\nvocabularies and works with any off-the-shelf models without retraining and\nmodification. We conduct comprehensive experiments on various tasks,\ndemonstrating 1.57x speedup. This work enables a universal approach for draft\nmodel selection, making SD a more versatile and practical tool for LLM\nacceleration.\n","authors":["Sibo Xiao","Jinyuan Fu","Zhongle Xie","Lidan Shou"],"pdf_url":"https://arxiv.org/pdf/2510.15545v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24505v1","updated":"2025-10-28T15:16:06Z","published":"2025-10-28T15:16:06Z","title":"CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?","summary":"  Accurate confidence calibration in Large Language Models (LLMs) is critical\nfor safe use in high-stakes domains, where clear verbalized confidence enhances\nuser trust. Traditional methods that mimic reference confidence expressions\noften fail to capture the reasoning needed for accurate confidence assessment.\nWe propose natural language critiques as a solution, ideally suited for\nconfidence calibration, as precise gold confidence labels are hard to obtain\nand often require multiple generations. This paper studies how natural language\ncritiques can enhance verbalized confidence, addressing: (1) What to critique:\nuncertainty (question-focused) or confidence (answer-specific)? Analysis shows\nconfidence suits multiple-choice tasks, while uncertainty excels in open-ended\nscenarios. (2) How to critique: self-critique or critique calibration training?\nWe propose Self-Critique, enabling LLMs to critique and optimize their\nconfidence beyond mere accuracy, and CritiCal, a novel Critique Calibration\ntraining method that leverages natural language critiques to improve confidence\ncalibration, moving beyond direct numerical optimization. Experiments show that\nCritiCal significantly outperforms Self-Critique and other competitive\nbaselines, even surpassing its teacher model, GPT-4o, in complex reasoning\ntasks. CritiCal also shows robust generalization in out-of-distribution\nsettings, advancing LLM's reliability.\n","authors":["Qing Zong","Jiayu Liu","Tianshi Zheng","Chunyang Li","Baixuan Xu","Haochen Shi","Weiqi Wang","Zhaowei Wang","Chunkit Chan","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2510.24505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24488v1","updated":"2025-10-28T15:03:18Z","published":"2025-10-28T15:03:18Z","title":"A word association network methodology for evaluating implicit biases in\n  LLMs compared to humans","summary":"  As Large language models (LLMs) become increasingly integrated into our\nlives, their inherent social biases remain a pressing concern. Detecting and\nevaluating these biases can be challenging because they are often implicit\nrather than explicit in nature, so developing evaluation methods that assess\nthe implicit knowledge representations of LLMs is essential. We present a novel\nword association network methodology for evaluating implicit biases in LLMs\nbased on simulating semantic priming within LLM-generated word association\nnetworks. Our prompt-based approach taps into the implicit relational\nstructures encoded in LLMs, providing both quantitative and qualitative\nassessments of bias. Unlike most prompt-based evaluation methods, our method\nenables direct comparisons between various LLMs and humans, providing a\nvaluable point of reference and offering new insights into the alignment of\nLLMs with human cognition. To demonstrate the utility of our methodology, we\napply it to both humans and several widely used LLMs to investigate social\nbiases related to gender, religion, ethnicity, sexual orientation, and\npolitical party. Our results reveal both convergences and divergences between\nLLM and human biases, providing new perspectives on the potential risks of\nusing LLMs. Our methodology contributes to a systematic, scalable, and\ngeneralizable framework for evaluating and comparing biases across multiple\nLLMs and humans, advancing the goal of transparent and socially responsible\nlanguage technologies.\n","authors":["Katherine Abramski","Giulio Rossetti","Massimo Stella"],"pdf_url":"https://arxiv.org/pdf/2510.24488v1.pdf","comment":"24 pages, 13 figures, 3 tables"},{"id":"http://arxiv.org/abs/2510.24478v1","updated":"2025-10-28T14:50:03Z","published":"2025-10-28T14:50:03Z","title":"Talk2Ref: A Dataset for Reference Prediction from Scientific Talks","summary":"  Scientific talks are a growing medium for disseminating research, and\nautomatically identifying relevant literature that grounds or enriches a talk\nwould be highly valuable for researchers and students alike. We introduce\nReference Prediction from Talks (RPT), a new task that maps long, and\nunstructured scientific presentations to relevant papers. To support research\non RPT, we present Talk2Ref, the first large-scale dataset of its kind,\ncontaining 6,279 talks and 43,429 cited papers (26 per talk on average), where\nrelevance is approximated by the papers cited in the talk's corresponding\nsource publication. We establish strong baselines by evaluating\nstate-of-the-art text embedding models in zero-shot retrieval scenarios, and\npropose a dual-encoder architecture trained on Talk2Ref. We further explore\nstrategies for handling long transcripts, as well as training for domain\nadaptation. Our results show that fine-tuning on Talk2Ref significantly\nimproves citation prediction performance, demonstrating both the challenges of\nthe task and the effectiveness of our dataset for learning semantic\nrepresentations from spoken scientific content. The dataset and trained models\nare released under an open license to foster future research on integrating\nspoken scientific communication into citation recommendation systems.\n","authors":["Frederik Broy","Maike ZÃ¼fle","Jan Niehues"],"pdf_url":"https://arxiv.org/pdf/2510.24478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24476v1","updated":"2025-10-28T14:48:57Z","published":"2025-10-28T14:48:57Z","title":"Mitigating Hallucination in Large Language Models (LLMs): An\n  Application-Oriented Survey on RAG, Reasoning, and Agentic Systems","summary":"  Hallucination remains one of the key obstacles to the reliable deployment of\nlarge language models (LLMs), particularly in real-world applications. Among\nvarious mitigation strategies, Retrieval-Augmented Generation (RAG) and\nreasoning enhancement have emerged as two of the most effective and widely\nadopted approaches, marking a shift from merely suppressing hallucinations to\nbalancing creativity and reliability. However, their synergistic potential and\nunderlying mechanisms for hallucination mitigation have not yet been\nsystematically examined. This survey adopts an application-oriented perspective\nof capability enhancement to analyze how RAG, reasoning enhancement, and their\nintegration in Agentic Systems mitigate hallucinations. We propose a taxonomy\ndistinguishing knowledge-based and logic-based hallucinations, systematically\nexamine how RAG and reasoning address each, and present a unified framework\nsupported by real-world applications, evaluations, and benchmarks.\n","authors":["Yihan Li","Xiyuan Fu","Ghanshyam Verma","Paul Buitelaar","Mingming Liu"],"pdf_url":"https://arxiv.org/pdf/2510.24476v1.pdf","comment":"25 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2510.22830v2","updated":"2025-10-28T14:43:58Z","published":"2025-10-26T20:59:22Z","title":"Exploration of Summarization by Generative Language Models for Automated\n  Scoring of Long Essays","summary":"  BERT and its variants are extensively explored for automated scoring.\nHowever, a limit of 512 tokens for these encoder-based models showed the\ndeficiency in automated scoring of long essays. Thus, this research explores\ngenerative language models for automated scoring of long essays via\nsummarization and prompting. The results revealed great improvement of scoring\naccuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab\nAutomated Essay Scoring 2.0 dataset.\n","authors":["Haowei Hua","Hong Jiao","Xinyi Wang"],"pdf_url":"https://arxiv.org/pdf/2510.22830v2.pdf","comment":"19 pages, 5 Tables 7 Figures, Presentation at Artificial Intelligence\n  in Measurement and Education Conference (AIME-Con)"},{"id":"http://arxiv.org/abs/2510.24469v1","updated":"2025-10-28T14:36:22Z","published":"2025-10-28T14:36:22Z","title":"Iterative Critique-Refine Framework for Enhancing LLM Personalization","summary":"  Personalized text generation requires models not only to produce coherent\ntext but also to align with a target user's style, tone, and topical focus.\nExisting retrieval-augmented approaches such as LaMP and PGraphRAG enrich\nprofiles with user and neighbor histories, but they stop at generation and\noften yield outputs that drift in tone, topic, or style. We present PerFine, a\nunified, training-free critique-refine framework that enhances personalization\nthrough iterative, profile-grounded feedback. In each iteration, an LLM\ngenerator produces a draft conditioned on the retrieved profile, and a critic\nLLM - also conditioned on the same profile - provides structured feedback on\ntone, vocabulary, sentence structure, and topicality. The generator then\nrevises, while a novel knockout strategy retains the stronger draft across\niterations. We further study additional inference-time strategies such as\nBest-of-N and Topic Extraction to balance quality and efficiency. Across Yelp,\nGoodreads, and Amazon datasets, PerFine consistently improves personalization\nover PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5\nrefinement iterations, and scalability with increasing critic size. These\nresults highlight that post-hoc, profile-aware feedback offers a powerful\nparadigm for personalized LLM generation that is both training-free and\nmodel-agnostic.\n","authors":["Durga Prasad Maram","Dhruvin Gandhi","Zonghai Yao","Gayathri Akkinapalli","Franck Dernoncourt","Yu Wang","Ryan A. Rossi","Nesreen K. Ahmed"],"pdf_url":"https://arxiv.org/pdf/2510.24469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.20039v3","updated":"2025-10-28T14:35:32Z","published":"2025-04-28T17:59:28Z","title":"AutoJudge: Judge Decoding Without Manual Annotation","summary":"  We introduce AutoJudge, a method that accelerates large language model (LLM)\ninference with task-specific lossy speculative decoding. Instead of matching\nthe original model output distribution token-by-token, we identify which of the\ngenerated tokens affect the downstream quality of the response, relaxing the\ndistribution match guarantee so that the \"unimportant\" tokens can be generated\nfaster. Our approach relies on a semi-greedy search algorithm to test which of\nthe mismatches between target and draft models should be corrected to preserve\nquality and which ones may be skipped. We then train a lightweight classifier\nbased on existing LLM embeddings to predict, at inference time, which\nmismatching tokens can be safely accepted without compromising the final answer\nquality. We evaluate the effectiveness of AutoJudge with multiple draft/target\nmodel pairs on mathematical reasoning and programming benchmarks, achieving\nsignificant speedups at the cost of a minor accuracy reduction. Notably, on\nGSM8k with the Llama 3.1 70B target model, our approach achieves up to\n$\\approx2\\times$ speedup over speculative decoding at the cost of $\\le 1\\%$\ndrop in accuracy. When applied to the LiveCodeBench benchmark, AutoJudge\nautomatically detects programming-specific important tokens, accepting $\\ge 25$\ntokens per speculation cycle at $2\\%$ drop in Pass@1. Our approach requires no\nhuman annotation and is easy to integrate with modern LLM inference frameworks.\n","authors":["Roman Garipov","Fedor Velikonivtsev","Ivan Ermakov","Ruslan Svirschevski","Vage Egiazarian","Max Ryabinin"],"pdf_url":"https://arxiv.org/pdf/2504.20039v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.17336v2","updated":"2025-10-28T14:31:14Z","published":"2025-09-22T03:13:58Z","title":"Mano Technical Report","summary":"  Graphical user interfaces (GUIs) are the primary medium for human-computer\ninteraction, yet automating GUI interactions remains challenging due to the\ncomplexity of visual elements, dynamic environments, and the need for\nmulti-step reasoning. Existing methods based on vision-language models (VLMs)\noften suffer from limited resolution, domain mismatch, and insufficient\nsequential decisionmaking capability. To address these issues, we propose Mano,\na robust GUI agent built upon a multi-modal foundation model pre-trained on\nextensive web and computer system data. Our approach integrates a novel\nsimulated environment for high-fidelity data generation, a three-stage training\npipeline (supervised fine-tuning, offline reinforcement learning, and online\nreinforcement learning), and a verification module for error recovery. Mano\ndemonstrates state-of-the-art performance on multiple GUI benchmarks, including\nMind2Web and OSWorld, achieving significant improvements in success rate and\noperational accuracy. Our work provides new insights into the effective\nintegration of reinforcement learning with VLMs for practical GUI agent\ndeployment, highlighting the importance of domain-specific data, iterative\ntraining, and holistic reward design.\n","authors":["Tianyu Fu","Anyang Su","Chenxu Zhao","Hanning Wang","Minghui Wu","Zhe Yu","Fei Hu","Mingjia Shi","Wei Dong","Jiayao Wang","Yuyang Chen","Ruiyang Yu","Siran Peng","Menglin Li","Nan Huang","Haitian Wei","Jiawei Yu","Yi Xin","Xilin Zhao","Kai Gu","Ping Jiang","Sifan Zhou","Shuo Wang"],"pdf_url":"https://arxiv.org/pdf/2509.17336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24450v1","updated":"2025-10-28T14:13:44Z","published":"2025-10-28T14:13:44Z","title":"Charting the European LLM Benchmarking Landscape: A New Taxonomy and a\n  Set of Best Practices","summary":"  While new benchmarks for large language models (LLMs) are being developed\ncontinuously to catch up with the growing capabilities of new models and AI in\ngeneral, using and evaluating LLMs in non-English languages remains a\nlittle-charted landscape. We give a concise overview of recent developments in\nLLM benchmarking, and then propose a new taxonomy for the categorization of\nbenchmarks that is tailored to multilingual or non-English use scenarios. We\nfurther propose a set of best practices and quality standards that could lead\nto a more coordinated development of benchmarks for European languages. Among\nother recommendations, we advocate for a higher language and culture\nsensitivity of evaluation methods.\n","authors":["Å pela Vintar","Taja Kuzman PungerÅ¡ek","Mojca Brglez","Nikola LjubeÅ¡iÄ"],"pdf_url":"https://arxiv.org/pdf/2510.24450v1.pdf","comment":"12 pages, 1 figure. Submitted to the LREC 2026 conference"},{"id":"http://arxiv.org/abs/2509.22699v2","updated":"2025-10-28T14:11:48Z","published":"2025-09-21T08:54:06Z","title":"Are you sure? Measuring models bias in content moderation through\n  uncertainty","summary":"  Automatic content moderation is crucial to ensuring safety in social media.\nLanguage Model-based classifiers are being increasingly adopted for this task,\nbut it has been shown that they perpetuate racial and social biases. Even if\nseveral resources and benchmark corpora have been developed to challenge this\nissue, measuring the fairness of models in content moderation remains an open\nissue. In this work, we present an unsupervised approach that benchmarks models\non the basis of their uncertainty in classifying messages annotated by people\nbelonging to vulnerable groups. We use uncertainty, computed by means of the\nconformal prediction technique, as a proxy to analyze the bias of 11 models\nagainst women and non-white annotators and observe to what extent it diverges\nfrom metrics based on performance, such as the $F_1$ score. The results show\nthat some pre-trained models predict with high accuracy the labels coming from\nminority groups, even if the confidence in their prediction is low. Therefore,\nby measuring the confidence of models, we are able to see which groups of\nannotators are better represented in pre-trained models and lead the debiasing\nprocess of these models before their effective use.\n","authors":["Alessandra Urbinati","Mirko Lai","Simona Frenda","Marco Antonio Stranisci"],"pdf_url":"https://arxiv.org/pdf/2509.22699v2.pdf","comment":"accepted at Findings of ACL: EMNLP 2025"},{"id":"http://arxiv.org/abs/2510.24446v1","updated":"2025-10-28T14:09:05Z","published":"2025-10-28T14:09:05Z","title":"SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box\n  Adversarial Paraphrasing in Text Autoencoder Latent Space","summary":"  Multimodal large language models (MLLMs) have shown impressive capabilities\nin vision-language tasks such as reasoning segmentation, where models generate\nsegmentation masks based on textual queries. While prior work has primarily\nfocused on perturbing image inputs, semantically equivalent textual\nparaphrases-crucial in real-world applications where users express the same\nintent in varied ways-remain underexplored. To address this gap, we introduce a\nnovel adversarial paraphrasing task: generating grammatically correct\nparaphrases that preserve the original query meaning while degrading\nsegmentation performance. To evaluate the quality of adversarial paraphrases,\nwe develop a comprehensive automatic evaluation protocol validated with human\nstudies. Furthermore, we introduce SPARTA-a black-box, sentence-level\noptimization method that operates in the low-dimensional semantic latent space\nof a text autoencoder, guided by reinforcement learning. SPARTA achieves\nsignificantly higher success rates, outperforming prior methods by up to 2x on\nboth the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive\nbaselines to assess the robustness of advanced reasoning segmentation models.\nWe reveal that they remain vulnerable to adversarial paraphrasing-even under\nstrict semantic and grammatical constraints. All code and data will be released\npublicly upon acceptance.\n","authors":["Viktoriia Zinkovich","Anton Antonov","Andrei Spiridonov","Denis Shepelev","Andrey Moskalenko","Daria Pugacheva","Elena Tutubalina","Andrey Kuznetsov","Vlad Shakhuro"],"pdf_url":"https://arxiv.org/pdf/2510.24446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24442v1","updated":"2025-10-28T14:07:10Z","published":"2025-10-28T14:07:10Z","title":"Law in Silico: Simulating Legal Society with LLM-Based Agents","summary":"  Since real-world legal experiments are often costly or infeasible, simulating\nlegal societies with Artificial Intelligence (AI) systems provides an effective\nalternative for verifying and developing legal theory, as well as supporting\nlegal administration. Large Language Models (LLMs), with their world knowledge\nand role-playing capabilities, are strong candidates to serve as the foundation\nfor legal society simulation. However, the application of LLMs to simulate\nlegal systems remains underexplored. In this work, we introduce Law in Silico,\nan LLM-based agent framework for simulating legal scenarios with individual\ndecision-making and institutional mechanisms of legislation, adjudication, and\nenforcement. Our experiments, which compare simulated crime rates with\nreal-world data, demonstrate that LLM-based agents can largely reproduce\nmacro-level crime trends and provide insights that align with real-world\nobservations. At the same time, micro-level simulations reveal that a\nwell-functioning, transparent, and adaptive legal system offers better\nprotection of the rights of vulnerable individuals.\n","authors":["Yiding Wang","Yuxuan Chen","Fanxu Meng","Xifan Chen","Xiaolei Yang","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.24442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24438v1","updated":"2025-10-28T14:05:55Z","published":"2025-10-28T14:05:55Z","title":"Can LLMs Write Faithfully? An Agent-Based Evaluation of LLM-generated\n  Islamic Content","summary":"  Large language models are increasingly used for Islamic guidance, but risk\nmisquoting texts, misapplying jurisprudence, or producing culturally\ninconsistent responses. We pilot an evaluation of GPT-4o, Ansari AI, and Fanar\non prompts from authentic Islamic blogs. Our dual-agent framework uses a\nquantitative agent for citation verification and six-dimensional scoring (e.g.,\nStructure, Islamic Consistency, Citations) and a qualitative agent for\nfive-dimensional side-by-side comparison (e.g., Tone, Depth, Originality).\nGPT-4o scored highest in Islamic Accuracy (3.93) and Citation (3.38), Ansari AI\nfollowed (3.68, 3.32), and Fanar lagged (2.76, 1.82). Despite relatively strong\nperformance, models still fall short in reliably producing accurate Islamic\ncontent and citations -- a paramount requirement in faith-sensitive writing.\nGPT-4o had the highest mean quantitative score (3.90/5), while Ansari AI led\nqualitative pairwise wins (116/200). Fanar, though trailing, introduces\ninnovations for Islamic and Arabic contexts. This study underscores the need\nfor community-driven benchmarks centering Muslim perspectives, offering an\nearly step toward more reliable AI in Islamic knowledge and other high-stakes\ndomains such as medicine, law, and journalism.\n","authors":["Abdullah Mushtaq","Rafay Naeem","Ezieddin Elmahjub","Ibrahim Ghaznavi","Shawqi Al-Maliki","Mohamed Abdallah","Ala Al-Fuqaha","Junaid Qadir"],"pdf_url":"https://arxiv.org/pdf/2510.24438v1.pdf","comment":"Accepted at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025) Workshop: 5th Muslims in Machine Learning (MusIML) Workshop"},{"id":"http://arxiv.org/abs/2510.24434v1","updated":"2025-10-28T14:02:55Z","published":"2025-10-28T14:02:55Z","title":"LuxIT: A Luxembourgish Instruction Tuning Dataset from Monolingual Seed\n  Data","summary":"  The effectiveness of instruction-tuned Large Language Models (LLMs) is often\nlimited in low-resource linguistic settings due to a lack of high-quality\ntraining data. We introduce LuxIT, a novel, monolingual instruction tuning\ndataset for Luxembourgish developed to mitigate this challenge. We synthesize\nthe dataset from a corpus of native Luxembourgish texts, utilizing\nDeepSeek-R1-0528, chosen for its shown proficiency in Luxembourgish. Following\ngeneration, we apply a quality assurance process, employing an LLM-as-a-judge\napproach. To investigate the practical utility of the dataset, we fine-tune\nseveral smaller-scale LLMs on LuxIT. Subsequent benchmarking against their base\nmodels on Luxembourgish language proficiency examinations, however, yields\nmixed results, with performance varying significantly across different models.\nLuxIT represents a critical contribution to Luxembourgish natural language\nprocessing and offers a replicable monolingual methodology, though our findings\nhighlight the need for further research to optimize its application.\n","authors":["Julian Valline","Cedric Lothritz","Jordi Cabot"],"pdf_url":"https://arxiv.org/pdf/2510.24434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.17281v2","updated":"2025-10-28T13:52:29Z","published":"2025-08-24T10:02:51Z","title":"From Language to Action: A Review of Large Language Models as Autonomous\n  Agents and Tool Users","summary":"  The pursuit of human-level artificial intelligence (AI) has significantly\nadvanced the development of autonomous agents and Large Language Models (LLMs).\nLLMs are now widely utilized as decision-making agents for their ability to\ninterpret instructions, manage sequential tasks, and adapt through feedback.\nThis review examines recent developments in employing LLMs as autonomous agents\nand tool users and comprises seven research questions. We only used the papers\npublished between 2023 and 2025 in conferences of the A* and A rank and Q1\njournals. A structured analysis of the LLM agents' architectural design\nprinciples, dividing their applications into single-agent and multi-agent\nsystems, and strategies for integrating external tools is presented. In\naddition, the cognitive mechanisms of LLM, including reasoning, planning, and\nmemory, and the impact of prompting methods and fine-tuning procedures on agent\nperformance are also investigated. Furthermore, we evaluated current benchmarks\nand assessment protocols and have provided an analysis of 68 publicly available\ndatasets to assess the performance of LLM-based agents in various tasks. In\nconducting this review, we have identified critical findings on verifiable\nreasoning of LLMs, the capacity for self-improvement, and the personalization\nof LLM-based agents. Finally, we have discussed ten future research directions\nto overcome these gaps.\n","authors":["Sadia Sultana Chowa","Riasad Alvi","Subhey Sadi Rahman","Md Abdur Rahman","Mohaimenul Azam Khan Raiaan","Md Rafiqul Islam","Mukhtar Hussain","Sami Azam"],"pdf_url":"https://arxiv.org/pdf/2508.17281v2.pdf","comment":"Submitted to Artificial Intelligence Review for peer review"},{"id":"http://arxiv.org/abs/2510.24427v1","updated":"2025-10-28T13:47:23Z","published":"2025-10-28T13:47:23Z","title":"SynthWorlds: Controlled Parallel Worlds for Disentangling Reasoning and\n  Knowledge in Language Models","summary":"  Evaluating the reasoning ability of language models (LMs) is complicated by\ntheir extensive parametric world knowledge, where benchmark performance often\nreflects factual recall rather than genuine reasoning. Existing datasets and\napproaches (e.g., temporal filtering, paraphrasing, adversarial substitution)\ncannot cleanly separate the two. We present SynthWorlds, a framework that\ndisentangles task reasoning complexity from factual knowledge. In SynthWorlds,\nwe construct parallel corpora representing two worlds with identical\ninterconnected structure: a real-mapped world, where models may exploit\nparametric knowledge, and a synthetic-mapped world, where such knowledge is\nmeaningless. On top of these corpora, we design two mirrored tasks as case\nstudies: multi-hop question answering and page navigation, which maintain equal\nreasoning difficulty across worlds. Experiments in parametric-only (e.g.,\nclosed-book QA) and knowledge-augmented (e.g., retrieval-augmented) LM settings\nreveal a persistent knowledge advantage gap, defined as the performance boost\nmodels gain from memorized parametric world knowledge. Knowledge acquisition\nand integration mechanisms reduce but do not eliminate this gap, highlighting\nopportunities for system improvements. Fully automatic and scalable,\nSynthWorlds provides a controlled environment for evaluating LMs in ways that\nwere previously challenging, enabling precise and testable comparisons of\nreasoning and memorization.\n","authors":["Ken Gu","Advait Bhat","Mike A Merrill","Robert West","Xin Liu","Daniel McDuff","Tim Althoff"],"pdf_url":"https://arxiv.org/pdf/2510.24427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24425v1","updated":"2025-10-28T13:46:48Z","published":"2025-10-28T13:46:48Z","title":"Comprehensive and Efficient Distillation for Lightweight Sentiment\n  Analysis Models","summary":"  Recent efforts leverage knowledge distillation techniques to develop\nlightweight and practical sentiment analysis models. These methods are grounded\nin human-written instructions and large-scale user texts. Despite the promising\nresults, two key challenges remain: (1) manually written instructions are\nlimited in diversity and quantity, making them insufficient to ensure\ncomprehensive coverage of distilled knowledge; (2) large-scale user texts incur\nhigh computational cost, hindering the practicality of these methods. To this\nend, we introduce COMPEFFDIST, a comprehensive and efficient distillation\nframework for sentiment analysis. Our framework consists of two key modules:\nattribute-based automatic instruction construction and difficulty-based data\nfiltering, which correspondingly tackle the aforementioned challenges. Applying\nour method across multiple model series (Llama-3, Qwen-3, and Gemma-3), we\nenable 3B student models to match the performance of 20x larger teacher models\non most tasks. In addition, our approach greatly outperforms baseline methods\nin data efficiency, attaining the same performance level with only 10% of the\ndata.\n","authors":["Guangyu Xie","Yice Zhang","Jianzhu Bao","Qianlong Wang","Yang Sun","Bingbing Wang","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2510.24425v1.pdf","comment":"Accepted by EMNLP 2025. 22 pages, 9 figures. The first two authors\n  contribute equally"},{"id":"http://arxiv.org/abs/2510.24411v1","updated":"2025-10-28T13:22:39Z","published":"2025-10-28T13:22:39Z","title":"OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid\n  Validation in Realistic Workflows","summary":"  Computer-using agents powered by Vision-Language Models (VLMs) have\ndemonstrated human-like capabilities in operating digital environments like\nmobile platforms. While these agents hold great promise for advancing digital\nautomation, their potential for unsafe operations, such as system compromise\nand privacy leakage, is raising significant concerns. Detecting these safety\nconcerns across the vast and complex operational space of mobile environments\npresents a formidable challenge that remains critically underexplored. To\nestablish a foundation for mobile agent safety research, we introduce\nMobileRisk-Live, a dynamic sandbox environment accompanied by a safety\ndetection benchmark comprising realistic trajectories with fine-grained\nannotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety\ndetection framework that synergistically combines a Formal Verifier for\ndetecting explicit system-level violations with a VLM-based Contextual Judge\nfor assessing contextual risks and agent actions. Experiments show that\nOS-Sentinel achieves 10%-30% improvements over existing approaches across\nmultiple metrics. Further analysis provides critical insights that foster the\ndevelopment of safer and more reliable autonomous mobile agents.\n","authors":["Qiushi Sun","Mukai Li","Zhoumianze Liu","Zhihui Xie","Fangzhi Xu","Zhangyue Yin","Kanzhi Cheng","Zehao Li","Zichen Ding","Qi Liu","Zhiyong Wu","Zhuosheng Zhang","Ben Kao","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2510.24411v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2510.19585v2","updated":"2025-10-28T13:04:38Z","published":"2025-10-22T13:37:52Z","title":"Detecting Latin in Historical Books with Large Language Models: A\n  Multimodal Benchmark","summary":"  This paper presents a novel task of extracting Latin fragments from\nmixed-language historical documents with varied layouts. We benchmark and\nevaluate the performance of large foundation models against a multimodal\ndataset of 724 annotated pages. The results demonstrate that reliable Latin\ndetection with contemporary models is achievable. Our study provides the first\ncomprehensive analysis of these models' capabilities and limits for this task.\n","authors":["Yu Wu","Ke Shu","Jonas Fischer","Lidia Pivovarova","David Rosson","Eetu MÃ¤kelÃ¤","Mikko Tolonen"],"pdf_url":"https://arxiv.org/pdf/2510.19585v2.pdf","comment":"Under review. Both the dataset and code will be published"},{"id":"http://arxiv.org/abs/2505.11842v3","updated":"2025-10-28T12:44:07Z","published":"2025-05-17T05:06:38Z","title":"Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs","summary":"  The increasing deployment of Large Vision-Language Models (LVLMs) raises\nsafety concerns under potential malicious inputs. However, existing multimodal\nsafety evaluations primarily focus on model vulnerabilities exposed by static\nimage inputs, ignoring the temporal dynamics of video that may induce distinct\nsafety risks. To bridge this gap, we introduce Video-SafetyBench, the first\ncomprehensive benchmark designed to evaluate the safety of LVLMs under\nvideo-text attacks. It comprises 2,264 video-text pairs spanning 48\nfine-grained unsafe categories, each pairing a synthesized video with either a\nharmful query, which contains explicit malice, or a benign query, which appears\nharmless but triggers harmful behavior when interpreted alongside the video. To\ngenerate semantically accurate videos for safety evaluation, we design a\ncontrollable pipeline that decomposes video semantics into subject images (what\nis shown) and motion text (how it moves), which jointly guide the synthesis of\nquery-relevant videos. To effectively evaluate uncertain or borderline harmful\noutputs, we propose RJScore, a novel LLM-based metric that incorporates the\nconfidence of judge models and human-aligned decision threshold calibration.\nExtensive experiments show that benign-query video composition achieves average\nattack success rates of 67.2%, revealing consistent vulnerabilities to\nvideo-induced attacks. We believe Video-SafetyBench will catalyze future\nresearch into video-based safety evaluation and defense strategies.\n","authors":["Xuannan Liu","Zekun Li","Zheqi He","Peipei Li","Shuhan Xia","Xing Cui","Huaibo Huang","Xi Yang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2505.11842v3.pdf","comment":"Accepted by NeurIPS 2025 Dataset and Benchmark Track, Project page:\n  https://liuxuannan.github.io/Video-SafetyBench.github.io/"},{"id":"http://arxiv.org/abs/2510.24365v1","updated":"2025-10-28T12:41:10Z","published":"2025-10-28T12:41:10Z","title":"Text Simplification with Sentence Embeddings","summary":"  Sentence embeddings can be decoded to give approximations of the original\ntexts used to create them. We explore this effect in the context of text\nsimplification, demonstrating that reconstructed text embeddings preserve\ncomplexity levels. We experiment with a small feed forward neural network to\neffectively learn a transformation between sentence embeddings representing\nhigh-complexity and low-complexity texts. We provide comparison to a Seq2Seq\nand LLM-based approach, showing encouraging results in our much smaller\nlearning setting. Finally, we demonstrate the applicability of our\ntransformation to an unseen simplification dataset (MedEASI), as well as\ndatasets from languages outside the training data (ES,DE). We conclude that\nlearning transformations in sentence embedding space is a promising direction\nfor future research and has potential to unlock the ability to develop small,\nbut powerful models for text simplification and other natural language\ngeneration tasks.\n","authors":["Matthew Shardlow"],"pdf_url":"https://arxiv.org/pdf/2510.24365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07883v2","updated":"2025-10-28T12:30:22Z","published":"2024-05-13T16:17:10Z","title":"Zero-Shot Tokenizer Transfer","summary":"  Language models (LMs) are bound to their tokenizer, which maps raw text to a\nsequence of vocabulary items (tokens). This restricts their flexibility: for\nexample, LMs trained primarily on English may still perform well in other\nnatural and programming languages, but have vastly decreased efficiency due to\ntheir English-centric tokenizer. To mitigate this, we should be able to swap\nthe original LM tokenizer with an arbitrary one, on the fly, without degrading\nperformance. Hence, in this work we define a new problem: Zero-Shot Tokenizer\nTransfer (ZeTT). The challenge at the core of ZeTT is finding embeddings for\nthe tokens in the vocabulary of the new tokenizer. Since prior heuristics for\ninitializing embeddings often perform at chance level in a ZeTT setting, we\npropose a new solution: we train a hypernetwork taking a tokenizer as input and\npredicting the corresponding embeddings. We empirically demonstrate that the\nhypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and\ndecoder LLMs (e.g., Mistral-7B). Our method comes close to the original models'\nperformance in cross-lingual and coding tasks while markedly reducing the\nlength of the tokenized sequence. We also find that the remaining gap can be\nquickly closed by continued training on less than 1B tokens. Finally, we show\nthat a ZeTT hypernetwork trained for a base (L)LM can also be applied to\nfine-tuned variants without extra training. Overall, our results make\nsubstantial strides toward detaching LMs from their tokenizer.\n","authors":["Benjamin Minixhofer","Edoardo Maria Ponti","Ivan VuliÄ"],"pdf_url":"https://arxiv.org/pdf/2405.07883v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2510.24358v1","updated":"2025-10-28T12:26:45Z","published":"2025-10-28T12:26:45Z","title":"Automatically Benchmarking LLM Code Agents through Agent-Driven\n  Annotation and Evaluation","summary":"  Recent advances in code agents have enabled automated software development at\nthe project level, supported by large language models (LLMs) and widely adopted\ntools. However, existing benchmarks for code agent evaluation face two major\nlimitations: high annotation cost and expertise requirements, and rigid\nevaluation metrics that rely primarily on unit tests. To address these\nchallenges, we propose an agent-driven benchmark construction pipeline that\nleverages human supervision to efficiently generate diverse and challenging\nproject-level tasks. Based on this approach, we introduce PRDBench, a novel\nbenchmark comprising 50 real-world Python projects across 20 domains, each with\nstructured Product Requirement Document (PRD) requirements, comprehensive\nevaluation criteria, and reference implementations. PRDBench features rich data\nsources, high task complexity, and flexible metrics. We further employ an\nAgent-as-a-Judge paradigm to score agent outputs, enabling the evaluation of\nvarious test types beyond unit tests. Extensive experiments on PRDBench\ndemonstrate its effectiveness in assessing the capabilities of both code agents\nand evaluation agents, providing a scalable and robust framework for annotation\nand evaluation.\n","authors":["Lingyue Fu","Bolun Zhang","Hao Guan","Yaoming Zhu","Lin Qiu","Weiwen Liu","Xuezhi Cao","Xunliang Cai","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2510.24358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24345v1","updated":"2025-10-28T12:11:12Z","published":"2025-10-28T12:11:12Z","title":"LongWeave: A Long-Form Generation Benchmark Bridging Real-World\n  Relevance and Verifiability","summary":"  Generating long, informative, and factual outputs remains a major challenge\nfor Large Language Models (LLMs). Existing benchmarks for long-form generation\ntypically assess real-world queries with hard-to-verify metrics or use\nsynthetic setups that ease evaluation but overlook real-world intricacies. In\nthis paper, we introduce \\textbf{LongWeave}, which balances real-world and\nverifiable assessment with Constraint-Verifier Evaluation (CoV-Eval). CoV-Eval\nconstructs tasks by first defining verifiable targets within real-world\nscenarios, then systematically generating corresponding queries, textual\nmaterials, and constraints based on these targets. This ensures that tasks are\nboth realistic and objectively assessable, enabling rigorous assessment of\nmodel capabilities in meeting complex real-world constraints. LongWeave\nsupports customizable input/output lengths (up to 64K/8K tokens) across seven\ndistinct tasks. Evaluation on 23 LLMs shows that even state-of-the-art models\nencounter significant challenges in long-form generation as real-world\ncomplexity and output length increase.\n","authors":["Zikai Xiao","Fei Huang","Jianhong Tu","Jianhui Wei","Wen Ma","Yuxuan Zhou","Jian Wu","Bowen Yu","Zuozhu Liu","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2510.24345v1.pdf","comment":"EMNLP Findings 2025"},{"id":"http://arxiv.org/abs/2412.15189v2","updated":"2025-10-28T12:02:14Z","published":"2024-12-19T18:57:11Z","title":"Face the Facts! Evaluating RAG-based Fact-checking Pipelines in\n  Realistic Settings","summary":"  Natural Language Processing and Generation systems have recently shown the\npotential to complement and streamline the costly and time-consuming job of\nprofessional fact-checkers. In this work, we lift several constraints of\ncurrent state-of-the-art pipelines for automated fact-checking based on the\nRetrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark, under\nmore realistic scenarios, RAG-based methods for the generation of verdicts -\ni.e., short texts discussing the veracity of a claim - evaluating them on\nstylistically complex claims and heterogeneous, yet reliable, knowledge bases.\nOur findings show a complex landscape, where, for example, LLM-based retrievers\noutperform other retrieval techniques, though they still struggle with\nheterogeneous knowledge bases; larger models excel in verdict faithfulness,\nwhile smaller models provide better context adherence, with human evaluations\nfavouring zero-shot and one-shot approaches for informativeness, and fine-tuned\nmodels for emotional alignment.\n","authors":["Daniel Russo","Stefano Menini","Jacopo Staiano","Marco Guerini"],"pdf_url":"https://arxiv.org/pdf/2412.15189v2.pdf","comment":"Code and data at https://github.com/drusso98/face-the-facts -\n  Accepted for publication at INLG 2025"},{"id":"http://arxiv.org/abs/2411.19477v5","updated":"2025-10-28T11:59:43Z","published":"2024-11-29T05:29:47Z","title":"Provable Scaling Laws for the Test-Time Compute of Large Language Models","summary":"  We propose two simple, principled and practical algorithms that enjoy\nprovable scaling laws for the test-time compute of large language models\n(LLMs). The first one is a two-stage knockout-style algorithm: given an input\nproblem, it first generates multiple candidate solutions, and then aggregate\nthem via a knockout tournament for the final output. Assuming that the LLM can\ngenerate a correct solution with non-zero probability and do better than a\nrandom guess in comparing a pair of correct and incorrect solutions, we prove\ntheoretically that the failure probability of this algorithm decays to zero\nexponentially or by a power law (depending on the specific way of scaling) as\nits test-time compute grows. The second one is a two-stage league-style\nalgorithm, where each candidate is evaluated by its average win rate against\nmultiple opponents, rather than eliminated upon loss to a single opponent.\nUnder analogous but more robust assumptions, we prove that its failure\nprobability also decays to zero exponentially with more test-time compute. Both\nalgorithms require a black-box LLM and nothing else (e.g., no verifier or\nreward model) for a minimalistic implementation, which makes them appealing for\npractical applications and easy to adapt for different tasks. Through extensive\nexperiments with diverse models and datasets, we validate the proposed theories\nand demonstrate the outstanding scaling properties of both algorithms.\n","authors":["Yanxi Chen","Xuchen Pan","Yaliang Li","Bolin Ding","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.19477v5.pdf","comment":"NeurIPS 2025 camera-ready version"},{"id":"http://arxiv.org/abs/2510.24328v1","updated":"2025-10-28T11:52:51Z","published":"2025-10-28T11:52:51Z","title":"Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect\n  Variants","summary":"  Large Language Models (LLMs) are increasingly used to answer everyday\nquestions, yet their performance on culturally grounded and dialectal content\nremains uneven across languages. We propose a comprehensive method that (i)\ntranslates Modern Standard Arabic (MSA) multiple-choice questions (MCQs) into\nEnglish and several Arabic dialects, (ii) converts them into open-ended\nquestions (OEQs), (iii) benchmarks a range of zero-shot and fine-tuned LLMs\nunder both MCQ and OEQ settings, and (iv) generates chain-of-thought (CoT)\nrationales to fine-tune models for step-by-step reasoning. Using this method,\nwe extend an existing dataset in which QAs are parallelly aligned across\nmultiple language varieties, making it, to our knowledge, the first of its\nkind. We conduct extensive experiments with both open and closed models. Our\nfindings show that (i) models underperform on Arabic dialects, revealing\npersistent gaps in culturally grounded and dialect-specific knowledge; (ii)\nArabic-centric models perform well on MCQs but struggle with OEQs; and (iii)\nCoT improves judged correctness while yielding mixed n-gram-based metrics. The\ndeveloped dataset will be publicly released to support further research on\nculturally and linguistically inclusive evaluation.\n","authors":["Hunzalah Hassan Bhatti","Firoj Alam"],"pdf_url":"https://arxiv.org/pdf/2510.24328v1.pdf","comment":"Cultural Knowledge, Everyday Knowledge, Open-Ended Question,\n  Chain-of-Thought, Large Language Models, Native, Multilingual, Language\n  Diversity"},{"id":"http://arxiv.org/abs/2510.24320v1","updated":"2025-10-28T11:37:01Z","published":"2025-10-28T11:37:01Z","title":"Critique-RL: Training Language Models for Critiquing through Two-Stage\n  Reinforcement Learning","summary":"  Training critiquing language models to assess and provide feedback on model\noutputs is a promising way to improve LLMs for complex reasoning tasks.\nHowever, existing approaches typically rely on stronger supervisors for\nannotating critique data. To address this, we propose Critique-RL, an online RL\napproach for developing critiquing language models without stronger\nsupervision. Our approach operates on a two-player paradigm: the actor\ngenerates a response, the critic provides feedback, and the actor refines the\nresponse accordingly. We first reveal that relying solely on indirect reward\nsignals from the actor's outputs for RL optimization often leads to\nunsatisfactory critics: while their helpfulness (i.e., providing constructive\nfeedback) improves, the discriminability (i.e., determining whether a response\nis high-quality or not) remains poor, resulting in marginal performance gains.\nTo overcome this, Critique-RL adopts a two-stage optimization strategy. In\nstage I, it reinforces the discriminability of the critic with direct\nrule-based reward signals; in stage II, it introduces indirect rewards based on\nactor refinement to improve the critic's helpfulness, while maintaining its\ndiscriminability via appropriate regularization. Extensive experiments across\nvarious tasks and models show that Critique-RL delivers substantial performance\nimprovements. For example, it achieves a 9.02% gain on in-domain tasks and a\n5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.\n","authors":["Zhiheng Xi","Jixuan Huang","Xin Guo","Boyang Hong","Dingwen Yang","Xiaoran Fan","Shuo Li","Zehui Chen","Junjie Ye","Siyu Yuan","Zhengyin Du","Xuesong Yao","Yufei Xu","Jiecao Chen","Rui Zheng","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2510.24320v1.pdf","comment":"Preprint, 25 pages, 9 figures. Code:\n  https://github.com/WooooDyy/Critique-RL"},{"id":"http://arxiv.org/abs/2504.11364v4","updated":"2025-10-28T11:36:51Z","published":"2025-04-15T16:30:02Z","title":"Offline Learning and Forgetting for Reasoning with Large Language Models","summary":"  Leveraging inference-time search in large language models has proven\neffective in further enhancing a trained model's capability to solve complex\nmathematical and reasoning problems. However, this approach significantly\nincreases computational costs and inference time, as the model must generate\nand evaluate multiple candidate solutions to identify a viable reasoning path.\nTo address this, we propose an effective approach that integrates search\ncapabilities directly into the model by fine-tuning it on unpaired successful\n(learning) and failed reasoning paths (forgetting) derived from diverse search\nmethods. A key challenge we identify is that naive fine-tuning can degrade the\nmodel's search capability; we show this can be mitigated with a smaller\nlearning rate. Extensive experiments on the challenging Game-of-24 and\nCountdown arithmetic puzzles show that, replacing CoT-generated data with\nsearch-generated data for offline fine-tuning improves success rates by around\n23% over inference-time search baselines, while reducing inference time by\n180$\\times$. On top of this, our learning and forgetting objective consistently\noutperforms both supervised fine-tuning and preference-based methods.\n","authors":["Tianwei Ni","Allen Nie","Sapana Chaudhary","Yao Liu","Huzefa Rangwala","Rasool Fakoor"],"pdf_url":"https://arxiv.org/pdf/2504.11364v4.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR), 2025.\n  Code: https://github.com/twni2016/llm-reasoning-uft"},{"id":"http://arxiv.org/abs/2510.24302v1","updated":"2025-10-28T11:12:02Z","published":"2025-10-28T11:12:02Z","title":"Lookahead Tree-Based Rollouts for Enhanced Trajectory-Level Exploration\n  in Reinforcement Learning with Verifiable Rewards","summary":"  Reinforcement Learning with Verifiable Rewards (RLVR), particularly with\nalgorithms like Group Relative Policy Optimization (GRPO), has proven highly\neffective in enhancing the reasoning capabilities of large language models.\nHowever, a critical bottleneck in current pipelines lies in the limited\ndiversity of sampled trajectories during group rollouts. Homogeneous\ntrajectories and their associated rewards would diminish the return signals for\npolicy updates, thereby hindering effective policy learning. This lack of\ndiversity stems primarily from token-level stochastic sampling, where local\nvariations are likely to collapse into near-identical reasoning paths. To\naddress this limitation, we propose Lookahead Tree-Based Rollouts (LATR), a\nnovel rollout strategy designed to explicitly promotes trajectory-level\ndiversity by enforcing branching into different candidate tokens likely to\nyield distinct continuations. Specifically, LATR iteratively operates in three\nstages: (1) branching at high-uncertainty generation steps, (2) performing\nlookahead simulation for each new branch, and (3) pruning branches that\nexhibits prolonged similarity during simulation. Compared with stochastic\nSampling, LATR accelerates policy learning by 131% on average and improves\nfinal pass@1 performance by 4.2% on both GRPO and Dynamic sAmpling Policy\nOptimization (DAPO) algorithms across different reasoning tasks. Our code and\ndata are publicly available at https://github.com/starreeze/latr.\n","authors":["Shangyu Xing","Siyuan Wang","Chenyuan Yang","Xinyu Dai","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2510.24302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24295v1","updated":"2025-10-28T10:58:59Z","published":"2025-10-28T10:58:59Z","title":"MERGE: Minimal Expression-Replacement GEneralization Test for Natural\n  Language Inference","summary":"  In recent years, many generalization benchmarks have shown language models'\nlack of robustness in natural language inference (NLI). However, manually\ncreating new benchmarks is costly, while automatically generating high-quality\nones, even by modifying existing benchmarks, is extremely difficult. In this\npaper, we propose a methodology for automatically generating high-quality\nvariants of original NLI problems by replacing open-class words, while\ncrucially preserving their underlying reasoning. We dub our generalization test\nas MERGE (Minimal Expression-Replacements GEneralization), which evaluates the\ncorrectness of models' predictions across reasoning-preserving variants of the\noriginal problem. Our results show that NLI models' perform 4-20% worse on\nvariants, suggesting low generalizability even on such minimally altered\nproblems. We also analyse how word class of the replacements, word probability,\nand plausibility influence NLI models' performance.\n","authors":["MÄdÄlina ZgreabÄn","Tejaswini Deoskar","Lasha Abzianidze"],"pdf_url":"https://arxiv.org/pdf/2510.24295v1.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2506.13771v2","updated":"2025-10-28T10:57:14Z","published":"2025-05-30T06:43:03Z","title":"LittleBit: Ultra Low-Bit Quantization via Latent Factorization","summary":"  Deploying large language models (LLMs) often faces challenges from\nsubstantial memory and computational costs. Quantization offers a solution, yet\nperformance degradation in the sub-1-bit regime remains particularly difficult.\nThis paper introduces LittleBit, a novel method for extreme LLM compression. It\ntargets levels like 0.1 bits per weight (BPW), achieving nearly 31$\\times$\nmemory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents\nweights in a low-rank form using latent matrix factorization, subsequently\nbinarizing these factors. To counteract information loss from this extreme\nprecision, it integrates a multi-scale compensation mechanism. This includes\nrow, column, and an additional latent dimension that learns per-rank\nimportance. Two key contributions enable effective training: Dual\nSign-Value-Independent Decomposition (Dual-SVID) for quantization-aware\ntraining (QAT) initialization, and integrated Residual Compensation to mitigate\nerrors. Extensive experiments confirm LittleBit's superiority in sub-1-bit\nquantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading\nmethod's 0.7 BPW. LittleBit establishes a new, viable size-performance\ntrade-off--unlocking a potential 11.6$\\times$ speedup over FP16 at the kernel\nlevel--and makes powerful LLMs practical for resource-constrained environments.\n","authors":["Banseok Lee","Dongkyu Kim","Youngcheon You","Youngmin Kim"],"pdf_url":"https://arxiv.org/pdf/2506.13771v2.pdf","comment":"Accepted to NeurIPS 2025. Banseok Lee and Dongkyu Kim contributed\n  equally"},{"id":"http://arxiv.org/abs/2510.24285v1","updated":"2025-10-28T10:42:57Z","published":"2025-10-28T10:42:57Z","title":"ViPER: Empowering the Self-Evolution of Visual Perception Abilities in\n  Vision-Language Model","summary":"  The limited capacity for fine-grained visual perception presents a critical\nbottleneck for Vision-Language Models (VLMs) in real-world applications.\nAddressing this is challenging due to the scarcity of high-quality data and the\nlimitations of existing methods: supervised fine-tuning (SFT) often compromises\ngeneral capabilities, while reinforcement fine-tuning (RFT) prioritizes textual\nreasoning over visual perception. To bridge this gap, we propose a novel\ntwo-stage task that structures visual perception learning as a coarse-to-fine\nprogressive process. Based on this task formulation, we develop ViPER, a\nself-bootstrapping framework specifically designed to enable iterative\nevolution through self-critiquing and self-prediction. By synergistically\nintegrating image-level and instance-level reconstruction with a two-stage\nreinforcement learning strategy, ViPER establishes a closed-loop training\nparadigm, where internally synthesized data directly fuel the enhancement of\nperceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the\nQwen-Viper series. With an average gain of 1.7% on seven comprehensive\nbenchmarks spanning various tasks and up to 6.0% on fine-grained perception,\nQwen-Viper consistently demonstrates superior performance across different\nvision-language scenarios while maintaining generalizability. Beyond enabling\nself-improvement in perceptual capabilities, ViPER provides concrete evidence\nfor the reciprocal relationship between generation and understanding, a\nbreakthrough to developing more autonomous and capable VLMs.\n","authors":["Juntian Zhang","Song Jin","Chuanqi Cheng","Yuhan Liu","Yankai Lin","Xun Zhang","Yufei Zhang","Fei Jiang","Guojun Yin","Wei Lin","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2510.24285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24259v1","updated":"2025-10-28T10:13:43Z","published":"2025-10-28T10:13:43Z","title":"Can LLMs Translate Human Instructions into a Reinforcement Learning\n  Agent's Internal Emergent Symbolic Representation?","summary":"  Emergent symbolic representations are critical for enabling developmental\nlearning agents to plan and generalize across tasks. In this work, we\ninvestigate whether large language models (LLMs) can translate human natural\nlanguage instructions into the internal symbolic representations that emerge\nduring hierarchical reinforcement learning. We apply a structured evaluation\nframework to measure the translation performance of commonly seen LLMs -- GPT,\nClaude, Deepseek and Grok -- across different internal symbolic partitions\ngenerated by a hierarchical reinforcement learning algorithm in the Ant Maze\nand Ant Fall environments. Our findings reveal that although LLMs demonstrate\nsome ability to translate natural language into a symbolic representation of\nthe environment dynamics, their performance is highly sensitive to partition\ngranularity and task complexity. The results expose limitations in current LLMs\ncapacity for representation alignment, highlighting the need for further\nresearch on robust alignment between language and internal agent\nrepresentations.\n","authors":["Ziqi Ma","Sao Mai Nguyen","Philippe Xu"],"pdf_url":"https://arxiv.org/pdf/2510.24259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24256v1","updated":"2025-10-28T10:09:35Z","published":"2025-10-28T10:09:35Z","title":"From Memorization to Reasoning in the Spectrum of Loss Curvature","summary":"  We characterize how memorization is represented in transformer models and\nshow that it can be disentangled in the weights of both language models (LMs)\nand vision transformers (ViTs) using a decomposition based on the loss\nlandscape curvature. This insight is based on prior theoretical and empirical\nwork showing that the curvature for memorized training points is much sharper\nthan non memorized, meaning ordering weight components from high to low\ncurvature can reveal a distinction without explicit labels. This motivates a\nweight editing procedure that suppresses far more recitation of untargeted\nmemorized data more effectively than a recent unlearning method\n(BalancedSubnet), while maintaining lower perplexity. Since the basis of\ncurvature has a natural interpretation for shared structure in model weights,\nwe analyze the editing procedure extensively on its effect on downstream tasks\nin LMs, and find that fact retrieval and arithmetic are specifically and\nconsistently negatively affected, even though open book fact retrieval and\ngeneral logical reasoning is conserved. We posit these tasks rely heavily on\nspecialized directions in weight space rather than general purpose mechanisms,\nregardless of whether those individual datapoints are memorized. We support\nthis by showing a correspondence between task data's activation strength with\nlow curvature components that we edit out, and the drop in task performance\nafter the edit. Our work enhances the understanding of memorization in neural\nnetworks with practical applications towards removing it, and provides evidence\nfor idiosyncratic, narrowly-used structures involved in solving tasks like math\nand fact retrieval.\n","authors":["Jack Merullo","Srihita Vatsavaya","Lucius Bushnaq","Owen Lewis"],"pdf_url":"https://arxiv.org/pdf/2510.24256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24250v1","updated":"2025-10-28T10:00:52Z","published":"2025-10-28T10:00:52Z","title":"Evaluating LLMs on Generating Age-Appropriate Child-Like Conversations","summary":"  Large Language Models (LLMs), predominantly trained on adult conversational\ndata, face significant challenges when generating authentic, child-like\ndialogue for specialized applications. We present a comparative study\nevaluating five different LLMs (GPT-4, RUTER-LLAMA-2-13b, GPTSW, NorMistral-7b,\nand NorBloom-7b) to generate age-appropriate Norwegian conversations for\nchildren aged 5 and 9 years. Through a blind evaluation by eleven education\nprofessionals using both real child interview data and LLM-generated text\nsamples, we assessed authenticity and developmental appropriateness. Our\nresults show that evaluators achieved strong inter-rater reliability (ICC=0.75)\nand demonstrated higher accuracy in age prediction for younger children\n(5-year-olds) compared to older children (9-year-olds). While GPT-4 and\nNorBloom-7b performed relatively well, most models generated language perceived\nas more linguistically advanced than the target age groups. These findings\nhighlight critical data-related challenges in developing LLM systems for\nspecialized applications involving children, particularly in low-resource\nlanguages where comprehensive age-appropriate lexical resources are scarce.\n","authors":["Syed Zohaib Hassan","PÃ¥l Halvorsen","Miriam S. Johnson","Pierre Lison"],"pdf_url":"https://arxiv.org/pdf/2510.24250v1.pdf","comment":"11 pages excluding references and appendix. 3 figures and 6 tables"},{"id":"http://arxiv.org/abs/2510.24247v1","updated":"2025-10-28T09:58:18Z","published":"2025-10-28T09:58:18Z","title":"Abjad AI at NADI 2025: CATT-Whisper: Multimodal Diacritic Restoration\n  Using Text and Speech Representations","summary":"  In this work, we tackle the Diacritic Restoration (DR) task for Arabic\ndialectal sentences using a multimodal approach that combines both textual and\nspeech information. We propose a model that represents the text modality using\nan encoder extracted from our own pre-trained model named CATT. The speech\ncomponent is handled by the encoder module of the OpenAI Whisper base model.\nOur solution is designed following two integration strategies. The former\nconsists of fusing the speech tokens with the input at an early stage, where\nthe 1500 frames of the audio segment are averaged over 10 consecutive frames,\nresulting in 150 speech tokens. To ensure embedding compatibility, these\naveraged tokens are processed through a linear projection layer prior to\nmerging them with the text tokens. Contextual encoding is guaranteed by the\nCATT encoder module. The latter strategy relies on cross-attention, where text\nand speech embeddings are fused. The cross-attention output is then fed to the\nCATT classification head for token-level diacritic prediction. To further\nimprove model robustness, we randomly deactivate the speech input during\ntraining, allowing the model to perform well with or without speech. Our\nexperiments show that the proposed approach achieves a word error rate (WER) of\n0.25 and a character error rate (CER) of 0.9 on the development set. On the\ntest set, our model achieved WER and CER scores of 0.55 and 0.13, respectively.\n","authors":["Ahmad Ghannam","Naif Alharthi","Faris Alasmary","Kholood Al Tabash","Shouq Sadah","Lahouari Ghouti"],"pdf_url":"https://arxiv.org/pdf/2510.24247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24236v1","updated":"2025-10-28T09:43:49Z","published":"2025-10-28T09:43:49Z","title":"Towards Transparent Reasoning: What Drives Faithfulness in Large\n  Language Models?","summary":"  Large Language Models (LLMs) often produce explanations that do not\nfaithfully reflect the factors driving their predictions. In healthcare\nsettings, such unfaithfulness is especially problematic: explanations that omit\nsalient clinical cues or mask spurious shortcuts can undermine clinician trust\nand lead to unsafe decision support. We study how inference and training-time\nchoices shape explanation faithfulness, focusing on factors practitioners can\ncontrol at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA\n8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions),\nand manipulate the number and type of few-shot examples, prompting strategies,\nand training procedure. Our results show: (i) both the quantity and quality of\nfew-shot examples significantly impact model faithfulness; (ii) faithfulness is\nsensitive to prompting design; (iii) the instruction-tuning phase improves\nmeasured faithfulness on MedQA. These findings offer insights into strategies\nfor enhancing the interpretability and trustworthiness of LLMs in sensitive\ndomains.\n","authors":["Teague McMillan","Gabriele Dominici","Martin Gjoreski","Marc Langheinrich"],"pdf_url":"https://arxiv.org/pdf/2510.24236v1.pdf","comment":"39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: NeurIPS 2025 Workshop on Evaluating the Evolving LLM\n  Lifecycle: Benchmarks, Emergent Abilities, and Scaling"},{"id":"http://arxiv.org/abs/2504.06560v4","updated":"2025-10-28T09:42:41Z","published":"2025-04-09T03:46:56Z","title":"NeedleInATable: Exploring Long-Context Capability of Large Language\n  Models towards Long-Structured Tables","summary":"  Processing structured tabular data, particularly large and lengthy tables,\nconstitutes a fundamental yet challenging task for large language models\n(LLMs). However, existing long-context benchmarks like Needle-in-a-Haystack\nprimarily focus on unstructured text, neglecting the challenge of diverse\nstructured tables. Meanwhile, previous tabular benchmarks mainly consider\ndownstream tasks that require high-level reasoning abilities, and overlook\nmodels' underlying fine-grained perception of individual table cells, which is\ncrucial for practical and robust LLM-based table applications. To address this\ngap, we introduce \\textsc{NeedleInATable} (NIAT), a new long-context tabular\nbenchmark that treats each table cell as a ``needle'' and requires models to\nextract the target cell based on cell locations or lookup questions. Our\ncomprehensive evaluation of various LLMs and multimodal LLMs reveals a\nsubstantial performance gap between popular downstream tabular tasks and the\nsimpler NIAT task, suggesting that they may rely on dataset-specific\ncorrelations or shortcuts to obtain better benchmark results but lack truly\nrobust long-context understanding towards structured tables. Furthermore, we\ndemonstrate that using synthesized NIAT training data can effectively improve\nperformance on both NIAT task and downstream tabular tasks, which validates the\nimportance of NIAT capability for LLMs' genuine table understanding ability.\n","authors":["Lanrui Wang","Mingyu Zheng","Hongyin Tang","Zheng Lin","Yanan Cao","Jingang Wang","Xunliang Cai","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2504.06560v4.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24222v1","updated":"2025-10-28T09:34:31Z","published":"2025-10-28T09:34:31Z","title":"HACK: Hallucinations Along Certainty and Knowledge Axes","summary":"  Hallucinations in LLMs present a critical barrier to their reliable usage.\nExisting research usually categorizes hallucination by their external\nproperties rather than by the LLMs' underlying internal properties. This\nexternal focus overlooks that hallucinations may require tailored mitigation\nstrategies based on their underlying mechanism. We propose a framework for\ncategorizing hallucinations along two axes: knowledge and certainty. Since\nparametric knowledge and certainty may vary across models, our categorization\nmethod involves a model-specific dataset construction process that\ndifferentiates between those types of hallucinations. Along the knowledge axis,\nwe distinguish between hallucinations caused by a lack of knowledge and those\noccurring despite the model having the knowledge of the correct response. To\nvalidate our framework along the knowledge axis, we apply steering mitigation,\nwhich relies on the existence of parametric knowledge to manipulate model\nactivations. This addresses the lack of existing methods to validate knowledge\ncategorization by showing a significant difference between the two\nhallucination types. We further analyze the distinct knowledge and\nhallucination patterns between models, showing that different hallucinations do\noccur despite shared parametric knowledge. Turning to the certainty axis, we\nidentify a particularly concerning subset of hallucinations where models\nhallucinate with certainty despite having the correct knowledge internally. We\nintroduce a new evaluation metric to measure the effectiveness of mitigation\nmethods on this subset, revealing that while some methods perform well on\naverage, they fail disproportionately on these critical cases. Our findings\nhighlight the importance of considering both knowledge and certainty in\nhallucination analysis and call for targeted mitigation approaches that\nconsider the hallucination underlying factors.\n","authors":["Adi Simhi","Jonathan Herzig","Itay Itzhak","Dana Arad","Zorik Gekhman","Roi Reichart","Fazl Barez","Gabriel Stanovsky","Idan Szpektor","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2510.24222v1.pdf","comment":"The code is available at\n  https://github.com/technion-cs-nlp/HACK_Hallucinations_Along_Certainty_and_Knowledge_axes"},{"id":"http://arxiv.org/abs/2510.24208v1","updated":"2025-10-28T09:25:40Z","published":"2025-10-28T09:25:40Z","title":"Beyond Neural Incompatibility: Easing Cross-Scale Knowledge Transfer in\n  Large Language Models through Latent Semantic Alignment","summary":"  Large Language Models (LLMs) encode vast amounts of knowledge in their\nmassive parameters, which is accessible to locate, trace, and analyze. Despite\nadvances in neural interpretability, it is still not clear how to transfer\nknowledge in a fine-grained manner, namely parametric knowledge transfer (PKT).\nA key problem is enabling effective and efficient knowledge transfer across\nLLMs of different scales, which is essential for achieving greater flexibility\nand broader applicability in transferring knowledge between LLMs. Due to neural\nincompatibility, referring to the architectural and parametric differences\nbetween LLMs of varying scales, existing methods that directly reuse layer\nparameters are severely limited. In this paper, we identify the semantic\nalignment in latent space as the fundamental prerequisite for LLM cross-scale\nknowledge transfer. Instead of directly using the layer parameters, our\napproach takes activations as the medium of layer-wise knowledge transfer.\nLeveraging the semantics in latent space, our approach is simple and\noutperforms prior work, better aligning model behaviors across varying scales.\nEvaluations on four benchmarks demonstrate the efficacy of our method. Further\nanalysis reveals the key factors easing cross-scale knowledge transfer and\nprovides insights into the nature of latent semantic alignment.\n","authors":["Jian Gu","Aldeida Aleti","Chunyang Chen","Hongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.24208v1.pdf","comment":"an early-stage version"},{"id":"http://arxiv.org/abs/2506.09349v3","updated":"2025-10-28T09:04:11Z","published":"2025-06-11T02:57:22Z","title":"DrVoice: Parallel Speech-Text Voice Conversation Model via\n  Dual-Resolution Speech Representations","summary":"  Recent studies on end-to-end (E2E) speech generation with large language\nmodels (LLMs) have attracted significant community attention, with multiple\nworks extending text-based LLMs to generate discrete speech tokens. Existing\nE2E approaches primarily fall into two categories: (1) Methods that generate\ndiscrete speech tokens independently without incorporating them into the LLM's\nautoregressive process, resulting in text generation being unaware of\nconcurrent speech synthesis. (2) Models that generate interleaved or parallel\nspeech-text tokens through joint autoregressive modeling, enabling mutual\nmodality awareness during generation. This paper presents DrVoice, a parallel\nspeech-text voice conversation model based on joint autoregressive modeling,\nfeaturing dual-resolution speech representations. Notably, while current\nmethods utilize mainly 12.5Hz input audio representation, our proposed\ndual-resolution mechanism reduces the input frequency for the LLM to 5Hz,\nsignificantly reducing computational cost and alleviating the frequency\ndiscrepancy between speech and text tokens and in turn better exploiting LLMs'\ncapabilities. Experimental results demonstrate that DRVOICE-7B establishes new\nstate-of-the-art (SOTA) on OpenAudioBench and Big Bench Audio benchmarks, while\nachieving performance comparable to the SOTA on VoiceBench and UltraEval-Audio\nbenchmarks, making it a leading open-source speech foundation model in ~7B\nmodels.\n","authors":["Chao-Hong Tan","Qian Chen","Wen Wang","Chong Deng","Qinglin Zhang","Luyao Cheng","Hai Yu","Xin Zhang","Xiang Lv","Tianyu Zhao","Chong Zhang","Yukun Ma","Yafeng Chen","Hui Wang","Jiaqing Liu","Xiangang Li","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2506.09349v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2404.17401v2","updated":"2025-10-28T08:44:48Z","published":"2024-04-26T13:22:28Z","title":"Evaluation of Geographical Distortions in Language Models","summary":"  Language models now constitute essential tools for improving efficiency for\nmany professional tasks such as writing, coding, or learning. For this reason,\nit is imperative to identify inherent biases. In the field of Natural Language\nProcessing, five sources of bias are well-identified: data, annotation,\nrepresentation, models, and research design. This study focuses on biases\nrelated to geographical knowledge. We explore the connection between geography\nand language models by highlighting their tendency to misrepresent spatial\ninformation, thus leading to distortions in the representation of geographical\ndistances. This study introduces four indicators to assess these distortions,\nby comparing geographical and semantic distances. Experiments are conducted\nfrom these four indicators with ten widely used language models. Results\nunderscore the critical necessity of inspecting and rectifying spatial biases\nin language models to ensure accurate and equitable representations.\n","authors":["RÃ©my Decoupes","Roberto Interdonato","Mathieu Roche","Maguelonne Teisseire","Sarah Valentin"],"pdf_url":"https://arxiv.org/pdf/2404.17401v2.pdf","comment":"Accepted version. Published in Machine Learning (Springer) 114:263\n  (2025). Open access under a CC BY-NC-ND 4.0 license. DOI:\n  10.1007/s10994-025-06916-9"},{"id":"http://arxiv.org/abs/2510.22672v2","updated":"2025-10-28T08:39:14Z","published":"2025-10-26T13:27:59Z","title":"Look and Tell: A Dataset for Multimodal Grounding Across Egocentric and\n  Exocentric Views","summary":"  We introduce Look and Tell, a multimodal dataset for studying referential\ncommunication across egocentric and exocentric perspectives. Using Meta Project\nAria smart glasses and stationary cameras, we recorded synchronized gaze,\nspeech, and video as 25 participants instructed a partner to identify\ningredients in a kitchen. Combined with 3D scene reconstructions, this setup\nprovides a benchmark for evaluating how different spatial representations (2D\nvs. 3D; ego vs. exo) affect multimodal grounding. The dataset contains 3.67\nhours of recordings, including 2,707 richly annotated referential expressions,\nand is designed to advance the development of embodied agents that can\nunderstand and engage in situated dialogue.\n","authors":["Anna Deichler","Jonas Beskow"],"pdf_url":"https://arxiv.org/pdf/2510.22672v2.pdf","comment":"10 pages, 6 figures, 2 tables. Accepted to the NeurIPS 2025 Workshop\n  on SPACE in Vision, Language, and Embodied AI (SpaVLE). Dataset:\n  https://huggingface.co/datasets/annadeichler/KTH-ARIA-referential"},{"id":"http://arxiv.org/abs/2510.24179v1","updated":"2025-10-28T08:34:01Z","published":"2025-10-28T08:34:01Z","title":"Exploring the Influence of Relevant Knowledge for Natural Language\n  Generation Interpretability","summary":"  This paper explores the influence of external knowledge integration in\nNatural Language Generation (NLG), focusing on a commonsense generation task.\nWe extend the CommonGen dataset by creating KITGI, a benchmark that pairs input\nconcept sets with retrieved semantic relations from ConceptNet and includes\nmanually annotated outputs. Using the T5-Large model, we compare sentence\ngeneration under two conditions: with full external knowledge and with filtered\nknowledge where highly relevant relations were deliberately removed. Our\ninterpretability benchmark follows a three-stage method: (1) identifying and\nremoving key knowledge, (2) regenerating sentences, and (3) manually assessing\noutputs for commonsense plausibility and concept coverage. Results show that\nsentences generated with full knowledge achieved 91\\% correctness across both\ncriteria, while filtering reduced performance drastically to 6\\%. These\nfindings demonstrate that relevant external knowledge is critical for\nmaintaining both coherence and concept coverage in NLG. This work highlights\nthe importance of designing interpretable, knowledge-enhanced NLG systems and\ncalls for evaluation frameworks that capture the underlying reasoning beyond\nsurface-level metrics.\n","authors":["IvÃ¡n MartÃ­nez-Murillo","Paloma Moreda","Elena Lloret"],"pdf_url":"https://arxiv.org/pdf/2510.24179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24178v1","updated":"2025-10-28T08:33:45Z","published":"2025-10-28T08:33:45Z","title":"MuSaG: A Multimodal German Sarcasm Dataset with Full-Modal Annotations","summary":"  Sarcasm is a complex form of figurative language in which the intended\nmeaning contradicts the literal one. Its prevalence in social media and popular\nculture poses persistent challenges for natural language understanding,\nsentiment analysis, and content moderation. With the emergence of multimodal\nlarge language models, sarcasm detection extends beyond text and requires\nintegrating cues from audio and vision. We present MuSaG, the first German\nmultimodal sarcasm detection dataset, consisting of 33 minutes of manually\nselected and human-annotated statements from German television shows. Each\ninstance provides aligned text, audio, and video modalities, annotated\nseparately by humans, enabling evaluation in unimodal and multimodal settings.\nWe benchmark nine open-source and commercial models, spanning text, audio,\nvision, and multimodal architectures, and compare their performance to human\nannotations. Our results show that while humans rely heavily on audio in\nconversational settings, models perform best on text. This highlights a gap in\ncurrent multimodal models and motivates the use of MuSaG for developing models\nbetter suited to realistic scenarios. We release MuSaG publicly to support\nfuture research on multimodal sarcasm detection and human-model alignment.\n","authors":["Aaron Scott","Maike ZÃ¼fle","Jan Niehues"],"pdf_url":"https://arxiv.org/pdf/2510.24178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.10114v3","updated":"2025-10-28T07:51:04Z","published":"2025-10-11T08:43:45Z","title":"LinearRAG: Linear Graph Retrieval Augmented Generation on Large-scale\n  Corpora","summary":"  Retrieval-Augmented Generation (RAG) is widely used to mitigate\nhallucinations of Large Language Models (LLMs) by leveraging external\nknowledge. While effective for simple queries, traditional RAG systems struggle\nwith large-scale, unstructured corpora where information is fragmented. Recent\nadvances incorporate knowledge graphs to capture relational structures,\nenabling more comprehensive retrieval for complex, multi-hop reasoning tasks.\nHowever, existing graph-based RAG (GraphRAG) methods rely on unstable and\ncostly relation extraction for graph construction, often producing noisy graphs\nwith incorrect or inconsistent relations that degrade retrieval quality. In\nthis paper, we revisit the pipeline of existing GraphRAG systems and propose\nLinearRAG (Linear Graph-based Retrieval-Augmented Generation), an efficient\nframework that enables reliable graph construction and precise passage\nretrieval. Specifically, LinearRAG constructs a relation-free hierarchical\ngraph, termed Tri-Graph, using only lightweight entity extraction and semantic\nlinking, avoiding unstable relation modeling. This new paradigm of graph\nconstruction scales linearly with corpus size and incurs no extra token\nconsumption, providing an economical and reliable indexing of the original\npassages. For retrieval, LinearRAG adopts a two-stage strategy: (i) relevant\nentity activation via local semantic bridging, followed by (ii) passage\nretrieval through global importance aggregation. Extensive experiments on four\ndatasets demonstrate that LinearRAG significantly outperforms baseline models.\nOur code and datasets are available at https://github.com/DEEP-PolyU/LinearRAG.\n","authors":["Luyao Zhuang","Shengyuan Chen","Yilin Xiao","Huachi Zhou","Yujing Zhang","Hao Chen","Qinggang Zhang","Xiao Huang"],"pdf_url":"https://arxiv.org/pdf/2510.10114v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23169v2","updated":"2025-10-28T07:44:06Z","published":"2025-10-27T09:51:49Z","title":"MATCH: Task-Driven Code Evaluation through Contrastive Learning","summary":"  AI-based code generation is increasingly prevalent, with GitHub Copilot\nestimated to generate 46% of the code on GitHub. Accurately evaluating how well\ngenerated code aligns with developer intent remains a critical challenge.\nTraditional evaluation methods, such as unit tests, are often unscalable and\ncostly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture code\nfunctionality, and metrics like CodeBERTScore require reference code, which is\nnot always available. To address the gap in reference-free evaluation, with few\nalternatives such as ICE-Score, this paper introduces MATCH, a novel\nreference-free metric. MATCH uses Contrastive Learning to generate meaningful\nembeddings for code and natural language task descriptions, enabling similarity\nscoring that reflects how well generated code implements the task. We show that\nMATCH achieves stronger correlations with functional correctness and human\npreference than existing metrics across multiple programming languages.\n","authors":["Marah Ghoummaid","Vladimir Tchuiev","Ofek Glick","Michal Moshkovitz","Dotan Di Castro"],"pdf_url":"https://arxiv.org/pdf/2510.23169v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24150v1","updated":"2025-10-28T07:42:59Z","published":"2025-10-28T07:42:59Z","title":"Ko-MuSR: A Multistep Soft Reasoning Benchmark for LLMs Capable of\n  Understanding Korean","summary":"  We present Ko-MuSR, the first benchmark to comprehensively evaluate\nmultistep, soft reasoning in long Korean narratives while minimizing data\ncontamination. Built following MuSR, Ko-MuSR features fully Korean narratives,\nreasoning chains, and multiple-choice questions verified by human annotators\nfor logical consistency and answerability. Evaluations of four large language\nmodels -- two multilingual and two Korean-specialized -- show that multilingual\nmodels outperform Korean-focused ones even in Korean reasoning tasks,\nindicating cross-lingual generalization of reasoning ability. Carefully\ndesigned prompting strategies, which combine few-shot examples, reasoning\ntraces, and task-specific hints, further boost accuracy, approaching\nhuman-level performance. Ko-MuSR offers a solid foundation for advancing Korean\nNLP by enabling systematic evaluation of long-context reasoning and prompting\nstrategies.\n","authors":["Chanwoo Park","Suyoung Park","JiA Kang","Jongyeon Park","Sangho Kim","Hyunji M. Park","Sumin Bae","Mingyu Kang","Jaejin Lee"],"pdf_url":"https://arxiv.org/pdf/2510.24150v1.pdf","comment":"submitted to ACL ARR Rolling Review"},{"id":"http://arxiv.org/abs/2509.24494v2","updated":"2025-10-28T07:36:45Z","published":"2025-09-29T09:07:45Z","title":"GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient\n  Chain-of-Thought Training","summary":"  Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a\nReinforcement Learning (RL) approach, can effectively train Chain-of-Thought\n(CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models\n(VLMs). In this paper, we analyze three challenges of GRPO: gradient coupling\nbetween thoughts and answers, sparse reward signals caused by limited parallel\nsampling, and unstable advantage estimation. To mitigate these challenges, we\npropose GRPO-MA, a simple yet theoretically grounded method that leverages\nmulti-answer generation from each thought process, enabling more robust and\nefficient optimization. Theoretically, we show that the variance of thought\nadvantage decreases as the number of answers per thought increases.\nEmpirically, our gradient analysis confirms this effect, showing that GRPO-MA\nreduces gradient spikes compared to GRPO. Experiments on math, code, and\ndiverse multimodal tasks demonstrate that GRPO-MA substantially improves\nperformance and training efficiency. Our ablation studies further reveal that\nincreasing the number of answers per thought consistently enhances model\nperformance.\n","authors":["Hongcheng Wang","Yinuo Huang","Sukai Wang","Guanghui Ren","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2509.24494v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2510.20280v2","updated":"2025-10-28T07:35:34Z","published":"2025-10-23T07:09:45Z","title":"Context-level Language Modeling by Learning Predictive Context\n  Embeddings","summary":"  Next-token prediction (NTP) is the cornerstone of modern large language\nmodels (LLMs) pretraining, driving their unprecedented capabilities in text\ngeneration, reasoning, and instruction following. However, the token-level\nprediction limits the model's capacity to capture higher-level semantic\nstructures and long-range contextual relationships. To overcome this\nlimitation, we introduce \\textbf{ContextLM}, a framework that augments standard\npretraining with an inherent \\textbf{next-context prediction} objective. This\nmechanism trains the model to learn predictive representations of multi-token\ncontexts, leveraging error signals derived from future token chunks. Crucially,\nContextLM achieves this enhancement while remaining fully compatible with the\nstandard autoregressive, token-by-token evaluation paradigm (e.g., perplexity).\nExtensive experiments on the GPT2 and Pythia model families, scaled up to\n$1.5$B parameters, show that ContextLM delivers consistent improvements in both\nperplexity and downstream task performance. Our analysis indicates that\nnext-context prediction provides a scalable and efficient pathway to stronger\nlanguage modeling, yielding better long-range coherence and more effective\nattention allocation with minimal computational overhead.\n","authors":["Beiya Dai","Yuliang Liu","Daozheng Xue","Qipeng Guo","Kai Chen","Xinbing Wang","Bowen Zhou","Zhouhan Lin"],"pdf_url":"https://arxiv.org/pdf/2510.20280v2.pdf","comment":"16pages,6 figures"},{"id":"http://arxiv.org/abs/2510.22162v2","updated":"2025-10-28T07:27:11Z","published":"2025-10-25T05:17:45Z","title":"Surface Reading LLMs: Synthetic Text and its Styles","summary":"  Despite a potential plateau in ML advancement, the societal impact of large\nlanguage models lies not in approaching superintelligence but in generating\ntext surfaces indistinguishable from human writing. While Critical AI Studies\nprovides essential material and socio-technical critique, it risks overlooking\nhow LLMs phenomenologically reshape meaning-making. This paper proposes a\nsemiotics of \"surface integrity\" as attending to the immediate plane where LLMs\ninscribe themselves into human communication. I distinguish three knowledge\ninterests in ML research (epistemology, epist\\=em\\=e, and epistemics) and argue\nfor integrating surface-level stylistic analysis alongside depth-oriented\ncritique. Through two case studies examining stylistic markers of synthetic\ntext, I argue how attending to style as a semiotic phenomenon reveals LLMs as\ncultural actors that transform the conditions of meaning emergence and\ncirculation in contemporary discourse, independent of questions about machine\nconsciousness.\n","authors":["Hannes Bajohr"],"pdf_url":"https://arxiv.org/pdf/2510.22162v2.pdf","comment":"12 pages, 1 figure"},{"id":"http://arxiv.org/abs/2510.24139v1","updated":"2025-10-28T07:24:32Z","published":"2025-10-28T07:24:32Z","title":"Beyond Line-Level Filtering for the Pretraining Corpora of LLMs","summary":"  While traditional line-level filtering techniques, such as line-level\ndeduplication and trailing-punctuation filters, are commonly used, these basic\nmethods can sometimes discard valuable content, negatively affecting downstream\nperformance. In this paper, we introduce two methods-pattern-aware line-level\ndeduplication (PLD) and pattern-aware trailing punctuation filtering (PTF)-by\nenhancing the conventional filtering techniques. Our approach not only\nconsiders line-level signals but also takes into account their sequential\ndistribution across documents, enabling us to retain structurally important\ncontent that might otherwise be removed. We evaluate these proposed methods by\ntraining small language models (1 B parameters) in both English and Korean. The\nresults demonstrate that our methods consistently improve performance on\nmultiple-choice benchmarks and significantly enhance generative\nquestion-answering accuracy on both SQuAD v1 and KorQuAD v1.\n","authors":["Chanwoo Park","Suyoung Park","Yelim Ahn","Jongmin Kim","Jongyeon Park","Jaejin Lee"],"pdf_url":"https://arxiv.org/pdf/2510.24139v1.pdf","comment":"submitted to ACL ARR Rolling Review"},{"id":"http://arxiv.org/abs/2510.24134v1","updated":"2025-10-28T07:19:01Z","published":"2025-10-28T07:19:01Z","title":"VC4VG: Optimizing Video Captions for Text-to-Video Generation","summary":"  Recent advances in text-to-video (T2V) generation highlight the critical role\nof high-quality video-text pairs in training models capable of producing\ncoherent and instruction-aligned videos. However, strategies for optimizing\nvideo captions specifically for T2V training remain underexplored. In this\npaper, we introduce VC4VG (Video Captioning for Video Generation), a\ncomprehensive caption optimization framework tailored to the needs of T2V\nmodels.We begin by analyzing caption content from a T2V perspective,\ndecomposing the essential elements required for video reconstruction into\nmultiple dimensions, and proposing a principled caption design methodology. To\nsupport evaluation, we construct VC4VG-Bench, a new benchmark featuring\nfine-grained, multi-dimensional, and necessity-graded metrics aligned with\nT2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a\nstrong correlation between improved caption quality and video generation\nperformance, validating the effectiveness of our approach. We release all\nbenchmark tools and code at https://github.com/qyr0403/VC4VG to support further\nresearch.\n","authors":["Yang Du","Zhuoran Lin","Kaiqiang Song","Biao Wang","Zhicheng Zheng","Tiezheng Ge","Bo Zheng","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2510.24134v1.pdf","comment":"Accepted by EMNLP 2025"},{"id":"http://arxiv.org/abs/2506.15355v2","updated":"2025-10-28T07:12:22Z","published":"2025-06-18T11:19:25Z","title":"SANSKRITI: A Comprehensive Benchmark for Evaluating Language Models'\n  Knowledge of Indian Culture","summary":"  Language Models (LMs) are indispensable tools shaping modern workflows, but\ntheir global effectiveness depends on understanding local socio-cultural\ncontexts. To address this, we introduce SANSKRITI, a benchmark designed to\nevaluate language models' comprehension of India's rich cultural diversity.\nComprising 21,853 meticulously curated question-answer pairs spanning 28 states\nand 8 union territories, SANSKRITI is the largest dataset for testing Indian\ncultural knowledge. It covers sixteen key attributes of Indian culture: rituals\nand ceremonies, history, tourism, cuisine, dance and music, costume, language,\nart, festivals, religion, medicine, transport, sports, nightlife, and\npersonalities, providing a comprehensive representation of India's cultural\ntapestry. We evaluate SANSKRITI on leading Large Language Models (LLMs), Indic\nLanguage Models (ILMs), and Small Language Models (SLMs), revealing significant\ndisparities in their ability to handle culturally nuanced queries, with many\nmodels struggling in region-specific contexts. By offering an extensive,\nculturally rich, and diverse dataset, SANSKRITI sets a new standard for\nassessing and improving the cultural understanding of LMs.\n","authors":["Arijit Maji","Raghvendra Kumar","Akash Ghosh"," Anushka","Sriparna Saha"],"pdf_url":"https://arxiv.org/pdf/2506.15355v2.pdf","comment":"ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2510.24126v1","updated":"2025-10-28T07:00:42Z","published":"2025-10-28T07:00:42Z","title":"Reinforcement Learning for Long-Horizon Multi-Turn Search Agents","summary":"  Large Language Model (LLM) agents can leverage multiple turns and tools to\nsolve complex tasks, with prompt-based approaches achieving strong performance.\nThis work demonstrates that Reinforcement Learning (RL) can push capabilities\nsignificantly further by learning from experience. Through experiments on a\nlegal document search benchmark, we show that our RL-trained 14 Billion\nparameter model outperforms frontier class models (85% vs 78% accuracy). In\naddition, we explore turn-restricted regimes, during training and at test-time,\nthat show these agents achieve better results if allowed to operate over longer\nmulti-turn horizons.\n","authors":["Vivek Kalyan","Martin Andrews"],"pdf_url":"https://arxiv.org/pdf/2510.24126v1.pdf","comment":"4 pages plus references and appendices. Accepted into the First\n  Workshop on Multi-Turn Interactions in Large Language Models at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24102v1","updated":"2025-10-28T06:16:38Z","published":"2025-10-28T06:16:38Z","title":"Squrve: A Unified and Modular Framework for Complex Real-World\n  Text-to-SQL Tasks","summary":"  Text-to-SQL technology has evolved rapidly, with diverse academic methods\nachieving impressive results. However, deploying these techniques in real-world\nsystems remains challenging due to limited integration tools. Despite these\nadvances, we introduce Squrve, a unified, modular, and extensive Text-to-SQL\nframework designed to bring together research advances and real-world\napplications. Squrve first establishes a universal execution paradigm that\nstandardizes invocation interfaces, then proposes a multi-actor collaboration\nmechanism based on seven abstracted effective atomic actor components.\nExperiments on widely adopted benchmarks demonstrate that the collaborative\nworkflows consistently outperform the original individual methods, thereby\nopening up a new effective avenue for tackling complex real-world queries. The\ncodes are available at https://github.com/Satissss/Squrve.\n","authors":["Yihan Wang","Peiyu Liu","Runyu Chen","Jiaxing Pu","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2510.24102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24096v1","updated":"2025-10-28T06:08:42Z","published":"2025-10-28T06:08:42Z","title":"RegSpeech12: A Regional Corpus of Bengali Spontaneous Speech Across\n  Dialects","summary":"  The Bengali language, spoken extensively across South Asia and among\ndiasporic communities, exhibits considerable dialectal diversity shaped by\ngeography, culture, and history. Phonological and pronunciation-based\nclassifications broadly identify five principal dialect groups: Eastern\nBengali, Manbhumi, Rangpuri, Varendri, and Rarhi. Within Bangladesh, further\ndistinctions emerge through variation in vocabulary, syntax, and morphology, as\nobserved in regions such as Chittagong, Sylhet, Rangpur, Rajshahi, Noakhali,\nand Barishal. Despite this linguistic richness, systematic research on the\ncomputational processing of Bengali dialects remains limited. This study seeks\nto document and analyze the phonetic and morphological properties of these\ndialects while exploring the feasibility of building computational models\nparticularly Automatic Speech Recognition (ASR) systems tailored to regional\nvarieties. Such efforts hold potential for applications in virtual assistants\nand broader language technologies, contributing to both the preservation of\ndialectal diversity and the advancement of inclusive digital tools for\nBengali-speaking communities. The dataset created for this study is released\nfor public use.\n","authors":["Md. Rezuwan Hassan","Azmol Hossain","Kanij Fatema","Rubayet Sabbir Faruque","Tanmoy Shome","Ruwad Naswan","Trina Chakraborty","Md. Foriduzzaman Zihad","Tawsif Tashwar Dipto","Nazia Tasnim","Nazmuddoha Ansary","Md. Mehedi Hasan Shawon","Ahmed Imtiaz Humayun","Md. Golam Rabiul Alam","Farig Sadeque","Asif Sushmit"],"pdf_url":"https://arxiv.org/pdf/2510.24096v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2510.24081v1","updated":"2025-10-28T05:46:25Z","published":"2025-10-28T05:46:25Z","title":"Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+\n  Languages and Cultures","summary":"  To date, there exist almost no culturally-specific evaluation benchmarks for\nlarge language models (LLMs) that cover a large number of languages and\ncultures. In this paper, we present Global PIQA, a participatory commonsense\nreasoning benchmark for over 100 languages, constructed by hand by 335\nresearchers from 65 countries around the world. The 116 language varieties in\nGlobal PIQA cover five continents, 14 language families, and 23 writing\nsystems. In the non-parallel split of Global PIQA, over 50% of examples\nreference local foods, customs, traditions, or other culturally-specific\nelements. We find that state-of-the-art LLMs perform well on Global PIQA in\naggregate, but they exhibit weaker performance in lower-resource languages (up\nto a 37% accuracy gap, despite random chance at 50%). Open models generally\nperform worse than proprietary models. Global PIQA highlights that in many\nlanguages and cultures, everyday knowledge remains an area for improvement,\nalongside more widely-discussed capabilities such as complex reasoning and\nexpert knowledge. Beyond its uses for LLM evaluation, we hope that Global PIQA\nprovides a glimpse into the wide diversity of cultures in which human language\nis embedded.\n","authors":["Tyler A. Chang","Catherine Arnett","Abdelrahman Eldesokey","Abdelrahman Sadallah","Abeer Kashar","Abolade Daud","Abosede Grace Olanihun","Adamu Labaran Mohammed","Adeyemi Praise","Adhikarinayum Meerajita Sharma","Aditi Gupta","Afitab Iyigun","Afonso SimplÃ­cio","Ahmed Essouaied","Aicha Chorana","Akhil Eppa","Akintunde Oladipo","Akshay Ramesh","Aleksei Dorkin","Alfred Malengo Kondoro","Alham Fikri Aji","Ali Eren ÃetintaÅ","Allan Hanbury","Alou Dembele","Alp Niksarli","Ãlvaro Arroyo","Amin Bajand","Amol Khanna","Ana Chkhaidze","Ana Condez","Andiswa Mkhonto","Andrew Hoblitzell","Andrew Tran","Angelos Poulis","Anirban Majumder","Anna Vacalopoulou","Annette Kuuipolani Kanahele Wong","Annika Simonsen","Anton Kovalev","Ashvanth. S","Ayodeji Joseph Lana","Barkin Kinay","Bashar Alhafni","Benedict Cibalinda Busole","Bernard Ghanem","Bharti Nathani","Biljana Stojanovska ÄuriÄ","Bola Agbonile","Bragi Bergsson","Bruce Torres Fischer","Burak Tutar","Burcu AlakuÅ ÃÄ±nar","Cade J. Kanoniakapueo Kane","Can Udomcharoenchaikit","Catherine Arnett","Chadi Helwe","Chaithra Reddy Nerella","Chen Cecilia Liu","Chiamaka Glory Nwokolo","Cristina EspaÃ±a-Bonet","Cynthia Amol","DaeYeop Lee","Dana Arad","Daniil Dzenhaliou","Daria Pugacheva","Dasol Choi","Daud Abolade","David Liu","David Semedo","Deborah Popoola","Deividas Mataciunas","Delphine Nyaboke","Dhyuthy Krishna Kumar","Diogo GlÃ³ria-Silva","Diogo Tavares","Divyanshu Goyal","DongGeon Lee","Ebele Nwamaka Anajemba","Egonu Ngozi Grace","Elena Mickel","Elena Tutubalina","Elias Herranen","Emile Anand","Emmanuel Habumuremyi","Emuobonuvie Maria Ajiboye","Eryawan Presma Yulianrifat","Esther Adenuga","Ewa Rudnicka","Faith Olabisi Itiola","Faran Taimoor Butt","Fathima Thekkekara","Fatima Haouari","Filbert Aurelian Tjiaranata","Firas Laakom","Francesca Grasso","Francesco Orabona","Francesco Periti","Gbenga Kayode Solomon","Gia Nghia Ngo","Gloria Udhehdhe-oze","GonÃ§alo Martins","Gopi Naga Sai Ram Challagolla","Guijin Son","Gulnaz Abdykadyrova","Hafsteinn Einarsson","Hai Hu","Hamidreza Saffari","Hamza Zaidi","Haopeng Zhang","Harethah Abu Shairah","Harry Vuong","Hele-Andra Kuulmets","Houda Bouamor","Hwanjo Yu","Iben Nyholm Debess","Ä°brahim Ethem Deveci","Ikhlasul Akmal Hanif","Ikhyun Cho","InÃªs Calvo","InÃªs Vieira","Isaac Manzi","Ismail Daud","Itay Itzhak"," Iuliia"," Alekseenko","Ivan Belashkin","Ivan Spada","Ivan Zhelyazkov","Jacob Brinton","Jafar Isbarov","Jaka Äibej","Jan Äuhel","Jan KocoÅ","Jauza Akbar Krito","Jebish Purbey","Jennifer Mickel","Jennifer Za","Jenny Kunz","Jihae Jeong","Jimena Tena DÃ¡valos","Jinu Lee","JoÃ£o MagalhÃ£es","John Yi","Jongin Kim","Joseph Chataignon","Joseph Marvin Imperial","Jubeerathan Thevakumar","Judith Land","Junchen Jiang","Jungwhan Kim","Kairit Sirts","Kamesh R","Kamesh V","Kanda Patrick Tshinu","KÃ¤triin Kukk","Kaustubh Ponkshe","Kavsar Huseynova","Ke He","Kelly Buchanan","Kengatharaiyer Sarveswaran","Kerem Zaman","Khalil Mrini","Kian Kyars","Krister Kruusmaa","Kusum Chouhan","Lainitha Krishnakumar","Laura Castro SÃ¡nchez","Laura Porrino Moscoso","Leshem Choshen","Levent Sencan","Lilja Ãvrelid","Lisa Alazraki","Lovina Ehimen-Ugbede","Luheerathan Thevakumar","Luxshan Thavarasa","Mahnoor Malik","Mamadou K. Keita","Mansi Jangid","Marco De Santis","Marcos GarcÃ­a","Marek Suppa","Mariam D'Ciofalo","Marii Ojastu","Maryam Sikander","Mausami Narayan","Maximos Skandalis","Mehak Mehak","Mehmet Ä°lteriÅ Bozkurt","Melaku Bayu Workie","Menan Velayuthan","Michael Leventhal","MichaÅ MarciÅczuk","Mirna PotoÄnjak","Mohammadamin Shafiei","Mridul Sharma","Mrityunjaya Indoria","Muhammad Ravi Shulthan Habibi","Murat KoliÄ","Nada Galant","Naphat Permpredanun","Narada Maugin","Nicholas Kluge CorrÃªa","Nikola LjubeÅ¡iÄ","Nirmal Thomas","Nisansa de Silva","Nisheeth Joshi","Nitish Ponkshe","Nizar Habash","Nneoma C. Udeze","Noel Thomas","NoÃ©mi Ligeti-Nagy","Nouhoum Coulibaly","Nsengiyumva Faustin","Odunayo Kareemat Buliaminu","Odunayo Ogundepo","Oghojafor Godswill Fejiro","Ogundipe Blessing Funmilola","Okechukwu God'spraise","Olanrewaju Samuel","Olaoye Deborah Oluwaseun","Olasoji Akindejoye","Olga Popova","Olga Snissarenko","Onyinye Anulika Chiemezie","Orkun Kinay","Osman Tursun","Owoeye Tobiloba Moses","Oyelade Oluwafemi Joshua","Oyesanmi Fiyinfoluwa","Pablo Gamallo","Pablo RodrÃ­guez FernÃ¡ndez","Palak Arora","Pedro Valente","Peter Rupnik","Philip Oghenesuowho Ekiugbo","Pramit Sahoo","Prokopis Prokopidis","Pua Niau-Puhipau","Quadri Yahya","Rachele Mignone","Raghav Singhal","Ram Mohan Rao Kadiyala","Raphael Merx","Rapheal Afolayan","Ratnavel Rajalakshmi","Rishav Ghosh","Romina Oji","Ron Kekeha Solis","Rui Guerra","Rushikesh Zawar","Sa'ad Nasir Bashir","Saeed Alzaabi","Sahil Sandeep","Sai Pavan Batchu","SaiSandeep Kantareddy","Salsabila Zahirah Pranida","Sam Buchanan","Samuel Rutunda","Sander Land","Sarah Sulollari","Sardar Ali","Saroj Sapkota","Saulius Tautvaisas","Sayambhu Sen","Sayantani Banerjee","Sebastien Diarra","SenthilNathan. M","Sewoong Lee","Shaan Shah","Shankar Venkitachalam","Sharifa Djurabaeva","Sharon Ibejih","Shivanya Shomir Dutta","Siddhant Gupta","Silvia Paniagua SuÃ¡rez","Sina Ahmadi","Sivasuthan Sukumar","Siyuan Song","Snegha A.","Sokratis Sofianopoulos","Sona Elza Simon","Sonja BenÄina","Sophie Gvasalia","Sphurti Kirit More","Spyros Dragazis","Stephan P. Kaufhold","Suba. S","Sultan AlRashed","Surangika Ranathunga","Taiga Someya","Taja Kuzman PungerÅ¡ek","Tal Haklay","Tasi'u Jibril","Tatsuya Aoyama","Tea Abashidze","Terenz Jomar Dela Cruz","Terra Blevins","Themistoklis Nikas","Theresa Dora Idoko","Thu Mai Do","Tilek Chubakov","Tommaso Gargiani","Uma Rathore","Uni Johannesen","Uwuma Doris Ugwu","Vallerie Alexandra Putra","Vanya Bannihatti Kumar","Varsha Jeyarajalingam","Varvara Arzt","Vasudevan Nedumpozhimana","Viktoria Ondrejova","Viktoryia Horbik","Vishnu Vardhan Reddy Kummitha","Vuk DiniÄ","Walelign Tewabe Sewunetie","Winston Wu","Xiaojing Zhao","Yacouba Diarra","Yaniv Nikankin","Yash Mathur","Yixi Chen","Yiyuan Li","Yolanda Xavier","Yonatan Belinkov","Yusuf Ismail Abayomi","Zaid Alyafeai","Zhengyang Shan","Zhi Rui Tam","Zilu Tang","Zuzana Nadova","Baber Abbasi","Stella Biderman","David Stap","Duygu Ataman","Fabian Schmidt","Hila Gonen","Jiayi Wang","David Ifeoluwa Adelani"],"pdf_url":"https://arxiv.org/pdf/2510.24081v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2510.24073v1","updated":"2025-10-28T05:17:18Z","published":"2025-10-28T05:17:18Z","title":"Challenging Multilingual LLMs: A New Taxonomy and Benchmark for\n  Unraveling Hallucination in Translation","summary":"  Large Language Models (LLMs) have advanced machine translation but remain\nvulnerable to hallucinations. Unfortunately, existing MT benchmarks are not\ncapable of exposing failures in multilingual LLMs. To disclose hallucination in\nmultilingual LLMs, we introduce a diagnostic framework with a taxonomy that\nseparates Instruction Detachment from Source Detachment. Guided by this\ntaxonomy, we create HalloMTBench, a multilingual, human-verified benchmark\nacross 11 English-to-X directions. We employed 4 frontier LLMs to generate\ncandidates and scrutinize these candidates with an ensemble of LLM judges, and\nexpert validation. In this way, we curate 5,435 high-quality instances. We have\nevaluated 17 LLMs on HalloMTBench. Results reveal distinct ``hallucination\ntriggers'' -- unique failure patterns reflecting model scale, source length\nsensitivity, linguistic biases, and Reinforcement-Learning (RL) amplified\nlanguage mixing. HalloMTBench offers a forward-looking testbed for diagnosing\nLLM translation failures. HalloMTBench is available in\nhttps://huggingface.co/collections/AIDC-AI/marco-mt.\n","authors":["Xinwei Wu","Heng Liu","Jiang Zhou","Xiaohu Zhao","Linlong Xu","Longyue Wang","Weihua Luo","Kaifu Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.24073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.18383v2","updated":"2025-10-28T04:50:06Z","published":"2025-10-21T08:03:14Z","title":"MENTOR: A Reinforcement Learning Framework for Enabling Tool Use in\n  Small Models via Teacher-Optimized Rewards","summary":"  Distilling the tool-using capabilities of large language models (LLMs) into\nsmaller, more efficient small language models (SLMs) is a key challenge for\ntheir practical application. The predominant approach, supervised fine-tuning\n(SFT), suffers from poor generalization as it trains models to imitate a static\nset of teacher trajectories rather than learn a robust methodology. While\nreinforcement learning (RL) offers an alternative, the standard RL using sparse\nrewards fails to effectively guide SLMs, causing them to struggle with\ninefficient exploration and adopt suboptimal strategies. To address these\ndistinct challenges, we propose MENTOR, a framework that synergistically\ncombines RL with teacher-guided distillation. Instead of simple imitation,\nMENTOR employs an RL-based process to learn a more generalizable policy through\nexploration. In addition, to solve the problem of reward sparsity, it uses a\nteacher's reference trajectory to construct a dense, composite teacher-guided\nreward that provides fine-grained guidance. Extensive experiments demonstrate\nthat MENTOR significantly improves the cross-domain generalization and\nstrategic competence of SLMs compared to both SFT and standard sparse-reward RL\nbaselines.\n","authors":["ChangSu Choi","Hoyun Song","Dongyeon Kim","WooHyeon Jung","Minkyung Cho","Sunjin Park","NohHyeob Bae","Seona Yu","KyungTae Lim"],"pdf_url":"https://arxiv.org/pdf/2510.18383v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20445v5","updated":"2025-10-28T04:18:04Z","published":"2024-10-27T13:51:09Z","title":"TrajAgent: An LLM-Agent Framework for Trajectory Modeling via\n  Large-and-Small Model Collaboration","summary":"  Trajectory modeling, which includes research on trajectory data pattern\nmining and future prediction, has widespread applications in areas such as life\nservices, urban transportation, and public administration. Numerous methods\nhave been proposed to address specific problems within trajectory modeling.\nHowever, the heterogeneity of data and the diversity of trajectory tasks make\neffective and reliable trajectory modeling an important yet highly challenging\nendeavor, even for domain experts. In this paper, we propose TrajAgent, an\nagent framework powered by large language models, designed to facilitate robust\nand efficient trajectory modeling through automation modeling. This framework\nleverages and optimizes diverse specialized models to address various\ntrajectory modeling tasks across different datasets effectively. In TrajAgent,\nwe first develop UniEnv, an execution environment with a unified data and model\ninterface, to support the execution and training of various models. Building on\nUniEnv, we introduce an agentic workflow designed for automatic trajectory\nmodeling across various trajectory tasks and data. Furthermore, we introduce\ncollaborative learning schema between LLM-based agents and small speciallized\nmodels, to enhance the performance of the whole framework effectively.\nExtensive experiments on five tasks using four real-world datasets demonstrate\nthe effectiveness of TrajAgent in automated trajectory modeling, achieving a\nperformance improvement of 2.38%-69.91% over baseline methods. The codes and\ndata can be accessed via https://github.com/tsinghua-fib-lab/TrajAgent.\n","authors":["Yuwei Du","Jie Feng","Jie Zhao","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2410.20445v5.pdf","comment":"Accepted by NeurIPS 2025,\n  https://github.com/tsinghua-fib-lab/TrajAgent"},{"id":"http://arxiv.org/abs/2510.24051v1","updated":"2025-10-28T04:17:55Z","published":"2025-10-28T04:17:55Z","title":"Pie: A Programmable Serving System for Emerging LLM Applications","summary":"  Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations.\n","authors":["In Gim","Zhiyao Ma","Seung-seob Lee","Lin Zhong"],"pdf_url":"https://arxiv.org/pdf/2510.24051v1.pdf","comment":"SOSP 2025. Source code available at\n  https://github.com/pie-project/pie"},{"id":"http://arxiv.org/abs/2509.24958v2","updated":"2025-10-28T04:02:58Z","published":"2025-09-29T15:52:36Z","title":"The Dialogue That Heals: A Comprehensive Evaluation of Doctor Agents'\n  Inquiry Capability","summary":"  An effective physician should possess a combination of empathy, expertise,\npatience, and clear communication when treating a patient. Recent advances have\nsuccessfully endowed AI doctors with expert diagnostic skills, particularly the\nability to actively seek information through inquiry. However, other essential\nqualities of a good doctor remain overlooked. To bridge this gap, we present\nMAQuE(Medical Agent Questioning Evaluation), the largest-ever benchmark for the\nautomatic and comprehensive evaluation of medical multi-turn questioning. It\nfeatures 3,000 realistically simulated patient agents that exhibit diverse\nlinguistic patterns, cognitive limitations, emotional responses, and tendencies\nfor passive disclosure. We also introduce a multi-faceted evaluation framework,\ncovering task success, inquiry proficiency, dialogue competence, inquiry\nefficiency, and patient experience. Experiments on different LLMs reveal\nsubstantial challenges across the evaluation aspects. Even state-of-the-art\nmodels show significant room for improvement in their inquiry capabilities.\nThese models are highly sensitive to variations in realistic patient behavior,\nwhich considerably impacts diagnostic accuracy. Furthermore, our fine-grained\nmetrics expose trade-offs between different evaluation perspectives,\nhighlighting the challenge of balancing performance and practicality in\nreal-world clinical settings.\n","authors":["Linlu Gong","Ante Wang","Yunghwei Lai","Weizhi Ma","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2509.24958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01068v4","updated":"2025-10-28T04:00:18Z","published":"2025-02-03T05:25:09Z","title":"FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation","summary":"  While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial prefill computation and key-value (KV) cache, which\ncan heavily burden computational efficiency and memory usage in both prefill\nand decoding stages. Recent works that compress KV caches with prefill\nacceleration reduce this cost but inadvertently tie the prefill compute\nreduction to the decoding KV budget. This coupling arises from overlooking the\nlayer-dependent variation of critical context, often leading to accuracy\ndegradation. To address this issue, we introduce FastKV, a KV cache compression\nframework designed to reduce latency in both prefill and decoding by leveraging\nthe stabilization of token importance in later layers. FastKV performs\nfull-context computation until a Token-Selective Propagation (TSP) layer, which\nforwards only the most informative tokens to subsequent layers. From these\npropagated tokens, FastKV independently selects salient KV entries for caching,\nthereby decoupling KV budget from the prefill compute reduction based on the\nTSP decision. This independent control of the TSP rate and KV retention rate\nenables flexible optimization of efficiency and accuracy. Experimental results\nshow that FastKV achieves speedups of up to 1.82$\\times$ in prefill and\n2.87$\\times$ in decoding compared to the full-context baseline, while matching\nthe accuracy of the baselines that only accelerate the decoding stage. Our code\nis available at https://github.com/dongwonjo/FastKV.\n","authors":["Dongwon Jo","Jiwon Song","Yulhwa Kim","Jae-Joon Kim"],"pdf_url":"https://arxiv.org/pdf/2502.01068v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05316v3","updated":"2025-10-28T03:50:11Z","published":"2025-06-05T17:55:43Z","title":"Improving Data Efficiency for LLM Reinforcement Fine-tuning Through\n  Difficulty-targeted Online Data Selection and Rollout Replay","summary":"  Reinforcement learning (RL) has become an effective approach for fine-tuning\nlarge language models (LLMs), particularly to enhance their reasoning\ncapabilities. However, RL fine-tuning remains highly resource-intensive, and\nexisting work has largely overlooked the problem of data efficiency. In this\npaper, we propose two techniques to improve data efficiency in LLM RL\nfine-tuning: difficulty-targeted online data selection and rollout replay. We\nintroduce the notion of adaptive difficulty to guide online data selection,\nprioritizing questions of moderate difficulty that are more likely to yield\ninformative learning signals. To estimate adaptive difficulty efficiently, we\ndevelop an attention-based framework that requires rollouts for only a small\nreference set of questions. The adaptive difficulty of the remaining questions\nis then estimated based on their similarity to this set. To further reduce\nrollout cost, we introduce a rollout replay mechanism inspired by experience\nreplay in traditional RL. This technique reuses recent rollouts, lowering\nper-step computation while maintaining stable updates. Experiments across 6\nLLM-dataset combinations show that our method reduces RL fine-tuning time by\n23% to 62% while reaching the same level of performance as the original GRPO\nalgorithm. Our code is available at\nhttps://github.com/ASTRAL-Group/data-efficient-llm-rl.\n","authors":["Yifan Sun","Jingyan Shen","Yibin Wang","Tianyu Chen","Zhendong Wang","Mingyuan Zhou","Huan Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.05316v3.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24035v1","updated":"2025-10-28T03:36:05Z","published":"2025-10-28T03:36:05Z","title":"GraphNet: A Large-Scale Computational Graph Dataset for Tensor Compiler\n  Research","summary":"  We introduce GraphNet, a dataset of 2.7K real-world deep learning\ncomputational graphs with rich metadata, spanning six major task categories\nacross multiple deep learning frameworks. To evaluate tensor compiler\nperformance on these samples, we propose the benchmark metric Speedup Score\nS(t), which jointly considers runtime speedup and execution correctness under\ntunable tolerance levels, offering a reliable measure of general optimization\ncapability. Furthermore, we extend S(t) to the Error-aware Speedup Score ES(t),\nwhich incorporates error information and helps compiler developers identify key\nperformance bottlenecks. In this report, we benchmark the default tensor\ncompilers, CINN for PaddlePaddle and TorchInductor for PyTorch, on computer\nvision (CV) and natural language processing (NLP) samples to demonstrate the\npracticality of GraphNet. The full construction pipeline with graph extraction\nand compiler evaluation tools is available at\nhttps://github.com/PaddlePaddle/GraphNet .\n","authors":["Xinqi Li","Yiqun Liu","Shan Jiang","Enrong Zheng","Huaijin Zheng","Wenhao Dai","Haodong Deng","Dianhai Yu","Yanjun Ma"],"pdf_url":"https://arxiv.org/pdf/2510.24035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23564v2","updated":"2025-10-28T03:22:35Z","published":"2025-10-27T17:35:15Z","title":"ReCode: Unify Plan and Action for Universal Granularity Control","summary":"  Real-world tasks require decisions at varying granularities, and humans excel\nat this by leveraging a unified cognitive representation where planning is\nfundamentally understood as a high-level form of action. However, current Large\nLanguage Model (LLM)-based agents lack this crucial capability to operate\nfluidly across decision granularities. This limitation stems from existing\nparadigms that enforce a rigid separation between high-level planning and\nlow-level action, which impairs dynamic adaptability and limits generalization.\nWe propose ReCode (Recursive Code Generation), a novel paradigm that addresses\nthis limitation by unifying planning and action within a single code\nrepresentation. In this representation, ReCode treats high-level plans as\nabstract placeholder functions, which the agent then recursively decomposes\ninto finer-grained sub-functions until reaching primitive actions. This\nrecursive approach dissolves the rigid boundary between plan and action,\nenabling the agent to dynamically control its decision granularity.\nFurthermore, the recursive structure inherently generates rich,\nmulti-granularity training data, enabling models to learn hierarchical\ndecision-making processes. Extensive experiments show ReCode significantly\nsurpasses advanced baselines in inference performance and demonstrates\nexceptional data efficiency in training, validating our core insight that\nunifying planning and action through recursive code generation is a powerful\nand effective approach to achieving universal granularity control. The code is\navailable at https://github.com/FoundationAgents/ReCode.\n","authors":["Zhaoyang Yu","Jiayi Zhang","Huixue Su","Yufan Zhao","Yifan Wu","Mingyi Deng","Jinyu Xiang","Yizhang Lin","Lingxiao Tang","Yingchao Li","Yuyu Luo","Bang Liu","Chenglin Wu"],"pdf_url":"https://arxiv.org/pdf/2510.23564v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01381v2","updated":"2025-10-28T03:11:09Z","published":"2025-06-02T07:18:26Z","title":"AdaRewriter: Unleashing the Power of Prompting-based Conversational\n  Query Reformulation via Test-Time Adaptation","summary":"  Prompting-based conversational query reformulation has emerged as a powerful\napproach for conversational search, refining ambiguous user queries into\nstandalone search queries. Best-of-N reformulation over the generated\ncandidates via prompting shows impressive potential scaling capability.\nHowever, both the previous tuning methods (training time) and adaptation\napproaches (test time) can not fully unleash their benefits. In this paper, we\npropose AdaRewriter, a novel framework for query reformulation using an\noutcome-supervised reward model via test-time adaptation. By training a\nlightweight reward model with contrastive ranking loss, AdaRewriter selects the\nmost promising reformulation during inference. Notably, it can operate\neffectively in black-box systems, including commercial LLM APIs. Experiments on\nfive conversational search datasets show that AdaRewriter significantly\noutperforms the existing methods across most settings, demonstrating the\npotential of test-time adaptation for conversational query reformulation.\n","authors":["Yilong Lai","Jialong Wu","Zhenglin Wang","Deyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.01381v2.pdf","comment":"Accepted by EMNLP 2025"},{"id":"http://arxiv.org/abs/2510.19457v2","updated":"2025-10-28T03:06:40Z","published":"2025-10-22T10:41:57Z","title":"MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for\n  Large Multimodal Models","summary":"  Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal\npre-training, yet their static representations struggle to maintain an accurate\nunderstanding of time-sensitive factual knowledge. Existing benchmarks remain\nconstrained by static designs, inadequately evaluating LMMs' ability to\nunderstand time-sensitive knowledge. To address this gap, we propose MINED, a\ncomprehensive benchmark that evaluates temporal awareness along 6 key\ndimensions and 11 challenging tasks: cognition, awareness, trustworthiness,\nunderstanding, reasoning, and robustness. MINED is constructed from Wikipedia\nby two professional annotators, containing 2,104 time-sensitive knowledge\nsamples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED\nshows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07,\nwhile most open-source LMMs still lack time understanding ability. Meanwhile,\nLMMs perform best on organization knowledge, whereas their performance is\nweakest on sport. To address these challenges, we investigate the feasibility\nof updating time-sensitive knowledge in LMMs through knowledge editing methods\nand observe that LMMs can effectively update knowledge via knowledge editing\nmethods in single editing scenarios.\n","authors":["Kailin Jiang","Ning Jiang","Yuntao Du","Yuchen Ren","Yuchen Li","Yifan Gao","Jinhe Bi","Yunpu Ma","Qingqing Liu","Xianhao Wang","Yifan Jia","Hongbo Jiang","Yaocong Hu","Bin Li","Lei Liu"],"pdf_url":"https://arxiv.org/pdf/2510.19457v2.pdf","comment":"project page:https://mined-lmm.github.io/"},{"id":"http://arxiv.org/abs/2510.24023v1","updated":"2025-10-28T03:06:07Z","published":"2025-10-28T03:06:07Z","title":"Success and Cost Elicit Convention Formation for Efficient Communication","summary":"  Humans leverage shared conversational context to become increasingly\nsuccessful and efficient at communicating over time. One manifestation of this\nis the formation of ad hoc linguistic conventions, which allow people to\ncoordinate on short, less costly utterances that are understood using shared\nconversational context. We present a method to train large multimodal models to\nform conventions, enabling efficient communication. Our approach uses simulated\nreference games between models, and requires no additional human-produced data.\nIn repeated reference games involving photographs and tangram images, our\nmethod enables models to communicate efficiently with people: reducing the\nmessage length by up to 41% while increasing success by 15% over the course of\nthe interaction. Human listeners respond faster when interacting with our model\nthat forms conventions. We also show that training based on success or cost\nalone is insufficient - both are necessary to elicit convention formation.\n","authors":["Saujas Vaduguru","Yilun Hua","Yoav Artzi","Daniel Fried"],"pdf_url":"https://arxiv.org/pdf/2510.24023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24021v1","updated":"2025-10-28T03:02:22Z","published":"2025-10-28T03:02:22Z","title":"SpecKD: Speculative Decoding for Effective Knowledge Distillation of\n  LLMs","summary":"  Knowledge Distillation (KD) has become a cornerstone technique for\ncompressing Large Language Models (LLMs) into smaller, more efficient student\nmodels. However, conventional KD approaches typically apply the distillation\nloss uniformly across all tokens, regardless of the teacher's confidence. This\nindiscriminate mimicry can introduce noise, as the student is forced to learn\nfrom the teacher's uncertain or high-entropy predictions, which may ultimately\nharm student performance-especially when the teacher is much larger and more\npowerful. To address this, we propose Speculative Knowledge Distillation\n(SpecKD), a novel, plug-and-play framework that introduces a dynamic,\ntoken-level gating mechanism inspired by the \"propose-and-verify\" paradigm of\nspeculative decoding. At each step, the student's token proposal is verified\nagainst the teacher's distribution; the distillation loss is selectively\napplied only to \"accepted\" tokens, while \"rejected\" tokens are masked out.\nExtensive experiments on diverse text generation tasks show that SpecKD\nconsistently and significantly outperforms strong KD baselines, leading to more\nstable training and more capable student models, and achieving state-of-the-art\nresults.\n","authors":["Haiduo Huang","Jiangcheng Song","Yadong Zhang","Pengju Ren"],"pdf_url":"https://arxiv.org/pdf/2510.24021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24020v1","updated":"2025-10-28T03:00:35Z","published":"2025-10-28T03:00:35Z","title":"Teaching LLMs to Abstain via Fine-Grained Semantic Confidence Reward","summary":"  Mitigating hallucinations in Large Language Models (LLMs) is critical for\ntheir reliable deployment. Existing methods typically fine-tune LLMs to abstain\nfrom answering questions beyond their knowledge scope. However, these methods\noften rely on coarse-grained signals to guide LLMs to abstain, such as overall\nconfidence or uncertainty scores on multiple sampled answers, which may result\nin an imprecise awareness of the model's own knowledge boundaries. To this end,\nwe propose a novel reinforcement learning framework built on\n$\\textbf{\\underline{Fi}ne-grained \\underline{S}emantic \\underline{Co}nfidence\n\\underline{Re}ward (\\Ours)}$, which guides LLMs to abstain via sample-specific\nconfidence. Specifically, our method operates by sampling multiple candidate\nanswers and conducting semantic clustering, then training the LLM to retain\nanswers within high-confidence clusters and discard those within low-confidence\nones, thereby promoting accurate post-hoc abstention. Additionally, we propose\na new metric for evaluating the reliability of abstention fine-tuning tasks\nmore comprehensively. Our method significantly enhances reliability in both\nin-domain and out-of-distribution benchmarks.\n","authors":["Hao An","Yang Xu"],"pdf_url":"https://arxiv.org/pdf/2510.24020v1.pdf","comment":"23pages, 4figures"},{"id":"http://arxiv.org/abs/2510.24014v1","updated":"2025-10-28T02:49:40Z","published":"2025-10-28T02:49:40Z","title":"TEXT2DB: Integration-Aware Information Extraction with Large Language\n  Model Agents","summary":"  The task of information extraction (IE) is to extract structured knowledge\nfrom text. However, it is often not straightforward to utilize IE output due to\nthe mismatch between the IE ontology and the downstream application needs. We\npropose a new formulation of IE TEXT2DB that emphasizes the integration of IE\noutput and the target database (or knowledge base). Given a user instruction, a\ndocument set, and a database, our task requires the model to update the\ndatabase with values from the document set to satisfy the user instruction.\nThis task requires understanding user instructions for what to extract and\nadapting to the given DB/KB schema for how to extract on the fly. To evaluate\nthis new task, we introduce a new benchmark featuring common demands such as\ndata infilling, row population, and column addition. In addition, we propose an\nLLM agent framework OPAL (Observe-PlanAnalyze LLM) which includes an Observer\ncomponent that interacts with the database, the Planner component that\ngenerates a code-based plan with calls to IE models, and the Analyzer component\nthat provides feedback regarding code quality before execution. Experiments\nshow that OPAL can successfully adapt to diverse database schemas by generating\ndifferent code plans and calling the required IE models. We also highlight\ndifficult cases such as dealing with large databases with complex dependencies\nand extraction hallucination, which we believe deserve further investigation.\nSource code: https://github.com/yzjiao/Text2DB\n","authors":["Yizhu Jiao","Sha Li","Sizhe Zhou","Heng Ji","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2510.24014v1.pdf","comment":"ACL 2025. Source code: https://github.com/yzjiao/Text2DB"},{"id":"http://arxiv.org/abs/2410.02787v2","updated":"2025-10-28T02:45:04Z","published":"2024-09-18T02:29:00Z","title":"Navigation with VLM framework: Towards Going to Any Language","summary":"  Navigating towards fully open language goals and exploring open scenes in an\nintelligent way have always raised significant challenges. Recently, Vision\nLanguage Models (VLMs) have demonstrated remarkable capabilities to reason with\nboth language and visual data. Although many works have focused on leveraging\nVLMs for navigation in open scenes, they often require high computational cost,\nrely on object-centric approaches, or depend on environmental priors in\ndetailed human instructions. We introduce Navigation with VLM (NavVLM), a\ntraining-free framework that harnesses open-source VLMs to enable robots to\nnavigate effectively, even for human-friendly language goal such as abstract\nplaces, actions, or specific objects in open scenes. NavVLM leverages the VLM\nas its cognitive core to perceive environmental information and constantly\nprovides exploration guidance achieving intelligent navigation with only a neat\ntarget rather than a detailed instruction with environment prior. We evaluated\nand validated NavVLM in both simulation and real-world experiments. In\nsimulation, our framework achieves state-of-the-art performance in Success\nweighted by Path Length (SPL) on object-specifc tasks in richly detailed\nenvironments from Matterport 3D (MP3D), Habitat Matterport 3D (HM3D) and\nGibson. With navigation episode reported, NavVLM demonstrates the capabilities\nto navigate towards any open-set languages. In real-world validation, we\nvalidated our framework's effectiveness in real-world robot at indoor scene.\n","authors":["Zecheng Yin","Chonghao Cheng","and Yao Guo","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2410.02787v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2412.12679v2","updated":"2025-10-28T02:20:41Z","published":"2024-12-17T08:47:41Z","title":"Discourse Features Enhance Detection of Document-Level Machine-Generated\n  Content","summary":"  The availability of high-quality APIs for Large Language Models (LLMs) has\nfacilitated the widespread creation of Machine-Generated Content (MGC), posing\nchallenges such as academic plagiarism and the spread of misinformation.\nExisting MGC detectors often focus solely on surface-level information,\noverlooking implicit and structural features. This makes them susceptible to\ndeception by surface-level sentence patterns, particularly for longer texts and\nin texts that have been subsequently paraphrased. To overcome these challenges,\nwe introduce novel methodologies and datasets. Besides the publicly available\ndataset Plagbench, we developed the paraphrased Long-Form Question and Answer\n(paraLFQA) and paraphrased Writing Prompts (paraWP) datasets using GPT and\nDIPPER, a discourse paraphrasing tool, by extending artifacts from their\noriginal versions. To better capture the structure of longer texts at document\nlevel, we propose DTransformer, a model that integrates discourse analysis\nthrough PDTB preprocessing to encode structural features. It results in\nsubstantial performance gains across both datasets - 15.5% absolute improvement\non paraLFQA, 4% absolute improvement on paraWP, and 1.5% absolute improvemene\non M4 compared to SOTA approaches. The data and code are available at:\nhttps://github.com/myxp-lyp/Discourse-Features-Enhance-Detection-of-Document-Level-Machine-Generated-Content.git.\n","authors":["Yupei Li","Manuel Milling","Lucia Specia","BjÃ¶rn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2412.12679v2.pdf","comment":"Accepted by IJCNN 2025"},{"id":"http://arxiv.org/abs/2510.24003v1","updated":"2025-10-28T02:18:09Z","published":"2025-10-28T02:18:09Z","title":"META-RAG: Meta-Analysis-Inspired Evidence-Re-Ranking Method for\n  Retrieval-Augmented Generation in Evidence-Based Medicine","summary":"  Evidence-based medicine (EBM) holds a crucial role in clinical application.\nGiven suitable medical articles, doctors effectively reduce the incidence of\nmisdiagnoses. Researchers find it efficient to use large language models (LLMs)\ntechniques like RAG for EBM tasks. However, the EBM maintains stringent\nrequirements for evidence, and RAG applications in EBM struggle to efficiently\ndistinguish high-quality evidence. Therefore, inspired by the meta-analysis\nused in EBM, we provide a new method to re-rank and filter the medical\nevidence. This method presents multiple principles to filter the best evidence\nfor LLMs to diagnose. We employ a combination of several EBM methods to emulate\nthe meta-analysis, which includes reliability analysis, heterogeneity analysis,\nand extrapolation analysis. These processes allow the users to retrieve the\nbest medical evidence for the LLMs. Ultimately, we evaluate these high-quality\narticles and show an accuracy improvement of up to 11.4% in our experiments and\nresults. Our method successfully enables RAG to extract higher-quality and more\nreliable evidence from the PubMed dataset. This work can reduce the infusion of\nincorrect knowledge into responses and help users receive more effective\nreplies.\n","authors":["Mengzhou Sun","Sendong Zhao","Jianyu Chen","Haochun Wang","Bin Qin"],"pdf_url":"https://arxiv.org/pdf/2510.24003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23998v1","updated":"2025-10-28T02:01:05Z","published":"2025-10-28T02:01:05Z","title":"PICOs-RAG: PICO-supported Query Rewriting for Retrieval-Augmented\n  Generation in Evidence-Based Medicine","summary":"  Evidence-based medicine (EBM) research has always been of paramount\nimportance. It is important to find appropriate medical theoretical support for\nthe needs from physicians or patients to reduce the occurrence of medical\naccidents. This process is often carried out by human querying relevant\nliterature databases, which lacks objectivity and efficiency. Therefore,\nresearchers utilize retrieval-augmented generation (RAG) to search for evidence\nand generate responses automatically. However, current RAG methods struggle to\nhandle complex queries in real-world clinical scenarios. For example, when\nqueries lack certain information or use imprecise language, the model may\nretrieve irrelevant evidence and generate unhelpful answers. To address this\nissue, we present the PICOs-RAG to expand the user queries into a better\nformat. Our method can expand and normalize the queries into professional ones\nand use the PICO format, a search strategy tool present in EBM, to extract the\nmost important information used for retrieval. This approach significantly\nenhances retrieval efficiency and relevance, resulting in up to an 8.8\\%\nimprovement compared to the baseline evaluated by our method. Thereby the\nPICOs-RAG improves the performance of the large language models into a helpful\nand reliable medical assistant in EBM.\n","authors":["Mengzhou Sun","Sendong Zhao","Jianyu Chen","Bin Qin"],"pdf_url":"https://arxiv.org/pdf/2510.23998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23995v1","updated":"2025-10-28T01:57:40Z","published":"2025-10-28T01:57:40Z","title":"M-Eval: A Heterogeneity-Based Framework for Multi-evidence Validation in\n  Medical RAG Systems","summary":"  Retrieval-augmented Generation (RAG) has demonstrated potential in enhancing\nmedical question-answering systems through the integration of large language\nmodels (LLMs) with external medical literature. LLMs can retrieve relevant\nmedical articles to generate more professional responses efficiently. However,\ncurrent RAG applications still face problems. They generate incorrect\ninformation, such as hallucinations, and they fail to use external knowledge\ncorrectly. To solve these issues, we propose a new method named M-Eval. This\nmethod is inspired by the heterogeneity analysis approach used in\nEvidence-Based Medicine (EBM). Our approach can check for factual errors in RAG\nresponses using evidence from multiple sources. First, we extract additional\nmedical literature from external knowledge bases. Then, we retrieve the\nevidence documents generated by the RAG system. We use heterogeneity analysis\nto check whether the evidence supports different viewpoints in the response. In\naddition to verifying the accuracy of the response, we also assess the\nreliability of the evidence provided by the RAG system. Our method shows an\nimprovement of up to 23.31% accuracy across various LLMs. This work can help\ndetect errors in current RAG-based medical systems. It also makes the\napplications of LLMs more reliable and reduces diagnostic errors.\n","authors":["Mengzhou Sun","Sendong Zhao","Jianyu Chen","Haochun Wang","Bin Qin"],"pdf_url":"https://arxiv.org/pdf/2510.23995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.19467v3","updated":"2025-10-28T01:55:42Z","published":"2025-04-28T04:13:18Z","title":"BRIDGE: Benchmarking Large Language Models for Understanding Real-world\n  Clinical Practice Text","summary":"  Large language models (LLMs) hold great promise for medical applications and\nare evolving rapidly, with new models being released at an accelerated pace.\nHowever, benchmarking on large-scale real-world data such as electronic health\nrecords (EHRs) is critical, as clinical decisions are directly informed by\nthese sources, yet current evaluations remain limited. Most existing benchmarks\nrely on medical exam-style questions or PubMed-derived text, failing to capture\nthe complexity of real-world clinical data. Others focus narrowly on specific\napplication scenarios, limiting their generalizability across broader clinical\nuse. To address this gap, we present BRIDGE, a comprehensive multilingual\nbenchmark comprising 87 tasks sourced from real-world clinical data sources\nacross nine languages. It covers eight major task types spanning the entire\ncontinuum of patient care across six clinical stages and 20 representative\napplications, including triage and referral, consultation, information\nextraction, diagnosis, prognosis, and billing coding, and involves 14 clinical\nspecialties. We systematically evaluated 95 LLMs (including DeepSeek-R1,\nGPT-4o, Gemini series, and Qwen3 series) under various inference strategies.\nOur results reveal substantial performance variation across model sizes,\nlanguages, natural language processing tasks, and clinical specialties.\nNotably, we demonstrate that open-source LLMs can achieve performance\ncomparable to proprietary models, while medically fine-tuned LLMs based on\nolder architectures often underperform versus updated general-purpose models.\nThe BRIDGE and its corresponding leaderboard serve as a foundational resource\nand a unique reference for the development and evaluation of new LLMs in\nreal-world clinical text understanding.\n  The BRIDGE leaderboard:\nhttps://huggingface.co/spaces/YLab-Open/BRIDGE-Medical-Leaderboard\n","authors":["Jiageng Wu","Bowen Gu","Ren Zhou","Kevin Xie","Doug Snyder","Yixing Jiang","Valentina Carducci","Richard Wyss","Rishi J Desai","Emily Alsentzer","Leo Anthony Celi","Adam Rodman","Sebastian Schneeweiss","Jonathan H. Chen","Santiago Romero-Brufau","Kueiyu Joshua Lin","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2504.19467v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.03490v2","updated":"2025-10-28T01:07:57Z","published":"2025-10-03T20:15:24Z","title":"SEER: The Span-based Emotion Evidence Retrieval Benchmark","summary":"  We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to\ntest Large Language Models' (LLMs) ability to identify the specific spans of\ntext that express emotion. Unlike traditional emotion recognition tasks that\nassign a single label to an entire sentence, SEER targets the underexplored\ntask of emotion evidence detection: pinpointing which exact phrases convey\nemotion. This span-level approach is crucial for applications like empathetic\ndialogue and clinical support, which need to know how emotion is expressed, not\njust what the emotion is. SEER includes two tasks: identifying emotion evidence\nwithin a single sentence, and identifying evidence across a short passage of\nfive consecutive sentences. It contains new annotations for both emotion and\nemotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs\nand find that, while some models approach average human performance on\nsingle-sentence inputs, their accuracy degrades in longer passages. Our error\nanalysis reveals key failure modes, including overreliance on emotion keywords\nand false positives in neutral text.\n","authors":["Aneesha Sampath","Oya Aran","Emily Mower Provost"],"pdf_url":"https://arxiv.org/pdf/2510.03490v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00481v2","updated":"2025-10-28T00:59:36Z","published":"2025-05-31T09:21:57Z","title":"PVP: An Image Dataset for Personalized Visual Persuasion with Persuasion\n  Strategies, Viewer Characteristics, and Persuasiveness Ratings","summary":"  Visual persuasion, which uses visual elements to influence cognition and\nbehaviors, is crucial in fields such as advertising and political\ncommunication. With recent advancements in artificial intelligence, there is\ngrowing potential to develop persuasive systems that automatically generate\npersuasive images tailored to individuals. However, a significant bottleneck in\nthis area is the lack of comprehensive datasets that connect the persuasiveness\nof images with the personal information about those who evaluated the images.\nTo address this gap and facilitate technological advancements in personalized\nvisual persuasion, we release the Personalized Visual Persuasion (PVP) dataset,\ncomprising 28,454 persuasive images across 596 messages and 9 persuasion\nstrategies. Importantly, the PVP dataset provides persuasiveness scores of\nimages evaluated by 2,521 human annotators, along with their demographic and\npsychological characteristics (personality traits and values). We demonstrate\nthe utility of our dataset by developing a persuasive image generator and an\nautomated evaluator, and establish benchmark baselines. Our experiments reveal\nthat incorporating psychological characteristics enhances the generation and\nevaluation of persuasive images, providing valuable insights for personalized\nvisual persuasion.\n","authors":["Junseo Kim","Jongwook Han","Dongmin Choi","Jongwook Yoon","Eun-Ju Lee","Yohan Jo"],"pdf_url":"https://arxiv.org/pdf/2506.00481v2.pdf","comment":"ACL 2025 Main. Code and dataset are released at:\n  https://github.com/holi-lab/PVP_Personalized_Visual_Persuasion"},{"id":"http://arxiv.org/abs/2510.23969v1","updated":"2025-10-28T00:50:15Z","published":"2025-10-28T00:50:15Z","title":"emg2speech: synthesizing speech from electromyography using\n  self-supervised speech models","summary":"  We present a neuromuscular speech interface that translates electromyographic\n(EMG) signals collected from orofacial muscles during speech articulation\ndirectly into audio. We show that self-supervised speech (SS) representations\nexhibit a strong linear relationship with the electrical power of muscle action\npotentials: SS features can be linearly mapped to EMG power with a correlation\nof $r = 0.85$. Moreover, EMG power vectors corresponding to different\narticulatory gestures form structured and separable clusters in feature space.\nThis relationship: $\\text{SS features}$ $\\xrightarrow{\\texttt{linear mapping}}$\n$\\text{EMG power}$ $\\xrightarrow{\\texttt{gesture-specific clustering}}$\n$\\text{articulatory movements}$, highlights that SS models implicitly encode\narticulatory mechanisms. Leveraging this property, we directly map EMG signals\nto SS feature space and synthesize speech, enabling end-to-end EMG-to-speech\ngeneration without explicit articulatory models and vocoder training.\n","authors":["Harshavardhana T. Gowda","Lee M. Miller"],"pdf_url":"https://arxiv.org/pdf/2510.23969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23949v1","updated":"2025-10-28T00:05:00Z","published":"2025-10-28T00:05:00Z","title":"Uncovering the Potential Risks in Unlearning: Danger of English-only\n  Unlearning in Multilingual LLMs","summary":"  There have been a couple of studies showing that attempting to erase\nmultilingual knowledge using only English data is insufficient for multilingual\nLLMs. However, their analyses remain highly performance-oriented. In this\npaper, we switch the point of view to evaluation, and address an additional\nblind spot which reveals itself when the multilingual LLM is fully finetuned\nwith parallel multilingual dataset before unlearning. Here, language confusion\noccurs whereby a model responds in language different from that of the input\nprompt. Language confusion is a problematic phenomenon in unlearning, causing\nthe standard reference-based metrics to fail. We tackle this phenomenon in\nthree steps: (1) introduce N-gram-based Language-Mix (N-Mix) score to\nquantitatively show the language confusion is pervasive and consistent in\nmultilingual LLMs, (2) demonstrate that reference-based metrics result in false\nnegatives when N-Mix score is high, and(3) suggest the need of new type of\nunlearning evaluation that can directly assess the content of the generated\nsentences. We call this type of metrics as semantic-based metric.\n","authors":["Kyomin Hwang","Hyeonjin Kim","Seungyeon Kim","Sunghyun Wee","Nojun Kwak"],"pdf_url":"https://arxiv.org/pdf/2510.23949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25017v1","updated":"2025-10-28T22:33:14Z","published":"2025-10-28T22:33:14Z","title":"StorageXTuner: An LLM Agent-Driven Automatic Tuning Framework for\n  Heterogeneous Storage Systems","summary":"  Automatically configuring storage systems is hard: parameter spaces are large\nand conditions vary across workloads, deployments, and versions. Heuristic and\nML tuners are often system specific, require manual glue, and degrade under\nchanges. Recent LLM-based approaches help but usually treat tuning as a\nsingle-shot, system-specific task, which limits cross-system reuse, constrains\nexploration, and weakens validation. We present StorageXTuner, an LLM\nagent-driven auto-tuning framework for heterogeneous storage engines.\nStorageXTuner separates concerns across four agents - Executor (sandboxed\nbenchmarking), Extractor (performance digest), Searcher (insight-guided\nconfiguration exploration), and Reflector (insight generation and management).\nThe design couples an insight-driven tree search with layered memory that\npromotes empirically validated insights and employs lightweight checkers to\nguard against unsafe actions. We implement a prototype and evaluate it on\nRocksDB, LevelDB, CacheLib, and MySQL InnoDB with YCSB, MixGraph, and TPC-H/C.\nRelative to out-of-the-box settings and to ELMo-Tune, StorageXTuner reaches up\nto 575% and 111% higher throughput, reduces p99 latency by as much as 88% and\n56%, and converges with fewer trials.\n","authors":["Qi Lin","Zhenyu Zhang","Viraj Thakkar","Zhenjie Sun","Mai Zheng","Zhichao Cao"],"pdf_url":"https://arxiv.org/pdf/2510.25017v1.pdf","comment":"ArXiv version; Affiliations: Arizona State University (Lin, Zhang,\n  Thakkar, Sun, Cao) and Iowa State University (Zheng)"},{"id":"http://arxiv.org/abs/2510.25013v1","updated":"2025-10-28T22:25:19Z","published":"2025-10-28T22:25:19Z","title":"Emergence of Minimal Circuits for Indirect Object Identification in\n  Attention-Only Transformers","summary":"  Mechanistic interpretability aims to reverse-engineer large language models\n(LLMs) into human-understandable computational circuits. However, the\ncomplexity of pretrained models often obscures the minimal mechanisms required\nfor specific reasoning tasks. In this work, we train small, attention-only\ntransformers from scratch on a symbolic version of the Indirect Object\nIdentification (IOI) task -- a benchmark for studying coreference -- like\nreasoning in transformers. Surprisingly, a single-layer model with only two\nattention heads achieves perfect IOI accuracy, despite lacking MLPs and\nnormalization layers. Through residual stream decomposition, spectral analysis,\nand embedding interventions, we find that the two heads specialize into\nadditive and contrastive subcircuits that jointly implement IOI resolution.\nFurthermore, we show that a two-layer, one-head model achieves similar\nperformance by composing information across layers through query-value\ninteractions. These results demonstrate that task-specific training induces\nhighly interpretable, minimal circuits, offering a controlled testbed for\nprobing the computational foundations of transformer reasoning.\n","authors":["Rabin Adhikari"],"pdf_url":"https://arxiv.org/pdf/2510.25013v1.pdf","comment":"9 pages, 10 figures"},{"id":"http://arxiv.org/abs/2507.22811v2","updated":"2025-10-28T21:49:54Z","published":"2025-07-30T16:29:47Z","title":"DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph","summary":"  In this work we present an entity linker for DBLP's 2025 version of RDF-based\nKnowledge Graph. Compared to the 2022 version, DBLP now considers publication\nvenues as a new entity type called dblp:Stream. In the earlier version of\nDBLPLink, we trained KG-embeddings and re-rankers on a dataset to produce\nentity linkings. In contrast, in this work, we develop a zero-shot entity\nlinker using LLMs using a novel method, where we re-rank candidate entities\nbased on the log-probabilities of the \"yes\" token output at the penultimate\nlayer of the LLM.\n","authors":["Debayan Banerjee","Tilahun Abedissa Taffa","Ricardo Usbeck"],"pdf_url":"https://arxiv.org/pdf/2507.22811v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24992v1","updated":"2025-10-28T21:43:45Z","published":"2025-10-28T21:43:45Z","title":"POWSM: A Phonetic Open Whisper-Style Speech Foundation Model","summary":"  Recent advances in spoken language processing have led to substantial\nprogress in phonetic tasks such as automatic speech recognition (ASR), phone\nrecognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme\nconversion (P2G). Despite their conceptual similarity, these tasks have largely\nbeen studied in isolation, each relying on task-specific architectures and\ndatasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech\nModel), the first unified framework capable of jointly performing multiple\nphone-related tasks. POWSM enables seamless conversion between audio, text\n(graphemes), and phones, opening up new possibilities for universal and\nlow-resource speech processing. Our model outperforms or matches specialized PR\nmodels of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P,\nP2G, and ASR. Our training data, code and models are released to foster open\nscience.\n","authors":["Chin-Jou Li","Kalvin Chang","Shikhar Bharadwaj","Eunjung Yeo","Kwanghee Choi","Jian Zhu","David Mortensen","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2510.24992v1.pdf","comment":"14 pages, under review"},{"id":"http://arxiv.org/abs/2505.10844v4","updated":"2025-10-28T21:30:31Z","published":"2025-05-16T04:23:34Z","title":"Creativity or Brute Force? Using Brainteasers as a Window into the\n  Problem-Solving Abilities of Large Language Models","summary":"  Accuracy remains a standard metric for evaluating AI systems, but it offers\nlimited insight into how models arrive at their solutions. In this work, we\nintroduce a benchmark based on brainteasers written in long narrative form to\nprobe more deeply into the types of reasoning strategies that models use.\nBrainteasers are well-suited for this goal because they can be solved with\nmultiple approaches, such as a few-step solution that uses a creative insight\nor a longer solution that uses more brute force. We investigate large language\nmodels (LLMs) across multiple layers of reasoning, focusing not only on\ncorrectness but also on the quality and creativity of their solutions. We\ninvestigate many aspects of the reasoning process: (1) semantic parsing of the\nbrainteasers into precise mathematical competition style formats; (2)\ngenerating solutions from these mathematical forms; (3) self-correcting\nsolutions based on gold solutions; (4) producing step-by-step sketches of\nsolutions; and (5) making use of hints. We find that LLMs are in many cases\nable to find creative, insightful solutions to brainteasers, suggesting that\nthey capture some of the capacities needed to solve novel problems in creative\nways. Nonetheless, there also remain situations where they rely on brute force\ndespite the availability of more efficient, creative solutions, highlighting a\npotential direction for improvement in the reasoning abilities of LLMs.\n","authors":["Simeng Han","Howard Dai","Stephen Xia","Grant Zhang","Chen Liu","Lichang Chen","Hoang Huy Nguyen","Hongyuan Mei","Jiayuan Mao","R. Thomas McCoy"],"pdf_url":"https://arxiv.org/pdf/2505.10844v4.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2405.05583v3","updated":"2025-10-28T21:27:32Z","published":"2024-05-09T07:15:19Z","title":"OpenFactCheck: Building, Benchmarking Customized Fact-Checking Systems\n  and Evaluating the Factuality of Claims and LLMs","summary":"  The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for mechanisms to verify the factual accuracy of\ntheir outputs. Difficulties lie in assessing the factuality of free-form\nresponses in open domains. Also, different papers use disparate evaluation\nbenchmarks and measurements, which renders them hard to compare and hampers\nfuture progress. To mitigate these issues, we propose OpenFactCheck, a unified\nframework for building customized automatic fact-checking systems, benchmarking\ntheir accuracy, evaluating factuality of LLMs, and verifying claims in a\ndocument. OpenFactCheck consists of three modules: (i) CUSTCHECKER allows users\nto easily customize an automatic fact-checker and verify the factual\ncorrectness of documents and claims, (ii) LLMEVAL, a unified evaluation\nframework assesses LLM's factuality ability from various perspectives fairly,\nand (iii) CHECKEREVAL is an extensible solution for gauging the reliability of\nautomatic fact-checkers' verification results using human-annotated datasets.\nData and code are publicly available at\nhttps://github.com/yuxiaw/openfactcheck.\n","authors":["Yuxia Wang","Minghan Wang","Hasan Iqbal","Georgi Georgiev","Jiahui Geng","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2405.05583v3.pdf","comment":"23 pages, 8 tables, 11 figures, Published In Proceedings of the 31st\n  International Conference on Computational Linguistics 2025"},{"id":"http://arxiv.org/abs/2408.11832v3","updated":"2025-10-28T21:14:55Z","published":"2024-08-06T15:49:58Z","title":"OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs","summary":"  The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for automatic tools to check the factual accuracy\nof their outputs, as LLMs often hallucinate. This is difficult as it requires\nassessing the factuality of free-form open-domain responses. While there has\nbeen a lot of research on this topic, different papers use different evaluation\nbenchmarks and measures, which makes them hard to compare and hampers future\nprogress. To mitigate these issues, we developed OpenFactCheck, a unified\nframework, with three modules: (i) RESPONSEEVAL, which allows users to easily\ncustomize an automatic fact-checking system and to assess the factuality of all\nclaims in an input document using that system, (ii) LLMEVAL, which assesses the\noverall factuality of an LLM, and (iii) CHECKEREVAL, a module to evaluate\nautomatic fact-checking systems. OpenFactCheck is open-sourced\n(https://github.com/mbzuai-nlp/openfactcheck) and publicly released as a Python\nlibrary (https://pypi.org/project/openfactcheck/) and also as a web service\n(http://app.openfactcheck.com). A video describing the system is available at\nhttps://youtu.be/-i9VKL0HleI.\n","authors":["Hasan Iqbal","Yuxia Wang","Minghan Wang","Georgi Georgiev","Jiahui Geng","Iryna Gurevych","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2408.11832v3.pdf","comment":"11 pages, 4 Figures, 3 Tables, Published In Proceedings of The 2024\n  Conference on Empirical Methods in Natural Language Processing"},{"id":"http://arxiv.org/abs/2510.14040v2","updated":"2025-10-28T21:10:02Z","published":"2025-10-15T19:23:12Z","title":"Quantifying Phonosemantic Iconicity Distributionally in 6 Languages","summary":"  Language is, as commonly theorized, largely arbitrary. Yet, systematic\nrelationships between phonetics and semantics have been observed in many\nspecific cases. To what degree could those systematic relationships manifest\nthemselves in large scale, quantitative investigations--both in previously\nidentified and unidentified phenomena? This work undertakes a distributional\napproach to quantifying phonosemantic iconicity at scale across 6 diverse\nlanguages (English, Spanish, Hindi, Finnish, Turkish, and Tamil). In each\nlanguage, we analyze the alignment of morphemes' phonetic and semantic\nsimilarity spaces with a suite of statistical measures, and discover an array\nof interpretable phonosemantic alignments not previously identified in the\nliterature, along with crosslinguistic patterns. We also analyze 5 previously\nhypothesized phonosemantic alignments, finding support for some such alignments\nand mixed results for others.\n","authors":["George Flint","Kaustubh Kislay"],"pdf_url":"https://arxiv.org/pdf/2510.14040v2.pdf","comment":"IJCNLP-AACL 2025 Main Conference Proceedings"},{"id":"http://arxiv.org/abs/2510.24966v1","updated":"2025-10-28T20:55:58Z","published":"2025-10-28T20:55:58Z","title":"Sequences of Logits Reveal the Low Rank Structure of Language Models","summary":"  A major problem in the study of large language models is to understand their\ninherent low-dimensional structure. We introduce an approach to study the\nlow-dimensional structure of language models at a model-agnostic level: as\nsequential probabilistic models. We first empirically demonstrate that a wide\nrange of modern language models exhibit low-rank structure: in particular,\nmatrices built from the model's logits for varying sets of prompts and\nresponses have low approximate rank. We then show that this low-rank structure\ncan be leveraged for generation -- in particular, we can generate a response to\na target prompt using a linear combination of the model's outputs on unrelated,\nor even nonsensical prompts.\n  On the theoretical front, we observe that studying the approximate rank of\nlanguage models in the sense discussed above yields a simple universal\nabstraction whose theoretical predictions parallel our experiments. We then\nanalyze the representation power of the abstraction and give provable learning\nguarantees.\n","authors":["Noah Golowich","Allen Liu","Abhishek Shetty"],"pdf_url":"https://arxiv.org/pdf/2510.24966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15063v2","updated":"2025-10-28T20:55:49Z","published":"2025-05-21T03:31:44Z","title":"UrduFactCheck: An Agentic Fact-Checking Framework for Urdu with Evidence\n  Boosting and Benchmarking","summary":"  The rapid adoption of Large Language Models (LLMs) has raised important\nconcerns about the factual reliability of their outputs, particularly in\nlow-resource languages such as Urdu. Existing automated fact-checking systems\nare predominantly developed for English, leaving a significant gap for the more\nthan 200 million Urdu speakers worldwide. In this work, we present\nUrduFactBench and UrduFactQA, two novel hand-annotated benchmarks designed to\nenable fact-checking and factual consistency evaluation in Urdu. While\nUrduFactBench focuses on claim verification, UrduFactQA targets the factuality\nof LLMs in question answering. These resources, the first of their kind for\nUrdu, were developed through a multi-stage annotation process involving native\nUrdu speakers. To complement these benchmarks, we introduce UrduFactCheck, a\nmodular fact-checking framework that incorporates both monolingual and\ntranslation-based evidence retrieval strategies to mitigate the scarcity of\nhigh-quality Urdu evidence. Leveraging these resources, we conduct an extensive\nevaluation of twelve LLMs and demonstrate that translation-augmented pipelines\nconsistently enhance performance compared to monolingual ones. Our findings\nreveal persistent challenges for open-source LLMs in Urdu and underscore the\nimportance of developing targeted resources. All code and data are publicly\navailable at https://github.com/mbzuai-nlp/UrduFactCheck.\n","authors":["Sarfraz Ahmad","Hasan Iqbal","Momina Ahsan","Numaan Naeem","Muhammad Ahsan Riaz Khan","Arham Riaz","Muhammad Arslan Manzoor","Yuxia Wang","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2505.15063v2.pdf","comment":"15 pages, 4 figures, 5 tables, 6 Listings, Published in Proceeding of\n  The 2025 Conference on Empirical Methods in Natural Language Processing"},{"id":"http://arxiv.org/abs/2510.24963v1","updated":"2025-10-28T20:51:01Z","published":"2025-10-28T20:51:01Z","title":"Language Model Behavioral Phases are Consistent Across Architecture,\n  Training Data, and Scale","summary":"  We show that across architecture (Transformer vs. Mamba vs. RWKV), training\ndataset (OpenWebText vs. The Pile), and scale (14 million parameters to 12\nbillion parameters), autoregressive language models exhibit highly consistent\npatterns of change in their behavior over the course of pretraining. Based on\nour analysis of over 1,400 language model checkpoints on over 110,000 tokens of\nEnglish, we find that up to 98% of the variance in language model behavior at\nthe word level can be explained by three simple heuristics: the unigram\nprobability (frequency) of a given word, the $n$-gram probability of the word,\nand the semantic similarity between the word and its context. Furthermore, we\nsee consistent behavioral phases in all language models, with their predicted\nprobabilities for words overfitting to those words' $n$-gram probabilities for\nincreasing $n$ over the course of training. Taken together, these results\nsuggest that learning in neural language models may follow a similar trajectory\nirrespective of model details.\n","authors":["James A. Michaelov","Roger P. Levy","Benjamin K. Bergen"],"pdf_url":"https://arxiv.org/pdf/2510.24963v1.pdf","comment":"To be presented at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2509.23234v4","updated":"2025-10-28T20:33:49Z","published":"2025-09-27T10:33:41Z","title":"p-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding","summary":"  Obtaining high-quality outputs from Large Language Models (LLMs) often\ndepends upon the choice of a sampling-based decoding strategy to\nprobabilistically choose the next token at each generation step. While a\nvariety of such sampling methods have been proposed, their performance can be\nsensitive to the selection of hyperparameters which may require different\nsettings depending upon the generation task and temperature configuration. In\nthis work, we introduce $p$-less sampling: an information-theoretic approach to\nsampling which dynamically sets a truncation threshold at each decoding step\nbased on the entire token probability distribution. Unlike existing methods,\n$p$-less sampling has no hyperparameters and consistently produces high-quality\noutputs as temperature increases. We provide theoretical perspectives on\n$p$-less sampling to ground our proposed method and conduct experiments to\nempirically validate its effectiveness across a range of math, logical\nreasoning, and creative writing tasks. Our results demonstrate how $p$-less\nsampling consistently outperforms existing sampling approaches while exhibiting\nmuch less degradation in text quality at higher temperature values. We further\nshow how $p$-less achieves greater inference-time efficiency than alternative\nmethods through lower average token sampling times and shorter generation\nlengths, without sacrificing accuracy. Finally, we provide analyses to\nhighlight the benefits of $p$-less through qualitative examples, case studies,\nand diversity assessments. The code is available at\nhttps://github.com/ryttry/p-less .\n","authors":["Runyan Tan","Shuang Wu","Phillip Howard"],"pdf_url":"https://arxiv.org/pdf/2509.23234v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02745v3","updated":"2025-10-28T20:31:06Z","published":"2024-03-05T07:58:12Z","title":"CURATRON: Complete and Robust Preference Data for Rigorous Alignment of\n  Large Language Models","summary":"  This paper addresses the challenges of aligning large language models (LLMs)\nwith human values via preference learning (PL), focusing on incomplete and\ncorrupted data in preference datasets. We propose a novel method for robustly\nand completely recalibrating values within these datasets to enhance LLMs'\nresilience against the issues. In particular, we devise a guaranteed polynomial\ntime ranking algorithm that robustifies several existing models, such as the\nclassic Bradley-Terry-Luce (BTL) (Bradley and Terry, 1952) model and certain\ngeneralizations of it. To the best of our knowledge, our present work is the\nfirst to propose an algorithm that provably recovers an $\\epsilon$-optimal\nranking with high probability while allowing as large as $O(n)$ perturbed\npairwise comparison results per model response. Furthermore, we show robust\nrecovery results in the partially observed setting. Our experiments confirm\nthat our algorithms handle adversarial noise and unobserved comparisons well in\nboth general and LLM preference dataset settings. This work contributes to the\ndevelopment and scaling of more reliable and ethically aligned AI models by\nequipping the dataset curation pipeline with the ability to handle missing and\nmaliciously manipulated inputs.\n","authors":["Son The Nguyen","Niranjan Uma Naresh","Theja Tulabandhula"],"pdf_url":"https://arxiv.org/pdf/2403.02745v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.07619v6","updated":"2025-10-28T20:24:07Z","published":"2023-04-15T19:22:37Z","title":"Can ChatGPT Forecast Stock Price Movements? Return Predictability and\n  Large Language Models","summary":"  We document the capability of large language models (LLMs) like ChatGPT to\npredict stock market reactions from news headlines without direct financial\ntraining. Using post-knowledge-cutoff headlines, GPT-4 captures initial market\nresponses, achieving approximately 90% portfolio-day hit rates for the\nnon-tradable initial reaction. GPT-4 scores also significantly predict the\nsubsequent drift, especially for small stocks and negative news. Forecasting\nability generally increases with model size, suggesting that financial\nreasoning is an emerging capacity of complex LLMs. Strategy returns decline as\nLLM adoption rises, consistent with improved price efficiency. To rationalize\nthese findings, we develop a theoretical model that incorporates LLM\ntechnology, information-processing capacity constraints, underreaction, and\nlimits to arbitrage.\n","authors":["Alejandro Lopez-Lira","Yuehua Tang"],"pdf_url":"https://arxiv.org/pdf/2304.07619v6.pdf","comment":"Previously posted in SSRN\n  https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4412788"},{"id":"http://arxiv.org/abs/2506.01367v3","updated":"2025-10-28T20:23:50Z","published":"2025-06-02T06:50:58Z","title":"MMD-Flagger: Leveraging Maximum Mean Discrepancy to Detect\n  Hallucinations","summary":"  Large language models (LLMs) have become pervasive in our everyday life. Yet,\na fundamental obstacle prevents their use in many critical applications: their\npropensity to generate fluent, human-quality content that is not grounded in\nreality. The detection of such hallucinations is thus of the highest\nimportance. In this work, we propose a new method to flag hallucinated content:\nMMD-Flagger. It relies on Maximum Mean Discrepancy (MMD), a non-parametric\ndistance between distributions. On a high-level perspective, MMD-Flagger tracks\nthe MMD between the output to inspect and counterparts generated with various\ntemperature parameters. We show empirically that inspecting the shape of this\ntrajectory is sufficient to detect most hallucinations. This novel method is\nbenchmarked on machine translation and summarization datasets, on which it\nexhibits competitive performance relative to natural competitors.\n","authors":["Kensuke Mitsuzawa","Damien Garreau"],"pdf_url":"https://arxiv.org/pdf/2506.01367v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24942v1","updated":"2025-10-28T20:14:37Z","published":"2025-10-28T20:14:37Z","title":"Finding Culture-Sensitive Neurons in Vision-Language Models","summary":"  Despite their impressive performance, vision-language models (VLMs) still\nstruggle on culturally situated inputs. To understand how VLMs process\nculturally grounded information, we study the presence of culture-sensitive\nneurons, i.e. neurons whose activations show preferential sensitivity to inputs\nassociated with particular cultural contexts. We examine whether such neurons\nare important for culturally diverse visual question answering and where they\nare located. Using the CVQA benchmark, we identify neurons of culture\nselectivity and perform causal tests by deactivating the neurons flagged by\ndifferent identification methods. Experiments on three VLMs across 25 cultural\ngroups demonstrate the existence of neurons whose ablation disproportionately\nharms performance on questions about the corresponding cultures, while having\nminimal effects on others. Moreover, we propose a new margin-based selector -\nContrastive Activation Selection (CAS), and show that it outperforms existing\nprobability- and entropy-based methods in identifying culture-sensitive\nneurons. Finally, our layer-wise analyses reveals that such neurons tend to\ncluster in certain decoder layers. Overall, our findings shed new light on the\ninternal organization of multimodal representations.\n","authors":["Xiutian Zhao","Rochelle Choenni","Rohit Saxena","Ivan Titov"],"pdf_url":"https://arxiv.org/pdf/2510.24942v1.pdf","comment":"22 pages, 13 figures"},{"id":"http://arxiv.org/abs/2510.24940v1","updated":"2025-10-28T20:11:54Z","published":"2025-10-28T20:11:54Z","title":"SemCoT: Accelerating Chain-of-Thought Reasoning through\n  Semantically-Aligned Implicit Tokens","summary":"  The verbosity of Chain-of-Thought (CoT) reasoning hinders its mass deployment\nin efficiency-critical applications. Recently, implicit CoT approaches have\nemerged, which encode reasoning steps within LLM's hidden embeddings (termed\n``implicit reasoning'') rather than explicit tokens. This approach accelerates\nCoT by reducing the reasoning length and bypassing some LLM components.\nHowever, existing implicit CoT methods face two significant challenges: (1)\nthey fail to preserve the semantic alignment between the implicit reasoning\n(when transformed to natural language) and the ground-truth reasoning,\nresulting in a significant CoT performance degradation, and (2) they focus on\nreducing the length of the implicit reasoning; however, they neglect the\nconsiderable time cost for an LLM to generate one individual implicit reasoning\ntoken. To tackle these challenges, we propose a novel semantically-aligned\nimplicit CoT framework termed SemCoT. In particular, for the first challenge,\nwe design a contrastively trained sentence transformer that evaluates semantic\nalignment between implicit and explicit reasoning, which is used to enforce\nsemantic preservation during implicit reasoning optimization. To address the\nsecond challenge, we introduce an efficient implicit reasoning generator by\nfinetuning a lightweight language model using knowledge distillation. This\ngenerator is guided by our sentence transformer to distill ground-truth\nreasoning into semantically aligned implicit reasoning, while also optimizing\nfor accuracy. SemCoT is the first approach that enhances CoT efficiency by\njointly optimizing token-level generation speed and preserving semantic\nalignment with ground-truth reasoning. Extensive experiments demonstrate the\nsuperior performance of SemCoT compared to state-of-the-art methods in both\nefficiency and effectiveness. Our code can be found at\nhttps://github.com/YinhanHe123/SemCoT/.\n","authors":["Yinhan He","Wendy Zheng","Yaochen Zhu","Zaiyi Zheng","Lin Su","Sriram Vasudevan","Qi Guo","Liangjie Hong","Jundong Li"],"pdf_url":"https://arxiv.org/pdf/2510.24940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02103v3","updated":"2025-10-28T20:08:46Z","published":"2023-07-05T08:22:46Z","title":"Do predictability factors towards signing avatars hold across cultures?","summary":"  Avatar technology can offer accessibility possibilities and improve the\nDeaf-and-Hard of Hearing sign language users access to communication, education\nand services, such as the healthcare system. However, sign language users\nacceptance of signing avatars as well as their attitudes towards them vary and\ndepend on many factors. Furthermore, research on avatar technology is mostly\ndone by researchers who are not Deaf. The study examines the extent to which\nintrinsic or extrinsic factors contribute to predict the attitude towards\navatars across cultures. Intrinsic factors include the characteristics of the\navatar, such as appearance, movements and facial expressions. Extrinsic factors\ninclude users technology experience, their hearing status, age and their sign\nlanguage fluency. This work attempts to answer questions such as, if lower\nattitude ratings are related to poor technology experience with ASL users, for\nexample, is that also true for Moroccan Sign Language (MSL) users? For the\npurposes of the study, we designed a questionnaire to understand MSL users\nattitude towards avatars. Three groups of participants were surveyed: Deaf\n(57), Hearing (20) and Hard-of-Hearing (3). The results of our study were then\ncompared with those reported in other relevant studies.\n","authors":["Abdelhadi Soudi","Manal El Hakkaoui","Kristof Van Laerhoven"],"pdf_url":"https://arxiv.org/pdf/2307.02103v3.pdf","comment":"In Proceedings of SLTAT 2023: Eighth International Workshop on Sign\n  Language Translation and Avatar Technology, held in conjunction with ICASSP\n  2023: IEEE International Conference on Acoustics, Speech, and Signal\n  Processing, Rhodes, Greece, June 4-10, 2023"},{"id":"http://arxiv.org/abs/2505.20249v2","updated":"2025-10-28T20:06:49Z","published":"2025-05-26T17:23:29Z","title":"WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for\n  Evaluating Large Language Models","summary":"  Climate change adaptation requires the understanding of disruptive weather\nimpacts on society, where large language models (LLMs) might be applicable.\nHowever, their effectiveness is under-explored due to the difficulty of\nhigh-quality corpus collection and the lack of available benchmarks. The\nclimate-related events stored in regional newspapers record how communities\nadapted and recovered from disasters. However, the processing of the original\ncorpus is non-trivial. In this study, we first develop a disruptive weather\nimpact dataset with a four-stage well-crafted construction pipeline. Then, we\npropose WXImpactBench, the first benchmark for evaluating the capacity of LLMs\non disruptive weather impacts. The benchmark involves two evaluation tasks,\nmulti-label classification and ranking-based question answering. Extensive\nexperiments on evaluating a set of LLMs provide first-hand analysis of the\nchallenges in developing disruptive weather impact understanding and climate\nchange adaptation systems. The constructed dataset and the code for the\nevaluation framework are available to help society protect against\nvulnerabilities from disasters.\n","authors":["Yongan Yu","Qingchen Hu","Xianda Du","Jiayin Wang","Fengran Mo","Renee Sieber"],"pdf_url":"https://arxiv.org/pdf/2505.20249v2.pdf","comment":"Accepted by ACL 2025"},{"id":"http://arxiv.org/abs/2510.24934v1","updated":"2025-10-28T19:59:26Z","published":"2025-10-28T19:59:26Z","title":"Disaggregation Reveals Hidden Training Dynamics: The Case of Agreement\n  Attraction","summary":"  Language models generally produce grammatical text, but they are more likely\nto make errors in certain contexts. Drawing on paradigms from\npsycholinguistics, we carry out a fine-grained analysis of those errors in\ndifferent syntactic contexts. We demonstrate that by disaggregating over the\nconditions of carefully constructed datasets and comparing model performance on\neach over the course of training, it is possible to better understand the\nintermediate stages of grammatical learning in language models. Specifically,\nwe identify distinct phases of training where language model behavior aligns\nwith specific heuristics such as word frequency and local context rather than\ngeneralized grammatical rules. We argue that taking this approach to analyzing\nlanguage model behavior more generally can serve as a powerful tool for\nunderstanding the intermediate learning phases, overall training dynamics, and\nthe specific generalizations learned by language models.\n","authors":["James A. Michaelov","Catherine Arnett"],"pdf_url":"https://arxiv.org/pdf/2510.24934v1.pdf","comment":"Accepted to the First Workshop on Interpreting Cognition in Deep\n  Learning Models (CogInterp @ NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.24932v1","updated":"2025-10-28T19:58:24Z","published":"2025-10-28T19:58:24Z","title":"RiddleBench: A New Generative Reasoning Benchmark for LLMs","summary":"  Large Language Models have demonstrated strong performance on many\nestablished reasoning benchmarks. However, these benchmarks primarily evaluate\nstructured skills like quantitative problem-solving, leaving a gap in assessing\nflexible, multifaceted reasoning abilities that are central to human\nintelligence. These abilities require integrating logical deduction with\nspatial awareness and constraint satisfaction, which current evaluations do not\nmeasure well. To address this, we introduce RiddleBench, a benchmark of 1,737\nchallenging puzzles in English designed to probe these core reasoning\ncapabilities. Evaluation of state-of-the-art models on RiddleBench shows\nfundamental weaknesses. Even top proprietary models like Gemini 2.5 Pro, o3,\nand Claude 4 Sonnet achieve accuracy just above 60% (60.30%, 63.37%, and\n63.16%). Analysis further reveals deep failures, including hallucination\ncascades (accepting flawed reasoning from other models) and poor\nself-correction due to a strong self-confirmation bias. Their reasoning is also\nfragile, with performance degrading significantly when constraints are\nreordered or irrelevant information is introduced. RiddleBench functions as a\ndiagnostic tool for these issues and as a resource for guiding the development\nof more robust and reliable language models.\n","authors":["Deepon Halder","Alan Saji","Thanmay Jayakumar","Ratish Puduppully","Anoop Kunchukuttan","Raj Dabre"],"pdf_url":"https://arxiv.org/pdf/2510.24932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09767v3","updated":"2025-10-28T19:36:28Z","published":"2025-02-13T20:51:25Z","title":"Non-Markovian Discrete Diffusion with Causal Language Models","summary":"  Discrete diffusion models offer a flexible, controllable approach to\nstructured sequence generation, yet they still lag behind causal language\nmodels in expressive power. A key limitation lies in their reliance on the\nMarkovian assumption, which restricts each step to condition only on the\ncurrent state, leading to potential uncorrectable error accumulation. In this\npaper, we introduce CaDDi (Causal Discrete Diffusion Model), a discrete\ndiffusion model that conditions on the entire generative trajectory, thereby\nlifting the Markov constraint and allowing the model to revisit and improve\npast states. By unifying sequential (causal) and temporal (diffusion) reasoning\nin a single non-Markovian transformer, CaDDi also treats standard causal\nlanguage models as a special case and permits the direct reuse of pretrained\nLLM weights with no architectural changes. Empirically, CaDDi outperforms\nstate-of-the-art discrete diffusion baselines on natural-language benchmarks,\nsubstantially narrowing the remaining gap to large autoregressive transformers.\n","authors":["Yangtian Zhang","Sizhuang He","Daniel Levine","Lawrence Zhao","David Zhang","Syed A Rizvi","Shiyang Zhang","Emanuele Zappala","Rex Ying","David van Dijk"],"pdf_url":"https://arxiv.org/pdf/2502.09767v3.pdf","comment":"39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)"},{"id":"http://arxiv.org/abs/2510.19898v2","updated":"2025-10-28T19:10:09Z","published":"2025-10-22T17:58:56Z","title":"BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills","summary":"  High quality bugs are key to training the next generation of language model\nbased software engineering (SWE) agents. We introduce a novel method for\nsynthetic generation of difficult and diverse bugs. Our method instructs SWE\nAgents to introduce a feature into the codebase whereby they may\nunintentionally break tests, resulting in bugs. Prior approaches often induce\nan out-of-distribution effect by generating bugs intentionally (e.g. by\nintroducing local perturbation to existing code), which does not reflect\nrealistic development processes. We perform qualitative analysis to demonstrate\nthat our approach for generating bugs more closely reflects the patterns found\nin human-authored edits. Through extensive experiments, we demonstrate that our\nbugs provide more efficient training data for supervised fine-tuning,\noutperforming other bug datasets by 2% with half the training data (1.2k vs. 3k\nbugs). We train on our newly generated bugs in addition to existing bug\ndatasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench\nVerified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on\nSWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over\nthree seeds.\n","authors":["Atharv Sonwane","Isadora White","Hyunji Lee","Matheus Pereira","Lucas Caccia","Minseon Kim","Zhengyan Shi","Chinmay Singh","Alessandro Sordoni","Marc-Alexandre CÃ´tÃ©","Xingdi Yuan"],"pdf_url":"https://arxiv.org/pdf/2510.19898v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24891v1","updated":"2025-10-28T18:54:51Z","published":"2025-10-28T18:54:51Z","title":"Idea2Plan: Exploring AI-Powered Research Planning","summary":"  Large language models (LLMs) have demonstrated significant potential to\naccelerate scientific discovery as valuable tools for analyzing data,\ngenerating hypotheses, and supporting innovative approaches in various\nscientific fields. In this work, we investigate how LLMs can handle the\ntransition from conceptual research ideas to well-structured research plans.\nEffective research planning not only supports scientists in advancing their\nresearch but also represents a crucial capability for the development of\nautonomous research agents. Despite its importance, the field lacks a\nsystematic understanding of LLMs' research planning capability. To rigorously\nmeasure this capability, we introduce the Idea2Plan task and Idea2Plan Bench, a\nbenchmark built from 200 ICML 2025 Spotlight and Oral papers released after\nmajor LLM training cutoffs. Each benchmark instance includes a research idea\nand a grading rubric capturing the key components of valid plans. We further\npropose Idea2Plan JudgeEval, a complementary benchmark to assess the\nreliability of LLM-based judges against expert annotations. Experimental\nresults show that GPT-5 and GPT-5-mini achieve the strongest performance on the\nbenchmark, though substantial headroom remains for future improvement. Our\nstudy provides new insights into LLMs' capability for research planning and lay\nthe groundwork for future progress.\n","authors":["Jin Huang","Silviu Cucerzan","Sujay Kumar Jauhar","Ryen W. White"],"pdf_url":"https://arxiv.org/pdf/2510.24891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.21112v2","updated":"2025-10-28T18:52:14Z","published":"2025-07-12T23:10:59Z","title":"InsurTech innovation using natural language processing","summary":"  With the rapid rise of InsurTech, traditional insurance companies are\nincreasingly exploring alternative data sources and advanced technologies to\nsustain their competitive edge. This paper provides both a conceptual overview\nand practical case studies of natural language processing (NLP) and its\nemerging applications within insurance operations, focusing on transforming\nraw, unstructured text into structured data suitable for actuarial analysis and\ndecision-making. Leveraging real-world alternative data provided by an\nInsurTech industry partner that enriches traditional insurance data sources, we\napply various NLP techniques to demonstrate feature de-biasing, feature\ncompression, and industry classification in the commercial insurance context.\nThese enriched, text-derived insights not only add to and refine traditional\nrating factors for commercial insurance pricing but also offer novel\nperspectives for assessing underlying risk by introducing novel industry\nclassification techniques. Through these demonstrations, we show that NLP is\nnot merely a supplementary tool but a foundational element of modern,\ndata-driven insurance analytics.\n","authors":["Panyi Dong","Zhiyu Quan"],"pdf_url":"https://arxiv.org/pdf/2507.21112v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24870v1","updated":"2025-10-28T18:21:19Z","published":"2025-10-28T18:21:19Z","title":"Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented\n  Generation","summary":"  We introduce MiRAGE, an evaluation framework for retrieval-augmented\ngeneration (RAG) from multimodal sources. As audiovisual media becomes a\nprevalent source of information online, it is essential for RAG systems to\nintegrate information from these sources into generation. However, existing\nevaluations for RAG are text-centric, limiting their applicability to\nmultimodal, reasoning intensive settings because they don't verify information\nagainst sources. MiRAGE is a claim-centric approach to multimodal RAG\nevaluation, consisting of InfoF1, evaluating factuality and information\ncoverage, and CiteF1, measuring citation support and completeness. We show that\nMiRAGE, when applied by humans, strongly aligns with extrinsic quality\njudgments. We additionally introduce automatic variants of MiRAGE and three\nprominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the\nlimitations of text-centric work and laying the groundwork for automatic\nevaluation. We release open-source implementations and outline how to assess\nmultimodal RAG.\n","authors":["Alexander Martin","William Walden","Reno Kriz","Dengjia Zhang","Kate Sanders","Eugene Yang","Chihsheng Jin","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2510.24870v1.pdf","comment":"https://github.com/alexmartin1722/mirage"},{"id":"http://arxiv.org/abs/2506.17585v2","updated":"2025-10-28T18:06:24Z","published":"2025-06-21T04:48:05Z","title":"Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language\n  Models","summary":"  Trustworthy language models should provide both correct and verifiable\nanswers. However, citations generated directly by standalone LLMs are often\nunreliable. As a result, current systems insert citations by querying an\nexternal retriever at inference time, introducing latency, infrastructure\ndependence, and vulnerability to retrieval noise. We explore whether LLMs can\nbe made to reliably attribute to the documents seen during continual\npretraining without test-time retrieval, by revising the training process. To\nstudy this, we construct CitePretrainBench, a benchmark that mixes real-world\ncorpora (Wikipedia, Common Crawl, arXiv) with novel documents and probes both\nshort-form (single-fact) and long-form (multi-fact) citation tasks. Our\napproach follows a two-stage process: (1) continual pretraining to index\nfactual knowledge by binding it to persistent document identifiers; and (2)\ninstruction tuning to elicit citation behavior. We introduce Active Indexing\nfor the first stage, which creates generalizable, source-anchored bindings by\naugmenting training with synthetic data that (i) restate each fact in diverse,\ncompositional forms and (ii) enforce bidirectional training (source-to-fact and\nfact-to-source). This equips the model to both generate content from a cited\nsource and attribute its own answers, improving robustness to paraphrase and\ncomposition. Experiments with Qwen-2.5-7B&3B show that Active Indexing\nconsistently outperforms a Passive Indexing baseline, which simply appends an\nidentifier to each document, achieving citation precision gains of up to 30.2%\nacross all tasks and models. Our ablation studies reveal that performance\ncontinues to improve as we scale the amount of augmented data, showing a clear\nupward trend even at 16x the original token count. Finally, we show that\ninternal citations complement external ones by making the model more robust to\nretrieval noise.\n","authors":["Yukun Huang","Sanxing Chen","Jian Pei","Manzil Zaheer","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2506.17585v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24856v1","updated":"2025-10-28T18:02:51Z","published":"2025-10-28T18:02:51Z","title":"Do Large Language Models Grasp The Grammar? Evidence from\n  Grammar-Book-Guided Probing in Luxembourgish","summary":"  Grammar refers to the system of rules that governs the structural\norganization and the semantic relations among linguistic units such as\nsentences, phrases, and words within a given language. In natural language\nprocessing, there remains a notable scarcity of grammar focused evaluation\nprotocols, a gap that is even more pronounced for low-resource languages.\nMoreover, the extent to which large language models genuinely comprehend\ngrammatical structure, especially the mapping between syntactic structures and\nmeanings, remains under debate. To investigate this issue, we propose a Grammar\nBook Guided evaluation pipeline intended to provide a systematic and\ngeneralizable framework for grammar evaluation consisting of four key stages,\nand in this work we take Luxembourgish as a case study. The results show a weak\npositive correlation between translation performance and grammatical\nunderstanding, indicating that strong translations do not necessarily imply\ndeep grammatical competence. Larger models perform well overall due to their\nsemantic strength but remain weak in morphology and syntax, struggling\nparticularly with Minimal Pair tasks, while strong reasoning ability offers a\npromising way to enhance their grammatical understanding.\n","authors":["Lujun Li","Yewei Song","Lama Sleem","Yiqun Wang","Yangjie Xu","Cedric Lothritz","Niccolo Gentile","Radu State","Tegawende F. Bissyande","Jacques Klein"],"pdf_url":"https://arxiv.org/pdf/2510.24856v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2510.24718v1","updated":"2025-10-28T17:59:58Z","published":"2025-10-28T17:59:58Z","title":"Generative View Stitching","summary":"  Autoregressive video diffusion models are capable of long rollouts that are\nstable and consistent with history, but they are unable to guide the current\ngeneration with conditioning from the future. In camera-guided video generation\nwith a predefined camera trajectory, this limitation leads to collisions with\nthe generated scene, after which autoregression quickly collapses. To address\nthis, we propose Generative View Stitching (GVS), which samples the entire\nsequence in parallel such that the generated scene is faithful to every part of\nthe predefined camera trajectory. Our main contribution is a sampling algorithm\nthat extends prior work on diffusion stitching for robot planning to video\ngeneration. While such stitching methods usually require a specially trained\nmodel, GVS is compatible with any off-the-shelf video model trained with\nDiffusion Forcing, a prevalent sequence diffusion framework that we show\nalready provides the affordances necessary for stitching. We then introduce\nOmni Guidance, a technique that enhances the temporal consistency in stitching\nby conditioning on both the past and future, and that enables our proposed\nloop-closing mechanism for delivering long-range coherence. Overall, GVS\nachieves camera-guided video generation that is stable, collision-free,\nframe-to-frame consistent, and closes loops for a variety of predefined camera\npaths, including Oscar Reutersv\\\"ard's Impossible Staircase. Results are best\nviewed as videos at https://andrewsonga.github.io/gvs.\n","authors":["Chonghyuk Song","Michal Stary","Boyuan Chen","George Kopanas","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2510.24718v1.pdf","comment":"Project website: https://andrewsonga.github.io/gvs"},{"id":"http://arxiv.org/abs/2510.24717v1","updated":"2025-10-28T17:59:57Z","published":"2025-10-28T17:59:57Z","title":"Uniform Discrete Diffusion with Metric Path for Video Generation","summary":"  Continuous-space video generation has advanced rapidly, while discrete\napproaches lag behind due to error accumulation and long-context inconsistency.\nIn this work, we revisit discrete generative modeling and present Uniform\ndiscRete diffuSion with metric pAth (URSA), a simple yet powerful framework\nthat bridges the gap with continuous approaches for the scalable video\ngeneration. At its core, URSA formulates the video generation task as an\niterative global refinement of discrete spatiotemporal tokens. It integrates\ntwo key designs: a Linearized Metric Path and a Resolution-dependent Timestep\nShifting mechanism. These designs enable URSA to scale efficiently to\nhigh-resolution image synthesis and long-duration video generation, while\nrequiring significantly fewer inference steps. Additionally, we introduce an\nasynchronous temporal fine-tuning strategy that unifies versatile tasks within\na single model, including interpolation and image-to-video generation.\nExtensive experiments on challenging video and image generation benchmarks\ndemonstrate that URSA consistently outperforms existing discrete methods and\nachieves performance comparable to state-of-the-art continuous diffusion\nmethods. Code and models are available at https://github.com/baaivision/URSA\n","authors":["Haoge Deng","Ting Pan","Fan Zhang","Yang Liu","Zhuoyan Luo","Yufeng Cui","Wenxuan Wang","Chunhua Shen","Shiguang Shan","Zhaoxiang Zhang","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2510.24717v1.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2510.24711v1","updated":"2025-10-28T17:59:02Z","published":"2025-10-28T17:59:02Z","title":"Routing Matters in MoE: Scaling Diffusion Transformers with Explicit\n  Routing Guidance","summary":"  Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model\ncapacity while preserving computational efficiency. Despite its notable success\nin large language models (LLMs), existing attempts to apply MoE to Diffusion\nTransformers (DiTs) have yielded limited gains. We attribute this gap to\nfundamental differences between language and visual tokens. Language tokens are\nsemantically dense with pronounced inter-token variation, while visual tokens\nexhibit spatial redundancy and functional heterogeneity, hindering expert\nspecialization in vision MoE. To this end, we present ProMoE, an MoE framework\nfeaturing a two-step router with explicit routing guidance that promotes expert\nspecialization. Specifically, this guidance encourages the router to partition\nimage tokens into conditional and unconditional sets via conditional routing\naccording to their functional roles, and refine the assignments of conditional\nimage tokens through prototypical routing with learnable prototypes based on\nsemantic content. Moreover, the similarity-based expert allocation in latent\nspace enabled by prototypical routing offers a natural mechanism for\nincorporating explicit semantic guidance, and we validate that such guidance is\ncrucial for vision MoE. Building on this, we propose a routing contrastive loss\nthat explicitly enhances the prototypical routing process, promoting\nintra-expert coherence and inter-expert diversity. Extensive experiments on\nImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods\nunder both Rectified Flow and DDPM training objectives. Code and models will be\nmade publicly available.\n","authors":["Yujie Wei","Shiwei Zhang","Hangjie Yuan","Yujin Han","Zhekai Chen","Jiayu Wang","Difan Zou","Xihui Liu","Yingya Zhang","Yu Liu","Hongming Shan"],"pdf_url":"https://arxiv.org/pdf/2510.24711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24709v1","updated":"2025-10-28T17:57:05Z","published":"2025-10-28T17:57:05Z","title":"Does Object Binding Naturally Emerge in Large Pretrained Vision\n  Transformers?","summary":"  Object binding, the brain's ability to bind the many features that\ncollectively represent an object into a coherent whole, is central to human\ncognition. It groups low-level perceptual features into high-level object\nrepresentations, stores those objects efficiently and compositionally in\nmemory, and supports human reasoning about individual object instances. While\nprior work often imposes object-centric attention (e.g., Slot Attention)\nexplicitly to probe these benefits, it remains unclear whether this ability\nnaturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they\ncould: recognizing which patches belong to the same object should be useful for\ndownstream prediction and thus guide attention. Motivated by the quadratic\nnature of self-attention, we hypothesize that ViTs represent whether two\npatches belong to the same object, a property we term IsSameObject. We decode\nIsSameObject from patch embeddings across ViT layers using a similarity probe,\nwhich reaches over 90% accuracy. Crucially, this object-binding capability\nemerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker\nin ImageNet-supervised models, suggesting that binding is not a trivial\narchitectural artifact, but an ability acquired through specific pretraining\nobjectives. We further discover that IsSameObject is encoded in a\nlow-dimensional subspace on top of object features, and that this signal\nactively guides attention. Ablating IsSameObject from model activations\ndegrades downstream performance and works against the learning objective,\nimplying that emergent object binding naturally serves the pretraining\nobjective. Our findings challenge the view that ViTs lack object binding and\nhighlight how symbolic knowledge of \"which parts belong together\" emerges\nnaturally in a connectionist system.\n","authors":["Yihao Li","Saeed Salehi","Lyle Ungar","Konrad P. Kording"],"pdf_url":"https://arxiv.org/pdf/2510.24709v1.pdf","comment":"Accepted as a Spotlight at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24688v1","updated":"2025-10-28T17:49:42Z","published":"2025-10-28T17:49:42Z","title":"MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with\n  Relation-Aware Fusion for 3D Object Detection","summary":"  Infrastructure-based perception plays a crucial role in intelligent\ntransportation systems, offering global situational awareness and enabling\ncooperative autonomy. However, existing camera-based detection models often\nunderperform in such scenarios due to challenges such as multi-view\ninfrastructure setup, diverse camera configurations, degraded visual inputs,\nand various road layouts. We introduce MIC-BEV, a Transformer-based\nbird's-eye-view (BEV) perception framework for infrastructure-based\nmulti-camera 3D object detection. MIC-BEV flexibly supports a variable number\nof cameras with heterogeneous intrinsic and extrinsic parameters and\ndemonstrates strong robustness under sensor degradation. The proposed\ngraph-enhanced fusion module in MIC-BEV integrates multi-view image features\ninto the BEV space by exploiting geometric relationships between cameras and\nBEV cells alongside latent visual cues. To support training and evaluation, we\nintroduce M2I, a synthetic dataset for infrastructure-based object detection,\nfeaturing diverse camera configurations, road layouts, and environmental\nconditions. Extensive experiments on both M2I and the real-world dataset\nRoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D\nobject detection. It also remains robust under challenging conditions,\nincluding extreme weather and sensor degradation. These results highlight the\npotential of MIC-BEV for real-world deployment. The dataset and source code are\navailable at: https://github.com/HandsomeYun/MIC-BEV.\n","authors":["Yun Zhang","Zhaoliang Zheng","Johnson Liu","Zhiyu Huang","Zewei Zhou","Zonglin Meng","Tianhui Cai","Jiaqi Ma"],"pdf_url":"https://arxiv.org/pdf/2510.24688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07862v2","updated":"2025-10-28T17:37:03Z","published":"2025-02-11T17:19:44Z","title":"ADMN: A Layer-Wise Adaptive Multimodal Network for Dynamic Input Noise\n  and Compute Resources","summary":"  Multimodal deep learning systems are deployed in dynamic scenarios due to the\nrobustness afforded by multiple sensing modalities. Nevertheless, they struggle\nwith varying compute resource availability (due to multi-tenancy, device\nheterogeneity, etc.) and fluctuating quality of inputs (from sensor feed\ncorruption, environmental noise, etc.). Statically provisioned multimodal\nsystems cannot adapt when compute resources change over time, while existing\ndynamic networks struggle with strict compute budgets. Additionally, both\nsystems often neglect the impact of variations in modality quality.\nConsequently, modalities suffering substantial corruption may needlessly\nconsume resources better allocated towards other modalities. We propose ADMN, a\nlayer-wise Adaptive Depth Multimodal Network capable of tackling both\nchallenges: it adjusts the total number of active layers across all modalities\nto meet strict compute resource constraints and continually reallocates layers\nacross input modalities according to their modality quality. Our evaluations\nshowcase ADMN can match the accuracy of state-of-the-art networks while\nreducing up to 75% of their floating-point operations.\n","authors":["Jason Wu","Yuyang Yuan","Kang Yang","Lance Kaplan","Mani Srivastava"],"pdf_url":"https://arxiv.org/pdf/2502.07862v2.pdf","comment":"Accepted to Neurips 2025"},{"id":"http://arxiv.org/abs/2505.20426v3","updated":"2025-10-28T17:35:54Z","published":"2025-05-26T18:20:22Z","title":"MMPerspective: Do MLLMs Understand Perspective? A Comprehensive\n  Benchmark for Perspective Perception, Reasoning, and Robustness","summary":"  Understanding perspective is fundamental to human visual perception, yet the\nextent to which multimodal large language models (MLLMs) internalize\nperspective geometry remains unclear. We introduce MMPerspective, the first\nbenchmark specifically designed to systematically evaluate MLLMs' understanding\nof perspective through 10 carefully crafted tasks across three complementary\ndimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark\ncomprises 2,711 real-world and synthetic image instances with 5,083\nquestion-answer pairs that probe key capabilities, such as vanishing point\nperception and counting, perspective type reasoning, line relationship\nunderstanding in 3D space, invariance to perspective-preserving\ntransformations, etc. Through a comprehensive evaluation of 43 state-of-the-art\nMLLMs, we uncover significant limitations: while models demonstrate competence\non surface-level perceptual tasks, they struggle with compositional reasoning\nand maintaining spatial consistency under perturbations. Our analysis further\nreveals intriguing patterns between model architecture, scale, and perspective\ncapabilities, highlighting both robustness bottlenecks and the benefits of\nchain-of-thought prompting. MMPerspective establishes a valuable testbed for\ndiagnosing and advancing spatial understanding in vision-language systems.\nResources available at: https://yunlong10.github.io/MMPerspective/\n","authors":["Yolo Yunlong Tang","Pinxin Liu","Zhangyun Tan","Mingqian Feng","Rui Mao","Chao Huang","Jing Bi","Yunzhong Xiao","Susan Liang","Hang Hua","Ali Vosoughi","Luchuan Song","Zeliang Zhang","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2505.20426v3.pdf","comment":"Accepted to NeurIPS 2025 DB Track"},{"id":"http://arxiv.org/abs/2510.24667v1","updated":"2025-10-28T17:35:02Z","published":"2025-10-28T17:35:02Z","title":"SAGE: Structure-Aware Generative Video Transitions between Diverse Clips","summary":"  Video transitions aim to synthesize intermediate frames between two clips,\nbut naive approaches such as linear blending introduce artifacts that limit\nprofessional use or break temporal coherence. Traditional techniques\n(cross-fades, morphing, frame interpolation) and recent generative inbetweening\nmethods can produce high-quality plausible intermediates, but they struggle\nwith bridging diverse clips involving large temporal gaps or significant\nsemantic differences, leaving a gap for content-aware and visually coherent\ntransitions. We address this challenge by drawing on artistic workflows,\ndistilling strategies such as aligning silhouettes and interpolating salient\nfeatures to preserve structure and perceptual continuity. Building on this, we\npropose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot\napproach that combines structural guidance, provided via line maps and motion\nflow, with generative synthesis, enabling smooth, semantically consistent\ntransitions without fine-tuning. Extensive experiments and comparison with\ncurrent alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate\nthat SAGE outperforms both classical and generative baselines on quantitative\nmetrics and user studies for producing transitions between diverse clips. Code\nto be released on acceptance.\n","authors":["Mia Kan","Yilin Liu","Niloy Mitra"],"pdf_url":"https://arxiv.org/pdf/2510.24667v1.pdf","comment":"Website: https://kan32501.github.io/sage.github.io/"},{"id":"http://arxiv.org/abs/2510.24657v1","updated":"2025-10-28T17:22:44Z","published":"2025-10-28T17:22:44Z","title":"Group Relative Attention Guidance for Image Editing","summary":"  Recently, image editing based on Diffusion-in-Transformer models has\nundergone rapid development. However, existing editing methods often lack\neffective control over the degree of editing, limiting their ability to achieve\nmore customized results. To address this limitation, we investigate the\nMM-Attention mechanism within the DiT model and observe that the Query and Key\ntokens share a bias vector that is only layer-dependent. We interpret this bias\nas representing the model's inherent editing behavior, while the delta between\neach token and its corresponding bias encodes the content-specific editing\nsignals. Based on this insight, we propose Group Relative Attention Guidance, a\nsimple yet effective method that reweights the delta values of different tokens\nto modulate the focus of the model on the input image relative to the editing\ninstruction, enabling continuous and fine-grained control over editing\nintensity without any tuning. Extensive experiments conducted on existing image\nediting frameworks demonstrate that GRAG can be integrated with as few as four\nlines of code, consistently enhancing editing quality. Moreover, compared to\nthe commonly used Classifier-Free Guidance, GRAG achieves smoother and more\nprecise control over the degree of editing. Our code will be released at\nhttps://github.com/little-misfit/GRAG-Image-Editing.\n","authors":["Xuanpu Zhang","Xuesong Niu","Ruidong Chen","Dan Song","Jianhao Zeng","Penghui Du","Haoxiang Cao","Kai Wu","An-an Liu"],"pdf_url":"https://arxiv.org/pdf/2510.24657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24653v1","updated":"2025-10-28T17:18:43Z","published":"2025-10-28T17:18:43Z","title":"Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making\n  Datasets in Digital Pathology","summary":"  Interpretation of giga-pixel whole-slide images (WSIs) is an important but\ndifficult task for pathologists. Their diagnostic accuracy is estimated to\naverage around 70%. Adding a second pathologist does not substantially improve\ndecision consistency. The field lacks adequate behavioral data to explain\ndiagnostic errors and inconsistencies. To fill in this gap, we present\nPathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual\nsearch and decision-making processes of the full diagnostic workflow during\ncancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse\ninteraction, stimulus tracking, viewport navigation, and diagnostic decision\ndata (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data\ncollection process emphasizes ecological validity through an\napplication-grounded testbed, called PTAH. In total, we recorded 171,909\nfixations, 263,320 saccades, and 1,867,362 mouse interaction events. In\naddition, such data could also be used to improve the training of both\npathologists and AI systems that might support human experts. All experiments\nwere preregistered at https://osf.io/hj9a7, and the complete dataset along with\nanalysis code is available at https://go.osu.edu/pathogaze.\n","authors":["Veronica Thai","Rui Li","Meng Ling","Shuning Jiang","Jeremy Wolfe","Raghu Machiraju","Yan Hu","Zaibo Li","Anil Parwani","Jian Chen"],"pdf_url":"https://arxiv.org/pdf/2510.24653v1.pdf","comment":"16 pages, 9 figures, submitted to Nature Scientific Data"},{"id":"http://arxiv.org/abs/2510.24640v1","updated":"2025-10-28T17:06:40Z","published":"2025-10-28T17:06:40Z","title":"A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries","summary":"  The rapid advancement of generative AI has enabled the creation of highly\nrealistic forged facial images, posing significant threats to AI security,\ndigital media integrity, and public trust. Face forgery techniques, ranging\nfrom face swapping and attribute editing to powerful diffusion-based image\nsynthesis, are increasingly being used for malicious purposes such as\nmisinformation, identity fraud, and defamation. This growing challenge\nunderscores the urgent need for robust and generalizable face forgery detection\nmethods as a critical component of AI security infrastructure. In this work, we\npropose a novel dual-branch convolutional neural network for face forgery\ndetection that leverages complementary cues from both spatial and frequency\ndomains. The RGB branch captures semantic information, while the frequency\nbranch focuses on high-frequency artifacts that are difficult for generative\nmodels to suppress. A channel attention module is introduced to adaptively fuse\nthese heterogeneous features, highlighting the most informative channels for\nforgery discrimination. To guide the network's learning process, we design a\nunified loss function, FSC Loss, that combines focal loss, supervised\ncontrastive loss, and a frequency center margin loss to enhance class\nseparability and robustness. We evaluate our model on the DiFF benchmark, which\nincludes forged images generated from four representative methods:\ntext-to-image, image-to-image, face swap, and face edit. Our method achieves\nstrong performance across all categories and outperforms average human\naccuracy. These results demonstrate the model's effectiveness and its potential\ncontribution to safeguarding AI ecosystems against visual forgery attacks.\n","authors":["Xin Zhang","Yuqi Song","Fei Zuo"],"pdf_url":"https://arxiv.org/pdf/2510.24640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22693v2","updated":"2025-10-28T16:57:22Z","published":"2025-10-26T14:36:15Z","title":"VADTree: Explainable Training-Free Video Anomaly Detection via\n  Hierarchical Granularity-Aware Tree","summary":"  Video anomaly detection (VAD) focuses on identifying anomalies in videos.\nSupervised methods demand substantial in-domain training data and fail to\ndeliver clear explanations for anomalies. In contrast, training-free methods\nleverage the knowledge reserves and language interactivity of large pre-trained\nmodels to detect anomalies. However, the current fixed-length temporal window\nsampling approaches struggle to accurately capture anomalies with varying\ntemporal spans. Therefore, we propose VADTree that utilizes a Hierarchical\nGranularityaware Tree (HGTree) structure for flexible sampling in VAD. VADTree\nleverages the knowledge embedded in a pre-trained Generic Event Boundary\nDetection (GEBD) model to characterize potential anomaly event boundaries.\nSpecifically, VADTree decomposes the video into generic event nodes based on\nboundary confidence, and performs adaptive coarse-fine hierarchical structuring\nand redundancy removal to construct the HGTree. Then, the multi-dimensional\npriors are injected into the visual language models (VLMs) to enhance the\nnode-wise anomaly perception, and anomaly reasoning for generic event nodes is\nachieved via large language models (LLMs). Finally, an inter-cluster node\ncorrelation method is used to integrate the multi-granularity anomaly scores.\nExtensive experiments on three challenging datasets demonstrate that VADTree\nachieves state-of-the-art performance in training-free settings while\ndrastically reducing the number of sampled video segments. The code will be\navailable at https://github.com/wenlongli10/VADTree.\n","authors":["Wenlong Li","Yifei Xu","Yuan Rao","Zhenhua Wang","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2510.22693v2.pdf","comment":"NeurIPS 2025 poster"},{"id":"http://arxiv.org/abs/2510.24623v1","updated":"2025-10-28T16:51:50Z","published":"2025-10-28T16:51:50Z","title":"GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization","summary":"  In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline\ndesigned to localize a mobile robot in large-scale outdoor environments using\nprior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing\non the perceived ground area and utilizes the place recognition network R2D2,\nor alternatively, the non-learning approach Scale-Invariant Feature Transform\n(SIFT), to identify and select keypoints for BEV image map registration. Our\nresults demonstrate that GroundLoc outperforms state-of-the-art methods on the\nSemanticKITTI and HeLiPR datasets across various sensors. In the multi-session\nlocalization evaluation, GroundLoc reaches an Average Trajectory Error (ATE)\nwell below 50 cm on all Ouster OS2 128 sequences while meeting online runtime\nrequirements. The system supports various sensor models, as evidenced by\nevaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II,\nand Livox Avia sensors. The prior maps are stored as 2D raster image maps,\nwhich can be created from a single drive and require only 4 MB of storage per\nsquare kilometer. The source code is available at\nhttps://github.com/dcmlr/groundloc.\n","authors":["Nicolai Steinke","Daniel Goehring"],"pdf_url":"https://arxiv.org/pdf/2510.24623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.18513v2","updated":"2025-10-28T16:44:35Z","published":"2025-10-21T10:55:32Z","title":"DWaste: Greener AI for Waste Sorting using Mobile and Edge Devices","summary":"  The rise of convenience packaging has led to generation of enormous waste,\nmaking efficient waste sorting crucial for sustainable waste management. To\naddress this, we developed DWaste, a computer vision-powered platform designed\nfor real-time waste sorting on resource-constrained smartphones and edge\ndevices, including offline functionality. We benchmarked various image\nclassification models (EfficientNetV2S/M, ResNet50/101, MobileNet) and object\ndetection (YOLOv8n, YOLOv11n) including our purposed YOLOv8n-CBAM model using\nour annotated dataset designed for recycling. We found a clear trade-off\nbetween accuracy and resource consumption: the best classifier,\nEfficientNetV2S, achieved high accuracy(~ 96%) but suffered from high latency\n(~ 0.22s) and elevated carbon emissions. In contrast, lightweight object\ndetection models delivered strong performance (up to 80% mAP) with ultra-fast\ninference (~ 0.03s) and significantly smaller model sizes (< 7MB ), making them\nideal for real-time, low-power use. Model quantization further maximized\nefficiency, substantially reducing model size and VRAM usage by up to 75%. Our\nwork demonstrates the successful implementation of \"Greener AI\" models to\nsupport real-time, sustainable waste sorting on edge devices.\n","authors":["Suman Kunwar"],"pdf_url":"https://arxiv.org/pdf/2510.18513v2.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2405.07046v3","updated":"2025-10-28T16:43:19Z","published":"2024-05-11T16:22:00Z","title":"RETTA: Retrieval-Enhanced Test-Time Adaptation for Zero-Shot Video\n  Captioning","summary":"  Despite the significant progress of fully-supervised video captioning,\nzero-shot methods remain much less explored. In this paper, we propose a novel\nzero-shot video captioning framework named Retrieval-Enhanced Test-Time\nAdaptation (RETTA), which takes advantage of existing pretrained large-scale\nvision and language models to directly generate captions with test-time\nadaptation. Specifically, we bridge video and text using four key models: a\ngeneral video-text retrieval model XCLIP, a general image-text matching model\nCLIP, a text alignment model AnglE, and a text generation model GPT-2, due to\ntheir source-code availability. The main challenge is how to enable the text\ngeneration model to be sufficiently aware of the content in a given video so as\nto generate corresponding captions. To address this problem, we propose using\nlearnable tokens as a communication medium among these four frozen models\nGPT-2, XCLIP, CLIP, and AnglE. Different from the conventional way that trains\nthese tokens with training data, we propose to learn these tokens with soft\ntargets of the inference data under several carefully crafted loss functions,\nwhich enable the tokens to absorb video information catered for GPT-2. This\nprocedure can be efficiently done in just a few iterations (we use 16\niterations in the experiments) and does not require ground truth data.\nExtensive experimental results on three widely used datasets, MSR-VTT, MSVD,\nand VATEX, show absolute 5.1%-32.4% improvements in terms of the main metric\nCIDEr compared to several state-of-the-art zero-shot video captioning methods.\n","authors":["Yunchuan Ma","Laiyun Qing","Guorong Li","Yuankai Qi","Amin Beheshti","Quan Z. Sheng","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2405.07046v3.pdf","comment":"Published in Pattern Recognition"},{"id":"http://arxiv.org/abs/2510.24579v1","updated":"2025-10-28T16:13:14Z","published":"2025-10-28T16:13:14Z","title":"Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter\n  Correction in Cone-Beam CT","summary":"  Cone-beam CT (CBCT) employs a flat-panel detector to achieve\nthree-dimensional imaging with high spatial resolution. However, CBCT is\nsusceptible to scatter during data acquisition, which introduces CT value bias\nand reduced tissue contrast in the reconstructed images, ultimately degrading\ndiagnostic accuracy. To address this issue, we propose a deep learning-based\nscatter artifact correction method inspired by physical prior knowledge.\nLeveraging the fact that the observed point scatter probability density\ndistribution exhibits rotational symmetry in the projection domain. The method\nuses Gaussian Radial Basis Functions (RBF) to model the point scatter function\nand embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides\nefficient nonlinear mapping capabilities for learning high-dimensional scatter\nfeatures. By incorporating the physical characteristics of the scattered photon\ndistribution together with the complex function mapping capacity of KAN, the\nmodel improves its ability to accurately represent scatter. The effectiveness\nof the method is validated through both synthetic and real-scan experiments.\nExperimental results show that the model can effectively correct the scatter\nartifacts in the reconstructed images and is superior to the current methods in\nterms of quantitative metrics.\n","authors":["Xu Jiang","Huiying Pan","Ligen Shi","Jianing Sun","Wenfeng Xu","Xing Zhao"],"pdf_url":"https://arxiv.org/pdf/2510.24579v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.12427v4","updated":"2025-10-28T16:06:34Z","published":"2025-02-18T01:52:41Z","title":"Frequency-Aware Vision Transformers for High-Fidelity Super-Resolution\n  of Earth System Models","summary":"  Super-resolution (SR) is crucial for enhancing the spatial fidelity of Earth\nSystem Model (ESM) outputs, allowing fine-scale structures vital to climate\nscience to be recovered from coarse simulations. However, traditional deep\nsuper-resolution methods, including convolutional and transformer-based models,\ntend to exhibit spectral bias, reconstructing low-frequency content more\nreadily than valuable high-frequency details. In this work, we introduce two\nfrequency-aware frameworks: the Vision Transformer-Tuned Sinusoidal Implicit\nRepresentation (ViSIR), combining Vision Transformers and sinusoidal\nactivations to mitigate spectral bias, and the Vision Transformer Fourier\nRepresentation Network (ViFOR), which integrates explicit Fourier-based\nfiltering for independent low- and high-frequency learning. Evaluated on the\nE3SM-HR Earth system dataset across surface temperature, shortwave, and\nlongwave fluxes, these models outperform leading CNN, GAN, and vanilla\ntransformer baselines, with ViFOR demonstrating up to 2.6~dB improvements in\nPSNR and significantly higher SSIM. Detailed ablation and scaling studies\nhighlight the benefit of full-field training, the impact of frequency\nhyperparameters, and the potential for generalization. The results establish\nViFOR as a state-of-the-art, scalable solution for climate data downscaling.\nFuture extensions will address temporal super-resolution, multimodal climate\nvariables, automated parameter selection, and integration of physical\nconservation constraints to broaden scientific applicability.\n","authors":["Ehsan Zeraatkar","Salah A Faroughi","Jelena TeÅ¡iÄ"],"pdf_url":"https://arxiv.org/pdf/2502.12427v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06415v2","updated":"2025-10-28T16:04:58Z","published":"2025-03-09T03:17:28Z","title":"Polygonal network disorder and the turning distance","summary":"  The turning distance is a well-studied metric for measuring the similarity\nbetween two polygons. This metric is constructed by taking an $L^p$ distance\nbetween step functions which track each shape's tangent angle of a path tracing\nits boundary. In this study, we introduce \\textit{turning disorders} for\npolygonal planar networks, defined by averaging turning distances between\nnetwork faces and \"ordered\" shapes (regular polygons or circles). We derive\nclosed-form expressions of turning distances for special classes of regular\npolygons, related to the divisibility of $m$ and $n$, and also between regular\npolygons and circles. These formulas are used to show that the time for\ncomputing the 2-turning distances reduces to $O((m+n) \\log(m+n))$ when both\nshapes are regular polygons, an improvement from $O(mn\\log(mn))$ operations\nneeded to compute distances between general polygons of $n$ and $m$ sides. We\nalso apply these formulas to several examples of network microstructure with\nvarying disorder. For Archimedean lattices, a class of regular tilings, we can\nexpress turning disorders with exact expressions. We also consider turning\ndisorders applied to two examples of stochastic processes on networks: spring\nnetworks evolving under T1 moves and polygonal rupture processes. We find that\nthe two aspects of defining different turning disorders, the choice of ordered\nshape and whether to apply area-weighting, can capture different notions of\nnetwork disorder.\n","authors":["Alex Dolce","Ryan Lavelle","Bernard Scott","Ashlyn Urbanski","Joseph Klobusicky"],"pdf_url":"https://arxiv.org/pdf/2503.06415v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05177v3","updated":"2025-10-28T16:02:48Z","published":"2025-02-07T18:59:56Z","title":"Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with\n  Leading Short-Context Accuracy","summary":"  We introduce Long-VITA, a simple yet effective large multi-modal model for\nlong-context visual-language understanding tasks. It is adept at concurrently\nprocessing and analyzing modalities of image, video, and text over 4K frames or\n1M tokens while delivering advanced performances on short-context multi-modal\ntasks. We propose an effective multi-modal training schema that starts with\nlarge language models and proceeds through vision-language alignment, general\nknowledge learning, and two sequential stages of long-sequence fine-tuning. We\nfurther implement context-parallelism distributed inference and logits-masked\nlanguage modeling head to scale Long-VITA to infinitely long inputs of images\nand texts during model inference. Regarding training data, Long-VITA is built\non a mix of 17M samples from public datasets only and demonstrates\nstate-of-the-art performance on various multi-modal benchmarks, compared\nagainst recent cutting-edge models with internal data. Long-VITA is fully\nopen-source and reproducible.. By leveraging our inference designs, Long-VITA\nmodels achieve a remarkable 2x prefill speedup and 4x context length extension\nin a single node with 8 GPUs. We hope Long-VITA can serve as a competitive\nbaseline and offer valuable insights for the open-source community in advancing\nlong-context multi-modal understanding.\n","authors":["Yunhang Shen","Chaoyou Fu","Shaoqi Dong","Xiong Wang","Yi-Fan Zhang","Peixian Chen","Mengdan Zhang","Haoyu Cao","Ke Li","Shaohui Lin","Xiawu Zheng","Yan Zhang","Yiyi Zhou","Ran He","Caifeng Shan","Rongrong Ji","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2502.05177v3.pdf","comment":"https://github.com/VITA-MLLM/Long-VITA"},{"id":"http://arxiv.org/abs/2510.24563v1","updated":"2025-10-28T15:56:36Z","published":"2025-10-28T15:56:36Z","title":"OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents","summary":"  With advances in decision-making and reasoning capabilities, multimodal\nagents show strong potential in computer application scenarios. Past\nevaluations have mainly assessed GUI interaction skills, while tool invocation\nabilities, such as those enabled by the Model Context Protocol (MCP), have been\nlargely overlooked. Comparing agents with integrated tool invocation to those\nevaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,\nthe first comprehensive and fair benchmark for assessing computer-use agents'\ntool invocation, GUI operation, and decision-making abilities in a real-world\nenvironment. We design a novel automated code-generation pipeline to create\ntools and combine them with a curated selection from existing tools. Rigorous\nmanual validation yields 158 high-quality tools (covering 7 common\napplications), each verified for correct functionality, practical\napplicability, and versatility. Extensive evaluations of state-of-the-art\nmultimodal agents on OSWorld-MCP show that MCP tools generally improve task\nsuccess rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%\nto 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of\nassessing tool invocation capabilities. However, even the strongest models have\nrelatively low tool invocation rates, Only 36.3%, indicating room for\nimprovement and highlighting the benchmark's challenge. By explicitly measuring\nMCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents\nand sets a new standard for evaluating performance in complex, tool-assisted\nenvironments. Our code, environment, and data are publicly available at\nhttps://osworld-mcp.github.io.\n","authors":["Hongrui Jia","Jitong Liao","Xi Zhang","Haiyang Xu","Tianbao Xie","Chaoya Jiang","Ming Yan","Si Liu","Wei Ye","Fei Huang"],"pdf_url":"https://arxiv.org/pdf/2510.24563v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02293v2","updated":"2025-10-28T15:28:13Z","published":"2025-08-04T11:03:12Z","title":"Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning","summary":"  So-called unsupervised anomaly detection is better described as\nsemi-supervised, as it assumes all training data are nominal. This assumption\nsimplifies training but requires manual data curation, introducing bias and\nlimiting adaptability. We propose Confident Meta-learning (CoMet), a novel\ntraining strategy that enables deep anomaly detection models to learn from\nuncurated datasets where nominal and anomalous samples coexist, eliminating the\nneed for explicit filtering. Our approach integrates Soft Confident Learning,\nwhich assigns lower weights to low-confidence samples, and Meta-Learning, which\nstabilizes training by regularizing updates based on training validation loss\ncovariance. This prevents overfitting and enhances robustness to noisy data.\nCoMet is model-agnostic and can be applied to any anomaly detection method\ntrainable via gradient descent. Experiments on MVTec-AD, VIADUCT, and KSDD2\nwith two state-of-the-art models demonstrate the effectiveness of our approach,\nconsistently improving over the baseline methods, remaining insensitive to\nanomalies in the training set, and setting a new state-of-the-art across all\ndatasets. Code is available at https://github.com/aqeeelmirza/CoMet\n","authors":["Muhammad Aqeel","Shakiba Sharifi","Marco Cristani","Francesco Setti"],"pdf_url":"https://arxiv.org/pdf/2508.02293v2.pdf","comment":"Accepted to IEEE/CVF International Conference on Computer Vision\n  (ICCV2025)"},{"id":"http://arxiv.org/abs/2510.24514v1","updated":"2025-10-28T15:26:20Z","published":"2025-10-28T15:26:20Z","title":"Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal\n  Reasoning in MLLMs","summary":"  While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.\n","authors":["Huanyu Zhang","Wenshan Wu","Chengzu Li","Ning Shang","Yan Xia","Yangyu Huang","Yifan Zhang","Li Dong","Zhang Zhang","Liang Wang","Tieniu Tan","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2510.24514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17071v2","updated":"2025-10-28T15:20:36Z","published":"2025-03-21T11:54:16Z","title":"Superpowering Open-Vocabulary Object Detectors for X-ray Vision","summary":"  Open-vocabulary object detection (OvOD) is set to revolutionize security\nscreening by enabling systems to recognize any item in X-ray scans. However,\ndeveloping effective OvOD models for X-ray imaging presents unique challenges\ndue to data scarcity and the modality gap that prevents direct adoption of\nRGB-based solutions. To overcome these limitations, we propose RAXO, a\ntraining-free framework that repurposes off-the-shelf RGB OvOD detectors for\nrobust X-ray detection. RAXO builds high-quality X-ray class descriptors using\na dual-source retrieval strategy. It gathers relevant RGB images from the web\nand enriches them via a novel X-ray material transfer mechanism, eliminating\nthe need for labeled databases. These visual descriptors replace text-based\nclassification in OvOD, leveraging intra-modal feature distances for robust\ndetection. Extensive experiments demonstrate that RAXO consistently improves\nOvOD performance, providing an average mAP increase of up to 17.0 points over\nbase detectors. To further support research in this emerging field, we also\nintroduce DET-COMPASS, a new benchmark featuring bounding box annotations for\nover 300 object categories, enabling large-scale evaluation of OvOD in X-ray.\nCode and dataset available at: https://github.com/PAGF188/RAXO.\n","authors":["Pablo Garcia-Fernandez","Lorenzo Vaquero","Mingxuan Liu","Feng Xue","Daniel Cores","Nicu Sebe","Manuel Mucientes","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2503.17071v2.pdf","comment":"Accepted at ICCV 2025"},{"id":"http://arxiv.org/abs/2510.24503v1","updated":"2025-10-28T15:15:14Z","published":"2025-10-28T15:15:14Z","title":"Local Performance vs. Out-of-Distribution Generalization: An Empirical\n  Analysis of Personalized Federated Learning in Heterogeneous Data\n  Environments","summary":"  In the context of Federated Learning with heterogeneous data environments,\nlocal models tend to converge to their own local model optima during local\ntraining steps, deviating from the overall data distributions. Aggregation of\nthese local updates, e.g., with FedAvg, often does not align with the global\nmodel optimum (client drift), resulting in an update that is suboptimal for\nmost clients. Personalized Federated Learning approaches address this challenge\nby exclusively focusing on the average local performances of clients' models on\ntheir own data distribution. Generalization to out-of-distribution samples,\nwhich is a substantial benefit of FedAvg and represents a significant component\nof robustness, appears to be inadequately incorporated into the assessment and\nevaluation processes. This study involves a thorough evaluation of Federated\nLearning approaches, encompassing both their local performance and their\ngeneralization capabilities. Therefore, we examine different stages within a\nsingle communication round to enable a more nuanced understanding of the\nconsidered metrics. Furthermore, we propose and incorporate a modified approach\nof FedAvg, designated as Federated Learning with Individualized Updates (FLIU),\nextending the algorithm by a straightforward individualization step with an\nadaptive personalization factor. We evaluate and compare the approaches\nempirically using MNIST and CIFAR-10 under various distributional conditions,\nincluding benchmark IID and pathological non-IID, as well as additional novel\ntest environments with Dirichlet distribution specifically developed to stress\nthe algorithms on complex data heterogeneity.\n","authors":["Mortesa Hussaini","Jan TheiÃ","Anthony Stein"],"pdf_url":"https://arxiv.org/pdf/2510.24503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24486v1","updated":"2025-10-28T15:00:07Z","published":"2025-10-28T15:00:07Z","title":"Fast and accurate neural reflectance transformation imaging through\n  knowledge distillation","summary":"  Reflectance Transformation Imaging (RTI) is very popular for its ability to\nvisually analyze surfaces by enhancing surface details through interactive\nrelighting, starting from only a few tens of photographs taken with a fixed\ncamera and variable illumination. Traditional methods like Polynomial Texture\nMaps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle\nto accurately capture complex reflectance fields using few per-pixel\ncoefficients and fixed bases, leading to artifacts, especially in highly\nreflective or shadowed areas. The NeuralRTI approach, which exploits a neural\nautoencoder to learn a compact function that better approximates the local\nreflectance as a function of light directions, has been shown to produce\nsuperior quality at comparable storage cost. However, as it performs\ninteractive relighting with custom decoder networks with many parameters, the\nrendering step is computationally expensive and not feasible at full resolution\nfor large images on limited hardware. Earlier attempts to reduce costs by\ndirectly training smaller networks have failed to produce valid results. For\nthis reason, we propose to reduce its computational cost through a novel\nsolution based on Knowledge Distillation (DisK-NeuralRTI). ...\n","authors":["Tinsae G. Dulecha","Leonardo Righetto","Ruggero Pintus","Enrico Gobbetti","Andrea Giachetti"],"pdf_url":"https://arxiv.org/pdf/2510.24486v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2510.14383v2","updated":"2025-10-28T14:50:18Z","published":"2025-10-16T07:31:21Z","title":"DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with\n  Analytical Insights","summary":"  Accurate brain tumor segmentation is significant for clinical diagnosis and\ntreatment but remains challenging due to tumor heterogeneity. Mamba-based State\nSpace Models have demonstrated promising performance. However, despite their\ncomputational efficiency over other neural architectures, they incur\nconsiderable overhead for this task due to their sequential feature computation\nacross multiple spatial axes. Moreover, their robustness across diverse BraTS\ndata partitions remains largely unexplored, leaving a critical gap in reliable\nevaluation. To address this, we first propose a dual-resolution bi-directional\nMamba (DRBD-Mamba), an efficient 3D segmentation model that captures\nmulti-scale long-range dependencies with minimal computational overhead. We\nleverage a space-filling curve to preserve spatial locality during 3D-to-1D\nfeature mapping, thereby reducing reliance on computationally expensive\nmulti-axial feature scans. To enrich feature representation, we propose a gated\nfusion module that adaptively integrates forward and reverse contexts, along\nwith a quantization block that improves robustness. We further propose five\nsystematic folds on BraTS2023 for rigorous evaluation of segmentation\ntechniques under diverse conditions and present analysis of common failure\nscenarios. On the 20% test set used by recent methods, our model achieves Dice\nimprovements of 0.10% for whole tumor, 1.75% for tumor core, and 0.93% for\nenhancing tumor. Evaluations on the proposed systematic folds demonstrate that\nour model maintains competitive whole tumor accuracy while achieving clear\naverage Dice gains of 1.16% for tumor core and 1.68% for enhancing tumor over\nexisting state-of-the-art. Furthermore, our model achieves a 15x efficiency\nimprovement while maintaining high segmentation accuracy, highlighting its\nrobustness and computational advantage over existing methods.\n","authors":["Danish Ali","Ajmal Mian","Naveed Akhtar","Ghulam Mubashar Hassan"],"pdf_url":"https://arxiv.org/pdf/2510.14383v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.26386v2","updated":"2025-10-28T14:48:45Z","published":"2025-09-30T15:19:43Z","title":"PANDA: Towards Generalist Video Anomaly Detection via Agentic AI\n  Engineer","summary":"  Video anomaly detection (VAD) is a critical yet challenging task due to the\ncomplex and diverse nature of real-world scenarios. Previous methods typically\nrely on domain-specific training data and manual adjustments when applying to\nnew scenarios and unseen anomaly types, suffering from high labor costs and\nlimited generalization. Therefore, we aim to achieve generalist VAD, \\ie,\nautomatically handle any scene and any anomaly types without training data or\nhuman involvement. In this work, we propose PANDA, an agentic AI engineer based\non MLLMs. Specifically, we achieve PANDA by comprehensively devising four key\ncapabilities: (1) self-adaptive scene-aware strategy planning, (2) goal-driven\nheuristic reasoning, (3) tool-augmented self-reflection, and (4) self-improving\nchain-of-memory. Concretely, we develop a self-adaptive scene-aware RAG\nmechanism, enabling PANDA to retrieve anomaly-specific knowledge for anomaly\ndetection strategy planning. Next, we introduce a latent anomaly-guided\nheuristic prompt strategy to enhance reasoning precision. Furthermore, PANDA\nemploys a progressive reflection mechanism alongside a suite of context-aware\ntools to iteratively refine decision-making in complex scenarios. Finally, a\nchain-of-memory mechanism enables PANDA to leverage historical experiences for\ncontinual performance improvement. Extensive experiments demonstrate that PANDA\nachieves state-of-the-art performance in multi-scenario, open-set, and complex\nscenario settings without training and manual involvement, validating its\ngeneralizable and robust anomaly detection capability. Code is released at\nhttps://github.com/showlab/PANDA.\n","authors":["Zhiwei Yang","Chen Gao","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2509.26386v2.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24474v1","updated":"2025-10-28T14:43:48Z","published":"2025-10-28T14:43:48Z","title":"Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated\n  Sampling","summary":"  Denoising generative models, such as diffusion and flow-based models, produce\nhigh-quality samples but require many denoising steps due to discretization\nerror. Flow maps, which estimate the average velocity between timesteps,\nmitigate this error and enable faster sampling. However, their training\ntypically demands architectural changes that limit compatibility with\npretrained flow models. We introduce Decoupled MeanFlow, a simple decoding\nstrategy that converts flow models into flow map models without architectural\nmodifications. Our method conditions the final blocks of diffusion transformers\non the subsequent timestep, allowing pretrained flow models to be directly\nrepurposed as flow maps. Combined with enhanced training techniques, this\ndesign enables high-quality generation in as few as 1 to 4 steps. Notably, we\nfind that training flow models and subsequently converting them is more\nefficient and effective than training flow maps from scratch. On ImageNet\n256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12,\nrespectively, surpassing prior art by a large margin. Furthermore, we achieve\nFID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the\nperformance of flow models while delivering over 100x faster inference.\n","authors":["Kyungmin Lee","Sihyun Yu","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2510.24474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.17336v2","updated":"2025-10-28T14:31:14Z","published":"2025-09-22T03:13:58Z","title":"Mano Technical Report","summary":"  Graphical user interfaces (GUIs) are the primary medium for human-computer\ninteraction, yet automating GUI interactions remains challenging due to the\ncomplexity of visual elements, dynamic environments, and the need for\nmulti-step reasoning. Existing methods based on vision-language models (VLMs)\noften suffer from limited resolution, domain mismatch, and insufficient\nsequential decisionmaking capability. To address these issues, we propose Mano,\na robust GUI agent built upon a multi-modal foundation model pre-trained on\nextensive web and computer system data. Our approach integrates a novel\nsimulated environment for high-fidelity data generation, a three-stage training\npipeline (supervised fine-tuning, offline reinforcement learning, and online\nreinforcement learning), and a verification module for error recovery. Mano\ndemonstrates state-of-the-art performance on multiple GUI benchmarks, including\nMind2Web and OSWorld, achieving significant improvements in success rate and\noperational accuracy. Our work provides new insights into the effective\nintegration of reinforcement learning with VLMs for practical GUI agent\ndeployment, highlighting the importance of domain-specific data, iterative\ntraining, and holistic reward design.\n","authors":["Tianyu Fu","Anyang Su","Chenxu Zhao","Hanning Wang","Minghui Wu","Zhe Yu","Fei Hu","Mingjia Shi","Wei Dong","Jiayao Wang","Yuyang Chen","Ruiyang Yu","Siran Peng","Menglin Li","Nan Huang","Haitian Wei","Jiawei Yu","Yi Xin","Xilin Zhao","Kai Gu","Ping Jiang","Sifan Zhou","Shuo Wang"],"pdf_url":"https://arxiv.org/pdf/2509.17336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24464v1","updated":"2025-10-28T14:30:47Z","published":"2025-10-28T14:30:47Z","title":"Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras","summary":"  Markerless multiview motion capture is often constrained by the need for\nprecise camera calibration, limiting accessibility for non-experts and\nin-the-wild captures. Existing calibration-free approaches mitigate this\nrequirement but suffer from high computational cost and reduced reconstruction\naccuracy.\n  We present Kineo, a fully automatic, calibration-free pipeline for markerless\nmotion capture from videos captured by unsynchronized, uncalibrated,\nconsumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf\ndetectors to simultaneously calibrate cameras, including Brown-Conrady\ndistortion coefficients, and reconstruct 3D keypoints and dense scene point\nmaps at metric scale. A confidence-driven spatio-temporal keypoint sampling\nstrategy, combined with graph-based global optimization, ensures robust\ncalibration at a fixed computational cost independent of sequence length. We\nfurther introduce a pairwise reprojection consensus score to quantify 3D\nreconstruction reliability for downstream tasks.\n  Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements\nover prior calibration-free methods. Compared to previous state-of-the-art\napproaches, Kineo reduces camera translation error by approximately 83-85%,\ncamera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by\n83-91%.\n  Kineo is also efficient in real-world scenarios, processing multi-view\nsequences faster than their duration in specific configuration (e.g., 36min to\nprocess 1h20min of footage). The full pipeline and evaluation code are openly\nreleased to promote reproducibility and practical adoption at\nhttps://liris-xr.github.io/kineo/.\n","authors":["Charles Javerliat","Pierre Raimbaud","Guillaume LavouÃ©"],"pdf_url":"https://arxiv.org/pdf/2510.24464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.21724v2","updated":"2025-10-28T14:26:23Z","published":"2025-05-27T20:12:46Z","title":"OmniResponse: Online Multimodal Conversational Response Generation in\n  Dyadic Interactions","summary":"  In this paper, we introduce Online Multimodal Conversational Response\nGeneration (OMCRG), a novel task designed to produce synchronized verbal and\nnon-verbal listener feedback online, based on the speaker's multimodal inputs.\nOMCRG captures natural dyadic interactions and introduces new challenges in\naligning generated audio with listeners' facial responses. To tackle these\nchallenges, we incorporate text as an intermediate modality to connect audio\nand facial responses. We propose OmniResponse, a Multimodal Large Language\nModel (MLLM) that autoregressively generates accurate multimodal listener\nresponses. OmniResponse leverages a pretrained LLM enhanced with two core\ncomponents: Chrono-Text Markup, which precisely timestamps generated text\ntokens, and TempoVoice, a controllable online text-to-speech (TTS) module that\noutputs speech synchronized with facial responses. To advance OMCRG research,\nwe offer ResponseNet, a dataset of 696 detailed dyadic interactions featuring\nsynchronized split-screen videos, multichannel audio, transcripts, and\nannotated facial behaviors. Comprehensive evaluations on ResponseNet\ndemonstrate that OmniResponse outperforms baseline models in terms of semantic\nspeech content, audio-visual synchronization, and generation quality. Our\ndataset, code, and models are publicly available.\n","authors":["Cheng Luo","Jianghui Wang","Bing Li","Siyang Song","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2505.21724v2.pdf","comment":"25 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.24456v1","updated":"2025-10-28T14:24:34Z","published":"2025-10-28T14:24:34Z","title":"A Critical Study towards the Detection of Parkinsons Disease using ML\n  Technologies","summary":"  The proposed solution is Deep Learning Technique that will be able classify\nthree types of tea leaves diseases from which two diseases are caused by the\npests and one due to pathogens (infectious organisms) and environmental\nconditions and also show the area damaged by a disease in leaves. Namely Red\nRust, Helopeltis and Red spider mite respectively. In this paper we have\nevaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for\nthe object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU\nrange of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%.\nWhile Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95\nand recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than\nSSD. Also used Mask R-CNN for Object Instance Segmentation where we have\nimplemented our custom method to calculate the damaged diseased portion of\nleaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red\nSpider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.\n","authors":["Vivek Chetia","Abdul Taher Khan","Rahish Gogoi","David Kapsian Khual","Purnendu Bikash","Sajal Saha"],"pdf_url":"https://arxiv.org/pdf/2510.24456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.20072v2","updated":"2025-10-28T14:22:20Z","published":"2025-08-27T17:39:11Z","title":"Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding\n  in Vision-Language-Action Policies","summary":"  Vision-Language-Action (VLA) models adapt large vision-language backbones to\nmap images and instructions into robot actions. However, prevailing VLAs either\ngenerate actions auto-regressively in a fixed left-to-right order or attach\nseparate MLP or diffusion heads outside the backbone, leading to fragmented\ninformation pathways and specialized training requirements that hinder a\nunified, scalable architecture. We present Discrete Diffusion VLA, a\nunified-transformer policy that models discretized action chunks with discrete\ndiffusion. The design retains diffusion's progressive refinement paradigm while\nremaining natively compatible with the discrete token interface of VLMs. Our\nmethod achieves an adaptive decoding order that resolves easy action elements\nbefore harder ones and uses secondary re-masking to revisit uncertain\npredictions across refinement rounds, which improves consistency and enables\nrobust error correction. This unified decoder preserves pre-trained\nvision-language priors, supports parallel decoding, breaks the autoregressive\nbottleneck, and reduces the number of function evaluations. Discrete Diffusion\nVLA achieves 96.3% avg. success rates on LIBERO, 71.2% visual matching on\nSimplerEnv-Fractal and 54.2% overall on SimplerEnv-Bridge, improving over\nautoregressive, MLP decoder and continuous diffusion baselines. These findings\nindicate that discrete-diffusion VLA supports precise action modeling and\nconsistent training, laying groundwork for scaling VLA to larger models and\ndatasets. Our project page is https://github.com/Liang-ZX/DiscreteDiffusionVLA\n","authors":["Zhixuan Liang","Yizhuo Li","Tianshuo Yang","Chengyue Wu","Sitong Mao","Tian Nian","Liuao Pei","Shunbo Zhou","Xiaokang Yang","Jiangmiao Pang","Yao Mu","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2508.20072v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2510.22200v2","updated":"2025-10-28T14:19:57Z","published":"2025-10-25T07:41:02Z","title":"LongCat-Video Technical Report","summary":"  Video generation is a critical pathway toward world models, with efficient\nlong video inference as a key capability. Toward this end, we introduce\nLongCat-Video, a foundational video generation model with 13.6B parameters,\ndelivering strong performance across multiple video generation tasks. It\nparticularly excels in efficient and high-quality long video generation,\nrepresenting our first step toward world models. Key features include: Unified\narchitecture for multiple tasks: Built on the Diffusion Transformer (DiT)\nframework, LongCat-Video supports Text-to-Video, Image-to-Video, and\nVideo-Continuation tasks with a single model; Long video generation:\nPretraining on Video-Continuation tasks enables LongCat-Video to maintain high\nquality and temporal coherence in the generation of minutes-long videos;\nEfficient inference: LongCat-Video generates 720p, 30fps videos within minutes\nby employing a coarse-to-fine generation strategy along both the temporal and\nspatial axes. Block Sparse Attention further enhances efficiency, particularly\nat high resolutions; Strong performance with multi-reward RLHF: Multi-reward\nRLHF training enables LongCat-Video to achieve performance on par with the\nlatest closed-source and leading open-source models. Code and model weights are\npublicly available to accelerate progress in the field.\n","authors":[" Meituan LongCat Team","Xunliang Cai","Qilong Huang","Zhuoliang Kang","Hongyu Li","Shijun Liang","Liya Ma","Siyu Ren","Xiaoming Wei","Rixu Xie","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.22200v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24448v1","updated":"2025-10-28T14:12:11Z","published":"2025-10-28T14:12:11Z","title":"Rethinking Visual Intelligence: Insights from Video Pretraining","summary":"  Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.\n","authors":["Pablo Acuaviva","Aram Davtyan","Mariam Hassan","Sebastian Stapf","Ahmad Rahimi","Alexandre Alahi","Paolo Favaro"],"pdf_url":"https://arxiv.org/pdf/2510.24448v1.pdf","comment":"Updated version from preprint arXiv:2506.07280 (Gen2Gen) focused on\n  visual intelligence. This work can be considered as v2"},{"id":"http://arxiv.org/abs/2507.14643v2","updated":"2025-10-28T14:09:59Z","published":"2025-07-19T14:38:03Z","title":"Multispectral State-Space Feature Fusion: Bridging Shared and\n  Cross-Parametric Interactions for Object Detection","summary":"  Modern multispectral feature fusion for object detection faces two critical\nlimitations: (1) Excessive preference for local complementary features over\ncross-modal shared semantics adversely affects generalization performance; and\n(2) The trade-off between the receptive field size and computational complexity\npresent critical bottlenecks for scalable feature modeling. Addressing these\nissues, a novel Multispectral State-Space Feature Fusion framework, dubbed\nMS2Fusion, is proposed based on the state space model (SSM), achieving\nefficient and effective fusion through a dual-path parametric interaction\nmechanism. More specifically, the first cross-parameter interaction branch\ninherits the advantage of cross-attention in mining complementary information\nwith cross-modal hidden state decoding in SSM. The second shared-parameter\nbranch explores cross-modal alignment with joint embedding to obtain\ncross-modal similar semantic features and structures through parameter sharing\nin SSM. Finally, these two paths are jointly optimized with SSM for fusing\nmultispectral features in a unified framework, allowing our MS2Fusion to enjoy\nboth functional complementarity and shared semantic space. In our extensive\nexperiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our\nMS2Fusion significantly outperforms other state-of-the-art multispectral object\ndetection methods, evidencing its superiority. Moreover, MS2Fusion is general\nand applicable to other multispectral perception tasks. We show that, even\nwithout specific design, MS2Fusion achieves state-of-the-art results on RGB-T\nsemantic segmentation and RGBT salient object detection, showing its\ngenerality. The source code will be available at\nhttps://github.com/61s61min/MS2Fusion.git.\n","authors":["Jifeng Shen","Haibo Zhan","Shaohua Dong","Xin Zuo","Wankou Yang","Haibin Ling"],"pdf_url":"https://arxiv.org/pdf/2507.14643v2.pdf","comment":"submitted on 30/4/2025, Accepted by Information Fusion"},{"id":"http://arxiv.org/abs/2510.24446v1","updated":"2025-10-28T14:09:05Z","published":"2025-10-28T14:09:05Z","title":"SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box\n  Adversarial Paraphrasing in Text Autoencoder Latent Space","summary":"  Multimodal large language models (MLLMs) have shown impressive capabilities\nin vision-language tasks such as reasoning segmentation, where models generate\nsegmentation masks based on textual queries. While prior work has primarily\nfocused on perturbing image inputs, semantically equivalent textual\nparaphrases-crucial in real-world applications where users express the same\nintent in varied ways-remain underexplored. To address this gap, we introduce a\nnovel adversarial paraphrasing task: generating grammatically correct\nparaphrases that preserve the original query meaning while degrading\nsegmentation performance. To evaluate the quality of adversarial paraphrases,\nwe develop a comprehensive automatic evaluation protocol validated with human\nstudies. Furthermore, we introduce SPARTA-a black-box, sentence-level\noptimization method that operates in the low-dimensional semantic latent space\nof a text autoencoder, guided by reinforcement learning. SPARTA achieves\nsignificantly higher success rates, outperforming prior methods by up to 2x on\nboth the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive\nbaselines to assess the robustness of advanced reasoning segmentation models.\nWe reveal that they remain vulnerable to adversarial paraphrasing-even under\nstrict semantic and grammatical constraints. All code and data will be released\npublicly upon acceptance.\n","authors":["Viktoriia Zinkovich","Anton Antonov","Andrei Spiridonov","Denis Shepelev","Andrey Moskalenko","Daria Pugacheva","Elena Tutubalina","Andrey Kuznetsov","Vlad Shakhuro"],"pdf_url":"https://arxiv.org/pdf/2510.24446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24437v1","updated":"2025-10-28T14:04:19Z","published":"2025-10-28T14:04:19Z","title":"Deeply-Conditioned Image Compression via Self-Generated Priors","summary":"  Learned image compression (LIC) has shown great promise for achieving high\nrate-distortion performance. However, current LIC methods are often limited in\ntheir capability to model the complex correlation structures inherent in\nnatural images, particularly the entanglement of invariant global structures\nwith transient local textures within a single monolithic representation. This\nlimitation precipitates severe geometric deformation at low bitrates. To\naddress this, we introduce a framework predicated on functional decomposition,\nwhich we term Deeply-Conditioned Image Compression via self-generated priors\n(DCIC-sgp). Our central idea is to first encode a potent, self-generated prior\nto encapsulate the image's structural backbone. This prior is subsequently\nutilized not as mere side-information, but to holistically modulate the entire\ncompression pipeline. This deep conditioning, most critically of the analysis\ntransform, liberates it to dedicate its representational capacity to the\nresidual, high-entropy details. This hierarchical, dependency-driven approach\nachieves an effective disentanglement of information streams. Our extensive\nexperiments validate this assertion; visual analysis demonstrates that our\nmethod substantially mitigates the geometric deformation artifacts that plague\nconventional codecs at low bitrates. Quantitatively, our framework establishes\nhighly competitive performance, achieving significant BD-rate reductions of\n14.4%, 15.7%, and 15.1% against the VVC test model VTM-12.1 on the Kodak, CLIC,\nand Tecnick datasets.\n","authors":["Zhineng Zhao","Zhihai He","Zikun Zhou","Siwei Ma","Yaowei Wang"],"pdf_url":"https://arxiv.org/pdf/2510.24437v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08423v4","updated":"2025-10-28T13:53:29Z","published":"2025-05-13T10:35:57Z","title":"DArFace: Deformation Aware Robustness for Low Quality Face Recognition","summary":"  Facial recognition systems have achieved remarkable success by leveraging\ndeep neural networks, advanced loss functions, and large-scale datasets.\nHowever, their performance often deteriorates in real-world scenarios involving\nlow-quality facial images. Such degradations, common in surveillance footage or\nstandoff imaging include low resolution, motion blur, and various distortions,\nresulting in a substantial domain gap from the high-quality data typically used\nduring training. While existing approaches attempt to address robustness by\nmodifying network architectures or modeling global spatial transformations,\nthey frequently overlook local, non-rigid deformations that are inherently\npresent in real-world settings. In this work, we introduce \\textbf{DArFace}, a\n\\textbf{D}eformation-\\textbf{A}ware \\textbf{r}obust \\textbf{Face} recognition\nframework that enhances robustness to such degradations without requiring\npaired high- and low-quality training samples. Our method adversarially\nintegrates both global transformations (e.g., rotation, translation) and local\nelastic deformations during training to simulate realistic low-quality\nconditions. Moreover, we introduce a contrastive objective to enforce identity\nconsistency across different deformed views. Extensive evaluations on\nlow-quality benchmarks including TinyFace, IJB-B, and IJB-C demonstrate that\nDArFace surpasses state-of-the-art methods, with significant gains attributed\nto the inclusion of local deformation modeling.\n","authors":["Sadaf Gulshad","Abdullah Aldahlawi"],"pdf_url":"https://arxiv.org/pdf/2505.08423v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05900v2","updated":"2025-10-28T13:30:55Z","published":"2024-10-08T10:57:33Z","title":"MTFL: Multi-Timescale Feature Learning for Weakly-Supervised Anomaly\n  Detection in Surveillance Videos","summary":"  Detection of anomaly events is relevant for public safety and requires a\ncombination of fine-grained motion information and contextual events at\nvariable time-scales. To this end, we propose a Multi-Timescale Feature\nLearning (MTFL) method to enhance the representation of anomaly features.\nShort, medium, and long temporal tubelets are employed to extract\nspatio-temporal video features using a Video Swin Transformer. Experimental\nresults demonstrate that MTFL outperforms state-of-the-art methods on the\nUCF-Crime dataset, achieving an anomaly detection performance 89.78% AUC.\nMoreover, it performs complementary to SotA with 95.32% AUC on the ShanghaiTech\nand 84.57% AP on the XD-Violence dataset. Furthermore, we generate an extended\ndataset of the UCF-Crime for development and evaluation on a wider range of\nanomalies, namely Video Anomaly Detection Dataset (VADD), involving 2,591\nvideos in 18 classes with extensive coverage of realistic anomalies.\n","authors":["Yiling Zhang","Erkut Akdag","Egor Bondarev","Peter H. N. De With"],"pdf_url":"https://arxiv.org/pdf/2410.05900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24414v1","updated":"2025-10-28T13:27:38Z","published":"2025-10-28T13:27:38Z","title":"XAI Evaluation Framework for Semantic Segmentation","summary":"  Ensuring transparency and trust in artificial intelligence (AI) models is\nessential, particularly as they are increasingly applied in safety-critical and\nhigh-stakes domains. Explainable AI (XAI) has emerged as a promising approach\nto address this challenge, yet the rigorous evaluation of XAI methods remains\ncrucial for optimizing the trade-offs between model complexity, predictive\nperformance, and interpretability. While extensive progress has been achieved\nin evaluating XAI techniques for classification tasks, evaluation strategies\ntailored to semantic segmentation remain relatively underexplored. This work\nintroduces a comprehensive and systematic evaluation framework specifically\ndesigned for assessing XAI in semantic segmentation, explicitly accounting for\nboth spatial and contextual task complexities. The framework employs\npixel-level evaluation strategies and carefully designed metrics to provide\nfine-grained interpretability insights. Simulation results using recently\nadapted class activation mapping (CAM)-based XAI schemes demonstrate the\nefficiency, robustness, and reliability of the proposed methodology. These\nfindings contribute to advancing transparent, trustworthy, and accountable\nsemantic segmentation models.\n","authors":["Reem Hammoud","Abdul karim Gizzini","Ali J. Ghandour"],"pdf_url":"https://arxiv.org/pdf/2510.24414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24413v1","updated":"2025-10-28T13:23:32Z","published":"2025-10-28T13:23:32Z","title":"50 Years of Water Body Monitoring: The Case of Qaraaoun Reservoir,\n  Lebanon","summary":"  The sustainable management of the Qaraaoun Reservoir, the largest surface\nwater body in Lebanon located in the Bekaa Plain, depends on reliable\nmonitoring of its storage volume despite frequent sensor malfunctions and\nlimited maintenance capacity. This study introduces a sensor-free approach that\nintegrates open-source satellite imagery, advanced water-extent segmentation,\nand machine learning to estimate the reservoir surface area and volume in near\nreal time. Sentinel-2 and Landsat images are processed, where surface water is\ndelineated using a newly proposed water segmentation index. A machine learning\nmodel based on Support Vector Regression (SVR) is trained on a curated dataset\nthat includes water surface area, water level, and water volume calculations\nusing a reservoir bathymetry survey. The model is then able to estimate\nreservoir volume relying solely on surface area extracted from satellite\nimagery, without the need for ground measurements. Water segmentation using the\nproposed index aligns with ground truth for more than 95 percent of the\nshoreline. Hyperparameter tuning with GridSearchCV yields an optimized SVR\nperformance with error under 1.5 percent of full reservoir capacity and\ncoefficients of determination exceeding 0.98. These results demonstrate the\nrobustness and cost-effectiveness of the method, offering a practical solution\nfor continuous, sensor-independent monitoring of reservoir storage. The\nproposed methodology can be replicated for other water bodies, and the\nresulting 50 years of time-series data is valuable for research on climate\nchange and environmental patterns.\n","authors":["Ali Ahmad Faour","Nabil Amacha","Ali J. Ghandour"],"pdf_url":"https://arxiv.org/pdf/2510.24413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24411v1","updated":"2025-10-28T13:22:39Z","published":"2025-10-28T13:22:39Z","title":"OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid\n  Validation in Realistic Workflows","summary":"  Computer-using agents powered by Vision-Language Models (VLMs) have\ndemonstrated human-like capabilities in operating digital environments like\nmobile platforms. While these agents hold great promise for advancing digital\nautomation, their potential for unsafe operations, such as system compromise\nand privacy leakage, is raising significant concerns. Detecting these safety\nconcerns across the vast and complex operational space of mobile environments\npresents a formidable challenge that remains critically underexplored. To\nestablish a foundation for mobile agent safety research, we introduce\nMobileRisk-Live, a dynamic sandbox environment accompanied by a safety\ndetection benchmark comprising realistic trajectories with fine-grained\nannotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety\ndetection framework that synergistically combines a Formal Verifier for\ndetecting explicit system-level violations with a VLM-based Contextual Judge\nfor assessing contextual risks and agent actions. Experiments show that\nOS-Sentinel achieves 10%-30% improvements over existing approaches across\nmultiple metrics. Further analysis provides critical insights that foster the\ndevelopment of safer and more reliable autonomous mobile agents.\n","authors":["Qiushi Sun","Mukai Li","Zhoumianze Liu","Zhihui Xie","Fangzhi Xu","Zhangyue Yin","Kanzhi Cheng","Zehao Li","Zichen Ding","Qi Liu","Zhiyong Wu","Zhuosheng Zhang","Ben Kao","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2510.24411v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2510.24410v1","updated":"2025-10-28T13:22:24Z","published":"2025-10-28T13:22:24Z","title":"A Hybrid Approach for Visual Multi-Object Tracking","summary":"  This paper proposes a visual multi-object tracking method that jointly\nemploys stochastic and deterministic mechanisms to ensure identifier\nconsistency for unknown and time-varying target numbers under nonlinear\ndynamics. A stochastic particle filter addresses nonlinear dynamics and\nnon-Gaussian noise, with support from particle swarm optimization (PSO) to\nguide particles toward state distribution modes and mitigate divergence through\nproposed fitness measures incorporating motion consistency, appearance\nsimilarity, and social-interaction cues with neighboring targets. Deterministic\nassociation further enforces identifier consistency via a proposed cost matrix\nincorporating spatial consistency between particles and current detections,\ndetection confidences, and track penalties. Subsequently, a novel scheme is\nproposed for the smooth updating of target states while preserving their\nidentities, particularly for weak tracks during interactions with other targets\nand prolonged occlusions. Moreover, velocity regression over past states\nprovides trend-seed velocities, enhancing particle sampling and state updates.\nThe proposed tracker is designed to operate flexibly for both pre-recorded\nvideos and camera live streams, where future frames are unavailable.\nExperimental results confirm superior performance compared to state-of-the-art\ntrackers. The source-code reference implementations of both the proposed method\nand compared-trackers are provided on GitHub:\nhttps://github.com/SDU-VelKoTek/GenTrack2\n","authors":["Toan Van Nguyen","Rasmus G. K. Christiansen","Dirk Kraft","Leon Bodenhagen"],"pdf_url":"https://arxiv.org/pdf/2510.24410v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2510.24399v1","updated":"2025-10-28T13:13:20Z","published":"2025-10-28T13:13:20Z","title":"GenTrack: A New Generation of Multi-Object Tracking","summary":"  This paper introduces a novel multi-object tracking (MOT) method, dubbed\nGenTrack, whose main contributions include: a hybrid tracking approach\nemploying both stochastic and deterministic manners to robustly handle unknown\nand time-varying numbers of targets, particularly in maintaining target\nidentity (ID) consistency and managing nonlinear dynamics, leveraging particle\nswarm optimization (PSO) with some proposed fitness measures to guide\nstochastic particles toward their target distribution modes, enabling effective\ntracking even with weak and noisy object detectors, integration of social\ninteractions among targets to enhance PSO-guided particles as well as improve\ncontinuous updates of both strong (matched) and weak (unmatched) tracks,\nthereby reducing ID switches and track loss, especially during occlusions, a\nGenTrack-based redefined visual MOT baseline incorporating a comprehensive\nstate and observation model based on space consistency, appearance, detection\nconfidence, track penalties, and social scores for systematic and efficient\ntarget updates, and the first-ever publicly available source-code reference\nimplementation with minimal dependencies, featuring three variants, including\nGenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.\nExperimental results have shown that GenTrack provides superior performance on\nstandard benchmarks and real-world scenarios compared to state-of-the-art\ntrackers, with integrated implementations of baselines for fair comparison.\nPotential directions for future work are also discussed. The source-code\nreference implementations of both the proposed method and compared-trackers are\nprovided on GitHub: https://github.com/SDU-VelKoTek/GenTrack\n","authors":["Toan Van Nguyen","Rasmus G. K. Christiansen","Dirk Kraft","Leon Bodenhagen"],"pdf_url":"https://arxiv.org/pdf/2510.24399v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2510.24398v1","updated":"2025-10-28T13:13:01Z","published":"2025-10-28T13:13:01Z","title":"Unsupervised Detection of Post-Stroke Brain Abnormalities","summary":"  Post-stroke MRI not only delineates focal lesions but also reveals secondary\nstructural changes, such as atrophy and ventricular enlargement. These\nabnormalities, increasingly recognised as imaging biomarkers of recovery and\noutcome, remain poorly captured by supervised segmentation methods. We evaluate\nREFLECT, a flow-based generative model, for unsupervised detection of both\nfocal and non-lesional abnormalities in post-stroke patients. Using dual-expert\ncentral-slice annotations on ATLAS data, performance was assessed at the object\nlevel with Free-Response ROC analysis for anomaly maps. Two models were trained\non lesion-free slices from stroke patients (ATLAS) and on healthy controls\n(IXI) to test the effect of training data. On ATLAS test subjects, the\nIXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and\nimproved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43).\nTraining on fully healthy anatomy improves the modelling of normal variability,\nenabling broader and more reliable detection of structural abnormalities.\n","authors":["Youwan MahÃ©","Elise Bannier","StÃ©phanie Leplaideur","Elisa Fromont","Francesca Galassi"],"pdf_url":"https://arxiv.org/pdf/2510.24398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.19585v2","updated":"2025-10-28T13:04:38Z","published":"2025-10-22T13:37:52Z","title":"Detecting Latin in Historical Books with Large Language Models: A\n  Multimodal Benchmark","summary":"  This paper presents a novel task of extracting Latin fragments from\nmixed-language historical documents with varied layouts. We benchmark and\nevaluate the performance of large foundation models against a multimodal\ndataset of 724 annotated pages. The results demonstrate that reliable Latin\ndetection with contemporary models is achievable. Our study provides the first\ncomprehensive analysis of these models' capabilities and limits for this task.\n","authors":["Yu Wu","Ke Shu","Jonas Fischer","Lidia Pivovarova","David Rosson","Eetu MÃ¤kelÃ¤","Mikko Tolonen"],"pdf_url":"https://arxiv.org/pdf/2510.19585v2.pdf","comment":"Under review. Both the dataset and code will be published"},{"id":"http://arxiv.org/abs/2405.03520v2","updated":"2025-10-28T13:04:23Z","published":"2024-05-06T14:37:07Z","title":"Is Sora a World Simulator? A Comprehensive Survey on General World\n  Models and Beyond","summary":"  General world models represent a crucial pathway toward achieving Artificial\nGeneral Intelligence (AGI), serving as the cornerstone for various applications\nranging from virtual environments to decision-making systems. Recently, the\nemergence of the Sora model has attained significant attention due to its\nremarkable simulation capabilities, which exhibits an incipient comprehension\nof physical laws. In this survey, we embark on a comprehensive exploration of\nthe latest advancements in world models. Our analysis navigates through the\nforefront of generative methodologies in video generation, where world models\nstand as pivotal constructs facilitating the synthesis of highly realistic\nvisual content. Additionally, we scrutinize the burgeoning field of\nautonomous-driving world models, meticulously delineating their indispensable\nrole in reshaping transportation and urban mobility. Furthermore, we delve into\nthe intricacies inherent in world models deployed within autonomous agents,\nshedding light on their profound significance in enabling intelligent\ninteractions within dynamic environmental contexts. At last, we examine\nchallenges and limitations of world models, and discuss their potential future\ndirections. We hope this survey can serve as a foundational reference for the\nresearch community and inspire continued innovation. This survey will be\nregularly updated at:\nhttps://github.com/GigaAI-research/General-World-Models-Survey.\n","authors":["Zheng Zhu","Xiaofeng Wang","Wangbo Zhao","Chen Min","Bohan Li","Nianchen Deng","Min Dou","Yuqi Wang","Botian Shi","Kai Wang","Chi Zhang","Yang You","Zhaoxiang Zhang","Dawei Zhao","Liang Xiao","Jian Zhao","Jiwen Lu","Guan Huang"],"pdf_url":"https://arxiv.org/pdf/2405.03520v2.pdf","comment":"This survey will be regularly updated at:\n  https://github.com/GigaAI-research/General-World-Models-Survey"},{"id":"http://arxiv.org/abs/2510.24385v1","updated":"2025-10-28T13:01:42Z","published":"2025-10-28T13:01:42Z","title":"When are radiology reports useful for training medical image\n  classifiers?","summary":"  Medical images used to train machine learning models are often accompanied by\nradiology reports containing rich expert annotations. However, relying on these\nreports as inputs for clinical prediction requires the timely manual work of a\ntrained radiologist. This raises a natural question: when can radiology reports\nbe leveraged during training to improve image-only classification? Prior works\nare limited to evaluating pre-trained image representations by fine-tuning them\nto predict diagnostic labels, often extracted from reports, ignoring tasks with\nlabels that are weakly associated with the text. To address this gap, we\nconduct a systematic study of how radiology reports can be used during both\npre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,\n12-month readmission), and under varying training set sizes. Our findings\nreveal that: (1) Leveraging reports during pre-training is beneficial for\ndownstream classification tasks where the label is well-represented in the\ntext; however, pre-training through explicit image-text alignment can be\ndetrimental in settings where it's not; (2) Fine-tuning with reports can lead\nto significant improvements and even have a larger impact than the pre-training\nmethod in certain settings. These results provide actionable insights into when\nand how to leverage privileged text data to train medical image classifiers\nwhile highlighting gaps in current research.\n","authors":["Herman BergstrÃ¶m","Zhongqi Yue","Fredrik D. Johansson"],"pdf_url":"https://arxiv.org/pdf/2510.24385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24379v1","updated":"2025-10-28T12:57:42Z","published":"2025-10-28T12:57:42Z","title":"A Luminance-Aware Multi-Scale Network for Polarization Image Fusion with\n  a Multi-Scene Dataset","summary":"  Polarization image fusion combines S0 and DOLP images to reveal surface\nroughness and material properties through complementary texture features, which\nhas important applications in camouflage recognition, tissue pathology\nanalysis, surface defect detection and other fields. To intergrate\ncoL-Splementary information from different polarized images in complex\nluminance environment, we propose a luminance-aware multi-scale network (MLSN).\nIn the encoder stage, we propose a multi-scale spatial weight matrix through a\nbrightness-branch , which dynamically weighted inject the luminance into the\nfeature maps, solving the problem of inherent contrast difference in polarized\nimages. The global-local feature fusion mechanism is designed at the bottleneck\nlayer to perform windowed self-attention computation, to balance the global\ncontext and local details through residual linking in the feature dimension\nrestructuring stage. In the decoder stage, to further improve the adaptability\nto complex lighting, we propose a Brightness-Enhancement module, establishing\nthe mapping relationship between luminance distribution and texture features,\nrealizing the nonlinear luminance correction of the fusion result. We also\npresent MSP, an 1000 pairs of polarized images that covers 17 types of indoor\nand outdoor complex lighting scenes. MSP provides four-direction polarization\nraw maps, solving the scarcity of high-quality datasets in polarization image\nfusion. Extensive experiment on MSP, PIF and GAND datasets verify that the\nproposed MLSN outperms the state-of-the-art methods in subjective and objective\nevaluations, and the MS-SSIM and SD metircs are higher than the average values\nof other methods by 8.57%, 60.64%, 10.26%, 63.53%, 22.21%, and 54.31%,\nrespectively. The source code and dataset is avalable at\nhttps://github.com/1hzf/MLS-UNet.\n","authors":["Zhuangfan Huang","Xiaosong Li","Gao Wang","Tao Ye","Haishu Tan","Huafeng Li"],"pdf_url":"https://arxiv.org/pdf/2510.24379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24378v1","updated":"2025-10-28T12:56:48Z","published":"2025-10-28T12:56:48Z","title":"Stroke Lesion Segmentation in Clinical Workflows: A Modular,\n  Lightweight, and Deployment-Ready Tool","summary":"  Deep learning frameworks such as nnU-Net achieve state-of-the-art performance\nin brain lesion segmentation but remain difficult to deploy clinically due to\nheavy dependencies and monolithic design. We introduce \\textit{StrokeSeg}, a\nmodular and lightweight framework that translates research-grade stroke lesion\nsegmentation models into deployable applications. Preprocessing, inference, and\npostprocessing are decoupled: preprocessing relies on the Anima toolbox with\nBIDS-compliant outputs, and inference uses ONNX Runtime with \\texttt{Float16}\nquantisation, reducing model size by about 50\\%. \\textit{StrokeSeg} provides\nboth graphical and command-line interfaces and is distributed as Python scripts\nand as a standalone Windows executable. On a held-out set of 300 sub-acute and\nchronic stroke subjects, segmentation performance was equivalent to the\noriginal PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that\nhigh-performing research pipelines can be transformed into portable, clinically\nusable tools.\n","authors":["Yann Kerverdo","Florent Leray","Youwan MahÃ©","StÃ©phanie Leplaideur","Francesca Galassi"],"pdf_url":"https://arxiv.org/pdf/2510.24378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24374v1","updated":"2025-10-28T12:51:53Z","published":"2025-10-28T12:51:53Z","title":"Decoupling What to Count and Where to See for Referring Expression\n  Counting","summary":"  Referring Expression Counting (REC) extends class-level object counting to\nthe fine-grained subclass-level, aiming to enumerate objects matching a textual\nexpression that specifies both the class and distinguishing attribute. A\nfundamental challenge, however, has been overlooked: annotation points are\ntypically placed on class-representative locations (e.g., heads), forcing\nmodels to focus on class-level features while neglecting attribute information\nfrom other visual regions (e.g., legs for \"walking\"). To address this, we\npropose W2-Net, a novel framework that explicitly decouples the problem into\n\"what to count\" and \"where to see\" via a dual-query mechanism. Specifically,\nalongside the standard what-to-count (w2c) queries that localize the object, we\nintroduce dedicated where-to-see (w2s) queries. The w2s queries are guided to\nseek and extract features from attribute-specific visual regions, enabling\nprecise subclass discrimination. Furthermore, we introduce Subclass Separable\nMatching (SSM), a novel matching strategy that incorporates a repulsive force\nto enhance inter-subclass separability during label assignment. W2-Net\nsignificantly outperforms the state-of-the-art on the REC-8K dataset, reducing\ncounting error by 22.5% (validation) and 18.0% (test), and improving\nlocalization F1 by 7% and 8%, respectively. Code will be available.\n","authors":["Yuda Zou","Zijian Zhang","Yongchao Xu"],"pdf_url":"https://arxiv.org/pdf/2510.24374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11842v3","updated":"2025-10-28T12:44:07Z","published":"2025-05-17T05:06:38Z","title":"Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs","summary":"  The increasing deployment of Large Vision-Language Models (LVLMs) raises\nsafety concerns under potential malicious inputs. However, existing multimodal\nsafety evaluations primarily focus on model vulnerabilities exposed by static\nimage inputs, ignoring the temporal dynamics of video that may induce distinct\nsafety risks. To bridge this gap, we introduce Video-SafetyBench, the first\ncomprehensive benchmark designed to evaluate the safety of LVLMs under\nvideo-text attacks. It comprises 2,264 video-text pairs spanning 48\nfine-grained unsafe categories, each pairing a synthesized video with either a\nharmful query, which contains explicit malice, or a benign query, which appears\nharmless but triggers harmful behavior when interpreted alongside the video. To\ngenerate semantically accurate videos for safety evaluation, we design a\ncontrollable pipeline that decomposes video semantics into subject images (what\nis shown) and motion text (how it moves), which jointly guide the synthesis of\nquery-relevant videos. To effectively evaluate uncertain or borderline harmful\noutputs, we propose RJScore, a novel LLM-based metric that incorporates the\nconfidence of judge models and human-aligned decision threshold calibration.\nExtensive experiments show that benign-query video composition achieves average\nattack success rates of 67.2%, revealing consistent vulnerabilities to\nvideo-induced attacks. We believe Video-SafetyBench will catalyze future\nresearch into video-based safety evaluation and defense strategies.\n","authors":["Xuannan Liu","Zekun Li","Zheqi He","Peipei Li","Shuhan Xia","Xing Cui","Huaibo Huang","Xi Yang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2505.11842v3.pdf","comment":"Accepted by NeurIPS 2025 Dataset and Benchmark Track, Project page:\n  https://liuxuannan.github.io/Video-SafetyBench.github.io/"},{"id":"http://arxiv.org/abs/2510.24366v1","updated":"2025-10-28T12:42:33Z","published":"2025-10-28T12:42:33Z","title":"Adaptive Knowledge Transferring with Switching Dual-Student Framework\n  for Semi-Supervised Medical Image Segmentation","summary":"  Teacher-student frameworks have emerged as a leading approach in\nsemi-supervised medical image segmentation, demonstrating strong performance\nacross various tasks. However, the learning effects are still limited by the\nstrong correlation and unreliable knowledge transfer process between teacher\nand student networks. To overcome this limitation, we introduce a novel\nswitching Dual-Student architecture that strategically selects the most\nreliable student at each iteration to enhance dual-student collaboration and\nprevent error reinforcement. We also introduce a strategy of Loss-Aware\nExponential Moving Average to dynamically ensure that the teacher absorbs\nmeaningful information from students, improving the quality of pseudo-labels.\nOur plug-and-play framework is extensively evaluated on 3D medical image\nsegmentation datasets, where it outperforms state-of-the-art semi-supervised\nmethods, demonstrating its effectiveness in improving segmentation accuracy\nunder limited supervision.\n","authors":["Thanh-Huy Nguyen","Hoang-Thien Nguyen","Ba-Thinh Lam","Vi Vu","Bach X. Nguyen","Jianhua Xing","Tianyang Wang","Xingjian Li","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2510.24366v1.pdf","comment":"The paper is under review at Pattern Recognition Journal"},{"id":"http://arxiv.org/abs/2505.24424v2","updated":"2025-10-28T12:08:40Z","published":"2025-05-30T10:04:00Z","title":"Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning","summary":"  Vision-language models like CLIP have demonstrated remarkable zero-shot\ncapabilities in classification and retrieval. However, these models often\nstruggle with compositional reasoning - the ability to understand the\nrelationships between concepts. A recent benchmark, SugarCrepe++, reveals that\nprevious works on improving compositionality have mainly improved lexical\nsensitivity but neglected semantic understanding. In addition, downstream\nretrieval performance often deteriorates, although one would expect that\nimproving compositionality should enhance retrieval. In this work, we introduce\nCLIC (Compositionally-aware Learning in CLIP), a fine-tuning method based on a\nnovel training technique combining multiple images and their associated\ncaptions. CLIC improves compositionality across architectures as well as\ndifferently pre-trained CLIP models, both in terms of lexical and semantic\nunderstanding, and achieves consistent gains in retrieval performance. This\neven applies to the recent CLIPS, which achieves SOTA retrieval performance.\nNevertheless, the short fine-tuning with CLIC leads to an improvement in\nretrieval and to the best compositional CLIP model on SugarCrepe++. All our\nmodels and code are available at https://clic-compositional-clip.github.io\n","authors":["Amit Peleg","Naman Deep Singh","Matthias Hein"],"pdf_url":"https://arxiv.org/pdf/2505.24424v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24335v1","updated":"2025-10-28T11:57:33Z","published":"2025-10-28T11:57:33Z","title":"NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation","summary":"  We present NVSim, a framework that automatically constructs large-scale,\nnavigable indoor simulators from only common image sequences, overcoming the\ncost and scalability limitations of traditional 3D scanning. Our approach\nadapts 3D Gaussian Splatting to address visual artifacts on sparsely observed\nfloors a common issue in robotic traversal data. We introduce Floor-Aware\nGaussian Splatting to ensure a clean, navigable ground plane, and a novel\nmesh-free traversability checking algorithm that constructs a topological graph\nby directly analyzing rendered views. We demonstrate our system's ability to\ngenerate valid, large-scale navigation graphs from real-world data. A video\ndemonstration is avilable at https://youtu.be/tTiIQt6nXC8\n","authors":["Mingyu Jeong","Eunsung Kim","Sehun Park","Andrew Jaeyong Choi"],"pdf_url":"https://arxiv.org/pdf/2510.24335v1.pdf","comment":"9 pages, 10 figures"},{"id":"http://arxiv.org/abs/2510.24332v1","updated":"2025-10-28T11:55:45Z","published":"2025-10-28T11:55:45Z","title":"Sound Source Localization for Spatial Mapping of Surgical Actions in\n  Dynamic Scenes","summary":"  Purpose: Surgical scene understanding is key to advancing computer-aided and\nintelligent surgical systems. Current approaches predominantly rely on visual\ndata or end-to-end learning, which limits fine-grained contextual modeling.\nThis work aims to enhance surgical scene representations by integrating 3D\nacoustic information, enabling temporally and spatially aware multimodal\nunderstanding of surgical environments.\n  Methods: We propose a novel framework for generating 4D audio-visual\nrepresentations of surgical scenes by projecting acoustic localization\ninformation from a phased microphone array onto dynamic point clouds from an\nRGB-D camera. A transformer-based acoustic event detection module identifies\nrelevant temporal segments containing tool-tissue interactions which are\nspatially localized in the audio-visual scene representation. The system was\nexperimentally evaluated in a realistic operating room setup during simulated\nsurgical procedures performed by experts.\n  Results: The proposed method successfully localizes surgical acoustic events\nin 3D space and associates them with visual scene elements. Experimental\nevaluation demonstrates accurate spatial sound localization and robust fusion\nof multimodal data, providing a comprehensive, dynamic representation of\nsurgical activity.\n  Conclusion: This work introduces the first approach for spatial sound\nlocalization in dynamic surgical scenes, marking a significant advancement\ntoward multimodal surgical scene representations. By integrating acoustic and\nvisual data, the proposed framework enables richer contextual understanding and\nprovides a foundation for future intelligent and autonomous surgical systems.\n","authors":["Jonas Hein","Lazaros Vlachopoulos","Maurits Geert Laurent Olthof","Bastian Sigrist","Philipp FÃ¼rnstahl","Matthias Seibold"],"pdf_url":"https://arxiv.org/pdf/2510.24332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24331v1","updated":"2025-10-28T11:55:24Z","published":"2025-10-28T11:55:24Z","title":"What do vision-language models see in the context? Investigating\n  multimodal in-context learning","summary":"  In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks\nfrom demonstration examples without parameter updates. Although it has been\nextensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)\nremains underexplored. In this work, we present a systematic study of ICL in\nVLMs, evaluating seven models spanning four architectures on three image\ncaptioning benchmarks. We analyze how prompt design, architectural choices, and\ntraining strategies influence multimodal ICL. To our knowledge, we are the\nfirst to analyze how attention patterns in VLMs vary with an increasing number\nof in-context demonstrations. Our results reveal that training on imag-text\ninterleaved data enhances ICL performance but does not imply effective\nintegration of visual and textual information from demonstration examples. In\ncontrast, instruction tuning improves instruction-following but can reduce\nreliance on in-context demonstrations, suggesting a trade-off between\ninstruction alignment and in-context adaptation. Attention analyses further\nshow that current VLMs primarily focus on textual cues and fail to leverage\nvisual information, suggesting a limited capacity for multimodal integration.\nThese findings highlight key limitations in the ICL abilities of current VLMs\nand provide insights for enhancing their ability to learn from multimodal\nin-context examples.\n","authors":["Gabriel O. dos Santos","Esther Colombini","Sandra Avila"],"pdf_url":"https://arxiv.org/pdf/2510.24331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24321v1","updated":"2025-10-28T11:39:22Z","published":"2025-10-28T11:39:22Z","title":"Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt\n  Learning","summary":"  Remote sensing applications increasingly rely on deep learning for scene\nclassification. However, their performance is often constrained by the scarcity\nof labeled data and the high cost of annotation across diverse geographic and\nsensor domains. While recent vision-language models like CLIP have shown\npromise by learning transferable representations at scale by aligning visual\nand textual modalities, their direct application to remote sensing remains\nsuboptimal due to significant domain gaps and the need for task-specific\nsemantic adaptation. To address this critical challenge, we systematically\nexplore prompt learning as a lightweight and efficient adaptation strategy for\nfew-shot remote sensing image scene classification. We evaluate several\nrepresentative methods, including Context Optimization, Conditional Context\nOptimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating\nConstraints. These approaches reflect complementary design philosophies: from\nstatic context optimization to conditional prompts for enhanced generalization,\nmulti-modal prompts for joint vision-language adaptation, and semantically\nregularized prompts for stable learning without forgetting. We benchmark these\nprompt-learning methods against two standard baselines: zero-shot CLIP with\nhand-crafted prompts and a linear probe trained on frozen CLIP features.\nThrough extensive experiments on multiple benchmark remote sensing datasets,\nincluding cross-dataset generalization tests, we demonstrate that prompt\nlearning consistently outperforms both baselines in few-shot scenarios.\nNotably, Prompting with Self-Regulating Constraints achieves the most robust\ncross-domain performance. Our findings underscore prompt learning as a scalable\nand efficient solution for bridging the domain gap in satellite and aerial\nimagery, providing a strong foundation for future research in this field.\n","authors":["Ivica Dimitrovski","Vlatko Spasev","Ivan Kitanovski"],"pdf_url":"https://arxiv.org/pdf/2510.24321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.20234v3","updated":"2025-10-28T11:26:53Z","published":"2025-09-24T15:24:43Z","title":"ImageNet-trained CNNs are not biased towards texture: Revisiting feature\n  reliance through controlled suppression","summary":"  The hypothesis that Convolutional Neural Networks (CNNs) are inherently\ntexture-biased has shaped much of the discourse on feature use in deep\nlearning. We revisit this hypothesis by examining limitations in the\ncue-conflict experiment by Geirhos et al. To address these limitations, we\npropose a domain-agnostic framework that quantifies feature reliance through\nsystematic suppression of shape, texture, and color cues, avoiding the\nconfounds of forced-choice conflicts. By evaluating humans and neural networks\nunder controlled suppression conditions, we find that CNNs are not inherently\ntexture-biased but predominantly rely on local shape features. Nonetheless,\nthis reliance can be substantially mitigated through modern training strategies\nor architectures (ConvNeXt, ViTs). We further extend the analysis across\ncomputer vision, medical imaging, and remote sensing, revealing that reliance\npatterns differ systematically: computer vision models prioritize shape,\nmedical imaging models emphasize color, and remote sensing models exhibit a\nstronger reliance on texture. Code is available at\nhttps://github.com/tomburgert/feature-reliance.\n","authors":["Tom Burgert","Oliver Stoll","Paolo Rota","BegÃ¼m Demir"],"pdf_url":"https://arxiv.org/pdf/2509.20234v3.pdf","comment":"Accepted at NeurIPS 2025 (oral)"},{"id":"http://arxiv.org/abs/2510.23497v2","updated":"2025-10-28T11:09:37Z","published":"2025-10-27T16:32:12Z","title":"VOLD: Reasoning Transfer from LLMs to Vision-Language Models via\n  On-Policy Distillation","summary":"  Training vision-language models (VLMs) for complex reasoning remains a\nchallenging task, i.a. due to the scarcity of high-quality image-text reasoning\ndata. Conversely, text-based reasoning resources are abundant and scalable, but\nit is still an open question how to leveraging them for VLM reasoning. To\naddress this problem, we propose VOLD, a framework to transfer reasoning\ncapabilities from text-only teacher models to VLM student models. To this end,\nVOLD combines reinforcement learning via Group Relative Policy Optimization\n(GRPO) with on-policy distillation, which allows the student reasoning traces\nto be guided by the teacher model, resulting in a significant gain over using\nGRPO alone. We further show that a cold-start alignment is essential for an\neffective transfer during the online training phase in this scenario and that\nwithout sufficient distributional alignment between teacher and student,\non-policy distillation fails to provide meaningful guidance. We evaluate VOLD\nacross diverse benchmarks including MMMU-Pro, MathVision, MathVista, and\nLogicVista, showing that VOLD outperforms the baseline model significantly and\nimproves over the state of the art by a margin. Our ablation shows the\nimportance of a cold-start alignment via SFT for on-policy distillation with a\ntext-only teacher.\n","authors":["Walid Bousselham","Hilde Kuehne","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2510.23497v2.pdf","comment":"www.walidbousselham.com/VOLD/"},{"id":"http://arxiv.org/abs/2510.23444v2","updated":"2025-10-28T10:58:40Z","published":"2025-10-27T15:46:07Z","title":"FRBNet: Revisiting Low-Light Vision through Frequency-Domain Radial\n  Basis Network","summary":"  Low-light vision remains a fundamental challenge in computer vision due to\nsevere illumination degradation, which significantly affects the performance of\ndownstream tasks such as detection and segmentation. While recent\nstate-of-the-art methods have improved performance through invariant feature\nlearning modules, they still fall short due to incomplete modeling of low-light\nconditions. Therefore, we revisit low-light image formation and extend the\nclassical Lambertian model to better characterize low-light conditions. By\nshifting our analysis to the frequency domain, we theoretically prove that the\nfrequency-domain channel ratio can be leveraged to extract\nillumination-invariant features via a structured filtering process. We then\npropose a novel and end-to-end trainable module named \\textbf{F}requency-domain\n\\textbf{R}adial \\textbf{B}asis \\textbf{Net}work (\\textbf{FRBNet}), which\nintegrates the frequency-domain channel ratio operation with a learnable\nfrequency domain filter for the overall illumination-invariant feature\nenhancement. As a plug-and-play module, FRBNet can be integrated into existing\nnetworks for low-light downstream tasks without modifying loss functions.\nExtensive experiments across various downstream tasks demonstrate that FRBNet\nachieves superior performance, including +2.2 mAP for dark object detection and\n+2.9 mIoU for nighttime segmentation. Code is available at:\nhttps://github.com/Sing-Forevet/FRBNet.\n","authors":["Fangtong Sun","Congyu Li","Ke Yang","Yuchen Pan","Hanwen Yu","Xichuan Zhang","Yiying Li"],"pdf_url":"https://arxiv.org/pdf/2510.23444v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00129v2","updated":"2025-10-28T10:56:55Z","published":"2025-05-30T18:05:33Z","title":"Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically Aware\n  Sign Language Translation","summary":"  Recent progress in Sign Language Translation (SLT) has focussed primarily on\nimproving the representational capacity of large language models to incorporate\nSign Language features. This work explores an alternative direction: enhancing\nthe geometric properties of skeletal representations themselves. We propose\nGeo-Sign, a method that leverages the properties of hyperbolic geometry to\nmodel the hierarchical structure inherent in sign language kinematics. By\nprojecting skeletal features derived from Spatio-Temporal Graph Convolutional\nNetworks (ST-GCNs) into the Poincar\\'e ball model, we aim to create more\ndiscriminative embeddings, particularly for fine-grained motions like finger\narticulations. We introduce a hyperbolic projection layer, a weighted Fr\\'echet\nmean aggregation scheme, and a geometric contrastive loss operating directly in\nhyperbolic space. These components are integrated into an end-to-end\ntranslation framework as a regularisation function, to enhance the\nrepresentations within the language model. This work demonstrates the potential\nof hyperbolic geometry to improve skeletal representations for Sign Language\nTranslation, improving on SOTA RGB methods while preserving privacy and\nimproving computational efficiency. Code available here:\nhttps://github.com/ed-fish/geo-sign.\n","authors":["Edward Fish","Richard Bowden"],"pdf_url":"https://arxiv.org/pdf/2506.00129v2.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24285v1","updated":"2025-10-28T10:42:57Z","published":"2025-10-28T10:42:57Z","title":"ViPER: Empowering the Self-Evolution of Visual Perception Abilities in\n  Vision-Language Model","summary":"  The limited capacity for fine-grained visual perception presents a critical\nbottleneck for Vision-Language Models (VLMs) in real-world applications.\nAddressing this is challenging due to the scarcity of high-quality data and the\nlimitations of existing methods: supervised fine-tuning (SFT) often compromises\ngeneral capabilities, while reinforcement fine-tuning (RFT) prioritizes textual\nreasoning over visual perception. To bridge this gap, we propose a novel\ntwo-stage task that structures visual perception learning as a coarse-to-fine\nprogressive process. Based on this task formulation, we develop ViPER, a\nself-bootstrapping framework specifically designed to enable iterative\nevolution through self-critiquing and self-prediction. By synergistically\nintegrating image-level and instance-level reconstruction with a two-stage\nreinforcement learning strategy, ViPER establishes a closed-loop training\nparadigm, where internally synthesized data directly fuel the enhancement of\nperceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the\nQwen-Viper series. With an average gain of 1.7% on seven comprehensive\nbenchmarks spanning various tasks and up to 6.0% on fine-grained perception,\nQwen-Viper consistently demonstrates superior performance across different\nvision-language scenarios while maintaining generalizability. Beyond enabling\nself-improvement in perceptual capabilities, ViPER provides concrete evidence\nfor the reciprocal relationship between generation and understanding, a\nbreakthrough to developing more autonomous and capable VLMs.\n","authors":["Juntian Zhang","Song Jin","Chuanqi Cheng","Yuhan Liu","Yankai Lin","Xun Zhang","Yufei Zhang","Fei Jiang","Guojun Yin","Wei Lin","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2510.24285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24278v1","updated":"2025-10-28T10:39:04Z","published":"2025-10-28T10:39:04Z","title":"Training-free Source Attribution of AI-generated Images via Resynthesis","summary":"  Synthetic image source attribution is a challenging task, especially in data\nscarcity conditions requiring few-shot or zero-shot classification\ncapabilities. We present a new training-free one-shot attribution method based\non image resynthesis. A prompt describing the image under analysis is\ngenerated, then it is used to resynthesize the image with all the candidate\nsources. The image is attributed to the model which produced the resynthesis\nclosest to the original image in a proper feature space. We also introduce a\nnew dataset for synthetic image attribution consisting of face images from\ncommercial and open-source text-to-image generators. The dataset provides a\nchallenging attribution framework, useful for developing new attribution models\nand testing their capabilities on different generative architectures. The\ndataset structure allows to test approaches based on resynthesis and to compare\nthem to few-shot methods. Results from state-of-the-art few-shot approaches and\nother baselines show that the proposed resynthesis method outperforms existing\ntechniques when only a few samples are available for training or fine-tuning.\nThe experiments also demonstrate that the new dataset is a challenging one and\nrepresents a valuable benchmark for developing and evaluating future few-shot\nand zero-shot methods.\n","authors":["Pietro Bongini","Valentina Molinari","Andrea Costanzo","Benedetta Tondi","Mauro Barni"],"pdf_url":"https://arxiv.org/pdf/2510.24278v1.pdf","comment":"14 pages, 4 figures, 1 table, accepted at \"The 17th IEEE\n  INTERNATIONAL WORKSHOP ON INFORMATION FORENSICS AND SECURITY (WIFS2025)\",\n  Perth, Australia"},{"id":"http://arxiv.org/abs/2412.18833v2","updated":"2025-10-28T10:38:37Z","published":"2024-12-25T08:40:03Z","title":"Federated Learning with Partially Labeled Data: A Conditional\n  Distillation Approach","summary":"  In medical imaging, developing generalized segmentation models that can\nhandle multiple organs and lesions is crucial. However, the scarcity of fully\nannotated datasets and strict privacy regulations present significant barriers\nto data sharing. Federated Learning (FL) allows decentralized model training,\nbut existing FL methods often struggle with partial labeling, leading to model\ndivergence and catastrophic forgetting. We propose ConDistFL, a novel FL\nframework incorporating conditional distillation to address these challenges.\nConDistFL enables effective learning from partially labeled datasets,\nsignificantly improving segmentation accuracy across distributed and\nnon-uniform datasets. In addition to its superior segmentation performance,\nConDistFL maintains computational and communication efficiency, ensuring its\nscalability for real-world applications. Furthermore, ConDistFL demonstrates\nremarkable generalizability, significantly outperforming existing FL methods in\nout-of-federation tests, even adapting to unseen contrast phases (e.g.,\nnon-contrast CT images) in our experiments. Extensive evaluations on 3D CT and\n2D chest X-ray datasets show that ConDistFL is an efficient, adaptable solution\nfor collaborative medical image segmentation in privacy-constrained settings.\n","authors":["Pochuan Wang","Chen Shen","Masahiro Oda","Chiou-Shann Fuh","Kensaku Mori","Weichung Wang","Holger R. Roth"],"pdf_url":"https://arxiv.org/pdf/2412.18833v2.pdf","comment":"This manuscript was submitted to IEEE JBHI and is currently under\n  peer review"},{"id":"http://arxiv.org/abs/2510.23225v2","updated":"2025-10-28T10:25:00Z","published":"2025-10-27T11:23:04Z","title":"Through the Lens: Benchmarking Deepfake Detectors Against\n  MoirÃ©-Induced Distortions","summary":"  Deepfake detection remains a pressing challenge, particularly in real-world\nsettings where smartphone-captured media from digital screens often introduces\nMoir\\'e artifacts that can distort detection outcomes. This study\nsystematically evaluates state-of-the-art (SOTA) deepfake detectors on\nMoir\\'e-affected videos, an issue that has received little attention. We\ncollected a dataset of 12,832 videos, spanning 35.64 hours, from the Celeb-DF,\nDFD, DFDC, UADFV, and FF++ datasets, capturing footage under diverse real-world\nconditions, including varying screens, smartphones, lighting setups, and camera\nangles. To further examine the influence of Moir\\'e patterns on deepfake\ndetection, we conducted additional experiments using our DeepMoir\\'eFake,\nreferred to as (DMF) dataset and two synthetic Moir\\'e generation techniques.\nAcross 15 top-performing detectors, our results show that Moir\\'e artifacts\ndegrade performance by as much as 25.4%, while synthetically generated Moir\\'e\npatterns lead to a 21.4% drop in accuracy. Surprisingly, demoir\\'eing methods,\nintended as a mitigation approach, instead worsened the problem, reducing\naccuracy by up to 17.2%. These findings underscore the urgent need for\ndetection models that can robustly handle Moir\\'e distortions alongside other\nrealworld challenges, such as compression, sharpening, and blurring. By\nintroducing the DMF dataset, we aim to drive future research toward closing the\ngap between controlled experiments and practical deepfake detection.\n","authors":["Razaib Tariq","Minji Heo","Simon S. Woo","Shahroz Tariq"],"pdf_url":"https://arxiv.org/pdf/2510.23225v2.pdf","comment":"48 Pages, 29 Figures, 15 Tables"},{"id":"http://arxiv.org/abs/2510.24261v1","updated":"2025-10-28T10:17:11Z","published":"2025-10-28T10:17:11Z","title":"DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic\n  Manipulation","summary":"  Learning generalizable robotic manipulation policies remains a key challenge\ndue to the scarcity of diverse real-world training data. While recent\napproaches have attempted to mitigate this through self-supervised\nrepresentation learning, most either rely on 2D vision pretraining paradigms\nsuch as masked image modeling, which primarily focus on static semantics or\nscene geometry, or utilize large-scale video prediction models that emphasize\n2D dynamics, thus failing to jointly learn the geometry, semantics, and\ndynamics required for effective manipulation. In this paper, we present\nDynaRend, a representation learning framework that learns 3D-aware and\ndynamics-informed triplane features via masked reconstruction and future\nprediction using differentiable volumetric rendering. By pretraining on\nmulti-view RGB-D video data, DynaRend jointly captures spatial geometry, future\ndynamics, and task semantics in a unified triplane representation. The learned\nrepresentations can be effectively transferred to downstream robotic\nmanipulation tasks via action value map prediction. We evaluate DynaRend on two\nchallenging benchmarks, RLBench and Colosseum, as well as in real-world robotic\nexperiments, demonstrating substantial improvements in policy success rate,\ngeneralization to environmental perturbations, and real-world applicability\nacross diverse manipulation tasks.\n","authors":["Jingyi Tian","Le Wang","Sanping Zhou","Sen Wang","Jiayi Li","Gang Hua"],"pdf_url":"https://arxiv.org/pdf/2510.24261v1.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24262v1","updated":"2025-10-28T10:17:11Z","published":"2025-10-28T10:17:11Z","title":"UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level\n  Task Adaptation","summary":"  Data augmentation using generative models has emerged as a powerful paradigm\nfor enhancing performance in computer vision tasks. However, most existing\naugmentation approaches primarily focus on optimizing intrinsic data attributes\n-- such as fidelity and diversity -- to generate visually high-quality\nsynthetic data, while often neglecting task-specific requirements. Yet, it is\nessential for data generators to account for the needs of downstream tasks, as\ntraining data requirements can vary significantly across different tasks and\nnetwork architectures. To address these limitations, we propose UtilGen, a\nnovel utility-centric data augmentation framework that adaptively optimizes the\ndata generation process to produce task-specific, high-utility training data\nvia downstream task feedback. Specifically, we first introduce a weight\nallocation network to evaluate the task-specific utility of each synthetic\nsample. Guided by these evaluations, UtilGen iteratively refines the data\ngeneration process using a dual-level optimization strategy to maximize the\nsynthetic data utility: (1) model-level optimization tailors the generative\nmodel to the downstream task, and (2) instance-level optimization adjusts\ngeneration policies -- such as prompt embeddings and initial noise -- at each\ngeneration round. Extensive experiments on eight benchmark datasets of varying\ncomplexity and granularity demonstrate that UtilGen consistently achieves\nsuperior performance, with an average accuracy improvement of 3.87% over\nprevious SOTA. Further analysis of data influence and distribution reveals that\nUtilGen produces more impactful and task-relevant synthetic data, validating\nthe effectiveness of the paradigm shift from visual characteristics-centric to\ntask utility-centric data augmentation.\n","authors":["Jiyu Guo","Shuo Yang","Yiming Huang","Yancheng Long","Xiaobo Xia","Xiu Su","Bo Zhao","Zeke Xie","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2510.24262v1.pdf","comment":"39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)"},{"id":"http://arxiv.org/abs/2510.24260v1","updated":"2025-10-28T10:14:23Z","published":"2025-10-28T10:14:23Z","title":"DeshadowMamba: Deshadowing as 1D Sequential Similarity","summary":"  Recent deep models for image shadow removal often rely on attention-based\narchitectures to capture long-range dependencies. However, their fixed\nattention patterns tend to mix illumination cues from irrelevant regions,\nleading to distorted structures and inconsistent colors. In this work, we\nrevisit shadow removal from a sequence modeling perspective and explore the use\nof Mamba, a selective state space model that propagates global context through\ndirectional state transitions. These transitions yield an efficient global\nreceptive field while preserving positional continuity. Despite its potential,\ndirectly applying Mamba to image data is suboptimal, since it lacks awareness\nof shadow-non-shadow semantics and remains susceptible to color interference\nfrom nearby regions. To address these limitations, we propose CrossGate, a\ndirectional modulation mechanism that injects shadow-aware similarity into\nMamba's input gate, allowing selective integration of relevant context along\ntransition axes. To further ensure appearance fidelity, we introduce ColorShift\nregularization, a contrastive learning objective driven by global color\nstatistics. By synthesizing structured informative negatives, it guides the\nmodel to suppress color contamination and achieve robust color restoration.\nTogether, these components adapt sequence modeling to the structural integrity\nand chromatic consistency required for shadow removal. Extensive experiments on\npublic benchmarks demonstrate that DeshadowMamba achieves state-of-the-art\nvisual quality and strong quantitative performance.\n","authors":["Zhaotong Yang","Yi Chen","Yanying Li","Shengfeng He","Yangyang Xu","Junyu Dong","Jian Yang","Yong Du"],"pdf_url":"https://arxiv.org/pdf/2510.24260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.00037v3","updated":"2025-10-28T09:55:21Z","published":"2025-09-26T14:42:23Z","title":"On Robustness of Vision-Language-Action Model against Multi-Modal\n  Perturbations","summary":"  In Vision-Language-Action (VLA) models, robustness to real-world\nperturbations is critical for deployment. Existing methods target simple visual\ndisturbances, overlooking the broader multi-modal perturbations that arise in\nactions, instructions, environments, and observations. Here, we first evaluate\nthe robustness of mainstream VLAs under 17 perturbations across four\nmodalities. We find (1) actions as the most fragile modality, (2) Existing\nvisual-robust VLA do not gain robustness in other modality, and (3) pi0\ndemonstrates superior robustness with a diffusion-based action head. To build\nmulti-modal robust VLAs, we propose RobustVLA against perturbations in VLA\ninputs and outputs. For output robustness, we perform offline robust\noptimization against worst-case action noise that maximizes mismatch in flow\nmatching objective. This can be seen as adversarial training, label smoothing,\nand outlier penalization. For input robustness, we enforce consistent actions\nacross input variations that preserve task semantics. To account for multiple\nperturbations, we formulate robustness as a multi-armed bandit problem and\napply an upper confidence bound algorithm to automatically identify the most\nharmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers\nabsolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the\nOpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference\nthan existing visual-robust VLAs, and a 10.4% gain under mixed perturbations.\nOur RobustVLA is particularly effective on real-world FR5 robot with limited\ndemonstrations, showing absolute gains by 65.6% under perturbations of four\nmodalities.\n","authors":["Jianing Guo","Zhenhong Wu","Chang Tu","Yiyao Ma","Xiangqi Kong","Zhiqian Liu","Jiaming Ji","Shuning Zhang","Yuanpei Chen","Kai Chen","Qi Dou","Yaodong Yang","Xianglong Liu","Huijie Zhao","Weifeng Lv","Simin Li"],"pdf_url":"https://arxiv.org/pdf/2510.00037v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24232v1","updated":"2025-10-28T09:41:42Z","published":"2025-10-28T09:41:42Z","title":"Delving into Cascaded Instability: A Lipschitz Continuity View on Image\n  Restoration and Object Detection Synergy","summary":"  To improve detection robustness in adverse conditions (e.g., haze and low\nlight), image restoration is commonly applied as a pre-processing step to\nenhance image quality for the detector. However, the functional mismatch\nbetween restoration and detection networks can introduce instability and hinder\neffective integration -- an issue that remains underexplored. We revisit this\nlimitation through the lens of Lipschitz continuity, analyzing the functional\ndifferences between restoration and detection networks in both the input space\nand the parameter space. Our analysis shows that restoration networks perform\nsmooth, continuous transformations, while object detectors operate with\ndiscontinuous decision boundaries, making them highly sensitive to minor\nperturbations. This mismatch introduces instability in traditional cascade\nframeworks, where even imperceptible noise from restoration is amplified during\ndetection, disrupting gradient flow and hindering optimization. To address\nthis, we propose Lipschitz-regularized object detection (LROD), a simple yet\neffective framework that integrates image restoration directly into the\ndetector's feature learning, harmonizing the Lipschitz continuity of both tasks\nduring training. We implement this framework as Lipschitz-regularized YOLO\n(LR-YOLO), extending seamlessly to existing YOLO detectors. Extensive\nexperiments on haze and low-light benchmarks demonstrate that LR-YOLO\nconsistently improves detection stability, optimization smoothness, and overall\naccuracy.\n","authors":["Qing Zhao","Weijian Deng","Pengxu Wei","ZiYi Dong","Hannan Lu","Xiangyang Ji","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2510.24232v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24231v1","updated":"2025-10-28T09:41:30Z","published":"2025-10-28T09:41:30Z","title":"Benchmarking Microsaccade Recognition with Event Cameras: A Novel\n  Dataset and Evaluation","summary":"  Microsaccades are small, involuntary eye movements vital for visual\nperception and neural processing. Traditional microsaccade studies typically\nuse eye trackers or frame-based analysis, which, while precise, are costly and\nlimited in scalability and temporal resolution. Event-based sensing offers a\nhigh-speed, low-latency alternative by capturing fine-grained spatiotemporal\nchanges efficiently. This work introduces a pioneering event-based microsaccade\ndataset to support research on small eye movement dynamics in cognitive\ncomputing. Using Blender, we render high-fidelity eye movement scenarios and\nsimulate microsaccades with angular displacements from 0.5 to 2.0 degrees,\ndivided into seven distinct classes. These are converted to event streams using\nv2e, preserving the natural temporal dynamics of microsaccades, with durations\nranging from 0.25 ms to 2.25 ms. We evaluate the dataset using Spiking-VGG11,\nSpiking-VGG13, and Spiking-VGG16, and propose Spiking-VGG16Flow, an\noptical-flow-enhanced variant implemented in SpikingJelly. The models achieve\naround 90 percent average accuracy, successfully classifying microsaccades by\nangular displacement, independent of event count or duration. These results\ndemonstrate the potential of spiking neural networks for fine motion\nrecognition and establish a benchmark for event-based vision research. The\ndataset, code, and trained models will be publicly available at\nhttps://waseemshariff126.github.io/microsaccades/ .\n","authors":["Waseem Shariff","Timothy Hanley","Maciej Stec","Hossein Javidnia","Peter Corcoran"],"pdf_url":"https://arxiv.org/pdf/2510.24231v1.pdf","comment":"Accepted in British Machine Vision Conference (BMVC) 2025, Main\n  Conference"},{"id":"http://arxiv.org/abs/2505.20510v2","updated":"2025-10-28T09:39:55Z","published":"2025-05-26T20:22:19Z","title":"CPathAgent: An Agent-based Foundation Model for Interpretable\n  High-Resolution Pathology Image Analysis Mimicking Pathologists' Diagnostic\n  Logic","summary":"  Recent advances in computational pathology have led to the emergence of\nnumerous foundation models. These models typically rely on general-purpose\nencoders with multi-instance learning for whole slide image (WSI)\nclassification or apply multimodal approaches to generate reports directly from\nimages. However, these models cannot emulate the diagnostic approach of\npathologists, who systematically examine slides at low magnification to obtain\nan overview before progressively zooming in on suspicious regions to formulate\ncomprehensive diagnoses. Instead, existing models directly output final\ndiagnoses without revealing the underlying reasoning process. To address this\ngap, we introduce CPathAgent, an innovative agent-based approach that mimics\npathologists' diagnostic workflow by autonomously navigating across WSI based\non observed visual features, thereby generating substantially more transparent\nand interpretable diagnostic summaries. To achieve this, we develop a\nmulti-stage training strategy that unifies patch-level, region-level, and\nWSI-level capabilities within a single model, which is essential for\nreplicating how pathologists understand and reason across diverse image scales.\nAdditionally, we construct PathMMU-HR2, the first expert-validated benchmark\nfor large region analysis. This represents a critical intermediate scale\nbetween patches and whole slides, reflecting a key clinical reality where\npathologists typically examine several key large regions rather than entire\nslides at once. Extensive experiments demonstrate that CPathAgent consistently\noutperforms existing approaches across benchmarks at three different image\nscales, validating the effectiveness of our agent-based diagnostic approach and\nhighlighting a promising direction for computational pathology.\n","authors":["Yuxuan Sun","Yixuan Si","Chenglu Zhu","Kai Zhang","Zhongyi Shui","Bowen Ding","Tao Lin","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2505.20510v2.pdf","comment":"52 pages, 34 figures"},{"id":"http://arxiv.org/abs/2509.17550v3","updated":"2025-10-28T09:32:18Z","published":"2025-09-22T09:09:13Z","title":"Is It Certainly a Deepfake? Reliability Analysis in Detection &\n  Generation Ecosystem","summary":"  As generative models are advancing in quality and quantity for creating\nsynthetic content, deepfakes begin to cause online mistrust. Deepfake detectors\nare proposed to counter this effect, however, misuse of detectors claiming fake\ncontent as real or vice versa further fuels this misinformation problem. We\npresent the first comprehensive uncertainty analysis of deepfake detectors,\nsystematically investigating how generative artifacts influence prediction\nconfidence. As reflected in detectors' responses, deepfake generators also\ncontribute to this uncertainty as their generative residues vary, so we cross\nthe uncertainty analysis of deepfake detectors and generators. Based on our\nobservations, the uncertainty manifold holds enough consistent information to\nleverage uncertainty for deepfake source detection. Our approach leverages\nBayesian Neural Networks and Monte Carlo dropout to quantify both aleatoric and\nepistemic uncertainties across diverse detector architectures. We evaluate\nuncertainty on two datasets with nine generators, with four blind and two\nbiological detectors, compare different uncertainty methods, explore region-\nand pixel-based uncertainty, and conduct ablation studies. We conduct and\nanalyze binary real/fake, multi-class real/fake, source detection, and\nleave-one-out experiments between the generator/detector combinations to share\ntheir generalization capability, model calibration, uncertainty, and robustness\nagainst adversarial attacks. We further introduce uncertainty maps that\nlocalize prediction confidence at the pixel level, revealing distinct patterns\ncorrelated with generator-specific artifacts. Our analysis provides critical\ninsights for deploying reliable deepfake detection systems and establishes\nuncertainty quantification as a fundamental requirement for trustworthy\nsynthetic media detection.\n","authors":["Neslihan Kose","Anthony Rhodes","Umur Aybars Ciftci","Ilke Demir"],"pdf_url":"https://arxiv.org/pdf/2509.17550v3.pdf","comment":"Accepted for publication at the ICCV 2025 workshop - STREAM"},{"id":"http://arxiv.org/abs/2510.24214v1","updated":"2025-10-28T09:29:37Z","published":"2025-10-28T09:29:37Z","title":"SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel\n  LLMs","summary":"  Multimodal Large Language Models (MLLMs) typically process a large number of\nvisual tokens, leading to considerable computational overhead, even though many\nof these tokens are redundant. Existing visual token pruning methods primarily\nfocus on selecting the most salient tokens based on attention scores, resulting\nin the semantic incompleteness of the selected tokens. In this paper, we\npropose a novel visual token pruning strategy, called\n\\textbf{S}aliency-\\textbf{C}overage \\textbf{O}riented token \\textbf{P}runing\nfor \\textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and\ncoverage of the selected visual tokens to better preserve semantic\ncompleteness. Specifically, we introduce a set-coverage for a given set of\nselected tokens, computed based on the token relationships. We then define a\ntoken-coverage gain for each unselected token, quantifying how much additional\ncoverage would be obtained by including it. By integrating the saliency score\ninto the token-coverage gain, we propose our SCOPE score and iteratively select\nthe token with the highest SCOPE score. We conduct extensive experiments on\nmultiple vision-language understanding benchmarks using the LLaVA-1.5 and\nLLaVA-Next models. Experimental results demonstrate that our method\nconsistently outperforms prior approaches. Our code is available at\n\\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.\n","authors":["Jinhong Deng","Wen Li","Joey Tianyi Zhou","Yang He"],"pdf_url":"https://arxiv.org/pdf/2510.24214v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24213v1","updated":"2025-10-28T09:28:12Z","published":"2025-10-28T09:28:12Z","title":"Beyond Inference Intervention: Identity-Decoupled Diffusion for Face\n  Anonymization","summary":"  Face anonymization aims to conceal identity information while preserving\nnon-identity attributes. Mainstream diffusion models rely on inference-time\ninterventions such as negative guidance or energy-based optimization, which are\napplied post-training to suppress identity features. These interventions often\nintroduce distribution shifts and entangle identity with non-identity\nattributes, degrading visual fidelity and data utility. To address this, we\npropose \\textbf{ID\\textsuperscript{2}Face}, a training-centric anonymization\nframework that removes the need for inference-time optimization. The rationale\nof our method is to learn a structured latent space where identity and\nnon-identity information are explicitly disentangled, enabling direct and\ncontrollable anonymization at inference. To this end, we design a conditional\ndiffusion model with an identity-masked learning scheme. An Identity-Decoupled\nLatent Recomposer uses an Identity Variational Autoencoder to model identity\nfeatures, while non-identity attributes are extracted from same-identity pairs\nand aligned through bidirectional latent alignment. An Identity-Guided Latent\nHarmonizer then fuses these representations via soft-gating conditioned on\nnoisy feature prediction. The model is trained with a recomposition-based\nreconstruction loss to enforce disentanglement. At inference, anonymization is\nachieved by sampling a random identity vector from the learned identity space.\nTo further suppress identity leakage, we introduce an Orthogonal Identity\nMapping strategy that enforces orthogonality between sampled and source\nidentity vectors. Experiments demonstrate that ID\\textsuperscript{2}Face\noutperforms existing methods in visual quality, identity suppression, and\nutility preservation.\n","authors":["Haoxin Yang","Yihong Lin","Jingdan Kang","Xuemiao Xu","Yue Li","Cheng Xu","Shengfeng He"],"pdf_url":"https://arxiv.org/pdf/2510.24213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24211v1","updated":"2025-10-28T09:26:27Z","published":"2025-10-28T09:26:27Z","title":"MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive\n  Visual Generation Acceleration","summary":"  While autoregressive (AR) modeling has recently emerged as a new paradigm in\nvisual generation, its practical adoption is severely constrained by the slow\ninference speed of per-token generation, which often requires thousands of\nsteps to produce a single sample. To address this challenge, we propose MC-SJD,\na training-free, lossless parallel decoding framework designed to accelerate AR\nvisual generation by extending the recently introduced Speculative Jacobi\nDecoding (SJD). Although SJD shows strong potential for accelerating AR\ngeneration, we demonstrate that token instability across iterations\nsignificantly reduces the acceptance rate, a limitation that primarily arises\nfrom the independent sampling process used during draft token generation. To\novercome this, we introduce MC-SJD, an information-theoretic approach based on\ncoupling, which substantially accelerates standard SJD by maximizing the\nprobability of sampling identical draft tokens across consecutive iterations,\nall while preserving its lossless property. Remarkably, this method requires\nonly a single-line modification to the existing algorithm, yet achieves\nsubstantial performance gains, delivering up to a ~4.2x acceleration in image\ngeneration and ~13.3x acceleration in video generation compared to standard AR\ndecoding, without any degradation in output quality.\n","authors":["Junhyuk So","Hyunho Kook","Chaeyeon Jang","Eunhyeok Park"],"pdf_url":"https://arxiv.org/pdf/2510.24211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24202v1","updated":"2025-10-28T09:06:27Z","published":"2025-10-28T09:06:27Z","title":"CLFSeg: A Fuzzy-Logic based Solution for Boundary Clarity and\n  Uncertainty Reduction in Medical Image Segmentation","summary":"  Accurate polyp and cardiac segmentation for early detection and treatment is\nessential for the diagnosis and treatment planning of cancer-like diseases.\nTraditional convolutional neural network (CNN) based models have represented\nlimited generalizability, robustness, and inability to handle uncertainty,\nwhich affects the segmentation performance. To solve these problems, this paper\nintroduces CLFSeg, an encoder-decoder based framework that aggregates the\nFuzzy-Convolutional (FC) module leveraging convolutional layers and fuzzy\nlogic. This module enhances the segmentation performance by identifying local\nand global features while minimizing the uncertainty, noise, and ambiguity in\nboundary regions, ensuring computing efficiency. In order to handle class\nimbalance problem while focusing on the areas of interest with tiny and\nboundary regions, binary cross-entropy (BCE) with dice loss is incorporated.\nOur proposed model exhibits exceptional performance on four publicly available\ndatasets, including CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, and ACDC.\nExtensive experiments and visual studies show CLFSeg surpasses the existing\nSOTA performance and focuses on relevant regions of interest in anatomical\nstructures. The proposed CLFSeg improves performance while ensuring computing\nefficiency, which makes it a potential solution for real-world medical\ndiagnostic scenarios. Project page is available at\nhttps://visdomlab.github.io/CLFSeg/\n","authors":["Anshul Kaushal","Kunal Jangid","Vinod K. Kurmi"],"pdf_url":"https://arxiv.org/pdf/2510.24202v1.pdf","comment":"The 36th British Machine Vision Conference (BMVC) 2025"},{"id":"http://arxiv.org/abs/2510.24195v1","updated":"2025-10-28T08:59:11Z","published":"2025-10-28T08:59:11Z","title":"Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for\n  SAM2","summary":"  Recent studies reveal the vulnerability of the image segmentation foundation\nmodel SAM to adversarial examples. Its successor, SAM2, has attracted\nsignificant attention due to its strong generalization capability in video\nsegmentation. However, its robustness remains unexplored, and it is unclear\nwhether existing attacks on SAM can be directly transferred to SAM2. In this\npaper, we first analyze the performance gap of existing attacks between SAM and\nSAM2 and highlight two key challenges arising from their architectural\ndifferences: directional guidance from the prompt and semantic entanglement\nacross consecutive frames. To address these issues, we propose UAP-SAM2, the\nfirst cross-prompt universal adversarial attack against SAM2 driven by dual\nsemantic deviation. For cross-prompt transferability, we begin by designing a\ntarget-scanning strategy that divides each frame into k regions, each randomly\nassigned a prompt, to reduce prompt dependency during optimization. For\neffectiveness, we design a dual semantic deviation framework that optimizes a\nUAP by distorting the semantics within the current frame and disrupting the\nsemantic consistency across consecutive frames. Extensive experiments on six\ndatasets across two segmentation tasks demonstrate the effectiveness of the\nproposed method for SAM2. The comparative results show that UAP-SAM2\nsignificantly outperforms state-of-the-art (SOTA) attacks by a large margin.\n","authors":["Ziqi Zhou","Yifan Hu","Yufei Song","Zijing Li","Shengshan Hu","Leo Yu Zhang","Dezhong Yao","Long Zheng","Hai Jin"],"pdf_url":"https://arxiv.org/pdf/2510.24195v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2507.12841v2","updated":"2025-10-28T08:46:18Z","published":"2025-07-17T07:04:05Z","title":"AnyCap Project: A Unified Framework, Dataset, and Benchmark for\n  Controllable Omni-modal Captioning","summary":"  Controllable captioning is essential for precise multimodal alignment and\ninstruction following, yet existing models often lack fine-grained control and\nreliable evaluation protocols. To address this gap, we present the AnyCap\nProject, an integrated solution spanning model, dataset, and evaluation. We\nintroduce AnyCapModel (ACM), a lightweight plug-and-play framework that\nenhances the controllability of existing foundation models for omni-modal\ncaptioning without retraining the base model. ACM reuses the original captions\nfrom base models while incorporating user instructions and modality features to\ngenerate improved captions. To remedy the data scarcity in controllable\nmultimodal captioning, we build AnyCapDataset (ACD), covering three modalities,\n28 user-instruction types, and 300\\,k high-quality data entries. We further\npropose AnyCapEval, a new benchmark that provides more reliable evaluation\nmetrics for controllable captioning by decoupling content accuracy and\nstylistic fidelity. ACM markedly improves caption quality across a diverse set\nof base models on AnyCapEval. Notably, ACM-8B raises GPT-4o\\'s content scores\nby 45\\% and style scores by 12\\%, and it also achieves substantial gains on\nwidely used benchmarks such as MIA-Bench and VidCapBench.\n","authors":["Yiming Ren","Zhiqiang Lin","Yu Li","Gao Meng","Weiyun Wang","Junjie Wang","Zicheng Lin","Jifeng Dai","Yujiu Yang","Wenhai Wang","Ruihang Chu"],"pdf_url":"https://arxiv.org/pdf/2507.12841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22672v2","updated":"2025-10-28T08:39:14Z","published":"2025-10-26T13:27:59Z","title":"Look and Tell: A Dataset for Multimodal Grounding Across Egocentric and\n  Exocentric Views","summary":"  We introduce Look and Tell, a multimodal dataset for studying referential\ncommunication across egocentric and exocentric perspectives. Using Meta Project\nAria smart glasses and stationary cameras, we recorded synchronized gaze,\nspeech, and video as 25 participants instructed a partner to identify\ningredients in a kitchen. Combined with 3D scene reconstructions, this setup\nprovides a benchmark for evaluating how different spatial representations (2D\nvs. 3D; ego vs. exo) affect multimodal grounding. The dataset contains 3.67\nhours of recordings, including 2,707 richly annotated referential expressions,\nand is designed to advance the development of embodied agents that can\nunderstand and engage in situated dialogue.\n","authors":["Anna Deichler","Jonas Beskow"],"pdf_url":"https://arxiv.org/pdf/2510.22672v2.pdf","comment":"10 pages, 6 figures, 2 tables. Accepted to the NeurIPS 2025 Workshop\n  on SPACE in Vision, Language, and Embodied AI (SpaVLE). Dataset:\n  https://huggingface.co/datasets/annadeichler/KTH-ARIA-referential"},{"id":"http://arxiv.org/abs/2505.13043v2","updated":"2025-10-28T08:36:12Z","published":"2025-05-19T12:33:52Z","title":"A Generalized Label Shift Perspective for Cross-Domain Gaze Estimation","summary":"  Aiming to generalize the well-trained gaze estimation model to new target\ndomains, Cross-domain Gaze Estimation (CDGE) is developed for real-world\napplication scenarios. Existing CDGE methods typically extract the\ndomain-invariant features to mitigate domain shift in feature space, which is\nproved insufficient by Generalized Label Shift (GLS) theory. In this paper, we\nintroduce a novel GLS perspective to CDGE and modelize the cross-domain problem\nby label and conditional shift problem. A GLS correction framework is presented\nand a feasible realization is proposed, in which a importance reweighting\nstrategy based on truncated Gaussian distribution is introduced to overcome the\ncontinuity challenges in label shift correction. To embed the reweighted source\ndistribution to conditional invariant learning, we further derive a\nprobability-aware estimation of conditional operator discrepancy. Extensive\nexperiments on standard CDGE tasks with different backbone models validate the\nsuperior generalization capability across domain and applicability on various\nmodels of proposed method.\n","authors":["Hao-Ran Yang","Xiaohui Chen","Chuan-Xian Ren"],"pdf_url":"https://arxiv.org/pdf/2505.13043v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2503.08930v2","updated":"2025-10-28T08:30:20Z","published":"2025-03-11T22:18:57Z","title":"Acoustic Neural 3D Reconstruction Under Pose Drift","summary":"  We consider the problem of optimizing neural implicit surfaces for 3D\nreconstruction using acoustic images collected with drifting sensor poses. The\naccuracy of current state-of-the-art 3D acoustic modeling algorithms is highly\ndependent on accurate pose estimation; small errors in sensor pose can lead to\nsevere reconstruction artifacts. In this paper, we propose an algorithm that\njointly optimizes the neural scene representation and sonar poses. Our\nalgorithm does so by parameterizing the 6DoF poses as learnable parameters and\nbackpropagating gradients through the neural renderer and implicit\nrepresentation. We validated our algorithm on both real and simulated datasets.\nIt produces high-fidelity 3D reconstructions even under significant pose drift.\n","authors":["Tianxiang Lin","Mohamad Qadri","Kevin Zhang","Adithya Pediredla","Christopher A. Metzler","Michael Kaess"],"pdf_url":"https://arxiv.org/pdf/2503.08930v2.pdf","comment":"8 pages, 8 figures. This paper is accepted by 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2503.09336v3","updated":"2025-10-28T08:24:48Z","published":"2025-03-12T12:30:59Z","title":"Stealthy Patch-Wise Backdoor Attack in 3D Point Cloud via Curvature\n  Awareness","summary":"  Backdoor attacks pose a severe threat to deep neural networks (DNNs) by\nimplanting hidden backdoors that can be activated with predefined triggers to\nmanipulate model behaviors maliciously. Existing 3D point cloud backdoor\nattacks primarily rely on sample-wise global modifications, which suffer from\nlow imperceptibility. Although optimization can improve stealthiness,\noptimizing sample-wise triggers significantly increases computational cost. To\naddress these limitations, we propose the Stealthy Patch-Wise Backdoor Attack\n(SPBA), the first patch-wise backdoor attack framework for 3D point clouds.\nSpecifically, SPBA decomposes point clouds into local patches and employs a\ncurvature-based imperceptibility score to guide trigger injection into visually\nless sensitive patches. By optimizing a unified patch-wise trigger that\nperturbs spectral features of selected patches, SPBA significantly enhances\noptimization efficiency while maintaining high stealthiness. Extensive\nexperiments on ModelNet40 and ShapeNetPart further demonstrate that SPBA\nsurpasses prior state-of-the-art backdoor attacks in both attack effectiveness\nand resistance to defense methods. The code is available at\nhttps://github.com/HazardFY/SPBA.\n","authors":["Yu Feng","Dingxin Zhang","Runkai Zhao","Yong Xia","Heng Huang","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2503.09336v3.pdf","comment":"13 pages, 6 figures, 11 tables"},{"id":"http://arxiv.org/abs/2508.15256v2","updated":"2025-10-28T08:08:14Z","published":"2025-08-21T05:40:23Z","title":"Normal and Abnormal Pathology Knowledge-Augmented Vision-Language Model\n  for Anomaly Detection in Pathology Images","summary":"  Anomaly detection in computational pathology aims to identify rare and scarce\nanomalies where disease-related data are often limited or missing. Existing\nanomaly detection methods, primarily designed for industrial settings, face\nlimitations in pathology due to computational constraints, diverse tissue\nstructures, and lack of interpretability. To address these challenges, we\npropose Ano-NAViLa, a Normal and Abnormal pathology knowledge-augmented\nVision-Language model for Anomaly detection in pathology images. Ano-NAViLa is\nbuilt on a pre-trained vision-language model with a lightweight trainable MLP.\nBy incorporating both normal and abnormal pathology knowledge, Ano-NAViLa\nenhances accuracy and robustness to variability in pathology images and\nprovides interpretability through image-text associations. Evaluated on two\nlymph node datasets from different organs, Ano-NAViLa achieves the\nstate-of-the-art performance in anomaly detection and localization,\noutperforming competing models.\n","authors":["Jinsol Song","Jiamu Wang","Anh Tien Nguyen","Keunho Byeon","Sangjeong Ahn","Sung Hak Lee","Jin Tae Kwak"],"pdf_url":"https://arxiv.org/pdf/2508.15256v2.pdf","comment":"Accepted to ICCV 2025. Code is available at:\n  https://github.com/QuIIL/ICCV2025_Ano-NAViLa"},{"id":"http://arxiv.org/abs/2505.05229v2","updated":"2025-10-28T08:05:02Z","published":"2025-05-08T13:21:10Z","title":"Does CLIP perceive art the same way we do?","summary":"  CLIP has emerged as a powerful multimodal model capable of connecting images\nand text through joint embeddings, but to what extent does it 'see' the same\nway humans do - especially when interpreting artworks? In this paper, we\ninvestigate CLIP's ability to extract high-level semantic and stylistic\ninformation from paintings, including both human-created and AI-generated\nimagery. We evaluate its perception across multiple dimensions: content, scene\nunderstanding, artistic style, historical period, and the presence of visual\ndeformations or artifacts. By designing targeted probing tasks and comparing\nCLIP's responses to human annotations and expert benchmarks, we explore its\nalignment with human perceptual and contextual understanding. Our findings\nreveal both strengths and limitations in CLIP's visual representations,\nparticularly in relation to aesthetic cues and artistic intent. We further\ndiscuss the implications of these insights for using CLIP as a guidance\nmechanism during generative processes, such as style transfer or prompt-based\nimage synthesis. Our work highlights the need for deeper interpretability in\nmultimodal systems, especially when applied to creative domains where nuance\nand subjectivity play a central role.\n","authors":["Andrea Asperti","Leonardo DessÃ¬","Maria Chiara Tonetti","Nico Wu"],"pdf_url":"https://arxiv.org/pdf/2505.05229v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22035v2","updated":"2025-10-28T07:52:08Z","published":"2025-10-24T21:41:32Z","title":"Caption-Driven Explainability: Probing CNNs for Bias via CLIP","summary":"  Robustness has become one of the most critical problems in machine learning\n(ML). The science of interpreting ML models to understand their behavior and\nimprove their robustness is referred to as explainable artificial intelligence\n(XAI). One of the state-of-the-art XAI methods for computer vision problems is\nto generate saliency maps. A saliency map highlights the pixel space of an\nimage that excites the ML model the most. However, this property could be\nmisleading if spurious and salient features are present in overlapping pixel\nspaces. In this paper, we propose a caption-based XAI method, which integrates\na standalone model to be explained into the contrastive language-image\npre-training (CLIP) model using a novel network surgery approach. The resulting\ncaption-based XAI model identifies the dominant concept that contributes the\nmost to the models prediction. This explanation minimizes the risk of the\nstandalone model falling for a covariate shift and contributes significantly\ntowards developing robust ML models. Our code is available at\n<https://github.com/patch0816/caption-driven-xai>.\n","authors":["Patrick Koller","Amil V. Dravid","Guido M. Schuster","Aggelos K. Katsaggelos"],"pdf_url":"https://arxiv.org/pdf/2510.22035v2.pdf","comment":"Accepted and presented at the IEEE ICIP 2025 Satellite Workshop\n  \"Generative AI for World Simulations and Communications & Celebrating 40\n  Years of Excellence in Education: Honoring Professor Aggelos Katsaggelos\",\n  Anchorage, Alaska, USA, September 14, 2025. Camera-ready preprint; the\n  official IEEE Xplore publication will follow. Code is available at\n  <https://github.com/patch0816/caption-driven-xai>"},{"id":"http://arxiv.org/abs/2401.09962v3","updated":"2025-10-28T07:47:22Z","published":"2024-01-18T13:23:51Z","title":"CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects","summary":"  Customized text-to-video generation aims to generate high-quality videos\nguided by text prompts and subject references. Current approaches for\npersonalizing text-to-video generation suffer from tackling multiple subjects,\nwhich is a more challenging and practical scenario. In this work, our aim is to\npromote multi-subject guided text-to-video customization. We propose\nCustomVideo, a novel framework that can generate identity-preserving videos\nwith the guidance of multiple subjects. To be specific, firstly, we encourage\nthe co-occurrence of multiple subjects via composing them in a single image.\nFurther, upon a basic text-to-video diffusion model, we design a simple yet\neffective attention control strategy to disentangle different subjects in the\nlatent space of diffusion model. Moreover, to help the model focus on the\nspecific area of the object, we segment the object from given reference images\nand provide a corresponding object mask for attention learning. Also, we\ncollect a multi-subject text-to-video generation dataset as a comprehensive\nbenchmark. Extensive qualitative, quantitative, and user study results\ndemonstrate the superiority of our method compared to previous state-of-the-art\napproaches. The project page is https://kyfafyd.wang/projects/customvideo.\n","authors":["Zhao Wang","Aoxue Li","Lingting Zhu","Yong Guo","Qi Dou","Zhenguo Li"],"pdf_url":"https://arxiv.org/pdf/2401.09962v3.pdf","comment":"IEEE TMM 2025"},{"id":"http://arxiv.org/abs/2510.24152v1","updated":"2025-10-28T07:43:30Z","published":"2025-10-28T07:43:30Z","title":"Enhancing Vision-Language Models for Autonomous Driving through\n  Task-Specific Prompting and Spatial Reasoning","summary":"  This technical report presents our solution for the RoboSense Challenge at\nIROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving\nscene understanding across perception, prediction, planning, and corruption\ndetection tasks. We propose a systematic framework built on four core\ncomponents. First, a Mixture-of-Prompts router classifies questions and\ndispatches them to task-specific expert prompts, eliminating interference\nacross diverse question types. Second, task-specific prompts embed explicit\ncoordinate systems, spatial reasoning rules, role-playing,\nChain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to\neach task. Third, a visual assembly module composes multi-view images with\nobject crops, magenta markers, and adaptive historical frames based on question\nrequirements. Fourth, we configure model inference parameters (temperature,\ntop-p, message roles) per task to optimize output quality. Implemented on\nQwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean\ndata) and 72.85% on Phase-2 (corrupted data), demonstrating that structured\nprompting and spatial grounding substantially enhance VLM performance on\nsafety-critical autonomous driving tasks. Code and prompt are available at\nhttps://github.com/wuaodi/UCAS-CSU-phase2.\n","authors":["Aodi Wu","Xubo Luo"],"pdf_url":"https://arxiv.org/pdf/2510.24152v1.pdf","comment":"RoboSense Challenge with IROS 2025"},{"id":"http://arxiv.org/abs/2503.18384v2","updated":"2025-10-28T07:24:00Z","published":"2025-03-24T06:51:38Z","title":"LiDAR Remote Sensing Meets Weak Supervision: Concepts, Methods, and\n  Perspectives","summary":"  Light detection and ranging (LiDAR) remote sensing encompasses two major\ndirections: data interpretation and parameter inversion. However, both\ndirections rely heavily on costly and labor-intensive labeled data and field\nmeasurements, which constrains their scalability and spatiotemporal\nadaptability. Weakly Supervised Learning (WSL) provides a unified framework to\naddress these limitations. This paper departs from the traditional view that\ntreats interpretation and inversion as separate tasks and offers a systematic\nreview of recent advances in LiDAR remote sensing from a unified WSL\nperspective. We cover typical WSL settings including incomplete\nsupervision(e.g., sparse point labels), inexact supervision (e.g., scene-level\ntags), inaccurate supervision (e.g., noisy labels), and cross-domain\nsupervision (e.g., domain adaptation/generalization) and corresponding\ntechniques such as pseudo-labeling, consistency regularization, self-training,\nand label refinement, which collectively enable robust learning from limited\nand weak annotations.We further analyze LiDAR-specific challenges (e.g.,\nirregular geometry, data sparsity, domain heterogeneity) that require tailored\nweak supervision, and examine how sparse LiDAR observations can guide joint\nlearning with other remote-sensing data for continuous surface-parameter\nretrieval. Finally, we highlight future directions where WSL acts as a bridge\nbetween LiDAR and foundation models to leverage large-scale multimodal datasets\nand reduce labeling costs, while also enabling broader WSL-driven advances in\ngeneralization, open-world adaptation, and scalable LiDAR remote sensing.\n","authors":["Yuan Gao","Shaobo Xia","Pu Wang","Xiaohuan Xi","Sheng Nie","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2503.18384v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24136v1","updated":"2025-10-28T07:22:34Z","published":"2025-10-28T07:22:34Z","title":"MSRANetV2: An Explainable Deep Learning Architecture for Multi-class\n  Classification of Colorectal Histopathological Images","summary":"  Colorectal cancer (CRC) is a leading worldwide cause of cancer-related\nmortality, and the role of prompt precise detection is of paramount interest in\nimproving patient outcomes. Conventional diagnostic methods such as colonoscopy\nand histological examination routinely exhibit subjectivity, are extremely\ntime-consuming, and are susceptible to variation. Through the development of\ndigital pathology, deep learning algorithms have become a powerful approach in\nenhancing diagnostic precision and efficiency. In our work, we proposed a\nconvolutional neural network architecture named MSRANetV2, specially optimized\nfor the classification of colorectal tissue images. The model employs a\nResNet50V2 backbone, extended with residual attention mechanisms and\nsqueeze-and-excitation (SE) blocks, to extract deep semantic and fine-grained\nspatial features. With channel alignment and upsampling operations, MSRANetV2\neffectively fuses multi-scale representations, thereby enhancing the robustness\nof the classification. We evaluated our model on a five-fold stratified\ncross-validation strategy on two publicly available datasets: CRC-VAL-HE-7K and\nNCT-CRC-HE-100K. The proposed model achieved remarkable average Precision,\nrecall, F1-score, AUC, and test accuracy were 0.9884 plus-minus 0.0151, 0.9900\nplus-minus 0.0151, 0.9900 plus-minus 0.0145, 0.9999 plus-minus 0.00006, and\n0.9905 plus-minus 0.0025 on the 7K dataset. On the 100K dataset, they were\n0.9904 plus-minus 0.0091, 0.9900 plus-minus 0.0071, 0.9900 plus-minus 0.0071,\n0.9997 plus-minus 0.00016, and 0.9902 plus-minus 0.0006. Additionally, Grad-CAM\nvisualizations were incorporated to enhance model interpretability by\nhighlighting tissue areas that are medically relevant. These findings validate\nthat MSRANetV2 is a reliable, interpretable, and high-performing architectural\nmodel for classifying CRC tissues.\n","authors":["Ovi Sarkar","Md Shafiuzzaman","Md. Faysal Ahamed","Golam Mahmud","Muhammad E. H. Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2510.24136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24134v1","updated":"2025-10-28T07:19:01Z","published":"2025-10-28T07:19:01Z","title":"VC4VG: Optimizing Video Captions for Text-to-Video Generation","summary":"  Recent advances in text-to-video (T2V) generation highlight the critical role\nof high-quality video-text pairs in training models capable of producing\ncoherent and instruction-aligned videos. However, strategies for optimizing\nvideo captions specifically for T2V training remain underexplored. In this\npaper, we introduce VC4VG (Video Captioning for Video Generation), a\ncomprehensive caption optimization framework tailored to the needs of T2V\nmodels.We begin by analyzing caption content from a T2V perspective,\ndecomposing the essential elements required for video reconstruction into\nmultiple dimensions, and proposing a principled caption design methodology. To\nsupport evaluation, we construct VC4VG-Bench, a new benchmark featuring\nfine-grained, multi-dimensional, and necessity-graded metrics aligned with\nT2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a\nstrong correlation between improved caption quality and video generation\nperformance, validating the effectiveness of our approach. We release all\nbenchmark tools and code at https://github.com/qyr0403/VC4VG to support further\nresearch.\n","authors":["Yang Du","Zhuoran Lin","Kaiqiang Song","Biao Wang","Zhicheng Zheng","Tiezheng Ge","Bo Zheng","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2510.24134v1.pdf","comment":"Accepted by EMNLP 2025"},{"id":"http://arxiv.org/abs/2510.14431v2","updated":"2025-10-28T07:17:14Z","published":"2025-10-16T08:31:44Z","title":"Real-Time Neural Video Compression with Unified Intra and Inter Coding","summary":"  Neural video compression (NVC) technologies have advanced rapidly in recent\nyears, yielding state-of-the-art schemes such as DCVC-RT that offer superior\ncompression efficiency to H.266/VVC and real-time encoding/decoding\ncapabilities. Nonetheless, existing NVC schemes have several limitations,\nincluding inefficiency in dealing with disocclusion and new content, interframe\nerror propagation and accumulation, among others. To eliminate these\nlimitations, we borrow the idea from classic video coding schemes, which allow\nintra coding within inter-coded frames. With the intra coding tool enabled,\ndisocclusion and new content are properly handled, and interframe error\npropagation is naturally intercepted without the need for manual refresh\nmechanisms. We present an NVC framework with unified intra and inter coding,\nwhere every frame is processed by a single model that is trained to perform\nintra/inter coding adaptively. Moreover, we propose a simultaneous two-frame\ncompression design to exploit interframe redundancy not only forwardly but also\nbackwardly. Experimental results show that our scheme outperforms DCVC-RT by an\naverage of 10.7\\% BD-rate reduction, delivers more stable bitrate and quality\nper frame, and retains real-time encoding/decoding performances. Code and\nmodels will be released.\n","authors":["Hui Xiang","Yifan Bian","Li Li","Jingran Wu","Xianguo Zhang","Dong Liu"],"pdf_url":"https://arxiv.org/pdf/2510.14431v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2510.24133v1","updated":"2025-10-28T07:16:21Z","published":"2025-10-28T07:16:21Z","title":"Compositional Image Synthesis with Inference-Time Scaling","summary":"  Despite their impressive realism, modern text-to-image models still struggle\nwith compositionality, often failing to render accurate object counts,\nattributes, and spatial relations. To address this challenge, we present a\ntraining-free framework that combines an object-centric approach with\nself-refinement to improve layout faithfulness while preserving aesthetic\nquality. Specifically, we leverage large language models (LLMs) to synthesize\nexplicit layouts from input prompts, and we inject these layouts into the image\ngeneration process, where a object-centric vision-language model (VLM) judge\nreranks multiple candidates to select the most prompt-aligned outcome\niteratively. By unifying explicit layout-grounding with self-refine-based\ninference-time scaling, our framework achieves stronger scene alignment with\nprompts compared to recent text-to-image models. The code are available at\nhttps://github.com/gcl-inha/ReFocus.\n","authors":["Minsuk Ji","Sanghyeok Lee","Namhyuk Ahn"],"pdf_url":"https://arxiv.org/pdf/2510.24133v1.pdf","comment":"projcet page: https://github.com/gcl-inha/ReFocus"},{"id":"http://arxiv.org/abs/2510.24129v1","updated":"2025-10-28T07:08:09Z","published":"2025-10-28T07:08:09Z","title":"ETC: training-free diffusion models acceleration with Error-aware Trend\n  Consistency","summary":"  Diffusion models have achieved remarkable generative quality but remain\nbottlenecked by costly iterative sampling. Recent training-free methods\naccelerate diffusion process by reusing model outputs. However, these methods\nignore denoising trends and lack error control for model-specific tolerance,\nleading to trajectory deviations under multi-step reuse and exacerbating\ninconsistencies in the generated results. To address these issues, we introduce\nError-aware Trend Consistency (ETC), a framework that (1) introduces a\nconsistent trend predictor that leverages the smooth continuity of diffusion\ntrajectories, projecting historical denoising patterns into stable future\ndirections and progressively distributing them across multiple approximation\nsteps to achieve acceleration without deviating; (2) proposes a model-specific\nerror tolerance search mechanism that derives corrective thresholds by\nidentifying transition points from volatile semantic planning to stable quality\nrefinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX\nwith negligible (-0.074 SSIM score) degradation of consistency.\n","authors":["Jiajian Xie","Hubery Yin","Chen Li","Zhou Zhao","Shengyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.24129v1.pdf","comment":"17 pages, 10 figures"},{"id":"http://arxiv.org/abs/2402.02096v2","updated":"2025-10-28T06:51:19Z","published":"2024-02-03T09:27:33Z","title":"UMCFuse: A Unified Multiple Complex Scenes Infrared and Visible Image\n  Fusion Framework","summary":"  Infrared and visible image fusion has emerged as a prominent research area in\ncomputer vision. However, little attention has been paid to the fusion task in\ncomplex scenes, leading to sub-optimal results under interference. To fill this\ngap, we propose a unified framework for infrared and visible images fusion in\ncomplex scenes, termed UMCFuse. Specifically, we classify the pixels of visible\nimages from the degree of scattering of light transmission, allowing us to\nseparate fine details from overall intensity. Maintaining a balance between\ninterference removal and detail preservation is essential for the\ngeneralization capacity of the proposed method. Therefore, we propose an\nadaptive denoising strategy for the fusion of detail layers. Meanwhile, we fuse\nthe energy features from different modalities by analyzing them from multiple\ndirections. Extensive fusion experiments on real and synthetic complex scenes\ndatasets cover adverse weather conditions, noise, blur, overexposure, fire, as\nwell as downstream tasks including semantic segmentation, object detection,\nsalient object detection, and depth estimation, consistently indicate the\nsuperiority of the proposed method compared with the recent representative\nmethods. Our code is available at https://github.com/ixilai/UMCFuse.\n","authors":["Xilai Li","Xiaosong Li","Tianshu Tan","Huafeng Li","Tao Ye"],"pdf_url":"https://arxiv.org/pdf/2402.02096v2.pdf","comment":"Published in IEEE-TIP 2025"},{"id":"http://arxiv.org/abs/2510.24117v1","updated":"2025-10-28T06:41:49Z","published":"2025-10-28T06:41:49Z","title":"DogMo: A Large-Scale Multi-View RGB-D Dataset for 4D Canine Motion\n  Recovery","summary":"  We present DogMo, a large-scale multi-view RGB-D video dataset capturing\ndiverse canine movements for the task of motion recovery from images. DogMo\ncomprises 1.2k motion sequences collected from 10 unique dogs, offering rich\nvariation in both motion and breed. It addresses key limitations of existing\ndog motion datasets, including the lack of multi-view and real 3D data, as well\nas limited scale and diversity. Leveraging DogMo, we establish four motion\nrecovery benchmark settings that support systematic evaluation across monocular\nand multi-view, RGB and RGB-D inputs. To facilitate accurate motion recovery,\nwe further introduce a three-stage, instance-specific optimization pipeline\nthat fits the SMAL model to the motion sequences. Our method progressively\nrefines body shape and pose through coarse alignment, dense correspondence\nsupervision, and temporal regularization. Our dataset and method provide a\nprincipled foundation for advancing research in dog motion recovery and open up\nnew directions at the intersection of computer vision, computer graphics, and\nanimal behavior modeling.\n","authors":["Zan Wang","Siyu Chen","Luya Mo","Xinfeng Gao","Yuxin Shen","Lebin Ding","Wei Liang"],"pdf_url":"https://arxiv.org/pdf/2510.24117v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2510.24116v1","updated":"2025-10-28T06:41:43Z","published":"2025-10-28T06:41:43Z","title":"UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via\n  Frequency-Domain Representations","summary":"  Knowledge distillation (KD) is an effective model compression technique that\ntransfers knowledge from a high-performance teacher to a lightweight student,\nreducing cost while maintaining accuracy. In visual applications, where\nlarge-scale image models are widely used, KD enables efficient deployment.\nHowever, architectural diversity introduces semantic discrepancies that hinder\nthe use of intermediate representations. Most existing KD methods are designed\nfor homogeneous models and degrade in heterogeneous scenarios, especially when\nintermediate features are involved. Prior studies mainly focus on the logits\nspace, making limited use of the semantic information in intermediate layers.\nTo address this limitation, Unified Heterogeneous Knowledge Distillation (UHKD)\nis proposed as a framework that leverages intermediate features in the\nfrequency domain for cross-architecture transfer. Fourier transform is applied\nto capture global feature information, alleviating representational\ndiscrepancies between heterogeneous teacher-student pairs. A Feature\nTransformation Module (FTM) produces compact frequency-domain representations\nof teacher features, while a learnable Feature Alignment Module (FAM) projects\nstudent features and aligns them via multi-level matching. Training is guided\nby a joint objective combining mean squared error on intermediate features with\nKullback-Leibler divergence on logits. Experiments on CIFAR-100 and ImageNet-1K\ndemonstrate gains of 5.59% and 0.83% over the latest method, highlighting UHKD\nas an effective approach for unifying heterogeneous representations and\nenabling efficient utilization of visual knowledge\n","authors":["Fengming Yu","Haiwei Pan","Kejia Zhang","Jian Guan","Haiying Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.24116v1.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2506.17939v2","updated":"2025-10-28T06:37:24Z","published":"2025-06-22T08:09:58Z","title":"GEMeX-RMCoT: An Enhanced Med-VQA Dataset for Region-Aware Multimodal\n  Chain-of-Thought Reasoning","summary":"  Medical visual question answering aims to support clinical decision-making by\nenabling models to answer natural language questions based on medical images.\nWhile recent advances in multi-modal learning have significantly improved\nperformance, current methods still suffer from limited answer reliability and\npoor interpretability, impairing the ability of clinicians and patients to\nunderstand and trust model outputs. To address these limitations, this work\nfirst proposes a Region-Aware Multimodal Chain-of-Thought (RMCoT) dataset, in\nwhich the process of producing an answer is preceded by a sequence of\nintermediate reasoning steps that explicitly ground relevant visual regions of\nthe medical image, thereby providing fine-grained explainability. Furthermore,\nwe introduce a novel verifiable reward mechanism for reinforcement learning to\nguide post-training, improving the alignment between the model's reasoning\nprocess and its final answer. Remarkably, our method achieves comparable\nperformance using only one-eighth of the training data, demonstrating the\nefficiency and effectiveness of the proposal. The dataset is available at\nhttps://www.med-vqa.com/GEMeX/.\n","authors":["Bo Liu","Xiangyu Zhao","Along He","Yidi Chen","Huazhu Fu","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2506.17939v2.pdf","comment":"Accepted at ACM MM 2025 (also known as GEMeX-ThinkVG)"},{"id":"http://arxiv.org/abs/2505.12758v4","updated":"2025-10-28T06:27:55Z","published":"2025-05-19T06:35:11Z","title":"Global urban visual perception varies across demographics and\n  personalities","summary":"  Understanding people's preferences is crucial for urban planning, yet current\napproaches often combine responses from multi-cultural populations, obscuring\ndemographic differences and risking amplifying biases. We conducted a\nlargescale urban visual perception survey of streetscapes worldwide using\nstreet view imagery, examining how demographics -- including gender, age,\nincome, education, race and ethnicity, and personality traits -- shape\nperceptions among 1,000 participants with balanced demographics from five\ncountries and 45 nationalities. This dataset, Street Perception Evaluation\nConsidering Socioeconomics (SPECS), reveals demographic- and personality-based\ndifferences across six traditional indicators -- safe, lively, wealthy,\nbeautiful, boring, depressing -- and four new ones -- live nearby, walk, cycle,\ngreen. Location-based sentiments further shape these preferences. Machine\nlearning models trained on existing global datasets tend to overestimate\npositive indicators and underestimate negative ones compared to human\nresponses, underscoring the need for local context. Our study aspires to\nrectify the myopic treatment of street perception, which rarely considers\ndemographics or personality traits.\n","authors":["Matias Quintana","Youlong Gu","Xiucheng Liang","Yujun Hou","Koichi Ito","Yihan Zhu","Mahmoud Abdelrahman","Filip Biljecki"],"pdf_url":"https://arxiv.org/pdf/2505.12758v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24108v1","updated":"2025-10-28T06:26:36Z","published":"2025-10-28T06:26:36Z","title":"ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory\n  Scoring","summary":"  End-to-end autonomous driving maps raw sensor inputs directly into\nego-vehicle trajectories to avoid cascading errors from perception modules and\nto leverage rich semantic cues. Existing frameworks largely rely on Imitation\nLearning (IL), which can be limited by sub-optimal expert demonstrations and\ncovariate shift during deployment. On the other hand, Reinforcement Learning\n(RL) has recently shown potential in scaling up with simulations, but is\ntypically confined to low-dimensional symbolic inputs (e.g. 3D objects and\nmaps), falling short of full end-to-end learning from raw sensor data. We\nintroduce ZTRS (Zero-Imitation End-to-End Autonomous Driving with Trajectory\nScoring), a framework that combines the strengths of both worlds: sensor inputs\nwithout losing information and RL training for robust planning. To the best of\nour knowledge, ZTRS is the first framework that eliminates IL entirely by only\nlearning from rewards while operating directly on high-dimensional sensor data.\nZTRS utilizes offline reinforcement learning with our proposed Exhaustive\nPolicy Optimization (EPO), a variant of policy gradient tailored for enumerable\nactions and rewards. ZTRS demonstrates strong performance across three\nbenchmarks: Navtest (generic real-world open-loop planning), Navhard (open-loop\nplanning in challenging real-world and synthetic scenarios), and HUGSIM\n(simulated closed-loop driving). Specifically, ZTRS achieves the\nstate-of-the-art result on Navhard and outperforms IL-based baselines on\nHUGSIM. Code will be available at https://github.com/woxihuanjiangguo/ZTRS.\n","authors":["Zhenxin Li","Wenhao Yao","Zi Wang","Xinglong Sun","Jingde Chen","Nadine Chang","Maying Shen","Jingyu Song","Zuxuan Wu","Shiyi Lan","Jose M. Alvarez"],"pdf_url":"https://arxiv.org/pdf/2510.24108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24105v1","updated":"2025-10-28T06:21:06Z","published":"2025-10-28T06:21:06Z","title":"Enhancing Pre-trained Representation Classifiability can Boost its\n  Interpretability","summary":"  The visual representation of a pre-trained model prioritizes the\nclassifiability on downstream tasks, while the widespread applications for\npre-trained visual models have posed new requirements for representation\ninterpretability. However, it remains unclear whether the pre-trained\nrepresentations can achieve high interpretability and classifiability\nsimultaneously. To answer this question, we quantify the representation\ninterpretability by leveraging its correlation with the ratio of interpretable\nsemantics within the representations. Given the pre-trained representations,\nonly the interpretable semantics can be captured by interpretations, whereas\nthe uninterpretable part leads to information loss. Based on this fact, we\npropose the Inherent Interpretability Score (IIS) that evaluates the\ninformation loss, measures the ratio of interpretable semantics, and quantifies\nthe representation interpretability. In the evaluation of the representation\ninterpretability with different classifiability, we surprisingly discover that\nthe interpretability and classifiability are positively correlated, i.e.,\nrepresentations with higher classifiability provide more interpretable\nsemantics that can be captured in the interpretations. This observation further\nsupports two benefits to the pre-trained representations. First, the\nclassifiability of representations can be further improved by fine-tuning with\ninterpretability maximization. Second, with the classifiability improvement for\nthe representations, we obtain predictions based on their interpretations with\nless accuracy degradation. The discovered positive correlation and\ncorresponding applications show that practitioners can unify the improvements\nin interpretability and classifiability for pre-trained vision models. Codes\nare available at https://github.com/ssfgunner/IIS.\n","authors":["Shufan Shen","Zhaobo Qi","Junshu Sun","Qingming Huang","Qi Tian","Shuhui Wang"],"pdf_url":"https://arxiv.org/pdf/2510.24105v1.pdf","comment":"ICLR 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2510.23118v2","updated":"2025-10-28T06:10:07Z","published":"2025-10-27T08:38:52Z","title":"Task-Agnostic Fusion of Time Series and Imagery for Earth Observation","summary":"  We propose a task-agnostic framework for multimodal fusion of time series and\nsingle timestamp images, enabling cross-modal generation and robust downstream\nperformance. Our approach explores deterministic and learned strategies for\ntime series quantization and then leverages a masked correlation learning\nobjective, aligning discrete image and time series tokens in a unified\nrepresentation space. Instantiated in the Earth observation domain, the\npretrained model generates consistent global temperature profiles from\nsatellite imagery and is validated through counterfactual experiments. Across\ndownstream tasks, our task-agnostic pretraining outperforms task-specific\nfusion by 6% in R^2 and 2% in RMSE on average, and exceeds baseline methods by\n50\\% in R$^2$ and 12\\% in RMSE. Finally, we analyze gradient sensitivity across\nmodalities, providing insights into model robustness. Code, data, and weights\nwill be released under a permissive license.\n","authors":["Gianfranco Basile","Johannes Jakubik","Benedikt Blumenstiel","Thomas Brunschwiler","Juan Bernabe Moreno"],"pdf_url":"https://arxiv.org/pdf/2510.23118v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24093v1","updated":"2025-10-28T06:06:52Z","published":"2025-10-28T06:06:52Z","title":"OmniText: A Training-Free Generalist for Controllable Text-Image\n  Manipulation","summary":"  Recent advancements in diffusion-based text synthesis have demonstrated\nsignificant performance in inserting and editing text within images via\ninpainting. However, despite the potential of text inpainting methods, three\nkey limitations hinder their applicability to broader Text Image Manipulation\n(TIM) tasks: (i) the inability to remove text, (ii) the lack of control over\nthe style of rendered text, and (iii) a tendency to generate duplicated\nletters. To address these challenges, we propose OmniText, a training-free\ngeneralist capable of performing a wide range of TIM tasks. Specifically, we\ninvestigate two key properties of cross- and self-attention mechanisms to\nenable text removal and to provide control over both text styles and content.\nOur findings reveal that text removal can be achieved by applying\nself-attention inversion, which mitigates the model's tendency to focus on\nsurrounding text, thus reducing text hallucinations. Additionally, we\nredistribute cross-attention, as increasing the probability of certain text\ntokens reduces text hallucination. For controllable inpainting, we introduce\nnovel loss functions in a latent optimization framework: a cross-attention\ncontent loss to improve text rendering accuracy and a self-attention style loss\nto facilitate style customization. Furthermore, we present OmniText-Bench, a\nbenchmark dataset for evaluating diverse TIM tasks. It includes input images,\ntarget text with masks, and style references, covering diverse applications\nsuch as text removal, rescaling, repositioning, and insertion and editing with\nvarious styles. Our OmniText framework is the first generalist method capable\nof performing diverse TIM tasks. It achieves state-of-the-art performance\nacross multiple tasks and metrics compared to other text inpainting methods and\nis comparable with specialist methods.\n","authors":["Agus Gunawan","Samuel Teodoro","Yun Chen","Soo Ye Kim","Jihyong Oh","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2510.24093v1.pdf","comment":"The first two authors contributed equally to this work. The last two\n  authors are co-corresponding authors"},{"id":"http://arxiv.org/abs/2508.17102v2","updated":"2025-10-28T06:06:16Z","published":"2025-08-23T18:05:06Z","title":"GRASP: Geospatial pixel Reasoning viA Structured Policy learning","summary":"  Geospatial pixel reasoning aims to generate segmentation masks in remote\nsensing imagery directly from natural-language instructions. Most existing\napproaches follow a paradigm that fine-tunes multimodal large language models\nunder supervision with dense pixel-level masks as ground truth. While effective\nwithin the training data distribution, this design suffers from two main\ndrawbacks: (1) the high cost of large-scale dense mask annotation, and (2) the\nlimited generalization capability of supervised fine-tuning in out-of-domain\nscenarios. To address these issues, we propose GRASP, a structured\npolicy-learning framework that integrates a multimodal large language model\nwith a pretrained segmentation model in a cascaded manner. To enhance\ngeneralization, we introduce PRIME, a training paradigm that replaces\nsupervised fine-tuning with reinforcement learning to better align reasoning\nand grounding behaviors with task objectives. To reduce annotation costs, we\ndesign BoP-Rewards, which substitutes dense mask labels with bounding box and\npositive points. It further verifies outputs through two complementary signals:\nformat, which constrains the reasoning and grounding structure to remain\nsyntactically parsable, and accuracy, which evaluates the quality of predicted\nboxes and points. For evaluation, we train our method and all baselines on\nEarthReason and GeoPixInstruct, constructing an in-domain benchmark by merging\ntheir test sets. We further release GRASP-1k, a fully out-of-domain benchmark\nwith reasoning-intensive queries, reasoning traces, and fine-grained masks.\nExperimental results demonstrate state-of-the-art (SOTA) in-domain performance\nand up to 54\\% improvement in out-of-domain scenarios, confirming that\nreinforcement learning with cost-aware rewards provides a robust and scalable\nparadigm for geospatial pixel reasoning. All code and datasets will be released\npublicly.\n","authors":["Chengjie Jiang","Yunqi Zhou","Jiafeng Yan","Jing Li","Jiayang Li","Yue Zhou","Hongjie He","Jonathan Li"],"pdf_url":"https://arxiv.org/pdf/2508.17102v2.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.22943v2","updated":"2025-10-28T05:57:57Z","published":"2025-10-27T02:56:17Z","title":"Switchable Token-Specific Codebook Quantization For Face Image\n  Compression","summary":"  With the ever-increasing volume of visual data, the efficient and lossless\ntransmission, along with its subsequent interpretation and understanding, has\nbecome a critical bottleneck in modern information systems. The emerged\ncodebook-based solution utilize a globally shared codebook to quantize and\ndequantize each token, controlling the bpp by adjusting the number of tokens or\nthe codebook size. However, for facial images, which are rich in attributes,\nsuch global codebook strategies overlook both the category-specific\ncorrelations within images and the semantic differences among tokens, resulting\nin suboptimal performance, especially at low bpp. Motivated by these\nobservations, we propose a Switchable Token-Specific Codebook Quantization for\nface image compression, which learns distinct codebook groups for different\nimage categories and assigns an independent codebook to each token. By\nrecording the codebook group to which each token belongs with a small number of\nbits, our method can reduce the loss incurred when decreasing the size of each\ncodebook group. This enables a larger total number of codebooks under a lower\noverall bpp, thereby enhancing the expressive capability and improving\nreconstruction performance. Owing to its generalizable design, our method can\nbe integrated into any existing codebook-based representation learning approach\nand has demonstrated its effectiveness on face recognition datasets, achieving\nan average accuracy of 93.51% for reconstructed images at 0.05 bpp.\n","authors":["Yongbo Wang","Haonan Wang","Guodong Mu","Ruixin Zhang","Jiaqi Chen","Jingyun Zhang","Jun Wang","Yuan Xie","Zhizhong Zhang","Shouhong Ding"],"pdf_url":"https://arxiv.org/pdf/2510.22943v2.pdf","comment":"NeurIPS 2025 accepted"},{"id":"http://arxiv.org/abs/2505.12702v2","updated":"2025-10-28T05:41:49Z","published":"2025-05-19T04:52:31Z","title":"Long-RVOS: A Comprehensive Benchmark for Long-term Referring Video\n  Object Segmentation","summary":"  Referring video object segmentation (RVOS) aims to identify, track and\nsegment the objects in a video based on language descriptions, which has\nreceived great attention in recent years. However, existing datasets remain\nfocus on short video clips within several seconds, with salient objects visible\nin most frames. To advance the task towards more practical scenarios, we\nintroduce \\textbf{Long-RVOS}, a large-scale benchmark for long-term referring\nvideo object segmentation. Long-RVOS contains 2,000+ videos of an average\nduration exceeding 60 seconds, covering a variety of objects that undergo\nocclusion, disappearance-reappearance and shot changing. The objects are\nmanually annotated with three different types of descriptions to individually\nevaluate the understanding of static attributes, motion patterns and\nspatiotemporal relationships. Moreover, unlike previous benchmarks that rely\nsolely on the per-frame spatial evaluation, we introduce two new metrics to\nassess the temporal and spatiotemporal consistency. We benchmark 6\nstate-of-the-art methods on Long-RVOS. The results show that current approaches\nstruggle severely with the long-video challenges. To address this, we further\npropose ReferMo, a promising baseline method that integrates motion information\nto expand the temporal receptive field, and employs a local-to-global\narchitecture to capture both short-term dynamics and long-term dependencies.\nDespite simplicity, ReferMo achieves significant improvements over current\nmethods in long-term scenarios. We hope that Long-RVOS and our baseline can\ndrive future RVOS research towards tackling more realistic and long-form\nvideos.\n","authors":["Tianming Liang","Haichao Jiang","Yuting Yang","Chaolei Tan","Shuai Li","Wei-Shi Zheng","Jian-Fang Hu"],"pdf_url":"https://arxiv.org/pdf/2505.12702v2.pdf","comment":"Project Page: \\url{https://isee-laboratory.github.io/Long-RVOS}"},{"id":"http://arxiv.org/abs/2510.24078v1","updated":"2025-10-28T05:40:14Z","published":"2025-10-28T05:40:14Z","title":"Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained\n  Classification","summary":"  Text-to-image (T2I) models are increasingly used for synthetic dataset\ngeneration, but generating effective synthetic training data for classification\nremains challenging. Fine-tuning a T2I model with a few real examples can help\nimprove the quality of synthetic training data; however, it may also cause\noverfitting and reduce diversity in the generated samples. We propose a\nfine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for\nfine-grained classification. Given a small set of real examples, we first\nextract class-agnostic attributes such as scene background and object pose. We\nthen explicitly condition on these attributes during fine-tuning of the T2I\nmodel and marginalize them out during generation. This design mitigates\noverfitting, preserves the T2I model's generative prior, reduces estimation\nerrors, and further minimizes unintended inter-class associations. Extensive\nexperiments across multiple T2I models, backbones, and datasets show that our\nmethod achieves state-of-the-art performance in low-shot fine-grained\nclassification when augmented with synthetic data. Concretely, BOB outperforms\nDataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning\na CLIP classifier with five real images augmented with 100 synthetic images).\nIn three of the four benchmarks, fine-tuning downstream models with 5 real\nimages augmented with BOB achieves better performance than fine-tuning with 10\nreal images. Collectively, BOB outperforms prior art in 18 of 24 experimental\nsettings, with 2+% accuracy improvements in 14 of these settings.\n","authors":["William Yang","Xindi Wu","Zhiwei Deng","Esin Tureci","Olga Russakovsky"],"pdf_url":"https://arxiv.org/pdf/2510.24078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06517v2","updated":"2025-10-28T05:40:02Z","published":"2025-06-06T20:28:37Z","title":"GS4: Generalizable Sparse Splatting Semantic SLAM","summary":"  Traditional SLAM algorithms excel at camera tracking, but typically produce\nincomplete and low-resolution maps that are not tightly integrated with\nsemantics prediction. Recent work integrates Gaussian Splatting (GS) into SLAM\nto enable dense, photorealistic 3D mapping, yet existing GS-based SLAM methods\nrequire per-scene optimization that is slow and consumes an excessive number of\nGaussians. We present GS4, the first generalizable GS-based semantic SLAM\nsystem. Compared with prior approaches, GS4 runs 10x faster, uses 10x fewer\nGaussians, and achieves state-of-the-art performance across color, depth,\nsemantic mapping and camera tracking. From an RGB-D video stream, GS4\nincrementally builds and updates a set of 3D Gaussians using a feed-forward\nnetwork. First, the Gaussian Prediction Model estimates a sparse set of\nGaussian parameters from input frame, which integrates both color and semantic\nprediction with the same backbone. Then, the Gaussian Refinement Network merges\nnew Gaussians with the existing set while avoiding redundancy. Finally, we\npropose to optimize GS for only 1-5 iterations that corrects drift and floaters\nwhen significant pose changes are detected. Experiments on the real-world\nScanNet and ScanNet++ benchmarks demonstrate state-of-the-art semantic SLAM\nperformance, with strong generalization capability shown through zero-shot\ntransfer to the NYUv2 and TUM RGB-D datasets.\n","authors":["Mingqi Jiang","Chanho Kim","Chen Ziwen","Li Fuxin"],"pdf_url":"https://arxiv.org/pdf/2506.06517v2.pdf","comment":"17 pages, 6 figures"},{"id":"http://arxiv.org/abs/2510.21412v2","updated":"2025-10-28T05:32:23Z","published":"2025-10-24T12:54:13Z","title":"Bridging the gap to real-world language-grounded visual concept learning","summary":"  Human intelligence effortlessly interprets visual scenes along a rich\nspectrum of semantic dimensions. However, existing approaches to\nlanguage-grounded visual concept learning are limited to a few predefined\nprimitive axes, such as color and shape, and are typically explored in\nsynthetic datasets. In this work, we propose a scalable framework that\nadaptively identifies image-related concept axes and grounds visual concepts\nalong these axes in real-world scenes. Leveraging a pretrained vision-language\nmodel and our universal prompting strategy, our framework identifies a diverse\nimage-related axes without any prior knowledge. Our universal concept encoder\nadaptively binds visual features to the discovered axes without introducing\nadditional model parameters for each concept. To ground visual concepts along\nthe discovered axes, we optimize a compositional anchoring objective, which\nensures that each axis can be independently manipulated without affecting\nothers. We demonstrate the effectiveness of our framework on subsets of\nImageNet, CelebA-HQ, and AFHQ, showcasing superior editing capabilities across\ndiverse real-world concepts that are too varied to be manually predefined. Our\nmethod also exhibits strong compositional generalization, outperforming\nexisting visual concept learning and text-based editing methods. The code is\navailable at https://github.com/whieya/Language-grounded-VCL.\n","authors":["Whie Jung","Semin Kim","Junee Kim","Seunghoon Hong"],"pdf_url":"https://arxiv.org/pdf/2510.21412v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18672v8","updated":"2025-10-28T05:22:48Z","published":"2025-03-24T13:44:12Z","title":"CalFuse: Multi-Modal Continual Learning via Feature Calibration and\n  Parameter Fusion","summary":"  With the proliferation of multi-modal data in large-scale visual recognition\nsystems, enabling models to continuously acquire knowledge from evolving data\nstreams while preserving prior information has become increasingly critical.\nClass-Continual Learning (CCL) addresses this challenge by incrementally\nincorporating new class knowledge without revisiting historical data, making it\nessential for real-world big data applications. While traditional CCL methods\nrely solely on visual features, recent advances in Vision-Language Models\n(VLMs) such as CLIP demonstrate significant potential for CCL by leveraging\npre-trained multi-modal knowledge. However, existing approaches face challenges\nin mitigating catastrophic forgetting while maintaining the cross-modal\ngeneralization capabilities of VLMs. To address these limitations, we propose\nCalFuse, a framework that synergizes feature Calibration with parameter Fusion\nto enable effective multi-modal knowledge integration in continual learning\nscenarios. CalFuse introduces a dynamic feature calibration mechanism that\nadaptively balances original CLIP visual representations with task-specific\nfeatures, preserving the model's intrinsic cross-modal generalization while\nadapting to new classes. Concurrently, a QR decomposition-based parameter\nfusion strategy progressively integrates newly acquired knowledge with\nhistorical task parameters, maintaining equilibrium between learning new class\nrepresentations and retaining prior knowledge across sequential tasks.\nExtensive experiments on benchmark datasets validate the effectiveness of our\napproach in large-scale multi-modal continual learning settings, demonstrating\nsuperior performance over state-of-the-art methods in both average accuracy and\nfinal task retention.\n","authors":["Juncen Guo","Siao Liu","Xiaoguang Zhu","Lianlong Sun","Liangyu Teng","Jingyi Wu","Di Li","Linxiao Gong","Weiwei Jiang","Wei Zhou","Liang Song"],"pdf_url":"https://arxiv.org/pdf/2503.18672v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09282v4","updated":"2025-10-28T04:40:41Z","published":"2025-02-13T12:54:13Z","title":"MsEdF: A Multi-stream Encoder-decoder Framework for Remote Sensing Image\n  Captioning","summary":"  Remote sensing images contain complex spatial patterns and semantic\nstructures, which makes the captioning model difficult to accurately describe.\nEncoder-decoder architectures have become the widely used approach for RSIC by\ntranslating visual content into descriptive text. However, many existing\nmethods rely on a single-stream architecture, which weakens the model to\naccurately describe the image. Such single-stream architectures typically\nstruggle to extract diverse spatial features or capture complex semantic\nrelationships, limiting their effectiveness in scenes with high intraclass\nsimilarity or contextual ambiguity. In this work, we propose a novel\nMulti-stream Encoder-decoder Framework (MsEdF) which improves the performance\nof RSIC by optimizing both the spatial representation and language generation\nof encoder-decoder architecture. The encoder fuses information from two\ncomplementary image encoders, thereby promoting feature diversity through the\nintegration of multiscale and structurally distinct cues. To improve the\ncapture of context-aware descriptions, we refine the input sequence's semantic\nmodeling on the decoder side using a stacked GRU architecture with an\nelement-wise aggregation scheme. Experiments on three benchmark RSIC datasets\nshow that MsEdF outperforms several baseline models.\n","authors":["Swadhin Das","Raksha Sharma"],"pdf_url":"https://arxiv.org/pdf/2502.09282v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02885v2","updated":"2025-10-28T04:18:29Z","published":"2025-01-06T09:55:55Z","title":"MDP3: A Training-free Approach for List-wise Frame Selection in\n  Video-LLMs","summary":"  Video large language models (Video-LLMs) have made significant progress in\nunderstanding videos. However, processing multiple frames leads to lengthy\nvisual token sequences, presenting challenges such as the limited context\nlength cannot accommodate the entire video, and the inclusion of irrelevant\nframes hinders visual perception. Hence, effective frame selection is crucial.\nThis paper emphasizes that frame selection should follow three key principles:\nquery relevance, list-wise diversity, and sequentiality. Existing methods, such\nas uniform frame sampling and query-frame matching, do not capture all of these\nprinciples. Thus, we propose Markov decision determinantal point process with\ndynamic programming (MDP3) for frame selection, a training-free and\nmodel-agnostic method that can be seamlessly integrated into existing\nVideo-LLMs. Our method first estimates frame similarities conditioned on the\nquery using a conditional Gaussian kernel within the reproducing kernel Hilbert\nspace~(RKHS). We then apply the determinantal point process~(DPP) to the\nsimilarity matrix to capture both query relevance and list-wise diversity. To\nincorporate sequentiality, we segment the video and apply DPP within each\nsegment, conditioned on the preceding segment selection, modeled as a Markov\ndecision process~(MDP) for allocating selection sizes across segments.\nTheoretically, MDP3 provides a \\((1 - 1/e)\\)-approximate solution to the\nNP-hard list-wise frame selection problem with pseudo-polynomial time\ncomplexity, demonstrating its efficiency. Empirically, MDP3 significantly\noutperforms existing methods, verifying its effectiveness and robustness.\n","authors":["Hui Sun","Shiyin Lu","Huanyu Wang","Qing-Guo Chen","Zhao Xu","Weihua Luo","Kaifu Zhang","Ming Li"],"pdf_url":"https://arxiv.org/pdf/2501.02885v2.pdf","comment":"26 pages, 14 figures"},{"id":"http://arxiv.org/abs/2510.22706v2","updated":"2025-10-28T04:16:45Z","published":"2025-10-26T14:57:44Z","title":"IGGT: Instance-Grounded Geometry Transformer for Semantic 3D\n  Reconstruction","summary":"  Humans naturally perceive the geometric structure and semantic content of a\n3D world as intertwined dimensions, enabling coherent and accurate\nunderstanding of complex scenes. However, most prior approaches prioritize\ntraining large geometry models for low-level 3D reconstruction and treat\nhigh-level spatial understanding in isolation, overlooking the crucial\ninterplay between these two fundamental aspects of 3D-scene analysis, thereby\nlimiting generalization and leading to poor performance in downstream 3D\nunderstanding tasks. Recent attempts have mitigated this issue by simply\naligning 3D models with specific language models, thus restricting perception\nto the aligned model's capacity and limiting adaptability to downstream tasks.\nIn this paper, we propose InstanceGrounded Geometry Transformer (IGGT), an\nend-to-end large unified transformer to unify the knowledge for both spatial\nreconstruction and instance-level contextual understanding. Specifically, we\ndesign a 3D-Consistent Contrastive Learning strategy that guides IGGT to encode\na unified representation with geometric structures and instance-grounded\nclustering through only 2D visual inputs. This representation supports\nconsistent lifting of 2D visual inputs into a coherent 3D scene with explicitly\ndistinct object instances. To facilitate this task, we further construct\nInsScene-15K, a large-scale dataset with high-quality RGB images, poses, depth\nmaps, and 3D-consistent instance-level mask annotations with a novel data\ncuration pipeline.\n","authors":["Hao Li","Zhengyu Zou","Fangfu Liu","Xuanyang Zhang","Fangzhou Hong","Yukang Cao","Yushi Lan","Manyuan Zhang","Gang Yu","Dingwen Zhang","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2510.22706v2.pdf","comment":"https://github.com/lifuguan/IGGT_official"},{"id":"http://arxiv.org/abs/2505.13389v5","updated":"2025-10-28T04:13:18Z","published":"2025-05-19T17:30:13Z","title":"VSA: Faster Video Diffusion with Trainable Sparse Attention","summary":"  Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D\nattention, even though most of the attention mass concentrates on a small\nsubset of positions. We turn this observation into VSA, a trainable,\nhardware-efficient sparse attention that replaces full attention at \\emph{both}\ntraining and inference. In VSA, a lightweight coarse stage pools tokens into\ntiles and identifies high-weight \\emph{critical tokens}; a fine stage computes\ntoken-level attention only inside those tiles subjecting to block computing\nlayout to ensure hard efficiency. This leads to a single differentiable kernel\nthat trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of\nFlashAttention3 MFU. We perform a large sweep of ablation studies and\nscaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA\nreaches a Pareto point that cuts training FLOPS by 2.53$\\times$ with no drop in\ndiffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention\ntime by 6$\\times$ and lowers end-to-end generation time from 31s to 18s with\ncomparable quality. These results establish trainable sparse attention as a\npractical alternative to full attention and a key enabler for further scaling\nof video diffusion models. Code will be available at\nhttps://github.com/hao-ai-lab/FastVideo.\n","authors":["Peiyuan Zhang","Yongqi Chen","Haofeng Huang","Will Lin","Zhengzhong Liu","Ion Stoica","Eric Xing","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.13389v5.pdf","comment":"Accepted by Neurips 2025"},{"id":"http://arxiv.org/abs/2506.22802v2","updated":"2025-10-28T03:55:35Z","published":"2025-06-28T08:08:16Z","title":"Riemannian-Geometric Fingerprints of Generative Models","summary":"  Recent breakthroughs and rapid integration of generative models (GMs) have\nsparked interest in the problem of model attribution and their fingerprints.\nFor instance, service providers need reliable methods of authenticating their\nmodels to protect their IP, while users and law enforcement seek to verify the\nsource of generated content for accountability and trust. In addition, a\ngrowing threat of model collapse is arising, as more model-generated data are\nbeing fed back into sources (e.g., YouTube) that are often harvested for\ntraining (\"regurgitative training\"), heightening the need to differentiate\nsynthetic from human data. Yet, a gap still exists in understanding generative\nmodels' fingerprints, we believe, stemming from the lack of a formal framework\nthat can define, represent, and analyze the fingerprints in a principled way.\nTo address this gap, we take a geometric approach and propose a new definition\nof artifact and fingerprint of GMs using Riemannian geometry, which allows us\nto leverage the rich theory of differential geometry. Our new definition\ngeneralizes previous work (Song et al., 2024) to non-Euclidean manifolds by\nlearning Riemannian metrics from data and replacing the Euclidean distances and\nnearest-neighbor search with geodesic distances and kNN-based Riemannian center\nof mass. We apply our theory to a new gradient-based algorithm for computing\nthe fingerprints in practice. Results show that it is more effective in\ndistinguishing a large array of GMs, spanning across 4 different datasets in 2\ndifferent resolutions (64 by 64, 256 by 256), 27 model architectures, and 2\nmodalities (Vision, Vision-Language). Using our proposed definition\nsignificantly improves the performance on model attribution, as well as a\ngeneralization to unseen datasets, model types, and modalities, suggesting its\npractical efficacy.\n","authors":["Hae Jin Song","Laurent Itti"],"pdf_url":"https://arxiv.org/pdf/2506.22802v2.pdf","comment":"ICCV 2025 Highlight paper"},{"id":"http://arxiv.org/abs/2510.24038v1","updated":"2025-10-28T03:47:44Z","published":"2025-10-28T03:47:44Z","title":"Enhancing CLIP Robustness via Cross-Modality Alignment","summary":"  Vision-language models (VLMs) such as CLIP demonstrate strong generalization\nin zero-shot classification but remain highly vulnerable to adversarial\nperturbations. Existing methods primarily focus on adversarial fine-tuning or\nprompt optimization; they often overlook the gaps in CLIP's encoded features,\nwhich is shown as the text and image features lie far apart from each other.\nThis misalignment is significantly amplified under adversarial perturbations,\nleading to severe degradation in classification performance. To address this\nproblem, we propose Cross-modality Alignment, dubbed COLA, an optimal\ntransport-based framework that explicitly addresses adversarial misalignment by\nrestoring both global image-text alignment and local structural consistency in\nthe feature space. (1) COLA first projects adversarial image embeddings onto a\nsubspace spanned by class text features, effectively filtering out non-semantic\ndistortions while preserving discriminative information. (2) It then models\nimages and texts as discrete distributions over multiple augmented views and\nrefines their alignment via OT, with the subspace projection seamlessly\nintegrated into the cost computation. This design ensures stable cross-modal\nalignment even under adversarial conditions. COLA is training-free and\ncompatible with existing fine-tuned models. Extensive evaluations across 14\nzero-shot classification benchmarks demonstrate the effectiveness of COLA,\nespecially with an average improvement of 6.7% on ImageNet and its variants\nunder PGD adversarial attacks, while maintaining high accuracy on clean\nsamples.\n","authors":["Xingyu Zhu","Beier Zhu","Shuo Wang","Kesen Zhao","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.24038v1.pdf","comment":"NeurIPS 2025 Spotlight"},{"id":"http://arxiv.org/abs/2510.24037v1","updated":"2025-10-28T03:39:18Z","published":"2025-10-28T03:39:18Z","title":"Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for\n  Vision Models","summary":"  Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision\nmodels to downstream tasks. Among PEFT paradigms, sparse tuning achieves\nremarkable performance by adjusting only the weights most relevant to\ndownstream tasks, rather than densely tuning the entire weight matrix. Current\nmethods follow a two-stage paradigm. First, it locates task-relevant weights by\ngradient information, which overlooks the parameter adjustments during\nfine-tuning and limits the performance. Second, it updates only the located\nweights by applying a sparse mask to the gradient of the weight matrix, which\nresults in high memory usage due to the storage of all weight matrices in the\noptimizer. In this paper, we propose a one-stage method named SNELLA to\novercome the above limitations. For memory usage, SNELLA selectively updates\nthe weight matrix by adding it to another sparse matrix that is merged by two\nlow-rank learnable matrices. We extend the low-rank decomposition by\nintroducing nonlinear kernel functions, thereby increasing the rank of the\nresulting merged matrix to prevent the interdependency among weight updates,\nenabling better adaptation to downstream tasks. For locating task-relevant\nweights, we propose an adaptive bi-level sparsity allocation mechanism that\nencourages weights to compete across and inside layers based on their\nimportance scores in an end-to-end manner. Extensive experiments are conducted\non classification, segmentation, and generation tasks using different\npre-trained vision models. The results show that SNELLA achieves SOTA\nperformance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.\n90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.\nCompared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%\nacross models with parameter scales from 86M to 632M. Our source codes are\navailable at https://github.com/ssfgunner/SNELL.\n","authors":["Shufan Shen","Junshu Sun","Shuhui Wang","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2510.24037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.16691v2","updated":"2025-10-28T03:37:32Z","published":"2025-09-20T13:37:37Z","title":"InstanceAssemble: Layout-Aware Image Generation via Instance Assembling\n  Attention","summary":"  Diffusion models have demonstrated remarkable capabilities in generating\nhigh-quality images. Recent advancements in Layout-to-Image (L2I) generation\nhave leveraged positional conditions and textual descriptions to facilitate\nprecise and controllable image synthesis. Despite overall progress, current L2I\nmethods still exhibit suboptimal performance. Therefore, we propose\nInstanceAssemble, a novel architecture that incorporates layout conditions via\ninstance-assembling attention, enabling position control with bounding boxes\n(bbox) and multimodal content control including texts and additional visual\ncontent. Our method achieves flexible adaption to existing DiT-based T2I models\nthrough light-weighted LoRA modules. Additionally, we propose a Layout-to-Image\nbenchmark, Denselayout, a comprehensive benchmark for layout-to-image\ngeneration, containing 5k images with 90k instances in total. We further\nintroduce Layout Grounding Score (LGS), an interpretable evaluation metric to\nmore precisely assess the accuracy of L2I generation. Experiments demonstrate\nthat our InstanceAssemble method achieves state-of-the-art performance under\ncomplex layout conditions, while exhibiting strong compatibility with diverse\nstyle LoRA modules. The code and pretrained models are publicly available at\nhttps://github.com/FireRedTeam/InstanceAssemble.\n","authors":["Qiang Xiang","Shuang Sun","Binglei Li","Dejia Song","Huaxia Li","Nemo Chen","Xu Tang","Yao Hu","Junping Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.16691v2.pdf","comment":"Accepted in NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24036v1","updated":"2025-10-28T03:36:15Z","published":"2025-10-28T03:36:15Z","title":"ResNet: Enabling Deep Convolutional Neural Networks through Residual\n  Learning","summary":"  Convolutional Neural Networks (CNNs) has revolutionized computer vision, but\ntraining very deep networks has been challenging due to the vanishing gradient\nproblem. This paper explores Residual Networks (ResNet), introduced by He et\nal. (2015), which overcomes this limitation by using skip connections. ResNet\nenables the training of networks with hundreds of layers by allowing gradients\nto flow directly through shortcut connections that bypass intermediate layers.\nIn our implementation on the CIFAR-10 dataset, ResNet-18 achieves 89.9%\naccuracy compared to 84.1% for a traditional deep CNN of similar depth, while\nalso converging faster and training more stably.\n","authors":["Xingyu Liu","Kun Ming Goh"],"pdf_url":"https://arxiv.org/pdf/2510.24036v1.pdf","comment":"3 pages, 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2505.20744v2","updated":"2025-10-28T03:32:37Z","published":"2025-05-27T05:34:56Z","title":"MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity\n  Recognition","summary":"  Human Activity Recognition (HAR) with wearable sensors is challenged by\nlimited interpretability, which significantly impacts cross-dataset\ngeneralization. To address this challenge, we propose Motion-Primitive\nTransformer (MoPFormer), a novel self-supervised framework that enhances\ninterpretability by tokenizing inertial measurement unit signals into\nsemantically meaningful motion primitives and leverages a Transformer\narchitecture to learn rich temporal representations. MoPFormer comprises two\nstages. The first stage is to partition multi-channel sensor streams into short\nsegments and quantize them into discrete ``motion primitive'' codewords, while\nthe second stage enriches those tokenized sequences through a context-aware\nembedding module and then processes them with a Transformer encoder. The\nproposed MoPFormer can be pre-trained using a masked motion-modeling objective\nthat reconstructs missing primitives, enabling it to develop robust\nrepresentations across diverse sensor configurations. Experiments on six HAR\nbenchmarks demonstrate that MoPFormer not only outperforms state-of-the-art\nmethods but also successfully generalizes across multiple datasets. More\nimportantly, the learned motion primitives significantly enhance both\ninterpretability and cross-dataset performance by capturing fundamental\nmovement patterns that remain consistent across similar activities, regardless\nof dataset origin.\n","authors":["Hao Zhang","Zhan Zhuang","Xuehao Wang","Xiaodong Yang","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.20744v2.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24034v1","updated":"2025-10-28T03:32:14Z","published":"2025-10-28T03:32:14Z","title":"AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven\n  Adversarial Prompts","summary":"  Despite rapid advancements in text-to-image (T2I) models, their safety\nmechanisms are vulnerable to adversarial prompts, which maliciously generate\nunsafe images. Current red-teaming methods for proactively assessing such\nvulnerabilities usually require white-box access to T2I models, and rely on\ninefficient per-prompt optimization, as well as inevitably generate\nsemantically meaningless prompts easily blocked by filters. In this paper, we\npropose APT (AutoPrompT), a black-box framework that leverages large language\nmodels (LLMs) to automatically generate human-readable adversarial suffixes for\nbenign prompts. We first introduce an alternating optimization-finetuning\npipeline between adversarial suffix optimization and fine-tuning the LLM\nutilizing the optimized suffix. Furthermore, we integrates a dual-evasion\nstrategy in optimization phase, enabling the bypass of both perplexity-based\nfilter and blacklist word filter: (1) we constrain the LLM generating\nhuman-readable prompts through an auxiliary LLM perplexity scoring, which\nstarkly contrasts with prior token-level gibberish, and (2) we also introduce\nbanned-token penalties to suppress the explicit generation of banned-tokens in\nblacklist. Extensive experiments demonstrate the excellent red-teaming\nperformance of our human-readable, filter-resistant adversarial prompts, as\nwell as superior zero-shot transferability which enables instant adaptation to\nunseen prompts and exposes critical vulnerabilities even in commercial APIs\n(e.g., Leonardo.Ai.).\n","authors":["Yufan Liu","Wanqian Zhang","Huashan Chen","Lin Wang","Xiaojun Jia","Zheng Lin","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2510.24034v1.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2508.05186v3","updated":"2025-10-28T03:21:38Z","published":"2025-08-07T09:21:20Z","title":"Learning to See and Act: Task-Aware View Planning for Robotic\n  Manipulation","summary":"  Recent vision-language-action (VLA) models for multi-task robotic\nmanipulation commonly rely on static viewpoints and shared visual encoders,\nwhich limit 3D perception and cause task interference, hindering robustness and\ngeneralization. In this work, we propose Task-Aware View Planning (TAVP), a\nframework designed to overcome these challenges by integrating active view\nplanning with task-specific representation learning. TAVP employs an efficient\nexploration policy, accelerated by a novel pseudo-environment, to actively\nacquire informative views. Furthermore, we introduce a Mixture-of-Experts (MoE)\nvisual encoder to disentangle features across different tasks, boosting both\nrepresentation fidelity and task generalization. By learning to see the world\nin a task-aware way, TAVP generates more complete and discriminative visual\nrepresentations, demonstrating significantly enhanced action prediction across\na wide array of manipulation challenges. Extensive experiments on RLBench tasks\nshow that our proposed TAVP model achieves superior performance over\nstate-of-the-art fixed-view approaches. Visual results and code are provided\nat: https://hcplab-sysu.github.io/TAVP.\n","authors":["Yongjie Bai","Zhouxia Wang","Yang Liu","Weixing Chen","Ziliang Chen","Mingtong Dai","Yongsen Zheng","Lingbo Liu","Guanbin Li","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2508.05186v3.pdf","comment":"14 pages, 8 figures, project page: https://hcplab-sysu.github.io/TAVP"},{"id":"http://arxiv.org/abs/2412.02542v3","updated":"2025-10-28T03:07:50Z","published":"2024-12-03T16:34:49Z","title":"Unveiling Concept Attribution in Diffusion Models","summary":"  Diffusion models have shown remarkable abilities in generating realistic and\nhigh-quality images from text prompts. However, a trained model remains largely\nblack-box; little do we know about the roles of its components in exhibiting a\nconcept such as objects or styles. Recent works employ causal tracing to\nlocalize knowledge-storing layers in generative models without showing how\nother layers contribute to the target concept. In this work, we approach\ndiffusion models' interpretability problem from a more general perspective and\npose a question: \\textit{``How do model components work jointly to demonstrate\nknowledge?''}. To answer this question, we decompose diffusion models using\ncomponent attribution, systematically unveiling the importance of each\ncomponent (specifically the model parameter) in generating a concept. The\nproposed framework, called \\textbf{C}omponent \\textbf{A}ttribution for\n\\textbf{D}iffusion Model (CAD), discovers the localization of concept-inducing\n(positive) components, while interestingly uncovers another type of components\nthat contribute negatively to generating a concept, which is missing in the\nprevious knowledge localization work. Based on this holistic understanding of\ndiffusion models, we introduce two fast, inference-time model editing\nalgorithms, CAD-Erase and CAD-Amplify; in particular, CAD-Erase enables erasure\nand CAD-Amplify allows amplification of a generated concept by ablating the\npositive and negative components, respectively, while retaining knowledge of\nother concepts. Extensive experimental results validate the significance of\nboth positive and negative components pinpointed by our framework,\ndemonstrating the potential of providing a complete view of interpreting\ngenerative models. Our code is available\n\\href{https://github.com/mail-research/CAD-attribution4diffusion}{here}.\n","authors":["Quang H. Nguyen","Hoang Phan","Khoa D. Doan"],"pdf_url":"https://arxiv.org/pdf/2412.02542v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24024v1","updated":"2025-10-28T03:06:28Z","published":"2025-10-28T03:06:28Z","title":"Listening without Looking: Modality Bias in Audio-Visual Captioning","summary":"  Audio-visual captioning aims to generate holistic scene descriptions by\njointly modeling sound and vision. While recent methods have improved\nperformance through sophisticated modality fusion, it remains unclear to what\nextent the two modalities are complementary in current audio-visual captioning\nmodels and how robust these models are when one modality is degraded. We\naddress these questions by conducting systematic modality robustness tests on\nLAVCap, a state-of-the-art audio-visual captioning model, in which we\nselectively suppress or corrupt the audio or visual streams to quantify\nsensitivity and complementarity. The analysis reveals a pronounced bias toward\nthe audio stream in LAVCap. To evaluate how balanced audio-visual captioning\nmodels are in their use of both modalities, we augment AudioCaps with textual\nannotations that jointly describe the audio and visual streams, yielding the\nAudioVisualCaps dataset. In our experiments, we report LAVCap baseline results\non AudioVisualCaps. We also evaluate the model under modality robustness tests\non AudioVisualCaps and the results indicate that LAVCap trained on\nAudioVisualCaps exhibits less modality bias than when trained on AudioCaps.\n","authors":["Yuchi Ishikawa","Toranosuke Manabe","Tatsuya Komatsu","Yoshimitsu Aoki"],"pdf_url":"https://arxiv.org/pdf/2510.24024v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2510.22379v2","updated":"2025-10-28T03:06:09Z","published":"2025-10-25T17:48:46Z","title":"TraceTrans: Translation and Spatial Tracing for Surgical Prediction","summary":"  Image-to-image translation models have achieved notable success in converting\nimages across visual domains and are increasingly used for medical tasks such\nas predicting post-operative outcomes and modeling disease progression.\nHowever, most existing methods primarily aim to match the target distribution\nand often neglect spatial correspondences between the source and translated\nimages. This limitation can lead to structural inconsistencies and\nhallucinations, undermining the reliability and interpretability of the\npredictions. These challenges are accentuated in clinical applications by the\nstringent requirement for anatomical accuracy. In this work, we present\nTraceTrans, a novel deformable image translation model designed for\npost-operative prediction that generates images aligned with the target\ndistribution while explicitly revealing spatial correspondences with the\npre-operative input. The framework employs an encoder for feature extraction\nand dual decoders for predicting spatial deformations and synthesizing the\ntranslated image. The predicted deformation field imposes spatial constraints\non the generated output, ensuring anatomical consistency with the source.\nExtensive experiments on medical cosmetology and brain MRI datasets demonstrate\nthat TraceTrans delivers accurate and interpretable post-operative predictions,\nhighlighting its potential for reliable clinical deployment.\n","authors":["Xiyu Luo","Haodong Li","Xinxing Cheng","He Zhao","Yang Hu","Xuan Song","Tianyang Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.22379v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04897v3","updated":"2025-10-28T02:59:19Z","published":"2025-06-05T11:28:02Z","title":"From Objects to Anywhere: A Holistic Benchmark for Multi-level Visual\n  Grounding in 3D Scenes","summary":"  3D visual grounding has made notable progress in localizing objects within\ncomplex 3D scenes. However, grounding referring expressions beyond objects in\n3D scenes remains unexplored. In this paper, we introduce Anywhere3D-Bench, a\nholistic 3D visual grounding benchmark consisting of 2,886 referring\nexpression-3D bounding box pairs spanning four different grounding levels:\nhuman-activity areas, unoccupied space beyond objects, individual objects in\nthe scene, and fine-grained object parts. We assess a range of state-of-the-art\n3D visual grounding methods alongside large language models (LLMs) and\nmultimodal LLMs (MLLMs) on Anywhere3D-Bench. Experimental results reveal that\nspace-level and part-level visual grounding pose the greatest challenges:\nspace-level tasks require a more comprehensive spatial reasoning ability, for\nexample, modeling distances and spatial relations within 3D space, while\npart-level tasks demand fine-grained perception of object composition. Even the\nbest-performing models, Google Gemini-2.5-Pro and OpenAI o3, achieve just\naround 30% accuracy on space-level tasks and around 40% on part-level tasks,\nsignificantly lower than its performance on area-level and object-level tasks.\nThese findings underscore a critical gap in current models' capacity to\nunderstand and reason about 3D scenes beyond object-level semantics.\n","authors":["Tianxu Wang","Zhuofan Zhang","Ziyu Zhu","Yue Fan","Jing Xiong","Pengxiang Li","Xiaojian Ma","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2506.04897v3.pdf","comment":"Update v3 of the NeurIPS 2025 Datasets and Benchmarks paper (v2),\n  including additional evaluations of state-of-the-art multimodal large\n  language models. Project page: https://anywhere-3d.github.io/"},{"id":"http://arxiv.org/abs/2406.09782v3","updated":"2025-10-28T02:46:13Z","published":"2024-06-14T07:31:20Z","title":"Unsupervised Monocular Depth Estimation Based on Hierarchical\n  Feature-Guided Diffusion","summary":"  Unsupervised monocular depth estimation has received widespread attention\nbecause of its capability to train without ground truth. In real-world\nscenarios, the images may be blurry or noisy due to the influence of weather\nconditions and inherent limitations of the camera. Therefore, it is\nparticularly important to develop a robust depth estimation model. Benefiting\nfrom the training strategies of generative networks, generative-based methods\noften exhibit enhanced robustness. In light of this, we employ a\nwell-converging diffusion model among generative networks for unsupervised\nmonocular depth estimation. Additionally, we propose a hierarchical\nfeature-guided denoising module. This model significantly enriches the model's\ncapacity for learning and interpreting depth distribution by fully leveraging\nimage features to guide the denoising process. Furthermore, we explore the\nimplicit depth within reprojection and design an implicit depth consistency\nloss. This loss function serves to enhance the performance of the model and\nensure the scale consistency of depth within a video sequence. We conduct\nexperiments on the KITTI, Make3D, and our self-collected SIMIT datasets. The\nresults indicate that our approach stands out among generative-based models,\nwhile also showcasing remarkable robustness.\n","authors":["Runze Liu","Dongchen Zhu","Guanghui Zhang","Yue Xu","Wenjun Shi","Xiaolin Zhang","Lei Wang","Jiamao Li"],"pdf_url":"https://arxiv.org/pdf/2406.09782v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02787v2","updated":"2025-10-28T02:45:04Z","published":"2024-09-18T02:29:00Z","title":"Navigation with VLM framework: Towards Going to Any Language","summary":"  Navigating towards fully open language goals and exploring open scenes in an\nintelligent way have always raised significant challenges. Recently, Vision\nLanguage Models (VLMs) have demonstrated remarkable capabilities to reason with\nboth language and visual data. Although many works have focused on leveraging\nVLMs for navigation in open scenes, they often require high computational cost,\nrely on object-centric approaches, or depend on environmental priors in\ndetailed human instructions. We introduce Navigation with VLM (NavVLM), a\ntraining-free framework that harnesses open-source VLMs to enable robots to\nnavigate effectively, even for human-friendly language goal such as abstract\nplaces, actions, or specific objects in open scenes. NavVLM leverages the VLM\nas its cognitive core to perceive environmental information and constantly\nprovides exploration guidance achieving intelligent navigation with only a neat\ntarget rather than a detailed instruction with environment prior. We evaluated\nand validated NavVLM in both simulation and real-world experiments. In\nsimulation, our framework achieves state-of-the-art performance in Success\nweighted by Path Length (SPL) on object-specifc tasks in richly detailed\nenvironments from Matterport 3D (MP3D), Habitat Matterport 3D (HM3D) and\nGibson. With navigation episode reported, NavVLM demonstrates the capabilities\nto navigate towards any open-set languages. In real-world validation, we\nvalidated our framework's effectiveness in real-world robot at indoor scene.\n","authors":["Zecheng Yin","Chonghao Cheng","and Yao Guo","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2410.02787v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2510.24010v1","updated":"2025-10-28T02:34:08Z","published":"2025-10-28T02:34:08Z","title":"Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars\n  Science Tasks","summary":"  Foundation models have enabled rapid progress across many specialized domains\nby leveraging large-scale pre-training on unlabeled data, demonstrating strong\ngeneralization to a variety of downstream tasks. While such models have gained\nsignificant attention in fields like Earth Observation, their application to\nMars science remains limited. A key enabler of progress in other domains has\nbeen the availability of standardized benchmarks that support systematic\nevaluation. In contrast, Mars science lacks such benchmarks and standardized\nevaluation frameworks, which have limited progress toward developing foundation\nmodels for Martian tasks. To address this gap, we introduce Mars-Bench, the\nfirst benchmark designed to systematically evaluate models across a broad range\nof Mars-related tasks using both orbital and surface imagery. Mars-Bench\ncomprises 20 datasets spanning classification, segmentation, and object\ndetection, focused on key geologic features such as craters, cones, boulders,\nand frost. We provide standardized, ready-to-use datasets and baseline\nevaluations using models pre-trained on natural images, Earth satellite data,\nand state-of-the-art vision-language models. Results from all analyses suggest\nthat Mars-specific foundation models may offer advantages over general-domain\ncounterparts, motivating further exploration of domain-adapted pre-training.\nMars-Bench aims to establish a standardized foundation for developing and\ncomparing machine learning models for Mars science. Our data, models, and code\nare available at: https://mars-bench.github.io/.\n","authors":["Mirali Purohit","Bimal Gajera","Vatsal Malaviya","Irish Mehta","Kunal Kasodekar","Jacob Adler","Steven Lu","Umaa Rebbapragada","Hannah Kerner"],"pdf_url":"https://arxiv.org/pdf/2510.24010v1.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24009v1","updated":"2025-10-28T02:33:45Z","published":"2025-10-28T02:33:45Z","title":"Towards the Automatic Segmentation, Modeling and Meshing of the Aortic\n  Vessel Tree from Multicenter Acquisitions: An Overview of the SEG.A. 2023\n  Segmentation of the Aorta Challenge","summary":"  The automated analysis of the aortic vessel tree (AVT) from computed\ntomography angiography (CTA) holds immense clinical potential, but its\ndevelopment has been impeded by a lack of shared, high-quality data. We\nlaunched the SEG.A. challenge to catalyze progress in this field by introducing\na large, publicly available, multi-institutional dataset for AVT segmentation.\nThe challenge benchmarked automated algorithms on a hidden test set, with\nsubsequent optional tasks in surface meshing for computational simulations. Our\nfindings reveal a clear convergence on deep learning methodologies, with 3D\nU-Net architectures dominating the top submissions. A key result was that an\nensemble of the highest-ranking algorithms significantly outperformed\nindividual models, highlighting the benefits of model fusion. Performance was\nstrongly linked to algorithmic design, particularly the use of customized\npost-processing steps, and the characteristics of the training data. This\ninitiative not only establishes a new performance benchmark but also provides a\nlasting resource to drive future innovation toward robust, clinically\ntranslatable tools.\n","authors":["Yuan Jin","Antonio Pepe","Gian Marco Melito","Yuxuan Chen","Yunsu Byeon","Hyeseong Kim","Kyungwon Kim","Doohyun Park","Euijoon Choi","Dosik Hwang","Andriy Myronenko","Dong Yang","Yufan He","Daguang Xu","Ayman El-Ghotni","Mohamed Nabil","Hossam El-Kady","Ahmed Ayyad","Amr Nasr","Marek Wodzinski","Henning MÃ¼ller","Hyeongyu Kim","Yejee Shin","Abbas Khan","Muhammad Asad","Alexander Zolotarev","Caroline Roney","Anthony Mathur","Martin Benning","Gregory Slabaugh","Theodoros Panagiotis Vagenas","Konstantinos Georgas","George K. Matsopoulos","Jihan Zhang","Zhen Zhang","Liqin Huang","Christian Mayer","Heinrich MÃ¤chler","Jan Egger"],"pdf_url":"https://arxiv.org/pdf/2510.24009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00034v2","updated":"2025-10-28T02:15:21Z","published":"2025-05-27T01:43:02Z","title":"GaussianFusion: Gaussian-Based Multi-Sensor Fusion for End-to-End\n  Autonomous Driving","summary":"  Multi-sensor fusion is crucial for improving the performance and robustness\nof end-to-end autonomous driving systems. Existing methods predominantly adopt\neither attention-based flatten fusion or bird's eye view fusion through\ngeometric transformations. However, these approaches often suffer from limited\ninterpretability or dense computational overhead. In this paper, we introduce\nGaussianFusion, a Gaussian-based multi-sensor fusion framework for end-to-end\nautonomous driving. Our method employs intuitive and compact Gaussian\nrepresentations as intermediate carriers to aggregate information from diverse\nsensors. Specifically, we initialize a set of 2D Gaussians uniformly across the\ndriving scene, where each Gaussian is parameterized by physical attributes and\nequipped with explicit and implicit features. These Gaussians are progressively\nrefined by integrating multi-modal features. The explicit features capture rich\nsemantic and spatial information about the traffic scene, while the implicit\nfeatures provide complementary cues beneficial for trajectory planning. To\nfully exploit rich spatial and semantic information in Gaussians, we design a\ncascade planning head that iteratively refines trajectory predictions through\ninteractions with Gaussians. Extensive experiments on the NAVSIM and\nBench2Drive benchmarks demonstrate the effectiveness and robustness of the\nproposed GaussianFusion framework. The source code will be released at\nhttps://github.com/Say2L/GaussianFusion.\n","authors":["Shuai Liu","Quanmin Liang","Zefeng Li","Boyang Li","Kai Huang"],"pdf_url":"https://arxiv.org/pdf/2506.00034v2.pdf","comment":"Accepted at NeurIPS2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2510.24000v1","updated":"2025-10-28T02:10:54Z","published":"2025-10-28T02:10:54Z","title":"AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification\n  and Cross-Domain Generalization","summary":"  Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet\nearly and accurate detection can significantly improve treatment outcomes.\nWhile numerous Deep learning (DL) models have been developed to predict DR from\nfundus images, many face challenges in maintaining robustness due to\ndistributional variations caused by differences in acquisition devices,\ndemographic disparities, and imaging conditions. This paper addresses this\ncritical limitation by proposing a novel DR classification approach, a method\ncalled AdvBlur. Our method integrates adversarial blurred images into the\ndataset and employs a dual-loss function framework to address domain\ngeneralization. This approach effectively mitigates the impact of unseen\ndistributional variations, as evidenced by comprehensive evaluations across\nmultiple datasets. Additionally, we conduct extensive experiments to explore\nthe effects of factors such as camera type, low-quality images, and dataset\nsize. Furthermore, we perform ablation studies on blurred images and the loss\nfunction to ensure the validity of our choices. The experimental results\ndemonstrate the effectiveness of our proposed method, achieving competitive\nperformance compared to state-of-the-art domain generalization DR models on\nunseen external datasets.\n","authors":["Heethanjan Kanagalingam","Thenukan Pathmanathan","Mokeeshan Vathanakumar","Tharmakulasingam Mukunthan"],"pdf_url":"https://arxiv.org/pdf/2510.24000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23594v2","updated":"2025-10-28T02:07:50Z","published":"2025-10-27T17:57:52Z","title":"PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error\n  Detection","summary":"  Multimodal large language models (MLLMs) have achieved remarkable progress on\nvision-language tasks, yet their reasoning processes remain sometimes\nunreliable. We introduce PRISM-Bench, a benchmark of puzzle-based visual\nchallenges designed to evaluate not only whether models can solve problems, but\nhow their reasoning unfolds. Unlike prior evaluations that measure only\nfinal-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual\npuzzle and a step-by-step chain-of-thought (CoT) containing exactly one error,\nmodels must identify the first incorrect step. This setting enables\nfine-grained assessment of logical consistency, error detection, and visual\nreasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric,\nand analogical reasoning, resisting shortcuts based on superficial pattern\nmatching. Evaluations across state-of-the-art MLLMs reveal a persistent gap\nbetween fluent generation and faithful reasoning: models that produce plausible\nCoTs often fail to locate simple logical faults. By disentangling answer\ngeneration from reasoning verification, PRISM-Bench offers a sharper lens on\nmultimodal reasoning competence and underscores the need for diagnostic\nevaluation protocols in the development of trustworthy MLLMs.\n","authors":["Yusu Qian","Cheng Wan","Chao Jia","Yinfei Yang","Qingyu Zhao","Zhe Gan"],"pdf_url":"https://arxiv.org/pdf/2510.23594v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.18443v2","updated":"2025-10-28T01:59:34Z","published":"2025-06-23T09:27:22Z","title":"Radar and Event Camera Fusion for Agile Robot Ego-Motion Estimation","summary":"  Achieving reliable ego motion estimation for agile robots, e.g., aerobatic\naircraft, remains challenging because most robot sensors fail to respond timely\nand clearly to highly dynamic robot motions, often resulting in measurement\nblurring, distortion, and delays. In this paper, we propose an IMU-free and\nfeature-association-free framework to achieve aggressive ego-motion velocity\nestimation of a robot platform in highly dynamic scenarios by combining two\ntypes of exteroceptive sensors, an event camera and a millimeter wave radar,\nFirst, we used instantaneous raw events and Doppler measurements to derive\nrotational and translational velocities directly. Without a sophisticated\nassociation process between measurement frames, the proposed method is more\nrobust in texture-less and structureless environments and is more\ncomputationally efficient for edge computing devices. Then, in the back-end, we\npropose a continuous-time state-space model to fuse the hybrid time-based and\nevent-based measurements to estimate the ego-motion velocity in a fixed-lagged\nsmoother fashion. In the end, we validate our velometer framework extensively\nin self-collected experiment datasets. The results indicate that our IMU-free\nand association-free ego motion estimation framework can achieve reliable and\nefficient velocity output in challenging environments. The source code,\nillustrative video and dataset are available at\nhttps://github.com/ZzhYgwh/TwistEstimator.\n","authors":["Yang Lyu","Zhenghao Zou","Yanfeng Li","Xiaohu Guo","Chunhui Zhao","Quan Pan"],"pdf_url":"https://arxiv.org/pdf/2506.18443v2.pdf","comment":"2025.10.28 version v2 for TwistEstimator"},{"id":"http://arxiv.org/abs/2503.23502v3","updated":"2025-10-28T01:42:48Z","published":"2025-03-30T16:24:22Z","title":"Boosting Omnidirectional Stereo Matching with a Pre-trained Depth\n  Foundation Model","summary":"  Omnidirectional depth perception is essential for mobile robotics\napplications that require scene understanding across a full 360{\\deg} field of\nview. Camera-based setups offer a cost-effective option by using stereo depth\nestimation to generate dense, high-resolution depth maps without relying on\nexpensive active sensing. However, existing omnidirectional stereo matching\napproaches achieve only limited depth accuracy across diverse environments,\ndepth ranges, and lighting conditions, due to the scarcity of real-world data.\nWe present DFI-OmniStereo, a novel omnidirectional stereo matching method that\nleverages a large-scale pre-trained foundation model for relative monocular\ndepth estimation within an iterative optimization-based stereo matching\narchitecture. We introduce a dedicated two-stage training strategy to utilize\nthe relative monocular depth features for our omnidirectional stereo matching\nbefore scale-invariant fine-tuning. DFI-OmniStereo achieves state-of-the-art\nresults on the real-world Helvipad dataset, reducing disparity MAE by\napproximately 16% compared to the previous best omnidirectional stereo method.\n","authors":["Jannik Endres","Oliver Hahn","Charles CorbiÃ¨re","Simone Schaub-Meyer","Stefan Roth","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2503.23502v3.pdf","comment":"Accepted at IROS 2025. Project page:\n  https://vita-epfl.github.io/DFI-OmniStereo-website/"},{"id":"http://arxiv.org/abs/2503.04852v2","updated":"2025-10-28T01:41:35Z","published":"2025-03-06T03:40:01Z","title":"CAUSAL3D: A Comprehensive Benchmark for Causal Learning from Visual Data","summary":"  True intelligence hinges on the ability to uncover and leverage hidden causal\nrelations. Despite significant progress in AI and computer vision (CV), there\nremains a lack of benchmarks for assessing models' abilities to infer latent\ncausality from complex visual data. In this paper, we introduce\n\\textsc{\\textbf{Causal3D}}, a novel and comprehensive benchmark that integrates\nstructured data (tables) with corresponding visual representations (images) to\nevaluate causal reasoning. Designed within a systematic framework, Causal3D\ncomprises 19 3D-scene datasets capturing diverse causal relations, views, and\nbackgrounds, enabling evaluations across scenes of varying complexity. We\nassess multiple state-of-the-art methods, including classical causal discovery,\ncausal representation learning, and large/vision-language models (LLMs/VLMs).\nOur experiments show that as causal structures grow more complex without prior\nknowledge, performance declines significantly, highlighting the challenges even\nadvanced methods face in complex causal scenarios. Causal3D serves as a vital\nresource for advancing causal reasoning in CV and fostering trustworthy AI in\ncritical domains.\n","authors":["Disheng Liu","Yiran Qiao","Wuche Liu","Yiren Lu","Yunlai Zhou","Tuo Liang","Yu Yin","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2503.04852v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23981v1","updated":"2025-10-28T01:24:24Z","published":"2025-10-28T01:24:24Z","title":"TeleEgo: Benchmarking Egocentric AI Assistants in the Wild","summary":"  Egocentric AI assistants in real-world settings must process multi-modal\ninputs (video, audio, text), respond in real time, and retain evolving\nlong-term memory. However, existing benchmarks typically evaluate these\nabilities in isolation, lack realistic streaming scenarios, or support only\nshort-term tasks. We introduce \\textbf{TeleEgo}, a long-duration, streaming,\nomni-modal benchmark for evaluating egocentric AI assistants in realistic daily\ncontexts. The dataset features over 14 hours per participant of synchronized\negocentric video, audio, and text across four domains: work \\& study, lifestyle\n\\& routines, social activities, and outings \\& culture. All data is aligned on\na unified global timeline and includes high-quality visual narrations and\nspeech transcripts, curated through human refinement.TeleEgo defines 12\ndiagnostic subtasks across three core capabilities: Memory (recalling past\nevents), Understanding (interpreting the current moment), and Cross-Memory\nReasoning (linking distant events). It contains 3,291 human-verified QA items\nspanning multiple question formats (single-choice, binary, multi-choice, and\nopen-ended), evaluated strictly in a streaming setting. We propose two key\nmetrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess\ncorrectness, temporal responsiveness, and long-term retention. TeleEgo provides\na realistic and comprehensive evaluation to advance the development of\npractical AI assistants.\n","authors":["Jiaqi Yan","Ruilong Ren","Jingren Liu","Shuning Xu","Ling Wang","Yiheng Wang","Yun Wang","Long Zhang","Xiangyu Chen","Changzhi Sun","Jixiang Luo","Dell Zhang","Hao Sun","Chi Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2510.23981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23978v1","updated":"2025-10-28T01:19:54Z","published":"2025-10-28T01:19:54Z","title":"Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution\n  with Fourier Constraints","summary":"  Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution is\ncrucial. Existing methods predict Fourier components one by one using a\nrecurrent neural network. However, this approach leads to performance\ndegradation and inefficiency due to independent prediction. This paper proposes\npredicting multiple components jointly to improve both quality and efficiency.\n","authors":["Kazutoshi Akita","Norimichi Ukita"],"pdf_url":"https://arxiv.org/pdf/2510.23978v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2510.23977v1","updated":"2025-10-28T01:18:00Z","published":"2025-10-28T01:18:00Z","title":"Synergistic Neural Forecasting of Air Pollution with Stochastic Sampling","summary":"  Air pollution remains a leading global health and environmental risk,\nparticularly in regions vulnerable to episodic air pollution spikes due to\nwildfires, urban haze and dust storms. Accurate forecasting of particulate\nmatter (PM) concentrations is essential to enable timely public health warnings\nand interventions, yet existing models often underestimate rare but hazardous\npollution events. Here, we present SynCast, a high-resolution neural\nforecasting model that integrates meteorological and air composition data to\nimprove predictions of both average and extreme pollution levels. Built on a\nregionally adapted transformer backbone and enhanced with a diffusion-based\nstochastic refinement module, SynCast captures the nonlinear dynamics driving\nPM spikes more accurately than existing approaches. Leveraging on harmonized\nERA5 and CAMS datasets, our model shows substantial gains in forecasting\nfidelity across multiple PM variables (PM$_1$, PM$_{2.5}$, PM$_{10}$),\nespecially under extreme conditions. We demonstrate that conventional loss\nfunctions underrepresent distributional tails (rare pollution events) and show\nthat SynCast, guided by domain-aware objectives and extreme value theory,\nsignificantly enhances performance in highly impacted regions without\ncompromising global accuracy. This approach provides a scalable foundation for\nnext-generation air quality early warning systems and supports climate-health\nrisk mitigation in vulnerable regions.\n","authors":["Yohan Abeysinghe","Muhammad Akhtar Munir","Sanoojan Baliah","Ron Sarafian","Fahad Shahbaz Khan","Yinon Rudich","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2510.23977v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22118v2","updated":"2025-10-28T00:53:28Z","published":"2025-10-25T02:07:23Z","title":"GRAID: Enhancing Spatial Reasoning of VLMs Through High-Fidelity Data\n  Generation","summary":"  Vision Language Models (VLMs) achieve strong performance on many\nvision-language tasks but often struggle with spatial\nreasoning$\\unicode{x2014}$a prerequisite for many applications. Empirically, we\nfind that a dataset produced by a current training data generation pipeline has\na 57.6% human validation rate. These rates stem from current limitations:\nsingle-image 3D reconstruction introduces cascading modeling errors and\nrequires wide answer tolerances, while caption-based methods require\nhyper-detailed annotations and suffer from generative hallucinations. We\npresent GRAID, built on the key insight that qualitative spatial relationships\ncan be reliably determined from 2D geometric primitives alone. By operating\nexclusively on 2D bounding boxes from standard object detectors, GRAID avoids\nboth 3D reconstruction errors and generative hallucinations, resulting in\ndatasets that are of higher quality than existing tools that produce similar\ndatasets as validated by human evaluations. We apply our framework to the\nBDD100k, NuImages, and Waymo datasets, generating over 8.5 million high-quality\nVQA pairs creating questions spanning spatial relations, counting, ranking, and\nsize comparisons. We evaluate one of the datasets and find it achieves 91.16%\nhuman-validated accuracy$\\unicode{x2014}$compared to 57.6% on a dataset\ngenerated by recent work. Critically, we demonstrate that when trained on GRAID\ndata, models learn spatial reasoning concepts that generalize: models\nfine-tuned on 6 question types improve on over 10 held-out types, with accuracy\ngains of 47.5% on BDD and 37.9% on NuImages for Llama 3.2B 11B, and when\ntrained on all questions types, achieve improvements on several existing\nbenchmarks such as BLINK. The GRAID framework, datasets, and additional\ninformation can be found $\\href{this https URL}{here}$.\n","authors":["Karim Elmaaroufi","Liheng Lai","Justin Svegliato","Yutong Bai","Sanjit A. Seshia","Matei Zaharia"],"pdf_url":"https://arxiv.org/pdf/2510.22118v2.pdf","comment":"22 pages, 3 figures, 3 tables, project page:\n  https://ke7.github.io/graid/"},{"id":"http://arxiv.org/abs/2510.23968v1","updated":"2025-10-28T00:48:00Z","published":"2025-10-28T00:48:00Z","title":"Reasoning Visual Language Model for Chest X-Ray Analysis","summary":"  Vision-language models (VLMs) have shown strong promise for medical image\nanalysis, but most remain opaque, offering predictions without the transparent,\nstepwise reasoning clinicians rely on. We present a framework that brings\nchain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by\nreasoning-first training paradigms, our approach is designed to learn how\nexperts reason, not just what they conclude, by aligning intermediate steps\nwith observable image evidence and radiology workflow. Beyond accuracy, the\nexplicit reasoning traces support clinical auditability: they reveal why a\nconclusion was reached, which alternatives were considered, and where\nuncertainty remains, enabling quality assurance, error analysis, and safer\nhuman-AI collaboration.\n  Our model couples high-fidelity visual encoding with a two-stage training\nrecipe: a reasoning-style supervised fine-tuning (SFT) followed by\nreinforcement learning (RL) that uses verifiable rewards over a list of X-ray\nabnormalities. The model outputs reasoning that mirrors radiologists systematic\nthought process, uncertainty, and differential diagnosis. In\nout-of-distribution evaluation, the approach achieves competitive multi-label\nclassification while improving interpretability. In a reader study with expert\nradiologists, full reasoning traces increased confidence, supported error\nauditing, and reduced time to finalize reports. We release code and the model\nNV-Reason-CXR-3B to support community progress toward trustworthy, explainable\nAI in chest radiography and other medical imaging tasks where reasoning quality\nis as critical as prediction quality.\n","authors":["Andriy Myronenko","Dong Yang","Baris Turkbey","Mariam Aboian","Sena Azamat","Esra Akcicek","Hongxu Yin","Pavlo Molchanov","Marc Edgar","Yufan He","Pengfei Guo","Yucheng Tang","Daguang Xu"],"pdf_url":"https://arxiv.org/pdf/2510.23968v1.pdf","comment":"NV-Reason-CXR-3B"},{"id":"http://arxiv.org/abs/2510.23960v1","updated":"2025-10-28T00:35:59Z","published":"2025-10-28T00:35:59Z","title":"SafeVision: Efficient Image Guardrail with Robust Policy Adherence and\n  Explainability","summary":"  With the rapid proliferation of digital media, the need for efficient and\ntransparent safeguards against unsafe content is more critical than ever.\nTraditional image guardrail models, constrained by predefined categories, often\nmisclassify content due to their pure feature-based learning without semantic\nreasoning. Moreover, these models struggle to adapt to emerging threats,\nrequiring costly retraining for new threats. To address these limitations, we\nintroduce SafeVision, a novel image guardrail that integrates human-like\nreasoning to enhance adaptability and transparency. Our approach incorporates\nan effective data collection and generation framework, a policy-following\ntraining pipeline, and a customized loss function. We also propose a diverse QA\ngeneration and training strategy to enhance learning effectiveness. SafeVision\ndynamically aligns with evolving safety policies at inference time, eliminating\nthe need for retraining while ensuring precise risk assessments and\nexplanations. Recognizing the limitations of existing unsafe image benchmarks,\nwhich either lack granularity or cover limited risks, we introduce VisionHarm,\na high-quality dataset comprising two subsets: VisionHarm Third-party\n(VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse\nharmful categories. Through extensive experiments, we show that SafeVision\nachieves state-of-the-art performance on different benchmarks. SafeVision\noutperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while\nbeing over 16x faster. SafeVision sets a comprehensive, policy-following, and\nexplainable image guardrail with dynamic adaptation to emerging threats.\n","authors":["Peiyang Xu","Minzhou Pan","Zhaorun Chen","Shuang Yang","Chaowei Xiao","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2510.23960v1.pdf","comment":"42 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.23956v1","updated":"2025-10-28T00:19:42Z","published":"2025-10-28T00:19:42Z","title":"Neural USD: An object-centric framework for iterative editing and\n  control","summary":"  Amazing progress has been made in controllable generative modeling,\nespecially over the last few years. However, some challenges remain. One of\nthem is precise and iterative object editing. In many of the current methods,\ntrying to edit the generated image (for example, changing the color of a\nparticular object in the scene or changing the background while keeping other\nelements unchanged) by changing the conditioning signals often leads to\nunintended global changes in the scene. In this work, we take the first steps\nto address the above challenges. Taking inspiration from the Universal Scene\nDescriptor (USD) standard developed in the computer graphics community, we\nintroduce the \"Neural Universal Scene Descriptor\" or Neural USD. In this\nframework, we represent scenes and objects in a structured, hierarchical\nmanner. This accommodates diverse signals, minimizes model-specific\nconstraints, and enables per-object control over appearance, geometry, and\npose. We further apply a fine-tuning approach which ensures that the above\ncontrol signals are disentangled from one another. We evaluate several design\nconsiderations for our framework, demonstrating how Neural USD enables\niterative and incremental workflows. More information at:\nhttps://escontrela.me/neural_usd .\n","authors":["Alejandro Escontrela","Shrinu Kushagra","Sjoerd van Steenkiste","Yulia Rubanova","Aleksander Holynski","Kelsey Allen","Kevin Murphy","Thomas Kipf"],"pdf_url":"https://arxiv.org/pdf/2510.23956v1.pdf","comment":"22 pages, 16 figures, 1 table"},{"id":"http://arxiv.org/abs/2510.03786v2","updated":"2025-10-28T23:43:17Z","published":"2025-10-04T11:25:10Z","title":"MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based\n  Fusion for Medical Image Segmentation","summary":"  In recent years, deep learning has shown near-expert performance in\nsegmenting complex medical tissues and tumors. However, existing models are\noften task-specific, with performance varying across modalities and anatomical\nregions. Balancing model complexity and performance remains challenging,\nparticularly in clinical settings where both accuracy and efficiency are\ncritical. To address these issues, we propose a hybrid segmentation\narchitecture featuring a three-branch encoder that integrates CNNs,\nTransformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture\nlocal, global, and long-range dependencies. A multi-scale attention-based CNN\ndecoder reconstructs fine-grained segmentation maps while preserving contextual\nconsistency. Additionally, a co-attention gate enhances feature selection by\nemphasizing relevant spatial and semantic information across scales during both\nencoding and decoding, improving feature interaction and cross-scale\ncommunication. Extensive experiments on multiple benchmark datasets show that\nour approach outperforms state-of-the-art methods in accuracy and\ngeneralization, while maintaining comparable computational complexity. By\neffectively balancing efficiency and effectiveness, our architecture offers a\npractical and scalable solution for diverse medical imaging tasks. Source code\nand trained models will be publicly released upon acceptance to support\nreproducibility and further research.\n","authors":["T-Mai Bui","Fares Bougourzi","Fadi Dornaika","Vinh Truong Hoang"],"pdf_url":"https://arxiv.org/pdf/2510.03786v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25032v1","updated":"2025-10-28T23:21:00Z","published":"2025-10-28T23:21:00Z","title":"Efficient License Plate Recognition via Pseudo-Labeled Supervision with\n  Grounding DINO and YOLOv8","summary":"  Developing a highly accurate automatic license plate recognition system\n(ALPR) is challenging due to environmental factors such as lighting, rain, and\ndust. Additional difficulties include high vehicle speeds, varying camera\nangles, and low-quality or low-resolution images. ALPR is vital in traffic\ncontrol, parking, vehicle tracking, toll collection, and law enforcement\napplications. This paper proposes a deep learning strategy using YOLOv8 for\nlicense plate detection and recognition tasks. This method seeks to enhance the\nperformance of the model using datasets from Ontario, Quebec, California, and\nNew York State. It achieved an impressive recall rate of 94% on the dataset\nfrom the Center for Pattern Recognition and Machine Intelligence (CENPARMI) and\n91% on the UFPR-ALPR dataset. In addition, our method follows a semi-supervised\nlearning framework, combining a small set of manually labeled data with\npseudo-labels generated by Grounding DINO to train our detection model.\nGrounding DINO, a powerful vision-language model, automatically annotates many\nimages with bounding boxes for license plates, thereby minimizing the reliance\non labor-intensive manual labeling. By integrating human-verified and\nmodel-generated annotations, we can scale our dataset efficiently while\nmaintaining label quality, which significantly enhances the training process\nand overall model performance. Furthermore, it reports character error rates\nfor both datasets, providing additional insight into system performance.\n","authors":["Zahra Ebrahimi Vargoorani","Amir Mohammad Ghoreyshi","Ching Yee Suen"],"pdf_url":"https://arxiv.org/pdf/2510.25032v1.pdf","comment":"6 pages, 8 figures. Presented at 2025 IEEE International Workshop on\n  Machine Learning for Signal Processing (MLSP), August 31 - September 3, 2025,\n  Istanbul, Turkey"},{"id":"http://arxiv.org/abs/2412.08619v3","updated":"2025-10-28T22:43:29Z","published":"2024-12-11T18:40:16Z","title":"Physics Context Builders: A Modular Framework for Physical Reasoning in\n  Vision-Language Models","summary":"  Physical reasoning remains a significant challenge for Vision-Language Models\n(VLMs). This limitation arises from an inability to translate learned knowledge\ninto predictions about physical behavior. Although continual fine-tuning can\nmitigate this issue, it is expensive for large models and impractical to\nperform repeatedly for every task. This necessitates the creation of modular\nand scalable ways to teach VLMs about physical reasoning. To that end, we\nintroduce Physics Context Builders (PCBs), a modular framework where\nspecialized smaller VLMs are fine-tuned to generate detailed physical scene\ndescriptions. These can be used as physical contexts to enhance the reasoning\ncapabilities of larger VLMs. PCBs enable the separation of visual perception\nfrom reasoning, allowing us to analyze their relative contributions to physical\nunderstanding. We perform experiments on CLEVRER and on Falling Tower, a\nstability detection dataset with both simulated and real-world scenes, to\ndemonstrate that PCBs provide substantial performance improvements, increasing\naverage accuracy by up to 13.8% on complex physical reasoning tasks. Notably,\nPCBs also show strong Sim2Real transfer, successfully generalizing from\nsimulated training data to real-world scenes.\n","authors":["Vahid Balazadeh","Mohammadmehdi Ataei","Hyunmin Cheong","Amir Hosein Khasahmadi","Rahul G. Krishnan"],"pdf_url":"https://arxiv.org/pdf/2412.08619v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.10266v2","updated":"2025-10-28T22:32:18Z","published":"2025-09-12T14:08:06Z","title":"SignMouth: Leveraging Mouthing Cues for Sign Language Translation by\n  Multimodal Contrastive Fusion","summary":"  Sign language translation (SLT) aims to translate natural language from sign\nlanguage videos, serving as a vital bridge for inclusive communication. While\nrecent advances leverage powerful visual backbones and large language models,\nmost approaches mainly focus on manual signals (hand gestures) and tend to\noverlook non-manual cues like mouthing. In fact, mouthing conveys essential\nlinguistic information in sign languages and plays a crucial role in\ndisambiguating visually similar signs. In this paper, we propose SignClip, a\nnovel framework to improve the accuracy of sign language translation. It fuses\nmanual and non-manual cues, specifically spatial gesture and lip movement\nfeatures. Besides, SignClip introduces a hierarchical contrastive learning\nframework with multi-level alignment objectives, ensuring semantic consistency\nacross sign-lip and visual-text modalities. Extensive experiments on two\nbenchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our\napproach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip\nsurpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from\n24.32 to 24.71, and ROUGE from 46.57 to 48.38.\n","authors":["Wenfang Wu","Tingting Yuan","Yupeng Li","Daling Wang","Xiaoming Fu"],"pdf_url":"https://arxiv.org/pdf/2509.10266v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25002v1","updated":"2025-10-28T22:02:36Z","published":"2025-10-28T22:02:36Z","title":"Resi-VidTok: An Efficient and Decomposed Progressive Tokenization\n  Framework for Ultra-Low-Rate and Lightweight Video Transmission","summary":"  Real-time transmission of video over wireless networks remains highly\nchallenging, even with advanced deep models, particularly under severe channel\nconditions such as limited bandwidth and weak connectivity. In this paper, we\npropose Resi-VidTok, a Resilient Tokenization-Enabled framework designed for\nultra-low-rate and lightweight video transmission that delivers strong\nrobustness while preserving perceptual and semantic fidelity on commodity\ndigital hardware. By reorganizing spatio--temporal content into a discrete,\nimportance-ordered token stream composed of key tokens and refinement tokens,\nResi-VidTok enables progressive encoding, prefix-decodable reconstruction, and\ngraceful quality degradation under constrained channels. A key contribution is\na resilient 1D tokenization pipeline for video that integrates differential\ntemporal token coding, explicitly supporting reliable recovery from incomplete\ntoken sets using a single shared framewise decoder--without auxiliary temporal\nextractors or heavy generative models. Furthermore, stride-controlled frame\nsparsification combined with a lightweight decoder-side interpolator reduces\ntransmission load while maintaining motion continuity. Finally, a\nchannel-adaptive source--channel coding and modulation scheme dynamically\nallocates rate and protection according to token importance and channel\ncondition, yielding stable quality across adverse SNRs. Evaluation results\nindicate robust visual and semantic consistency at channel bandwidth ratios\n(CBR) as low as 0.0004 and real-time reconstruction at over 30 fps,\ndemonstrating the practicality of Resi-VidTok for energy-efficient,\nlatency-sensitive, and reliability-critical wireless applications.\n","authors":["Zhenyu Liu","Yi Ma","Rahim Tafazolli","Zhi Ding"],"pdf_url":"https://arxiv.org/pdf/2510.25002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.22918v4","updated":"2025-10-28T21:55:57Z","published":"2025-05-28T22:39:12Z","title":"Re-ttention: Ultra Sparse Visual Generation via Attention Statistical\n  Reshape","summary":"  Diffusion Transformers (DiT) have become the de-facto model for generating\nhigh-quality visual content like videos and images. A huge bottleneck is the\nattention mechanism where complexity scales quadratically with resolution and\nvideo length. One logical way to lessen this burden is sparse attention, where\nonly a subset of tokens or patches are included in the calculation. However,\nexisting techniques fail to preserve visual quality at extremely high sparsity\nlevels and might even incur non-negligible compute overheads. To address this\nconcern, we propose Re-ttention, which implements very high sparse attention\nfor visual generation models by leveraging the temporal redundancy of Diffusion\nModels to overcome the probabilistic normalization shift within the attention\nmechanism. Specifically, Re-ttention reshapes attention scores based on the\nprior softmax distribution history in order to preserve the visual quality of\nthe full quadratic attention at very high sparsity levels. Experimental results\non T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate that\nRe-ttention requires as few as 3.1% of the tokens during inference,\noutperforming contemporary methods like FastDiTAttn, Sparse VideoGen and\nMInference.\n","authors":["Ruichen Chen","Keith G. Mills","Liyao Jiang","Chao Gao","Di Niu"],"pdf_url":"https://arxiv.org/pdf/2505.22918v4.pdf","comment":"author comment: This version was previously removed by arXiv\n  administrators as the submitter did not have the rights to agree to the\n  license at the time of submission. The authors have now obtained the\n  necessary permissions, and the paper is resubmitted accordingly"},{"id":"http://arxiv.org/abs/2510.24980v1","updated":"2025-10-28T21:23:32Z","published":"2025-10-28T21:23:32Z","title":"FT-ARM: Fine-Tuned Agentic Reflection Multimodal Language Model for\n  Pressure Ulcer Severity Classification with Reasoning","summary":"  Pressure ulcers (PUs) are a serious and prevalent healthcare concern.\nAccurate classification of PU severity (Stages I-IV) is essential for proper\ntreatment but remains challenging due to subtle visual distinctions and\nsubjective interpretation, leading to variability among clinicians. Prior\nAI-based approaches using Convolutional Neural Networks (CNNs) and Vision\nTransformers (ViTs) achieved promising accuracy but offered limited\ninterpretability. We present FT-ARM (Fine-Tuned Agentic Reflection Multimodal\nmodel), a fine-tuned multimodal large language model (MLLM) with an agentic\nself-reflection mechanism for pressure ulcer severity classification. Inspired\nby clinician-style diagnostic reassessment, FT-ARM iteratively refines its\npredictions by reasoning over visual features and encoded clinical knowledge\nfrom text, enhancing both accuracy and consistency. On the publicly available\nPressure Injury Image Dataset (PIID), FT-ARM, fine-tuned from LLaMA 3.2 90B,\nachieved 85% accuracy in classifying PU stages I-IV, surpassing prior CNN-based\nmodels by +4%. Unlike earlier CNN/ViT studies that relied solely on offline\nevaluations, FT-ARM is designed and tested for live inference, reflecting\nreal-time deployment conditions. Furthermore, it produces clinically grounded\nnatural-language explanations, improving interpretability and trust. By\nintegrating fine-tuning and reflective reasoning across multimodal inputs,\nFT-ARM advances the reliability, transparency, and clinical applicability of\nautomated wound assessment systems, addressing the critical need for consistent\nand explainable PU staging to support improved patient care.\n","authors":["Reza Saadati Fard","Emmanuel Agu","Palawat Busaranuvong","Deepak Kumar","Shefalika Gautam","Bengisu Tulu","Diane Strong","Lorraine Loretz"],"pdf_url":"https://arxiv.org/pdf/2510.24980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09066v5","updated":"2025-10-28T21:13:11Z","published":"2024-03-14T03:13:01Z","title":"Hyperparameters in Continual Learning: A Reality Check","summary":"  Continual learning (CL) aims to train a model on a sequence of tasks (i.e., a\nCL scenario) while balancing the trade-off between plasticity (learning new\ntasks) and stability (retaining prior knowledge). The dominantly adopted\nconventional evaluation protocol for CL algorithms selects the best\nhyperparameters (e.g., learning rate, mini-batch size, regularization\nstrengths, etc.) within a given scenario and then evaluates the algorithms\nusing these hyperparameters in the same scenario. However, this protocol has\nsignificant shortcomings: it overestimates the CL capacity of algorithms and\nrelies on unrealistic hyperparameter tuning, which is not feasible for\nreal-world applications. From the fundamental principles of evaluation in\nmachine learning, we argue that the evaluation of CL algorithms should focus on\nassessing the generalizability of their CL capacity to unseen scenarios. Based\non this, we propose the Generalizable Two-phase Evaluation Protocol (GTEP)\nconsisting of hyperparameter tuning and evaluation phases. Both phases share\nthe same scenario configuration (e.g., number of tasks) but are generated from\ndifferent datasets. Hyperparameters of CL algorithms are tuned in the first\nphase and applied in the second phase to evaluate the algorithms. We apply this\nprotocol to class-incremental learning, both with and without pretrained\nmodels. Across more than 8,000 experiments, our results show that most\nstate-of-the-art algorithms fail to replicate their reported performance,\nhighlighting that their CL capacity has been significantly overestimated in the\nconventional evaluation protocol. Our implementation can be found in\nhttps://github.com/csm9493/GTEP.\n","authors":["Sungmin Cha","Kyunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2403.09066v5.pdf","comment":"TMLR 2025 camera ready version"},{"id":"http://arxiv.org/abs/2507.22017v2","updated":"2025-10-28T20:48:51Z","published":"2025-07-29T17:06:22Z","title":"Cyst-X: A Federated AI System Outperforms Clinical Guidelines to Detect\n  Pancreatic Cancer Precursors and Reduce Unnecessary Surgery","summary":"  Pancreatic cancer is projected to be the second-deadliest cancer by 2030,\nmaking early detection critical. Intraductal papillary mucinous neoplasms\n(IPMNs), key cancer precursors, present a clinical dilemma, as current\nguidelines struggle to stratify malignancy risk, leading to unnecessary\nsurgeries or missed diagnoses. Here, we developed Cyst-X, an AI framework for\nIPMN risk prediction trained on a unique, multi-center dataset of 1,461 MRI\nscans from 764 patients. Cyst-X achieves significantly higher accuracy (AUC =\n0.82) than both the established Kyoto guidelines (AUC = 0.75) and expert\nradiologists, particularly in correct identification of high-risk lesions.\nClinically, this translates to a 20% increase in cancer detection sensitivity\n(87.8% vs. 64.1%) for high-risk lesions. We demonstrate that this performance\nis maintained in a federated learning setting, allowing for collaborative model\ntraining without compromising patient privacy. To accelerate research in early\npancreatic cancer detection, we publicly release the Cyst-X dataset and models,\nproviding the first large-scale, multi-center MRI resource for pancreatic cyst\nanalysis.\n","authors":["Hongyi Pan","Gorkem Durak","Elif Keles","Deniz Seyithanoglu","Zheyuan Zhang","Alpay Medetalibeyoglu","Halil Ertugrul Aktas","Andrea Mia Bejar","Ziliang Hong","Yavuz Taktak","Gulbiz Dagoglu Kartal","Mehmet Sukru Erturk","Timurhan Cebeci","Maria Jaramillo Gonzalez","Yury Velichko","Lili Zhao","Emil Agarunov","Federica Proietto Salanitri","Concetto Spampinato","Pallavi Tiwari","Ziyue Xu","Sachin Jambawalikar","Ivo G. Schoots","Marco J. Bruno","Chenchang Huang","Candice W. Bolan","Tamas Gonda","Frank H. Miller","Rajesh N. Keswani","Michael B. Wallace","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2507.22017v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24949v1","updated":"2025-10-28T20:31:19Z","published":"2025-10-28T20:31:19Z","title":"SCOUT: A Lightweight Framework for Scenario Coverage Assessment in\n  Autonomous Driving","summary":"  Assessing scenario coverage is crucial for evaluating the robustness of\nautonomous agents, yet existing methods rely on expensive human annotations or\ncomputationally intensive Large Vision-Language Models (LVLMs). These\napproaches are impractical for large-scale deployment due to cost and\nefficiency constraints. To address these shortcomings, we propose SCOUT\n(Scenario Coverage Oversight and Understanding Tool), a lightweight surrogate\nmodel designed to predict scenario coverage labels directly from an agent's\nlatent sensor representations. SCOUT is trained through a distillation process,\nlearning to approximate LVLM-generated coverage labels while eliminating the\nneed for continuous LVLM inference or human annotation. By leveraging\nprecomputed perception features, SCOUT avoids redundant computations and\nenables fast, scalable scenario coverage estimation. We evaluate our method\nacross a large dataset of real-life autonomous navigation scenarios,\ndemonstrating that it maintains high accuracy while significantly reducing\ncomputational cost. Our results show that SCOUT provides an effective and\npractical alternative for large-scale coverage analysis. While its performance\ndepends on the quality of LVLM-generated training labels, SCOUT represents a\nmajor step toward efficient scenario coverage oversight in autonomous systems.\n","authors":["Anil Yildiz","Sarah M. Thornton","Carl Hildebrandt","Sreeja Roy-Singh","Mykel J. Kochenderfer"],"pdf_url":"https://arxiv.org/pdf/2510.24949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22589v2","updated":"2025-10-28T20:08:24Z","published":"2025-10-26T09:09:52Z","title":"PSScreen V2: Partially Supervised Multiple Retinal Disease Screening","summary":"  In this work, we propose PSScreen V2, a partially supervised self-training\nframework for multiple retinal disease screening. Unlike previous methods that\nrely on fully labelled or single-domain datasets, PSScreen V2 is designed to\nlearn from multiple partially labelled datasets with different distributions,\naddressing both label absence and domain shift challenges. To this end,\nPSScreen V2 adopts a three-branch architecture with one teacher and two student\nnetworks. The teacher branch generates pseudo labels from weakly augmented\nimages to address missing labels, while the two student branches introduce\nnovel feature augmentation strategies: Low-Frequency Dropout (LF-Dropout),\nwhich enhances domain robustness by randomly discarding domain-related\nlow-frequency components, and Low-Frequency Uncertainty (LF-Uncert), which\nestimates uncertain domain variability via adversarially learned Gaussian\nperturbations of low-frequency statistics. Extensive experiments on multiple\nin-domain and out-of-domain fundus datasets demonstrate that PSScreen V2\nachieves state-of-the-art performance and superior domain generalization\nability. Furthermore, compatibility tests with diverse backbones, including the\nvision foundation model DINOv2, as well as evaluations on chest X-ray datasets,\nhighlight the universality and adaptability of the proposed framework. The\ncodes are available at https://github.com/boyiZheng99/PSScreen_V2.\n","authors":["Boyi Zheng","Yalin Zheng","Hrvoje BogunoviÄ","Qing Liu"],"pdf_url":"https://arxiv.org/pdf/2510.22589v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24936v1","updated":"2025-10-28T20:06:08Z","published":"2025-10-28T20:06:08Z","title":"IBIS: A Powerful Hybrid Architecture for Human Activity Recognition","summary":"  The increasing interest in Wi-Fi sensing stems from its potential to capture\nenvironmental data in a low-cost, non-intrusive way, making it ideal for\napplications like healthcare, space occupancy analysis, and gesture-based IoT\ncontrol. However, a major limitation in this field is the common problem of\noverfitting, where models perform well on training data but fail to generalize\nto new data. To overcome this, we introduce a novel hybrid architecture that\nintegrates Inception-BiLSTM with a Support Vector Machine (SVM), which we refer\nto as IBIS. Our IBIS approach is uniquely engineered to improve model\ngeneralization and create more robust classification boundaries. By applying\nthis method to Doppler-derived data, we achieve a movement recognition accuracy\nof nearly 99%. Comprehensive performance metrics and confusion matrices confirm\nthe significant effectiveness of our proposed solution.\n","authors":["Alison M. Fernandes","Hermes I. Del Monego","Bruno S. Chang","Anelise Munaretto","HÃ©lder M. Fontes","Rui L. Campos"],"pdf_url":"https://arxiv.org/pdf/2510.24936v1.pdf","comment":"8 pages. 8 figures. Wireless Days Conference, December 2025"},{"id":"http://arxiv.org/abs/2510.24919v1","updated":"2025-10-28T19:44:20Z","published":"2025-10-28T19:44:20Z","title":"Modality-Aware SAM: Sharpness-Aware-Minimization Driven Gradient\n  Modulation for Harmonized Multimodal Learning","summary":"  In multimodal learning, dominant modalities often overshadow others, limiting\ngeneralization. We propose Modality-Aware Sharpness-Aware Minimization (M-SAM),\na model-agnostic framework that applies to many modalities and supports early\nand late fusion scenarios. In every iteration, M-SAM in three steps optimizes\nlearning. \\textbf{First, it identifies the dominant modality} based on\nmodalities' contribution in the accuracy using Shapley. \\textbf{Second, it\ndecomposes the loss landscape}, or in another language, it modulates the loss\nto prioritize the robustness of the model in favor of the dominant modality,\nand \\textbf{third, M-SAM updates the weights} by backpropagation of modulated\ngradients. This ensures robust learning for the dominant modality while\nenhancing contributions from others, allowing the model to explore and exploit\ncomplementary features that strengthen overall performance. Extensive\nexperiments on four diverse datasets show that M-SAM outperforms the latest\nstate-of-the-art optimization and gradient manipulation methods and\nsignificantly balances and improves multimodal learning.\n","authors":["Hossein R. Nowdeh","Jie Ji","Xiaolong Ma","Fatemeh Afghah"],"pdf_url":"https://arxiv.org/pdf/2510.24919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24907v1","updated":"2025-10-28T19:19:35Z","published":"2025-10-28T19:19:35Z","title":"Understanding Multi-View Transformers","summary":"  Multi-view transformers such as DUSt3R are revolutionizing 3D vision by\nsolving 3D tasks in a feed-forward manner. However, contrary to previous\noptimization-based pipelines, the inner mechanisms of multi-view transformers\nare unclear. Their black-box nature makes further improvements beyond data\nscaling challenging and complicates usage in safety- and reliability-critical\napplications. Here, we present an approach for probing and visualizing 3D\nrepresentations from the residual connections of the multi-view transformers'\nlayers. In this manner, we investigate a variant of the DUSt3R model, shedding\nlight on the development of its latent state across blocks, the role of the\nindividual layers, and suggest how it differs from methods with stronger\ninductive biases of explicit global pose. Finally, we show that the\ninvestigated variant of DUSt3R estimates correspondences that are refined with\nreconstructed geometry. The code used for the analysis is available at\nhttps://github.com/JulienGaubil/und3rstand .\n","authors":["Michal Stary","Julien Gaubil","Ayush Tewari","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2510.24907v1.pdf","comment":"Presented at the ICCV 2025 E2E3D Workshop"},{"id":"http://arxiv.org/abs/2506.03089v2","updated":"2025-10-28T19:19:25Z","published":"2025-06-03T17:13:51Z","title":"Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End\n  Improves CNN Robustness","summary":"  Convolutional neural networks (CNNs) trained on object recognition achieve\nhigh task performance but continue to exhibit vulnerability under a range of\nvisual perturbations and out-of-domain images, when compared with biological\nvision. Prior work has demonstrated that coupling a standard CNN with a\nfront-end (VOneBlock) that mimics the primate primary visual cortex (V1) can\nimprove overall model robustness. Expanding on this, we introduce Early Vision\nNetworks (EVNets), a new class of hybrid CNNs that combine the VOneBlock with a\nnovel SubcorticalBlock, whose architecture draws from computational models in\nneuroscience and is parameterized to maximize alignment with subcortical\nresponses reported across multiple experimental studies. Without being\noptimized to do so, the assembly of the SubcorticalBlock with the VOneBlock\nimproved V1 alignment across most standard V1 benchmarks, and better modeled\nextra-classical receptive field phenomena. In addition, EVNets exhibit stronger\nemergent shape bias and outperform the base CNN architecture by 9.3% on an\naggregate benchmark of robustness evaluations, including adversarial\nperturbations, common corruptions, and domain shifts. Finally, we show that\nEVNets can be further improved when paired with a state-of-the-art data\naugmentation technique, surpassing the performance of the isolated data\naugmentation approach by 6.2% on our robustness benchmark. This result reveals\ncomplementary benefits between changes in architecture to better mimic biology\nand training-based machine learning approaches.\n","authors":["Lucas Piper","Arlindo L. Oliveira","Tiago Marques"],"pdf_url":"https://arxiv.org/pdf/2506.03089v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24904v1","updated":"2025-10-28T19:12:22Z","published":"2025-10-28T19:12:22Z","title":"VividCam: Learning Unconventional Camera Motions from Virtual Synthetic\n  Videos","summary":"  Although recent text-to-video generative models are getting more capable of\nfollowing external camera controls, imposed by either text descriptions or\ncamera trajectories, they still struggle to generalize to unconventional camera\nmotions, which is crucial in creating truly original and artistic videos. The\nchallenge lies in the difficulty of finding sufficient training videos with the\nintended uncommon camera motions. To address this challenge, we propose\nVividCam, a training paradigm that enables diffusion models to learn complex\ncamera motions from synthetic videos, releasing the reliance on collecting\nrealistic training videos. VividCam incorporates multiple disentanglement\nstrategies that isolates camera motion learning from synthetic appearance\nartifacts, ensuring more robust motion representation and mitigating domain\nshift. We demonstrate that our design synthesizes a wide range of precisely\ncontrolled and complex camera motions using surprisingly simple synthetic data.\nNotably, this synthetic data often consists of basic geometries within a\nlow-poly 3D scene and can be efficiently rendered by engines like Unity. Our\nvideo results can be found in https://wuqiuche.github.io/VividCamDemoPage/ .\n","authors":["Qiucheng Wu","Handong Zhao","Zhixin Shu","Jing Shi","Yang Zhang","Shiyu Chang"],"pdf_url":"https://arxiv.org/pdf/2510.24904v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.24902v1","updated":"2025-10-28T19:04:53Z","published":"2025-10-28T19:04:53Z","title":"Pixels to Signals: A Real-Time Framework for Traffic Demand Estimation","summary":"  Traffic congestion is becoming a challenge in the rapidly growing urban\ncities, resulting in increasing delays and inefficiencies within urban\ntransportation systems. To address this issue a comprehensive methodology is\ndesigned to optimize traffic flow and minimize delays. The framework is\nstructured with three primary components: (a) vehicle detection, (b) traffic\nprediction, and (c) traffic signal optimization. This paper presents the first\ncomponent, vehicle detection. The methodology involves analyzing multiple\nsequential frames from a camera feed to compute the background, i.e. the\nunderlying roadway, by averaging pixel values over time. The computed\nbackground is then utilized to extract the foreground, where the Density-Based\nSpatial Clustering of Applications with Noise (DBSCAN) algorithm is applied to\ndetect vehicles. With its computational efficiency and minimal infrastructure\nmodification requirements, the proposed methodology offers a practical and\nscalable solution for real-world deployment.\n","authors":["H Mhatre","M Vyas","A Mittal"],"pdf_url":"https://arxiv.org/pdf/2510.24902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.21609v3","updated":"2025-10-28T18:57:29Z","published":"2025-09-25T21:21:00Z","title":"VLCE: A Knowledge-Enhanced Framework for Image Description in Disaster\n  Assessment","summary":"  Immediate damage assessment is essential after natural catastrophes; yet,\nconventional hand evaluation techniques are sluggish and perilous. Although\nsatellite and unmanned aerial vehicle (UAV) photos offer extensive perspectives\nof impacted regions, current computer vision methodologies generally yield just\nclassification labels or segmentation masks, so constraining their capacity to\ndeliver a thorough situational comprehension. We introduce the Vision Language\nCaption Enhancer (VLCE), a multimodal system designed to produce comprehensive,\ncontextually-informed explanations of disaster imagery. VLCE employs a\ndual-architecture approach: a CNN-LSTM model with a ResNet50 backbone\npretrained on EuroSat satellite imagery for the xBD dataset, and a Vision\nTransformer (ViT) model pretrained on UAV pictures for the RescueNet dataset.\nBoth systems utilize external semantic knowledge from ConceptNet and WordNet to\nexpand vocabulary coverage and improve description accuracy. We assess VLCE in\ncomparison to leading vision-language models (LLaVA and QwenVL) utilizing\nCLIPScore for semantic alignment and InfoMetIC for caption informativeness.\nExperimental findings indicate that VLCE markedly surpasses baseline models,\nattaining a maximum of 95.33% on InfoMetIC while preserving competitive\nsemantic alignment. Our dual-architecture system demonstrates significant\npotential for improving disaster damage assessment by automating the production\nof actionable, information-dense descriptions from satellite and drone photos.\n","authors":["Md. Mahfuzur Rahman","Kishor Datta Gupta","Marufa Kamal","Fahad Rahman","Sunzida Siddique","Ahmed Rafi Hasan","Mohd Ariful Haque","Roy George"],"pdf_url":"https://arxiv.org/pdf/2509.21609v3.pdf","comment":"29 pages, 40 figures, 3 algorithms"},{"id":"http://arxiv.org/abs/2510.24887v1","updated":"2025-10-28T18:43:17Z","published":"2025-10-28T18:43:17Z","title":"Proper Body Landmark Subset Enables More Accurate and 5X Faster\n  Recognition of Isolated Signs in LIBRAS","summary":"  This paper investigates the feasibility of using lightweight body landmark\ndetection for the recognition of isolated signs in Brazilian Sign Language\n(LIBRAS). Although the skeleton-based approach by Alves et al. (2024) enabled\nsubstantial improvements in recognition performance, the use of OpenPose for\nlandmark extraction hindered time performance. In a preliminary investigation,\nwe observed that simply replacing OpenPose with the lightweight MediaPipe,\nwhile improving processing speed, significantly reduced accuracy. To overcome\nthis limitation, we explored landmark subset selection strategies aimed at\noptimizing recognition performance. Experimental results showed that a proper\nlandmark subset achieves comparable or superior performance to state-of-the-art\nmethods while reducing processing time by more than 5X compared to Alves et al.\n(2024). As an additional contribution, we demonstrated that spline-based\nimputation effectively mitigates missing landmark issues, leading to\nsubstantial accuracy gains. These findings highlight that careful landmark\nselection, combined with simple imputation techniques, enables efficient and\naccurate isolated sign recognition, paving the way for scalable Sign Language\nRecognition systems.\n","authors":["Daniele L. V. dos Santos","Thiago B. Pereira","Carlos Eduardo G. R. Alves","Richard J. M. G. Tello","Francisco de A. Boldt","Thiago M. PaixÃ£o"],"pdf_url":"https://arxiv.org/pdf/2510.24887v1.pdf","comment":"Submitted to Int. Conf. on Computer Vision Theory and Applications\n  (VISAPP 2026)"},{"id":"http://arxiv.org/abs/2510.24885v1","updated":"2025-10-28T18:39:03Z","published":"2025-10-28T18:39:03Z","title":"FruitProm: Probabilistic Maturity Estimation and Detection of Fruits and\n  Vegetables","summary":"  Maturity estimation of fruits and vegetables is a critical task for\nagricultural automation, directly impacting yield prediction and robotic\nharvesting. Current deep learning approaches predominantly treat maturity as a\ndiscrete classification problem (e.g., unripe, ripe, overripe). This rigid\nformulation, however, fundamentally conflicts with the continuous nature of the\nbiological ripening process, leading to information loss and ambiguous class\nboundaries. In this paper, we challenge this paradigm by reframing maturity\nestimation as a continuous, probabilistic learning task. We propose a novel\narchitectural modification to the state-of-the-art, real-time object detector,\nRT-DETRv2, by introducing a dedicated probabilistic head. This head enables the\nmodel to predict a continuous distribution over the maturity spectrum for each\ndetected object, simultaneously learning the mean maturity state and its\nassociated uncertainty. This uncertainty measure is crucial for downstream\ndecision-making in robotics, providing a confidence score for tasks like\nselective harvesting. Our model not only provides a far richer and more\nbiologically plausible representation of plant maturity but also maintains\nexceptional detection performance, achieving a mean Average Precision (mAP) of\n85.6\\% on a challenging, large-scale fruit dataset. We demonstrate through\nextensive experiments that our probabilistic approach offers more granular and\naccurate maturity assessments than its classification-based counterparts,\npaving the way for more intelligent, uncertainty-aware automated systems in\nmodern agriculture\n","authors":["Sidharth Rai","Rahul Harsha Cheppally","Benjamin Vail","Keziban YalÃ§Ä±n DokumacÄ±","Ajay Sharda"],"pdf_url":"https://arxiv.org/pdf/2510.24885v1.pdf","comment":"Sidharth Rai, Rahul Harsha Cheppally contributed equally to this work"},{"id":"http://arxiv.org/abs/2510.24870v1","updated":"2025-10-28T18:21:19Z","published":"2025-10-28T18:21:19Z","title":"Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented\n  Generation","summary":"  We introduce MiRAGE, an evaluation framework for retrieval-augmented\ngeneration (RAG) from multimodal sources. As audiovisual media becomes a\nprevalent source of information online, it is essential for RAG systems to\nintegrate information from these sources into generation. However, existing\nevaluations for RAG are text-centric, limiting their applicability to\nmultimodal, reasoning intensive settings because they don't verify information\nagainst sources. MiRAGE is a claim-centric approach to multimodal RAG\nevaluation, consisting of InfoF1, evaluating factuality and information\ncoverage, and CiteF1, measuring citation support and completeness. We show that\nMiRAGE, when applied by humans, strongly aligns with extrinsic quality\njudgments. We additionally introduce automatic variants of MiRAGE and three\nprominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the\nlimitations of text-centric work and laying the groundwork for automatic\nevaluation. We release open-source implementations and outline how to assess\nmultimodal RAG.\n","authors":["Alexander Martin","William Walden","Reno Kriz","Dengjia Zhang","Kate Sanders","Eugene Yang","Chihsheng Jin","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2510.24870v1.pdf","comment":"https://github.com/alexmartin1722/mirage"},{"id":"http://arxiv.org/abs/2510.05034v5","updated":"2025-10-28T18:02:26Z","published":"2025-10-06T17:10:44Z","title":"Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large\n  Multimodal Models","summary":"  Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training\n","authors":["Yolo Yunlong Tang","Jing Bi","Pinxin Liu","Zhenyu Pan","Zhangyun Tan","Qianxiang Shen","Jiani Liu","Hang Hua","Junjia Guo","Yunzhong Xiao","Chao Huang","Zhiyuan Wang","Susan Liang","Xinyi Liu","Yizhi Song","Junhua Huang","Jia-Xing Zhong","Bozheng Li","Daiqing Qi","Ziyun Zeng","Ali Vosoughi","Luchuan Song","Zeliang Zhang","Daiki Shimada","Han Liu","Jiebo Luo","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2510.05034v5.pdf","comment":"Version v1.1"},{"id":"http://arxiv.org/abs/2510.24830v1","updated":"2025-10-28T16:42:53Z","published":"2025-10-28T16:42:53Z","title":"The Generation Phases of Flow Matching: a Denoising Perspective","summary":"  Flow matching has achieved remarkable success, yet the factors influencing\nthe quality of its generation process remain poorly understood. In this work,\nwe adopt a denoising perspective and design a framework to empirically probe\nthe generation process. Laying down the formal connections between flow\nmatching models and denoisers, we provide a common ground to compare their\nperformances on generation and denoising. This enables the design of principled\nand controlled perturbations to influence sample generation: noise and drift.\nThis leads to new insights on the distinct dynamical phases of the generative\nprocess, enabling us to precisely characterize at which stage of the generative\nprocess denoisers succeed or fail and why this matters.\n","authors":["Anne Gagneux","SÃ©golÃ¨ne Martin","RÃ©mi Gribonval","Mathurin Massias"],"pdf_url":"https://arxiv.org/pdf/2510.24830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24827v1","updated":"2025-10-28T16:04:03Z","published":"2025-10-28T16:04:03Z","title":"MCIHN: A Hybrid Network Model Based on Multi-path Cross-modal\n  Interaction for Multimodal Emotion Recognition","summary":"  Multimodal emotion recognition is crucial for future human-computer\ninteraction. However, accurate emotion recognition still faces significant\nchallenges due to differences between different modalities and the difficulty\nof characterizing unimodal emotional information. To solve these problems, a\nhybrid network model based on multipath cross-modal interaction (MCIHN) is\nproposed. First, adversarial autoencoders (AAE) are constructed separately for\neach modality. The AAE learns discriminative emotion features and reconstructs\nthe features through a decoder to obtain more discriminative information about\nthe emotion classes. Then, the latent codes from the AAE of different\nmodalities are fed into a predefined Cross-modal Gate Mechanism model (CGMM) to\nreduce the discrepancy between modalities, establish the emotional relationship\nbetween interacting modalities, and generate the interaction features between\ndifferent modalities. Multimodal fusion using the Feature Fusion module (FFM)\nfor better emotion recognition. Experiments were conducted on publicly\navailable SIMS and MOSI datasets, demonstrating that MCIHN achieves superior\nperformance.\n","authors":["Haoyang Zhang","Zhou Yang","Ke Sun","Yucai Pang","Guoliang Xu"],"pdf_url":"https://arxiv.org/pdf/2510.24827v1.pdf","comment":"The paper will be published in the MMAsia2025 conference proceedings"},{"id":"http://arxiv.org/abs/2510.24821v1","updated":"2025-10-28T15:24:13Z","published":"2025-10-28T15:24:13Z","title":"Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal\n  Perception and Generation","summary":"  We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a\nsparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion\ntotal parameters, of which only 6.1 billion are active per token. This\narchitecture enables highly efficient scaling (dramatically improving\ncomputational efficiency while significantly expanding model capacity) and\nempowers stronger unified multimodal intelligence across vision, speech, and\nlanguage, representing a key step toward Artificial General Intelligence (AGI).\nCompared to its predecessor, the upgraded version exhibits substantial\nimprovements across multimodal understanding and generation. We significantly\nadvance speech recognition capabilities, achieving state-of-the-art performance\nin contextual ASR and highly competitive results in dialect-aware ASR. In image\ngeneration, Ming-Flash-Omni introduces high-fidelity text rendering and\ndemonstrates marked gains in scene consistency and identity preservation during\nimage editing. Furthermore, Ming-Flash-Omni introduces generative segmentation,\na capability that not only achieves strong standalone segmentation performance\nbut also enhances spatial control in image generation and improves editing\nconsistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in\ntext-to-image generation and generative segmentation, and sets new records on\nall 12 contextual ASR benchmarks, all within a single unified architecture.\n","authors":["Inclusion AI"," :","Bowen Ma","Cheng Zou","Canxiang Yan","Chunxiang Jin","Chunjie Shen","Dandan Zheng","Fudong Wang","Furong Xu","GuangMing Yao","Jun Zhou","Jingdong Chen","Jianing Li","Jianxin Sun","Jiajia Liu","Jianjiang Zhu","Jianping Jiang","Jun Peng","Kaixiang Ji","Kaimeng Ren","Libin Wang","Lixiang Ru","Longhua Tan","Lan Wang","Mochen Bai","Ning Gao","Qingpei Guo","Qinglong Zhang","Qiang Xu","Rui Liu","Ruijie Xiong","Ruobing Zheng","Sirui Gao","Tianqi Li","Tinghao Liu","Weilong Chai","Xinyu Xiao","Xiaomei Wang","Xiaolong Wang","Xiao Lu","Xiaoyu Li","Xingning Dong","Xuzheng Yu","Yi Yuan","Yuting Gao","Yuting Xiao","Yunxiao Sun","Yipeng Chen","Yifan Mao","Yifei Wu","Yongjie Lyu","Ziping Ma","Zhiqiang Fang","Zhihao Qiu","Ziyuan Huang","Zizheng Yang","Zhengyu He"],"pdf_url":"https://arxiv.org/pdf/2510.24821v1.pdf","comment":"18 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.24820v1","updated":"2025-10-28T15:12:15Z","published":"2025-10-28T15:12:15Z","title":"SafeEditor: Unified MLLM for Efficient Post-hoc T2I Safety Editing","summary":"  With the rapid advancement of text-to-image (T2I) models, ensuring their\nsafety has become increasingly critical. Existing safety approaches can be\ncategorized into training-time and inference-time methods. While inference-time\nmethods are widely adopted due to their cost-effectiveness, they often suffer\nfrom limitations such as over-refusal and imbalance between safety and utility.\nTo address these challenges, we propose a multi-round safety editing framework\nthat functions as a model-agnostic, plug-and-play module, enabling efficient\nsafety alignment for any text-to-image model. Central to this framework is\nMR-SafeEdit, a multi-round image-text interleaved dataset specifically\nconstructed for safety editing in text-to-image generation. We introduce a\npost-hoc safety editing paradigm that mirrors the human cognitive process of\nidentifying and refining unsafe content. To instantiate this paradigm, we\ndevelop SafeEditor, a unified MLLM capable of multi-round safety editing on\ngenerated images. Experimental results show that SafeEditor surpasses prior\nsafety approaches by reducing over-refusal while achieving a more favorable\nsafety-utility balance.\n","authors":["Ruiyang Zhang","Jiahao Luo","Xiaoru Feng","Qiufan Pang","Yaodong Yang","Juntao Dai"],"pdf_url":"https://arxiv.org/pdf/2510.24820v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2510.24701v1","updated":"2025-10-28T17:53:02Z","published":"2025-10-28T17:53:02Z","title":"Tongyi DeepResearch Technical Report","summary":"  We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.\n","authors":[" Tongyi DeepResearch Team","Baixuan Li","Bo Zhang","Dingchu Zhang","Fei Huang","Guangyu Li","Guoxin Chen","Huifeng Yin","Jialong Wu","Jingren Zhou","Kuan Li","Liangcai Su","Litu Ou","Liwen Zhang","Pengjun Xie","Rui Ye","Wenbiao Yin","Xinmiao Yu","Xinyu Wang","Xixi Wu","Xuanzhong Chen","Yida Zhao","Zhen Zhang","Zhengwei Tao","Zhongwang Zhang","Zile Qiao","Chenxi Wang","Donglei Yu","Gang Fu","Haiyang Shen","Jiayin Yang","Jun Lin","Junkai Zhang","Kui Zeng","Li Yang","Hailong Yin","Maojia Song","Ming Yan","Peng Xia","Qian Xiao","Rui Min","Ruixue Ding","Runnan Fang","Shaowei Chen","Shen Huang","Shihang Wang","Shihao Cai","Weizhou Shen","Xiaobin Wang","Xin Guan","Xinyu Geng","Yingcheng Shi","Yuning Wu","Zhuo Chen","Zijian Li","Yong Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.24701v1.pdf","comment":"https://tongyi-agent.github.io/blog"},{"id":"http://arxiv.org/abs/2510.24652v1","updated":"2025-10-28T17:18:30Z","published":"2025-10-28T17:18:30Z","title":"Optimizing Retrieval for RAG via Reinforced Contrastive Learning","summary":"  As retrieval-augmented generation (RAG) becomes increasingly widespread, the\nrole of information retrieval (IR) is shifting from retrieving information for\nhuman users to retrieving contextual knowledge for artificial intelligence (AI)\nsystems, where relevance becomes difficult to define or annotate beforehand. To\naddress this challenge, we propose R3, a Retrieval framework optimized for RAG\nthrough trialand-feedback Reinforced contrastive learning. Unlike prior\napproaches that rely on annotated or synthetic data for supervised fine-tuning,\nR3 enables the retriever to dynamically explore and optimize relevance within\nthe RAG environment. During training, the retrieved results interact with the\nenvironment to produce contrastive signals that automatically guide the\nretriever's self-improvement. Extensive experiments across diverse tasks\ndemonstrate that R3 improves RAG performance by 5.2% over the original\nretriever and surpasses state-of-the-art retrievers by 4.9%, while achieving\ncomparable results to LLM-augmented retrieval and RAG systems built on\npost-trained or instruction-tuned LLMs. It is both efficient and practical,\nrequiring only 4 GPUs and completing training within a single day.\n","authors":["Jiawei Zhou","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2510.24652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03187v2","updated":"2025-10-28T17:05:07Z","published":"2025-05-30T19:29:18Z","title":"Comparing Retrieval Strategies to Capture Interdisciplinary Scientific\n  Research: A Bibliometric Evaluation of the Integration of Neuroscience and\n  Computer Science","summary":"  Interdisciplinary scientific research is increasingly important in knowledge\nproduction, funding policies, and academic discussions on scholarly\ncommunication. While many studies focus on interdisciplinary corpora defined a\npriori -- usually through keyword-based searches within assumed\ninterdisciplinary domains -- few explore interdisciplinarity as an emergent\nintersection between two distinct fields. Thus, methodological proposals for\nbuilding databases at the intersection of two fields of knowledge are scarce.\nThe goal of this article is to develop and compare different strategies for\ndefining an interdisciplinary corpus between two bodies of knowledge. As a case\nstudy, we focus on the intersection between neuroscience and computer science.\nTo this end, we develop and compare four retrieval strategies, two of them\nbased on keywords and two based on citation and reference patterns. Our results\nshow that the reference-based strategy provides better retrieval, pseudorecall,\nand F1. While we focus on comparing strategies for the study of the\nintersection between the fields of neuroscience and computer science, this\nmethodological reflection is applicable to a wide range of interdisciplinary\ndomains.\n","authors":["Malena Mendez Isla","Agustin Mauro","Diego Kozlowski"],"pdf_url":"https://arxiv.org/pdf/2506.03187v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21729v2","updated":"2025-10-28T16:15:47Z","published":"2025-09-30T00:25:47Z","title":"CustomIR: Unsupervised Fine-Tuning of Dense Embeddings for Known\n  Document Corpora","summary":"  Dense embedding models have become critical for modern information retrieval,\nparticularly in RAG pipelines, but their performance often degrades when\napplied to specialized corpora outside their pre-training distribution. To\naddress thi we introduce CustomIR, a framework for unsupervised adaptation of\npre-trained language embedding models to domain-specific corpora using\nsynthetically generated query-document pairs. CustomIR leverages large language\nmodels (LLMs) to create diverse queries grounded in a known target corpus,\npaired with LLM-verified hard negatives, eliminating the need for costly human\nannotation. Experiments on enterprise email and messaging datasets show that\nCustomIR consistently improves retrieval effectiveness with small models\ngaining up to 2.3 points in Recall@10. This performance increase allows these\nsmall models to rival the performance of much larger alternatives, allowing for\ncheaper RAG deployments. These results highlight that targeted synthetic\nfine-tuning offers a scalable and cost-efficient strategy for increasing\ndomain-specific performance.\n","authors":["Nathan Paull"],"pdf_url":"https://arxiv.org/pdf/2510.21729v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24469v1","updated":"2025-10-28T14:36:22Z","published":"2025-10-28T14:36:22Z","title":"Iterative Critique-Refine Framework for Enhancing LLM Personalization","summary":"  Personalized text generation requires models not only to produce coherent\ntext but also to align with a target user's style, tone, and topical focus.\nExisting retrieval-augmented approaches such as LaMP and PGraphRAG enrich\nprofiles with user and neighbor histories, but they stop at generation and\noften yield outputs that drift in tone, topic, or style. We present PerFine, a\nunified, training-free critique-refine framework that enhances personalization\nthrough iterative, profile-grounded feedback. In each iteration, an LLM\ngenerator produces a draft conditioned on the retrieved profile, and a critic\nLLM - also conditioned on the same profile - provides structured feedback on\ntone, vocabulary, sentence structure, and topicality. The generator then\nrevises, while a novel knockout strategy retains the stronger draft across\niterations. We further study additional inference-time strategies such as\nBest-of-N and Topic Extraction to balance quality and efficiency. Across Yelp,\nGoodreads, and Amazon datasets, PerFine consistently improves personalization\nover PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5\nrefinement iterations, and scalability with increasing critic size. These\nresults highlight that post-hoc, profile-aware feedback offers a powerful\nparadigm for personalized LLM generation that is both training-free and\nmodel-agnostic.\n","authors":["Durga Prasad Maram","Dhruvin Gandhi","Zonghai Yao","Gayathri Akkinapalli","Franck Dernoncourt","Yu Wang","Ryan A. Rossi","Nesreen K. Ahmed"],"pdf_url":"https://arxiv.org/pdf/2510.24469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24431v1","updated":"2025-10-28T13:58:36Z","published":"2025-10-28T13:58:36Z","title":"MiniOneRec: An Open-Source Framework for Scaling Generative\n  Recommendation","summary":"  The recent success of large language models (LLMs) has renewed interest in\nwhether recommender systems can achieve similar scaling benefits. Conventional\nrecommenders, dominated by massive embedding tables, tend to plateau as\nembedding dimensions grow. In contrast, the emerging generative paradigm\nreplaces embeddings with compact Semantic ID (SID) sequences produced by\nautoregressive Transformers. Yet most industrial deployments remain\nproprietary, leaving two fundamental questions open: (1) Do the expected\nscaling laws hold on public benchmarks? (2) What is the minimal post-training\nrecipe that enables competitive performance?\n  We present MiniOneRec, to the best of our knowledge, the first fully\nopen-source generative recommendation framework, which provides an end-to-end\nworkflow spanning SID construction, supervised fine-tuning, and\nrecommendation-oriented reinforcement learning. We generate SIDs via a Residual\nQuantized VAE and post-train Qwen backbones ranging from 0.5B to 7B parameters\non the Amazon Review dataset. Our experiments reveal a consistent downward\ntrend in both training and evaluation losses with increasing model size,\nvalidating the parameter efficiency of the generative approach. To further\nenhance performance, we propose a lightweight yet effective post-training\npipeline that (1) enforces full-process SID alignment and (2) applies\nreinforcement learning with constrained decoding and hybrid rewards. Together,\nthese techniques yield significant improvements in both ranking accuracy and\ncandidate diversity.\n","authors":["Xiaoyu Kong","Leheng Sheng","Junfei Tan","Yuxin Chen","Jiancan Wu","An Zhang","Xiang Wang","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2510.24431v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2510.24430v1","updated":"2025-10-28T13:57:23Z","published":"2025-10-28T13:57:23Z","title":"From Time and Place to Preference: LLM-Driven Geo-Temporal Context in\n  Recommendations","summary":"  Most recommender systems treat timestamps as numeric or cyclical values,\noverlooking real-world context such as holidays, events, and seasonal patterns.\nWe propose a scalable framework that uses large language models (LLMs) to\ngenerate geo-temporal embeddings from only a timestamp and coarse location,\ncapturing holidays, seasonal trends, and local/global events. We then introduce\na geo-temporal embedding informativeness test as a lightweight diagnostic,\ndemonstrating on MovieLens, LastFM, and a production dataset that these\nembeddings provide predictive signal consistent with the outcomes of full model\nintegrations. Geo-temporal embeddings are incorporated into sequential models\nthrough (1) direct feature fusion with metadata embeddings or (2) an auxiliary\nloss that enforces semantic and geo-temporal alignment. Our findings highlight\nthe need for adaptive or hybrid recommendation strategies, and we release a\ncontext-enriched MovieLens dataset to support future research.\n","authors":["Yejin Kim","Shaghayegh Agah","Mayur Nankani","Neeraj Sharma","Feifei Peng","Maria Peifer","Sardar Hamidian","H Howie Huang"],"pdf_url":"https://arxiv.org/pdf/2510.24430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24402v1","updated":"2025-10-28T13:16:36Z","published":"2025-10-28T13:16:36Z","title":"Metadata-Driven Retrieval-Augmented Generation for Financial Question\n  Answering","summary":"  Retrieval-Augmented Generation (RAG) struggles on long, structured financial\nfilings where relevant evidence is sparse and cross-referenced. This paper\npresents a systematic investigation of advanced metadata-driven\nRetrieval-Augmented Generation (RAG) techniques, proposing and evaluating a\nnovel, multi-stage RAG architecture that leverages LLM-generated metadata. We\nintroduce a sophisticated indexing pipeline to create contextually rich\ndocument chunks and benchmark a spectrum of enhancements, including\npre-retrieval filtering, post-retrieval reranking, and enriched embeddings,\nbenchmarked on the FinanceBench dataset. Our results reveal that while a\npowerful reranker is essential for precision, the most significant performance\ngains come from embedding chunk metadata directly with text (\"contextual\nchunks\"). Our proposed optimal architecture combines LLM-driven pre-retrieval\noptimizations with these contextual embeddings to achieve superior performance.\nAdditionally, we present a custom metadata reranker that offers a compelling,\ncost-effective alternative to commercial solutions, highlighting a practical\ntrade-off between peak performance and operational efficiency. This study\nprovides a blueprint for building robust, metadata-aware RAG systems for\nfinancial document analysis.\n","authors":["Michail Dadopoulos","Anestis Ladas","Stratos Moschidis","Ioannis Negkakis"],"pdf_url":"https://arxiv.org/pdf/2510.24402v1.pdf","comment":"Preprint version submitted to the International Journal of Accounting\n  Information Systems; currently under major revision. 20 pages, 1 figure, 1\n  table"},{"id":"http://arxiv.org/abs/2510.14788v2","updated":"2025-10-28T12:58:38Z","published":"2025-10-16T15:20:49Z","title":"Cross-Scenario Unified Modeling of User Interests at Billion Scale","summary":"  User interests on content platforms are inherently diverse, manifesting\nthrough complex behavioral patterns across heterogeneous scenarios such as\nsearch, feed browsing, and content discovery. Traditional recommendation\nsystems typically prioritize business metric optimization within isolated\nspecific scenarios, neglecting cross-scenario behavioral signals and struggling\nto integrate advanced techniques like LLMs at billion-scale deployments, which\nfinally limits their ability to capture holistic user interests across platform\ntouchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender\nEngine for Diversified scenarios, tailored for industry-level content\nrecommendation systems. RED-Rec unifies user interest representations across\nmultiple behavioral contexts by aggregating and synthesizing actions from\nvaried scenarios, resulting in comprehensive item and user modeling. At its\ncore, a two-tower LLM-powered framework enables nuanced, multifaceted\nrepresentations with deployment efficiency, and a scenario-aware dense mixing\nand querying policy effectively fuses diverse behavioral signals to capture\ncross-scenario user intent patterns and express fine-grained, context-specific\nintents during serving. We validate RED-Rec through online A/B testing on\nhundreds of millions of users in RedNote through online A/B testing, showing\nsubstantial performance gains in both content recommendation and advertisement\ntargeting tasks. We further introduce a million-scale sequential recommendation\ndataset, RED-MMU, for comprehensive offline training and evaluation. Our work\nadvances unified user modeling, unlocking deeper personalization and fostering\nmore meaningful user engagement in large-scale UGC platforms.\n","authors":["Manjie Xu","Cheng Chen","Xin Jia","Jingyi Zhou","Yongji Wu","Zejian Wang","Chi Zhang","Kai Zuo","Yibo Chen","Xu Tang","Yao Hu","Yixin Zhu"],"pdf_url":"https://arxiv.org/pdf/2510.14788v2.pdf","comment":"https://github.com/ariesssxu/RedSeqRec"},{"id":"http://arxiv.org/abs/2510.24369v1","updated":"2025-10-28T12:46:33Z","published":"2025-10-28T12:46:33Z","title":"DUET: Dual Model Co-Training for Entire Space CTR Prediction","summary":"  The pre-ranking stage plays a pivotal role in large-scale recommender systems\nbut faces an intrinsic trade-off between model expressiveness and computational\nefficiency. Owing to the massive candidate pool and strict latency constraints,\nindustry systems often rely on lightweight two-tower architectures, which are\ncomputationally efficient yet limited in estimation capability. As a result,\nthey struggle to capture the complex synergistic and suppressive relationships\namong candidate items, which are essential for producing contextually coherent\nand diverse recommendation lists. Moreover, this simplicity further amplifies\nthe Sample Selection Bias (SSB) problem, as coarse-grained models trained on\nbiased exposure data must generalize to a much larger candidate space with\ndistinct distributions.\n  To address these issues, we propose \\textbf{DUET} (\\textbf{DU}al Model\nCo-Training for \\textbf{E}ntire Space C\\textbf{T}R Prediction), a set-wise\npre-ranking framework that achieves expressive modeling under tight\ncomputational budgets. Instead of scoring items independently, DUET performs\nset-level prediction over the entire candidate subset in a single forward pass,\nenabling information-aware interactions among candidates while amortizing the\ncomputational cost across the set. Moreover, a dual model co-training mechanism\nextends supervision to unexposed items via mutual pseudo-label refinement,\neffectively mitigating SSB. Validated through extensive offline experiments and\nonline A/B testing, DUET consistently outperforms state-of-the-art baselines\nand achieves improvements across multiple core business metrics. At present,\nDUET has been fully deployed in Kuaishou and Kuaishou Lite Apps, serving the\nmain traffic for hundreds of millions of users.\n","authors":["Yutian Xiao","Meng Yuan","Fuzhen Zhuang","Wei Chen","Shukuan Wang","Shanqi Liu","Chao Feng","Wenhui Yu","Xiang Li","Lantao Hu","Han Li","Zhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.24369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.20900v4","updated":"2025-10-28T09:32:28Z","published":"2025-08-28T15:29:51Z","title":"OneRec-V2 Technical Report","summary":"  Recent breakthroughs in generative AI have transformed recommender systems\nthrough end-to-end generation. OneRec reformulates recommendation as an\nautoregressive generation task, achieving high Model FLOPs Utilization. While\nOneRec-V1 has shown significant empirical success in real-world deployment, two\ncritical challenges hinder its scalability and performance: (1) inefficient\ncomputational allocation where 97.66% of resources are consumed by sequence\nencoding rather than generation, and (2) limitations in reinforcement learning\nrelying solely on reward models.\n  To address these challenges, we propose OneRec-V2, featuring: (1) Lazy\nDecoder-Only Architecture: Eliminates encoder bottlenecks, reducing total\ncomputation by 94% and training resources by 90%, enabling successful scaling\nto 8B parameters. (2) Preference Alignment with Real-World User Interactions:\nIncorporates Duration-Aware Reward Shaping and Adaptive Ratio Clipping to\nbetter align with user preferences using real-world feedback.\n  Extensive A/B tests on Kuaishou demonstrate OneRec-V2's effectiveness,\nimproving App Stay Time by 0.467%/0.741% while balancing multi-objective\nrecommendations. This work advances generative recommendation scalability and\nalignment with real-world feedback, representing a step forward in the\ndevelopment of end-to-end recommender systems.\n","authors":["Guorui Zhou","Hengrui Hu","Hongtao Cheng","Huanjie Wang","Jiaxin Deng","Jinghao Zhang","Kuo Cai","Lejian Ren","Lu Ren","Liao Yu","Pengfei Zheng","Qiang Luo","Qianqian Wang","Qigen Hu","Rui Huang","Ruiming Tang","Shiyao Wang","Shujie Yang","Tao Wu","Wuchao Li","Xinchen Luo","Xingmei Wang","Yi Su","Yunfan Wu","Zexuan Cheng","Zhanyu Liu","Zixing Zhang","Bin Zhang","Boxuan Wang","Chaoyi Ma","Chengru Song","Chenhui Wang","Chenglong Chu","Di Wang","Dongxue Meng","Dunju Zang","Fan Yang","Fangyu Zhang","Feng Jiang","Fuxing Zhang","Gang Wang","Guowang Zhang","Han Li","Honghui Bao","Hongyang Cao","Jiaming Huang","Jiapeng Chen","Jiaqiang Liu","Jinghui Jia","Kun Gai","Lantao Hu","Liang Zeng","Qiang Wang","Qidong Zhou","Rongzhou Zhang","Shengzhe Wang","Shihui He","Shuang Yang","Siyang Mao","Sui Huang","Tiantian He","Tingting Gao","Wei Yuan","Xiao Liang","Xiaoxiao Xu","Xugang Liu","Yan Wang","Yang Zhou","Yi Wang","Yiwu Liu","Yue Song","Yufei Zhang","Yunfeng Zhao","Zhixin Ling","Ziming Li"],"pdf_url":"https://arxiv.org/pdf/2508.20900v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.17281v2","updated":"2025-10-28T04:01:30Z","published":"2025-10-20T08:16:12Z","title":"MemoryBench: A Benchmark for Memory and Continual Learning in LLM\n  Systems","summary":"  Scaling up data, parameters, and test-time computation has been the\nmainstream methods to improve LLM systems (LLMsys), but their upper bounds are\nalmost reached due to the gradual depletion of high-quality data and marginal\ngains obtained from larger computational resource consumption. Inspired by the\nabilities of human and traditional AI systems in learning from practice,\nconstructing memory and continual learning frameworks for LLMsys has become an\nimportant and popular research direction in recent literature. Yet, existing\nbenchmarks for LLM memory often focus on evaluating the system on homogeneous\nreading comprehension tasks with long-form inputs rather than testing their\nabilities to learn from accumulated user feedback in service time. Therefore,\nwe propose a user feedback simulation framework and a comprehensive benchmark\ncovering multiple domains, languages, and types of tasks to evaluate the\ncontinual learning abilities of LLMsys. Experiments show that the effectiveness\nand efficiency of state-of-the-art baselines are far from satisfying, and we\nhope this benchmark could pave the way for future studies on LLM memory and\noptimization algorithms.\n","authors":["Qingyao Ai","Yichen Tang","Changyue Wang","Jianming Long","Weihang Su","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2510.17281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21727v2","updated":"2025-10-28T02:31:06Z","published":"2025-09-27T16:50:03Z","title":"Your Dense Retriever is Secretly an Expeditious Reasoner","summary":"  Dense retrievers enhance retrieval by encoding queries and documents into\ncontinuous vectors, but they often struggle with reasoning-intensive queries.\nAlthough Large Language Models (LLMs) can reformulate queries to capture\ncomplex reasoning, applying them universally incurs significant computational\ncost. In this work, we propose Adaptive Query Reasoning (AdaQR), a hybrid query\nrewriting framework. Within this framework, a Reasoner Router dynamically\ndirects each query to either fast dense reasoning or deep LLM reasoning. The\ndense reasoning is achieved by the Dense Reasoner, which performs LLM-style\nreasoning directly in the embedding space, enabling a controllable trade-off\nbetween efficiency and accuracy. Experiments on large-scale retrieval\nbenchmarks BRIGHT show that AdaQR reduces reasoning cost by 28% while\npreserving-or even improving-retrieval performance by 7%.\n","authors":["Yichi Zhang","Jun Bai","Zhixin Cai","Shuhan Qin","Zhuofan Chen","Jinghua Guan","Wenge Rong"],"pdf_url":"https://arxiv.org/pdf/2510.21727v2.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2510.23990v1","updated":"2025-10-28T01:49:10Z","published":"2025-10-28T01:49:10Z","title":"Resource-Efficient LLM Application for Structured Transformation of\n  Unstructured Financial Contracts","summary":"  The transformation of unstructured legal contracts into standardized,\nmachine-readable formats is essential for automating financial workflows. The\nCommon Domain Model (CDM) provides a standardized framework for this purpose,\nbut converting complex legal documents like Credit Support Annexes (CSAs) into\nCDM representations remains a significant challenge. In this paper, we present\nan extension of the CDMizer framework, a template-driven solution that ensures\nsyntactic correctness and adherence to the CDM schema during contract-to-CDM\nconversion. We apply this extended framework to a real-world task, comparing\nits performance with a benchmark developed by the International Swaps and\nDerivatives Association (ISDA) for CSA clause extraction. Our results show that\nCDMizer, when integrated with a significantly smaller, open-source Large\nLanguage Model (LLM), achieves competitive performance in terms of accuracy and\nefficiency against larger, proprietary models. This work underscores the\npotential of resource-efficient solutions to automate legal contract\ntransformation, offering a cost-effective and scalable approach that can meet\nthe needs of financial institutions with constrained resources or strict data\nprivacy requirements.\n","authors":["Maruf Ahmed Mridul","Oshani Seneviratne"],"pdf_url":"https://arxiv.org/pdf/2510.23990v1.pdf","comment":"5 pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2510.25025v1","updated":"2025-10-28T22:54:19Z","published":"2025-10-28T22:54:19Z","title":"Secure Retrieval-Augmented Generation against Poisoning Attacks","summary":"  Large language models (LLMs) have transformed natural language processing\n(NLP), enabling applications from content generation to decision support.\nRetrieval-Augmented Generation (RAG) improves LLMs by incorporating external\nknowledge but also introduces security risks, particularly from data poisoning,\nwhere the attacker injects poisoned texts into the knowledge database to\nmanipulate system outputs. While various defenses have been proposed, they\noften struggle against advanced attacks. To address this, we introduce RAGuard,\na detection framework designed to identify poisoned texts. RAGuard first\nexpands the retrieval scope to increase the proportion of clean texts, reducing\nthe likelihood of retrieving poisoned content. It then applies chunk-wise\nperplexity filtering to detect abnormal variations and text similarity\nfiltering to flag highly similar texts. This non-parametric approach enhances\nRAG security, and experiments on large-scale datasets demonstrate its\neffectiveness in detecting and mitigating poisoning attacks, including strong\nadaptive attacks.\n","authors":["Zirui Cheng","Jikai Sun","Anjun Gao","Yueyang Quan","Zhuqing Liu","Xiaohua Hu","Minghong Fang"],"pdf_url":"https://arxiv.org/pdf/2510.25025v1.pdf","comment":"To appear in IEEE BigData 2025"},{"id":"http://arxiv.org/abs/2510.24870v1","updated":"2025-10-28T18:21:19Z","published":"2025-10-28T18:21:19Z","title":"Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented\n  Generation","summary":"  We introduce MiRAGE, an evaluation framework for retrieval-augmented\ngeneration (RAG) from multimodal sources. As audiovisual media becomes a\nprevalent source of information online, it is essential for RAG systems to\nintegrate information from these sources into generation. However, existing\nevaluations for RAG are text-centric, limiting their applicability to\nmultimodal, reasoning intensive settings because they don't verify information\nagainst sources. MiRAGE is a claim-centric approach to multimodal RAG\nevaluation, consisting of InfoF1, evaluating factuality and information\ncoverage, and CiteF1, measuring citation support and completeness. We show that\nMiRAGE, when applied by humans, strongly aligns with extrinsic quality\njudgments. We additionally introduce automatic variants of MiRAGE and three\nprominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the\nlimitations of text-centric work and laying the groundwork for automatic\nevaluation. We release open-source implementations and outline how to assess\nmultimodal RAG.\n","authors":["Alexander Martin","William Walden","Reno Kriz","Dengjia Zhang","Kate Sanders","Eugene Yang","Chihsheng Jin","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2510.24870v1.pdf","comment":"https://github.com/alexmartin1722/mirage"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2510.24718v1","updated":"2025-10-28T17:59:58Z","published":"2025-10-28T17:59:58Z","title":"Generative View Stitching","summary":"  Autoregressive video diffusion models are capable of long rollouts that are\nstable and consistent with history, but they are unable to guide the current\ngeneration with conditioning from the future. In camera-guided video generation\nwith a predefined camera trajectory, this limitation leads to collisions with\nthe generated scene, after which autoregression quickly collapses. To address\nthis, we propose Generative View Stitching (GVS), which samples the entire\nsequence in parallel such that the generated scene is faithful to every part of\nthe predefined camera trajectory. Our main contribution is a sampling algorithm\nthat extends prior work on diffusion stitching for robot planning to video\ngeneration. While such stitching methods usually require a specially trained\nmodel, GVS is compatible with any off-the-shelf video model trained with\nDiffusion Forcing, a prevalent sequence diffusion framework that we show\nalready provides the affordances necessary for stitching. We then introduce\nOmni Guidance, a technique that enhances the temporal consistency in stitching\nby conditioning on both the past and future, and that enables our proposed\nloop-closing mechanism for delivering long-range coherence. Overall, GVS\nachieves camera-guided video generation that is stable, collision-free,\nframe-to-frame consistent, and closes loops for a variety of predefined camera\npaths, including Oscar Reutersv\\\"ard's Impossible Staircase. Results are best\nviewed as videos at https://andrewsonga.github.io/gvs.\n","authors":["Chonghyuk Song","Michal Stary","Boyuan Chen","George Kopanas","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2510.24718v1.pdf","comment":"Project website: https://andrewsonga.github.io/gvs"},{"id":"http://arxiv.org/abs/2501.08428v3","updated":"2025-10-28T17:58:31Z","published":"2025-01-14T20:38:30Z","title":"Physics-Informed Latent Neural Operator for Real-time Predictions of\n  time-dependent parametric PDEs","summary":"  Deep operator network (DeepONet) has shown significant promise as surrogate\nmodels for systems governed by partial differential equations (PDEs), enabling\naccurate mappings between infinite-dimensional function spaces. However, when\napplied to systems with high-dimensional input-output mappings arising from\nlarge numbers of spatial and temporal collocation points, these models often\nrequire heavily overparameterized networks, leading to long training times.\nLatent DeepONet addresses some of these challenges by introducing a two-step\napproach: first learning a reduced latent space using a separate model,\nfollowed by operator learning within this latent space. While efficient, this\nmethod is inherently data-driven and lacks mechanisms for incorporating\nphysical laws, limiting its robustness and generalizability in data-scarce\nsettings. In this work, we propose PI-Latent-NO, a physics-informed latent\nneural operator framework that integrates governing physics directly into the\nlearning process. Our architecture features two coupled DeepONets trained\nend-to-end: a Latent-DeepONet that learns a low-dimensional representation of\nthe solution, and a Reconstruction-DeepONet that maps this latent\nrepresentation back to the physical space. By embedding PDE constraints into\nthe training via automatic differentiation, our method eliminates the need for\nlabeled training data and ensures physics-consistent predictions. The proposed\nframework is both memory and compute-efficient, exhibiting near-constant\nscaling with problem size and demonstrating significant speedups over\ntraditional physics-informed operator models. We validate our approach on a\nrange of parametric PDEs, showcasing its accuracy, scalability, and suitability\nfor real-time prediction in complex physical systems.\n","authors":["Sharmila Karumuri","Lori Graham-Brady","Somdatta Goswami"],"pdf_url":"https://arxiv.org/pdf/2501.08428v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24710v1","updated":"2025-10-28T17:58:17Z","published":"2025-10-28T17:58:17Z","title":"A Single-Loop First-Order Algorithm for Linearly Constrained Bilevel\n  Optimization","summary":"  We study bilevel optimization problems where the lower-level problems are\nstrongly convex and have coupled linear constraints. To overcome the potential\nnon-smoothness of the hyper-objective and the computational challenges\nassociated with the Hessian matrix, we utilize penalty and augmented Lagrangian\nmethods to reformulate the original problem as a single-level one. Especially,\nwe establish a strong theoretical connection between the reformulated function\nand the original hyper-objective by characterizing the closeness of their\nvalues and derivatives. Based on this reformulation, we propose a single-loop,\nfirst-order algorithm for linearly constrained bilevel optimization (SFLCB). We\nprovide rigorous analyses of its non-asymptotic convergence rates, showing an\nimprovement over prior double-loop algorithms -- form\n$O(\\epsilon^{-3}\\log(\\epsilon^{-1}))$ to $O(\\epsilon^{-3})$. The experiments\ncorroborate our theoretical findings and demonstrate the practical efficiency\nof the proposed SFLCB algorithm. Simulation code is provided at\nhttps://github.com/ShenGroup/SFLCB.\n","authors":["Wei Shen","Jiawei Zhang","Minhui Huang","Cong Shen"],"pdf_url":"https://arxiv.org/pdf/2510.24710v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24709v1","updated":"2025-10-28T17:57:05Z","published":"2025-10-28T17:57:05Z","title":"Does Object Binding Naturally Emerge in Large Pretrained Vision\n  Transformers?","summary":"  Object binding, the brain's ability to bind the many features that\ncollectively represent an object into a coherent whole, is central to human\ncognition. It groups low-level perceptual features into high-level object\nrepresentations, stores those objects efficiently and compositionally in\nmemory, and supports human reasoning about individual object instances. While\nprior work often imposes object-centric attention (e.g., Slot Attention)\nexplicitly to probe these benefits, it remains unclear whether this ability\nnaturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they\ncould: recognizing which patches belong to the same object should be useful for\ndownstream prediction and thus guide attention. Motivated by the quadratic\nnature of self-attention, we hypothesize that ViTs represent whether two\npatches belong to the same object, a property we term IsSameObject. We decode\nIsSameObject from patch embeddings across ViT layers using a similarity probe,\nwhich reaches over 90% accuracy. Crucially, this object-binding capability\nemerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker\nin ImageNet-supervised models, suggesting that binding is not a trivial\narchitectural artifact, but an ability acquired through specific pretraining\nobjectives. We further discover that IsSameObject is encoded in a\nlow-dimensional subspace on top of object features, and that this signal\nactively guides attention. Ablating IsSameObject from model activations\ndegrades downstream performance and works against the learning objective,\nimplying that emergent object binding naturally serves the pretraining\nobjective. Our findings challenge the view that ViTs lack object binding and\nhighlight how symbolic knowledge of \"which parts belong together\" emerges\nnaturally in a connectionist system.\n","authors":["Yihao Li","Saeed Salehi","Lyle Ungar","Konrad P. Kording"],"pdf_url":"https://arxiv.org/pdf/2510.24709v1.pdf","comment":"Accepted as a Spotlight at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2406.09795v2","updated":"2025-10-28T17:56:59Z","published":"2024-06-14T07:45:07Z","title":"DeltaPhi: Physical States Residual Learning for Neural Operators in\n  Data-Limited PDE Solving","summary":"  The limited availability of high-quality training data poses a major obstacle\nin data-driven PDE solving, where expensive data collection and resolution\nconstraints severely impact the ability of neural operator networks to learn\nand generalize the underlying physical system. To address this challenge, we\npropose DeltaPhi, a novel learning framework that transforms the PDE solving\ntask from learning direct input-output mappings to learning the residuals\nbetween similar physical states, a fundamentally different approach to neural\noperator learning. This reformulation provides implicit data augmentation by\nexploiting the inherent stability of physical systems where closer initial\nstates lead to closer evolution trajectories. DeltaPhi is architecture-agnostic\nand can be seamlessly integrated with existing neural operators to enhance\ntheir performance. Extensive experiments demonstrate consistent and significant\nimprovements across diverse physical systems including regular and irregular\ndomains, different neural architectures, multiple training data amount, and\ncross-resolution scenarios, confirming its effectiveness as a general\nenhancement for neural operators in data-limited PDE solving.\n","authors":["Xihang Yue","Yi Yang","Linchao Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.09795v2.pdf","comment":"Neurips 2025"},{"id":"http://arxiv.org/abs/2306.08848v4","updated":"2025-10-28T17:53:16Z","published":"2023-06-15T04:24:13Z","title":"Datasheets for Machine Learning Sensors","summary":"  Machine learning (ML) is becoming prevalent in embedded AI sensing systems.\nThese \"ML sensors\" enable context-sensitive, real-time data collection and\ndecision-making across diverse applications ranging from anomaly detection in\nindustrial settings to wildlife tracking for conservation efforts. As such,\nthere is a need to provide transparency in the operation of such ML-enabled\nsensing systems through comprehensive documentation. This is needed to enable\ntheir reproducibility, to address new compliance and auditing regimes mandated\nin regulation and industry-specific policy, and to verify and validate the\nresponsible nature of their operation. To address this gap, we introduce the\ndatasheet for ML sensors framework. We provide a comprehensive template,\ncollaboratively developed in academia-industry partnerships, that captures the\ndistinct attributes of ML sensors, including hardware specifications, ML model\nand dataset characteristics, end-to-end performance metrics, and environmental\nimpacts. Our framework addresses the continuous streaming nature of sensor\ndata, real-time processing requirements, and embeds benchmarking methodologies\nthat reflect real-world deployment conditions, ensuring practical viability.\nAligned with the FAIR principles (Findability, Accessibility, Interoperability,\nand Reusability), our approach enhances the transparency and reusability of ML\nsensor documentation across academic, industrial, and regulatory domains. To\nshow the application of our approach, we present two datasheets: the first for\nan open-source ML sensor designed in-house and the second for a commercial ML\nsensor developed by industry collaborators, both performing computer\nvision-based person detection.\n","authors":["Matthew Stewart","Yuke Zhang","Pete Warden","Yasmine Omri","Shvetank Prakash","Jacob Huckelberry","Joao Henrique Santos","Shawn Hymel","Benjamin Yeager Brown","Jim MacArthur","Nat Jeffries","Emanuel Moss","Mona Sloane","Brian Plancher","Vijay Janapa Reddi"],"pdf_url":"https://arxiv.org/pdf/2306.08848v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24701v1","updated":"2025-10-28T17:53:02Z","published":"2025-10-28T17:53:02Z","title":"Tongyi DeepResearch Technical Report","summary":"  We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.\n","authors":[" Tongyi DeepResearch Team","Baixuan Li","Bo Zhang","Dingchu Zhang","Fei Huang","Guangyu Li","Guoxin Chen","Huifeng Yin","Jialong Wu","Jingren Zhou","Kuan Li","Liangcai Su","Litu Ou","Liwen Zhang","Pengjun Xie","Rui Ye","Wenbiao Yin","Xinmiao Yu","Xinyu Wang","Xixi Wu","Xuanzhong Chen","Yida Zhao","Zhen Zhang","Zhengwei Tao","Zhongwang Zhang","Zile Qiao","Chenxi Wang","Donglei Yu","Gang Fu","Haiyang Shen","Jiayin Yang","Jun Lin","Junkai Zhang","Kui Zeng","Li Yang","Hailong Yin","Maojia Song","Ming Yan","Peng Xia","Qian Xiao","Rui Min","Ruixue Ding","Runnan Fang","Shaowei Chen","Shen Huang","Shihang Wang","Shihao Cai","Weizhou Shen","Xiaobin Wang","Xin Guan","Xinyu Geng","Yingcheng Shi","Yuning Wu","Zhuo Chen","Zijian Li","Yong Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.24701v1.pdf","comment":"https://tongyi-agent.github.io/blog"},{"id":"http://arxiv.org/abs/2510.24700v1","updated":"2025-10-28T17:52:08Z","published":"2025-10-28T17:52:08Z","title":"Greedy Sampling Is Provably Efficient for RLHF","summary":"  Reinforcement Learning from Human Feedback (RLHF) has emerged as a key\ntechnique for post-training large language models. Despite its empirical\nsuccess, the theoretical understanding of RLHF is still limited, as learning\nthe KL-regularized target with only preference feedback poses additional\nchallenges compared with canonical RL. Existing works mostly study the\nreward-based Bradley-Terry (BT) preference model, and extend classical designs\nutilizing optimism or pessimism. This work, instead, considers the general\npreference model (whose practical relevance has been observed recently) and\nobtains performance guarantees with major, order-wise improvements over\nexisting ones. Surprisingly, these results are derived from algorithms that\ndirectly use the empirical estimates (i.e., greedy sampling), as opposed to\nconstructing optimistic or pessimistic estimates in previous works. This\ninsight has a deep root in the unique structural property of the optimal policy\nclass under the KL-regularized target, and we further specialize it to the BT\nmodel, highlighting the surprising sufficiency of greedy sampling in RLHF.\n","authors":["Di Wu","Chengshuai Shi","Jing Yang","Cong Shen"],"pdf_url":"https://arxiv.org/pdf/2510.24700v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24699v1","updated":"2025-10-28T17:51:50Z","published":"2025-10-28T17:51:50Z","title":"AgentFold: Long-Horizon Web Agents with Proactive Context Management","summary":"  LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini.\n","authors":["Rui Ye","Zhongwang Zhang","Kuan Li","Huifeng Yin","Zhengwei Tao","Yida Zhao","Liangcai Su","Liwen Zhang","Zile Qiao","Xinyu Wang","Pengjun Xie","Fei Huang","Siheng Chen","Jingren Zhou","Yong Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.24699v1.pdf","comment":"26 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.24674v1","updated":"2025-10-28T17:40:04Z","published":"2025-10-28T17:40:04Z","title":"Learning to Drive Safely with Hybrid Options","summary":"  Out of the many deep reinforcement learning approaches for autonomous\ndriving, only few make use of the options (or skills) framework. That is\nsurprising, as this framework is naturally suited for hierarchical control\napplications in general, and autonomous driving tasks in specific. Therefore,\nin this work the options framework is applied and tailored to autonomous\ndriving tasks on highways. More specifically, we define dedicated options for\nlongitudinal and lateral manoeuvres with embedded safety and comfort\nconstraints. This way, prior domain knowledge can be incorporated into the\nlearning process and the learned driving behaviour can be constrained more\neasily. We propose several setups for hierarchical control with options and\nderive practical algorithms following state-of-the-art reinforcement learning\ntechniques. By separately selecting actions for longitudinal and lateral\ncontrol, the introduced policies over combined and hybrid options obtain the\nsame expressiveness and flexibility that human drivers have, while being easier\nto interpret than classical policies over continuous actions. Of all the\ninvestigated approaches, these flexible policies over hybrid options perform\nthe best under varying traffic conditions, outperforming the baseline policies\nover actions.\n","authors":["Bram De Cooman","Johan Suykens"],"pdf_url":"https://arxiv.org/pdf/2510.24674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24672v1","updated":"2025-10-28T17:37:12Z","published":"2025-10-28T17:37:12Z","title":"Eigenfunction Extraction for Ordered Representation Learning","summary":"  Recent advances in representation learning reveal that widely used\nobjectives, such as contrastive and non-contrastive, implicitly perform\nspectral decomposition of a contextual kernel, induced by the relationship\nbetween inputs and their contexts. Yet, these methods recover only the linear\nspan of top eigenfunctions of the kernel, whereas exact spectral decomposition\nis essential for understanding feature ordering and importance. In this work,\nwe propose a general framework to extract ordered and identifiable\neigenfunctions, based on modular building blocks designed to satisfy key\ndesiderata, including compatibility with the contextual kernel and scalability\nto modern settings. We then show how two main methodological paradigms,\nlow-rank approximation and Rayleigh quotient optimization, align with this\nframework for eigenfunction extraction. Finally, we validate our approach on\nsynthetic kernels and demonstrate on real-world image datasets that the\nrecovered eigenvalues act as effective importance scores for feature selection,\nenabling principled efficiency-accuracy tradeoffs via adaptive-dimensional\nrepresentations.\n","authors":["Burak VarÄ±cÄ±","Che-Ping Tsai","Ritabrata Ray","Nicholas M. Boffi","Pradeep Ravikumar"],"pdf_url":"https://arxiv.org/pdf/2510.24672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07862v2","updated":"2025-10-28T17:37:03Z","published":"2025-02-11T17:19:44Z","title":"ADMN: A Layer-Wise Adaptive Multimodal Network for Dynamic Input Noise\n  and Compute Resources","summary":"  Multimodal deep learning systems are deployed in dynamic scenarios due to the\nrobustness afforded by multiple sensing modalities. Nevertheless, they struggle\nwith varying compute resource availability (due to multi-tenancy, device\nheterogeneity, etc.) and fluctuating quality of inputs (from sensor feed\ncorruption, environmental noise, etc.). Statically provisioned multimodal\nsystems cannot adapt when compute resources change over time, while existing\ndynamic networks struggle with strict compute budgets. Additionally, both\nsystems often neglect the impact of variations in modality quality.\nConsequently, modalities suffering substantial corruption may needlessly\nconsume resources better allocated towards other modalities. We propose ADMN, a\nlayer-wise Adaptive Depth Multimodal Network capable of tackling both\nchallenges: it adjusts the total number of active layers across all modalities\nto meet strict compute resource constraints and continually reallocates layers\nacross input modalities according to their modality quality. Our evaluations\nshowcase ADMN can match the accuracy of state-of-the-art networks while\nreducing up to 75% of their floating-point operations.\n","authors":["Jason Wu","Yuyang Yuan","Kang Yang","Lance Kaplan","Mani Srivastava"],"pdf_url":"https://arxiv.org/pdf/2502.07862v2.pdf","comment":"Accepted to Neurips 2025"},{"id":"http://arxiv.org/abs/2510.24670v1","updated":"2025-10-28T17:36:51Z","published":"2025-10-28T17:36:51Z","title":"Pearl: A Foundation Model for Placing Every Atom in the Right Location","summary":"  Accurately predicting the three-dimensional structures of protein-ligand\ncomplexes remains a fundamental challenge in computational drug discovery that\nlimits the pace and success of therapeutic design. Deep learning methods have\nrecently shown strong potential as structural prediction tools, achieving\npromising accuracy across diverse biomolecular systems. However, their\nperformance and utility are constrained by scarce experimental data,\ninefficient architectures, physically invalid poses, and the limited ability to\nexploit auxiliary information available at inference. To address these issues,\nwe introduce Pearl (Placing Every Atom in the Right Location), a foundation\nmodel for protein-ligand cofolding at scale. Pearl addresses these challenges\nwith three key innovations: (1) training recipes that include large-scale\nsynthetic data to overcome data scarcity; (2) architectures that incorporate an\nSO(3)-equivariant diffusion module to inherently respect 3D rotational\nsymmetries, improving generalization and sample efficiency, and (3)\ncontrollable inference, including a generalized multi-chain templating system\nsupporting both protein and non-polymeric components as well as dual\nunconditional/conditional modes. Pearl establishes a new state-of-the-art\nperformance in protein-ligand cofolding. On the key metric of generating\naccurate (RMSD < 2 \\r{A}) and physically valid poses, Pearl surpasses AlphaFold\n3 and other open source baselines on the public Runs N' Poses and PoseBusters\nbenchmarks, delivering 14.5% and 14.2% improvements, respectively, over the\nnext best model. In the pocket-conditional cofolding regime, Pearl delivers\n$3.6\\times$ improvement on a proprietary set of challenging, real-world drug\ntargets at the more rigorous RMSD < 1 \\r{A} threshold. Finally, we demonstrate\nthat model performance correlates directly with synthetic dataset size used in\ntraining.\n","authors":[" Genesis Research Team","Alejandro Dobles","Nina Jovic","Kenneth Leidal","Pranav Murugan","David C. Williams","Drausin Wulsin","Nate Gruver","Christina X. Ji","Korrawat Pruegsanusak","Gianluca Scarpellini","Ansh Sharma","Wojciech Swiderski","Andrea Bootsma","Richard Strong Bowen","Charlotte Chen","Jamin Chen","Marc AndrÃ© DÃ¤mgen","Roy Tal Dew","Benjamin DiFrancesco","J. D. Fishman","Alla Ivanova","Zach Kagin","David Li-Bland","Zuli Liu","Igor Morozov","Jeffrey Ouyang-Zhang","Frank C. Pickard IV","Kushal S. Shah","Ben Shor","Gabriel Monteiro da Silva","Maxx Tessmer","Carl Tilbury","Cyr Vetcher","Daniel Zeng","Maruan Al-Shedivat","Aleksandra Faust","Evan N. Feinberg","Michael V. LeVine","Matteus Pan"],"pdf_url":"https://arxiv.org/pdf/2510.24670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23455v2","updated":"2025-10-28T17:15:50Z","published":"2025-10-27T15:56:19Z","title":"SGFusion: Stochastic Geographic Gradient Fusion in Federated Learning","summary":"  This paper proposes Stochastic Geographic Gradient Fusion (SGFusion), a novel\ntraining algorithm to leverage the geographic information of mobile users in\nFederated Learning (FL). SGFusion maps the data collected by mobile devices\nonto geographical zones and trains one FL model per zone, which adapts well to\nthe data and behaviors of users in that zone. SGFusion models the local\ndata-based correlation among geographical zones as a hierarchical random graph\n(HRG) optimized by Markov Chain Monte Carlo sampling. At each training step,\nevery zone fuses its local gradient with gradients derived from a small set of\nother zones sampled from the HRG. This approach enables knowledge fusion and\nsharing among geographical zones in a probabilistic and stochastic gradient\nfusion process with self-attention weights, such that \"more similar\" zones have\n\"higher probabilities\" of sharing gradients with \"larger attention weights.\"\nSGFusion remarkably improves model utility without introducing undue\ncomputational cost. Extensive theoretical and empirical results using a\nheart-rate prediction dataset collected across 6 countries show that models\ntrained with SGFusion converge with upper-bounded expected errors and\nsignificantly improve utility in all countries compared to existing approaches\nwithout notable cost in system scalability.\n","authors":["Khoa Nguyen","Khang Tran","NhatHai Phan","Cristian Borcea","Rouming Jin","Issa Khalil"],"pdf_url":"https://arxiv.org/pdf/2510.23455v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24643v1","updated":"2025-10-28T17:09:43Z","published":"2025-10-28T17:09:43Z","title":"The Cost of Robustness: Tighter Bounds on Parameter Complexity for\n  Robust Memorization in ReLU Nets","summary":"  We study the parameter complexity of robust memorization for $\\mathrm{ReLU}$\nnetworks: the number of parameters required to interpolate any given dataset\nwith $\\epsilon$-separation between differently labeled points, while ensuring\npredictions remain consistent within a $\\mu$-ball around each training sample.\nWe establish upper and lower bounds on the parameter count as a function of the\nrobustness ratio $\\rho = \\mu / \\epsilon$. Unlike prior work, we provide a\nfine-grained analysis across the entire range $\\rho \\in (0,1)$ and obtain\ntighter upper and lower bounds that improve upon existing results. Our findings\nreveal that the parameter complexity of robust memorization matches that of\nnon-robust memorization when $\\rho$ is small, but grows with increasing $\\rho$.\n","authors":["Yujun Kim","Chaewon Moon","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2510.24643v1.pdf","comment":"Accepted to NeurIPS 2025, 72 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.24639v1","updated":"2025-10-28T17:06:15Z","published":"2025-10-28T17:06:15Z","title":"Causal Ordering for Structure Learning From Time Series","summary":"  Predicting causal structure from time series data is crucial for\nunderstanding complex phenomena in physiology, brain connectivity, climate\ndynamics, and socio-economic behaviour. Causal discovery in time series is\nhindered by the combinatorial complexity of identifying true causal\nrelationships, especially as the number of variables and time points grow. A\ncommon approach to simplify the task is the so-called ordering-based methods.\nTraditional ordering methods inherently limit the representational capacity of\nthe resulting model. In this work, we fix this issue by leveraging multiple\nvalid causal orderings, instead of a single one as standard practice. We\npropose DOTS (Diffusion Ordered Temporal Structure), using diffusion-based\ncausal discovery for temporal data. By integrating multiple orderings, DOTS\neffectively recovers the transitive closure of the underlying directed acyclic\ngraph, mitigating spurious artifacts inherent in single-ordering approaches. We\nformalise the problem under standard assumptions such as stationarity and the\nadditive noise model, and leverage score matching with diffusion processes to\nenable efficient Hessian estimation. Extensive experiments validate the\napproach. Empirical evaluations on synthetic and real-world datasets\ndemonstrate that DOTS outperforms state-of-the-art baselines, offering a\nscalable and robust approach to temporal causal discovery. On synthetic\nbenchmarks ($d{=}\\!3-\\!6$ variables, $T{=}200\\!-\\!5{,}000$ samples), DOTS\nimproves mean window-graph $F1$ from $0.63$ (best baseline) to $0.81$. On the\nCausalTime real-world benchmark ($d{=}20\\!-\\!36$), while baselines remain the\nbest on individual datasets, DOTS attains the highest average summary-graph\n$F1$ while halving runtime relative to graph-optimisation methods. These\nresults establish DOTS as a scalable and accurate solution for temporal causal\ndiscovery.\n","authors":["Pedro P. Sanchez","Damian Machlanski","Steven McDonagh","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2510.24639v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2504.13883v3","updated":"2025-10-28T17:05:46Z","published":"2025-04-03T17:54:59Z","title":"Hybrid Deep Learning Model to Estimate Cognitive Effort from fNIRS\n  Signals","summary":"  This study estimates cognitive effort based on functional near-infrared\nspectroscopy data and performance scores using a hybrid DeepNet model. The\nestimation of cognitive effort enables educators to modify material to enhance\nlearning effectiveness and student engagement. In this study, we collected\noxygenated hemoglobin using functional near-infrared spectroscopy during an\neducational quiz game. Participants (n=16) responded to 16 questions in a\nUnity-based educational game, each within a 30-second response time limit. We\nused DeepNet models to predict the performance score from the oxygenated\nhemoglobin, and compared traditional machine learning and DeepNet models to\ndetermine which approach provides better accuracy in predicting performance\nscores. The result shows that the proposed CNN-GRU gives better performance\nwith 73% than other models. After the prediction, we used the predicted score\nand the oxygenated hemoglobin to observe cognitive effort by calculating\nrelative neural efficiency and involvement in our test cases. Our result shows\nthat even with moderate accuracy, the predicted cognitive effort closely follow\nthe actual trends. This findings can be helpful in designing and improving\nlearning environments and provide valuable insights into learning materials.\n","authors":["Shayla Sharmin","Roghayeh Leila Barmaki"],"pdf_url":"https://arxiv.org/pdf/2504.13883v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24633v1","updated":"2025-10-28T17:01:38Z","published":"2025-10-28T17:01:38Z","title":"Symbolic Snapshot Ensembles","summary":"  Inductive logic programming (ILP) is a form of logical machine learning. Most\nILP algorithms learn a single hypothesis from a single training run. Ensemble\nmethods train an ILP algorithm multiple times to learn multiple hypotheses. In\nthis paper, we train an ILP algorithm only once and save intermediate\nhypotheses. We then combine the hypotheses using a minimum description length\nweighting scheme. Our experiments on multiple benchmarks, including game\nplaying and visual reasoning, show that our approach improves predictive\naccuracy by 4% with less than 1% computational overhead.\n","authors":["Mingyue Liu","Andrew Cropper"],"pdf_url":"https://arxiv.org/pdf/2510.24633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24621v1","updated":"2025-10-28T16:49:03Z","published":"2025-10-28T16:49:03Z","title":"Coreset for Robust Geometric Median: Eliminating Size Dependency on\n  Outliers","summary":"  We study the robust geometric median problem in Euclidean space\n$\\mathbb{R}^d$, with a focus on coreset construction.A coreset is a compact\nsummary of a dataset $P$ of size $n$ that approximates the robust cost for all\ncenters $c$ within a multiplicative error $\\varepsilon$. Given an outlier count\n$m$, we construct a coreset of size $\\tilde{O}(\\varepsilon^{-2} \\cdot\n\\min\\{\\varepsilon^{-2}, d\\})$ when $n \\geq 4m$, eliminating the $O(m)$\ndependency present in prior work [Huang et al., 2022 & 2023]. For the special\ncase of $d = 1$, we achieve an optimal coreset size of\n$\\tilde{\\Theta}(\\varepsilon^{-1/2} + \\frac{m}{n} \\varepsilon^{-1})$, revealing\na clear separation from the vanilla case studied in [Huang et al., 2023;\nAfshani and Chris, 2024]. Our results further extend to robust\n$(k,z)$-clustering in various metric spaces, eliminating the $m$-dependence\nunder mild data assumptions. The key technical contribution is a novel\nnon-component-wise error analysis, enabling substantial reduction of outlier\ninfluence, unlike prior methods that retain them.Empirically, our algorithms\nconsistently outperform existing baselines in terms of size-accuracy tradeoffs\nand runtime, even when data assumptions are violated across a wide range of\ndatasets.\n","authors":["Ziyi Fang","Lingxiao Huang","Runkai Yang"],"pdf_url":"https://arxiv.org/pdf/2510.24621v1.pdf","comment":"This paper has been accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24619v1","updated":"2025-10-28T16:48:03Z","published":"2025-10-28T16:48:03Z","title":"Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation","summary":"  With the release of new large language models (LLMs) like Llama and Mistral,\nzero-shot cross-lingual transfer has become increasingly feasible due to their\nmultilingual pretraining and strong generalization capabilities. However,\nadapting these decoder-only LLMs to new tasks across languages remains\nchallenging. While parameter-efficient fine-tuning (PeFT) techniques like\nLow-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as\nsoft prompt tuning, prefix tuning, and Llama Adapter are less explored,\nespecially for zero-shot transfer in decoder-only models. We present a\ncomprehensive study of three prefix-based methods for zero-shot cross-lingual\ntransfer from English to 35+ high- and low-resource languages. Our analysis\nfurther explores transfer across linguistic families and scripts, as well as\nthe impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix\nmethods outperform LoRA-baselines by up to 6% on the Belebele benchmark.\nSimilar improvements were observed with Mistral v0.3 7B as well. Despite using\nonly 1.23M learning parameters with prefix tuning, we achieve consistent\nimprovements across diverse benchmarks. These findings highlight the potential\nof prefix-based techniques as an effective and scalable alternative to LoRA,\nparticularly in low-resource multilingual settings.\n","authors":["Snegha A","Sayambhu Sen","Piyush Singh Pasi","Abhishek Singhania","Preethi Jyothi"],"pdf_url":"https://arxiv.org/pdf/2510.24619v1.pdf","comment":"12 Pages"},{"id":"http://arxiv.org/abs/2410.16893v2","updated":"2025-10-28T16:44:42Z","published":"2024-10-22T10:56:52Z","title":"Global Optimization of Gaussian Process Acquisition Functions Using a\n  Piecewise-Linear Kernel Approximation","summary":"  Bayesian optimization relies on iteratively constructing and optimizing an\nacquisition function. The latter turns out to be a challenging, non-convex\noptimization problem itself. Despite the relative importance of this step, most\nalgorithms employ sampling- or gradient-based methods, which do not provably\nconverge to global optima. This work investigates mixed-integer programming\n(MIP) as a paradigm for global acquisition function optimization. Specifically,\nour Piecewise-linear Kernel Mixed Integer Quadratic Programming (PK-MIQP)\nformulation introduces a piecewise-linear approximation for Gaussian process\nkernels and admits a corresponding MIQP representation for acquisition\nfunctions. The proposed method is applicable to uncertainty-based acquisition\nfunctions for any stationary or dot-product kernel. We analyze the theoretical\nregret bounds of the proposed approximation, and empirically demonstrate the\nframework on synthetic functions, constrained benchmarks, and a hyperparameter\ntuning task.\n","authors":["Yilin Xie","Shiqiang Zhang","Joel A. Paulson","Calvin Tsay"],"pdf_url":"https://arxiv.org/pdf/2410.16893v2.pdf","comment":"18 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2510.24616v1","updated":"2025-10-28T16:44:34Z","published":"2025-10-28T16:44:34Z","title":"Statistical physics of deep learning: Optimal learning of a multi-layer\n  perceptron near interpolation","summary":"  For three decades statistical physics has been providing a framework to\nanalyse neural networks. A long-standing question remained on its capacity to\ntackle deep learning models capturing rich feature learning effects, thus going\nbeyond the narrow networks or kernel methods analysed until now. We positively\nanswer through the study of the supervised learning of a multi-layer\nperceptron. Importantly, (i) its width scales as the input dimension, making it\nmore prone to feature learning than ultra wide networks, and more expressive\nthan narrow ones or with fixed embedding layers; and (ii) we focus on the\nchallenging interpolation regime where the number of trainable parameters and\ndata are comparable, which forces the model to adapt to the task. We consider\nthe matched teacher-student setting. It provides the fundamental limits of\nlearning random deep neural network targets and helps in identifying the\nsufficient statistics describing what is learnt by an optimally trained network\nas the data budget increases. A rich phenomenology emerges with various\nlearning transitions. With enough data optimal performance is attained through\nmodel's \"specialisation\" towards the target, but it can be hard to reach for\ntraining algorithms which get attracted by sub-optimal solutions predicted by\nthe theory. Specialisation occurs inhomogeneously across layers, propagating\nfrom shallow towards deep ones, but also across neurons in each layer.\nFurthermore, deeper targets are harder to learn. Despite its simplicity, the\nBayesian-optimal setting provides insights on how the depth, non-linearity and\nfinite (proportional) width influence neural networks in the feature learning\nregime that are potentially relevant way beyond it.\n","authors":["Jean Barbier","Francesco Camilli","Minh-Toan Nguyen","Mauro Pastore","Rudy Skerk"],"pdf_url":"https://arxiv.org/pdf/2510.24616v1.pdf","comment":"30 pages, 19 figures + appendix. This submission supersedes both\n  arXiv:2505.24849 and arXiv:2501.18530"},{"id":"http://arxiv.org/abs/2510.24614v1","updated":"2025-10-28T16:44:11Z","published":"2025-10-28T16:44:11Z","title":"Semi-supervised and unsupervised learning for health indicator\n  extraction from guided waves in aerospace composite structures","summary":"  Health indicators (HIs) are central to diagnosing and prognosing the\ncondition of aerospace composite structures, enabling efficient maintenance and\noperational safety. However, extracting reliable HIs remains challenging due to\nvariability in material properties, stochastic damage evolution, and diverse\ndamage modes. Manufacturing defects (e.g., disbonds) and in-service incidents\n(e.g., bird strikes) further complicate this process. This study presents a\ncomprehensive data-driven framework that learns HIs via two learning approaches\nintegrated with multi-domain signal processing. Because ground-truth HIs are\nunavailable, a semi-supervised and an unsupervised approach are proposed: (i) a\ndiversity deep semi-supervised anomaly detection (Diversity-DeepSAD) approach\naugmented with continuous auxiliary labels used as hypothetical damage proxies,\nwhich overcomes the limitation of prior binary labels that only distinguish\nhealthy and failed states while neglecting intermediate degradation, and (ii) a\ndegradation-trend-constrained variational autoencoder (DTC-VAE), in which the\nmonotonicity criterion is embedded via an explicit trend constraint. Guided\nwaves with multiple excitation frequencies are used to monitor single-stiffener\ncomposite structures under fatigue loading. Time, frequency, and time-frequency\nrepresentations are explored, and per-frequency HIs are fused via unsupervised\nensemble learning to mitigate frequency dependence and reduce variance. Using\nfast Fourier transform features, the augmented Diversity-DeepSAD model achieved\n81.6% performance, while DTC-VAE delivered the most consistent HIs with 92.3%\nperformance, outperforming existing baselines.\n","authors":["James Josep Perry","Pablo Garcia-Conde Ortiz","George Konstantinou","Cornelie Vergouwen","Edlyn Santha Kumaran","Morteza Moradi"],"pdf_url":"https://arxiv.org/pdf/2510.24614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11390v3","updated":"2025-10-28T16:44:02Z","published":"2024-09-17T17:50:15Z","title":"Says Who? Effective Zero-Shot Annotation of Focalization","summary":"  Focalization describes the way in which access to narrative information is\nrestricted or controlled based on the knowledge available to knowledge of the\nnarrator. It is encoded via a wide range of lexico-grammatical features and is\nsubject to reader interpretation. Even trained annotators frequently disagree\non correct labels, suggesting this task is both qualitatively and\ncomputationally challenging. In this work, we test how well five contemporary\nlarge language model (LLM) families and two baselines perform when annotating\nshort literary excerpts for focalization. Despite the challenging nature of the\ntask, we find that LLMs show comparable performance to trained human\nannotators, with GPT-4o achieving an average F1 of 84.79%. Further, we\ndemonstrate that the log probabilities output by GPT-family models frequently\nreflect the difficulty of annotating particular excerpts. Finally, we provide a\ncase study analyzing sixteen Stephen King novels, demonstrating the usefulness\nof this approach for computational literary studies and the insights gleaned\nfrom examining focalization at scale.\n","authors":["Rebecca M. M. Hicke","Yuri Bizzoni","Pascale Feldkamp","Ross Deans Kristensen-McLachlan"],"pdf_url":"https://arxiv.org/pdf/2409.11390v3.pdf","comment":"Accepted at CHR 2025"},{"id":"http://arxiv.org/abs/2510.24601v1","updated":"2025-10-28T16:28:42Z","published":"2025-10-28T16:28:42Z","title":"Comparison of generalised additive models and neural networks in\n  applications: A systematic review","summary":"  Neural networks have become a popular tool in predictive modelling, more\ncommonly associated with machine learning and artificial intelligence than with\nstatistics. Generalised Additive Models (GAMs) are flexible non-linear\nstatistical models that retain interpretability. Both are state-of-the-art in\ntheir own right, with their respective advantages and disadvantages. This paper\nanalyses how these two model classes have performed on real-world tabular data.\nFollowing PRISMA guidelines, we conducted a systematic review of papers that\nperformed empirical comparisons of GAMs and neural networks. Eligible papers\nwere identified, yielding 143 papers, with 430 datasets. Key attributes at both\npaper and dataset levels were extracted and reported. Beyond summarising\ncomparisons, we analyse reported performance metrics using mixed-effects\nmodelling to investigate potential characteristics that can explain and\nquantify observed differences, including application area, study year, sample\nsize, number of predictors, and neural network complexity. Across datasets, no\nconsistent evidence of superiority was found for either GAMs or neural networks\nwhen considering the most frequently reported metrics (RMSE, $R^2$, and AUC).\nNeural networks tended to outperform in larger datasets and in those with more\npredictors, but this advantage narrowed over time. Conversely, GAMs remained\ncompetitive, particularly in smaller data settings, while retaining\ninterpretability. Reporting of dataset characteristics and neural network\ncomplexity was incomplete in much of the literature, limiting transparency and\nreproducibility. This review highlights that GAMs and neural networks should be\nviewed as complementary approaches rather than competitors. For many tabular\napplications, the performance trade-off is modest, and interpretability may\nfavour GAMs.\n","authors":["Jessica Doohan","Lucas Kook","Kevin Burke"],"pdf_url":"https://arxiv.org/pdf/2510.24601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24598v1","updated":"2025-10-28T16:27:10Z","published":"2025-10-28T16:27:10Z","title":"A Novel XAI-Enhanced Quantum Adversarial Networks for Velocity\n  Dispersion Modeling in MaNGA Galaxies","summary":"  Current quantum machine learning approaches often face challenges balancing\npredictive accuracy, robustness, and interpretability. To address this, we\npropose a novel quantum adversarial framework that integrates a hybrid quantum\nneural network (QNN) with classical deep learning layers, guided by an\nevaluator model with LIME-based interpretability, and extended through quantum\nGAN and self-supervised variants. In the proposed model, an adversarial\nevaluator concurrently guides the QNN by computing feedback loss, thereby\noptimizing both prediction accuracy and model explainability. Empirical\nevaluations show that the Vanilla model achieves RMSE = 0.27, MSE = 0.071, MAE\n= 0.21, and R^2 = 0.59, delivering the most consistent performance across\nregression metrics compared to adversarial counterparts. These results\ndemonstrate the potential of combining quantum-inspired methods with classical\narchitectures to develop lightweight, high-performance, and interpretable\npredictive models, advancing the applicability of QML beyond current\nlimitations.\n","authors":["Sathwik Narkedimilli","N V Saran Kumar","Aswath Babu H","Manjunath K Vanahalli","Manish M","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2510.24598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15737v4","updated":"2025-10-28T16:23:53Z","published":"2024-11-24T07:02:32Z","title":"TableTime: Reformulating Time Series Classification as Training-Free\n  Table Understanding with Large Language Models","summary":"  Large language models (LLMs) have demonstrated their effectiveness in\nmultivariate time series classification (MTSC). Effective adaptation of LLMs\nfor MTSC necessitates informative data representations. Existing LLM-based\nmethods directly encode embeddings for time series within the latent space of\nLLMs from scratch to align with semantic space of LLMs. Despite their\neffectiveness, we reveal that these methods conceal three inherent bottlenecks:\n(1) they struggle to encode temporal and channel-specific information in a\nlossless manner, both of which are critical components of multivariate time\nseries; (2) it is much difficult to align the learned representation space with\nthe semantic space of the LLMs; (3) they require task-specific retraining,\nwhich is both computationally expensive and labor-intensive. To bridge these\ngaps, we propose TableTime, which reformulates MTSC as a table understanding\ntask. Specifically, TableTime introduces the following strategies: (1) convert\nmultivariate time series into a tabular form, thus minimizing information loss\nto the greatest extent; (2) represent tabular time series in text format to\nachieve natural alignment with the semantic space of LLMs; (3) design a\nreasoning framework that integrates contextual text information, neighborhood\nassistance, multi-path inference and problem decomposition to enhance the\nreasoning ability of LLMs and realize zero-shot classification. Extensive\nexperiments performed on 10 publicly representative datasets from UEA archive\nverify the superiorities of the TableTime.\n","authors":["Jiahao Wang","Mingyue Cheng","Qingyang Mao","Yitong Zhou","Daoyu Wang","Qi Liu","Feiyang Xu","Xin Li"],"pdf_url":"https://arxiv.org/pdf/2411.15737v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19000v2","updated":"2025-10-28T16:16:00Z","published":"2024-05-29T11:28:06Z","title":"FedMAP: Personalised Federated Learning for Real Large-Scale Healthcare\n  Systems","summary":"  Federated learning (FL) promises to enable collaborative machine learning\nacross healthcare sites whilst preserving data privacy. Practical deployment\nremains limited by statistical heterogeneity arising from differences in\npatient demographics, treatments, and outcomes, and infrastructure constraints.\nWe introduce FedMAP, a personalised FL (PFL) framework that addresses\nheterogeneity through local Maximum a Posteriori (MAP) estimation with Input\nConvex Neural Network priors. These priors represent global knowledge gathered\nfrom other sites that guides the model while adapting to local data, and we\nprovide a formal proof of convergence. Unlike many PFL methods that rely on\nfixed regularisation, FedMAP's prior adaptively learns patterns that capture\ncomplex inter-site relationships. We demonstrate improved performance compared\nto local training, FedAvg, and several PFL methods across three large-scale\nclinical datasets: 10-year cardiovascular risk prediction (CPRD, 387 general\npractitioner practices, 258,688 patients), iron deficiency detection (INTERVAL,\n4 donor centres, 31,949 blood donors), and mortality prediction (eICU, 150\nhospitals, 44,842 patients). FedMAP incorporates a three-tier design that\nenables participation across healthcare sites with varying infrastructure and\ntechnical capabilities, from full federated training to inference-only\ndeployment. Geographical analysis reveals substantial equity improvements, with\nunderperforming regions achieving up to 14.3% performance gains. This framework\nprovides the first practical pathway for large-scale healthcare FL deployment,\nwhich ensures clinical sites at all scales can benefit, equity is enhanced, and\nprivacy is retained.\n","authors":["Fan Zhang","Daniel Kreuter","Carlos Esteve-YagÃ¼e","SÃ¶ren Dittmer","Javier Fernandez-Marques","Samantha Ip","BloodCounts! Consortium","Norbert C. J. de Wit","Angela Wood","James HF Rudd","Nicholas Lane","Nicholas S Gleadall","Carola-Bibiane SchÃ¶nlieb","Michael Roberts"],"pdf_url":"https://arxiv.org/pdf/2405.19000v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24577v1","updated":"2025-10-28T16:11:16Z","published":"2025-10-28T16:11:16Z","title":"Physics-Informed Extreme Learning Machine (PIELM): Opportunities and\n  Challenges","summary":"  We are very delighted to see the fast development of physics-informed extreme\nlearning machine (PIELM) in recent years for higher computation efficiency and\naccuracy in physics-informed machine learning. As a summary or review on PIELM\nis currently not available, we would like to take this opportunity to show our\nperspective and experience for this promising research direction. We can see\nmany efforts are made to solve PDEs with sharp gradients, nonlinearities,\nhigh-frequency behavior, hard constraints, uncertainty, multiphysics coupling.\nDespite the success, many urgent challenges remain to be tackled, which also\nprovides us opportunities to develop more robust, interpretable, and\ngeneralizable PIELM frameworks with applications in science and engineering.\n","authors":["He Yang","Fei Ren","Hai-Sui Yu","Xiaohui Chen","Pei-Zhi Zhuang"],"pdf_url":"https://arxiv.org/pdf/2510.24577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24574v1","updated":"2025-10-28T16:09:59Z","published":"2025-10-28T16:09:59Z","title":"DistDF: Time-Series Forecasting Needs Joint-Distribution Wasserstein\n  Alignment","summary":"  Training time-series forecast models requires aligning the conditional\ndistribution of model forecasts with that of the label sequence. The standard\ndirect forecast (DF) approach resorts to minimize the conditional negative\nlog-likelihood of the label sequence, typically estimated using the mean\nsquared error. However, this estimation proves to be biased in the presence of\nlabel autocorrelation. In this paper, we propose DistDF, which achieves\nalignment by alternatively minimizing a discrepancy between the conditional\nforecast and label distributions. Because conditional discrepancies are\ndifficult to estimate from finite time-series observations, we introduce a\nnewly proposed joint-distribution Wasserstein discrepancy for time-series\nforecasting, which provably upper bounds the conditional discrepancy of\ninterest. This discrepancy admits tractable, differentiable estimation from\nempirical samples and integrates seamlessly with gradient-based training.\nExtensive experiments show that DistDF improves the performance diverse\nforecast models and achieves the state-of-the-art forecasting performance. Code\nis available at https://anonymous.4open.science/r/DistDF-F66B.\n","authors":["Hao Wang","Licheng Pan","Yuan Lu","Zhixuan Chu","Xiaoxi Li","Shuting He","Zhichao Chen","Haoxuan Li","Qingsong Wen","Zhouchen Lin"],"pdf_url":"https://arxiv.org/pdf/2510.24574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05295v2","updated":"2025-10-28T16:01:40Z","published":"2025-02-07T19:56:01Z","title":"GST-UNet: A Neural Framework for Spatiotemporal Causal Inference with\n  Time-Varying Confounding","summary":"  Estimating causal effects from spatiotemporal observational data is essential\nin public health, environmental science, and policy evaluation, where\nrandomized experiments are often infeasible. Existing approaches, however,\neither rely on strong structural assumptions or fail to handle key challenges\nsuch as interference, spatial confounding, temporal carryover, and time-varying\nconfounding -- where covariates are influenced by past treatments and, in turn,\naffect future ones. We introduce GST-UNet (G-computation Spatio-Temporal UNet),\na theoretically grounded neural framework that combines a U-Net-based\nspatiotemporal encoder with regression-based iterative G-computation to\nestimate location-specific potential outcomes under complex intervention\nsequences. GST-UNet explicitly adjusts for time-varying confounders and\ncaptures non-linear spatial and temporal dependencies, enabling valid causal\ninference from a single observed trajectory in data-scarce settings. We\nvalidate its effectiveness in synthetic experiments and in a real-world\nanalysis of wildfire smoke exposure and respiratory hospitalizations during the\n2018 California Camp Fire. Together, these results position GST-UNet as a\nprincipled and ready-to-use framework for spatiotemporal causal inference,\nadvancing reliable estimation in policy-relevant and scientific domains.\n","authors":["Miruna Oprescu","David K. Park","Xihaier Luo","Shinjae Yoo","Nathan Kallus"],"pdf_url":"https://arxiv.org/pdf/2502.05295v2.pdf","comment":"29 pages, 6 figures, 6 tables, NeurIPS 2025"},{"id":"http://arxiv.org/abs/2409.11529v3","updated":"2025-10-28T15:59:49Z","published":"2024-09-17T19:59:57Z","title":"Adaptive Anomaly Detection in Network Flows with Low-Rank Tensor\n  Decompositions and Deep Unrolling","summary":"  Anomaly detection (AD) is increasingly recognized as a key component for\nensuring the resilience of future communication systems. While deep learning\nhas shown state-of-the-art AD performance, its application in critical systems\nis hindered by concerns regarding training data efficiency, domain adaptation\nand interpretability. This work considers AD in network flows using incomplete\nmeasurements, leveraging a robust tensor decomposition approach and deep\nunrolling techniques to address these challenges. We first propose a novel\nblock-successive convex approximation algorithm based on a regularized\nmodel-fitting objective where the normal flows are modeled as low-rank tensors\nand anomalies as sparse. An augmentation of the objective is introduced to\ndecrease the computational cost. We apply deep unrolling to derive a novel deep\nnetwork architecture based on our proposed algorithm, treating the\nregularization parameters as learnable weights. Inspired by Bayesian\napproaches, we extend the model architecture to perform online adaptation to\nper-flow and per-time-step statistics, improving AD performance while\nmaintaining a low parameter count and preserving the problem's permutation\nequivariances. To optimize the deep network weights for detection performance,\nwe employ a homotopy optimization approach based on an efficient approximation\nof the area under the receiver operating characteristic curve. Extensive\nexperiments on synthetic and real-world data demonstrate that our proposed deep\nnetwork architecture exhibits a high training data efficiency, outperforms\nreference methods, and adapts seamlessly to varying network topologies.\n","authors":["Lukas Schynol","Marius Pesavento"],"pdf_url":"https://arxiv.org/pdf/2409.11529v3.pdf","comment":"18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2510.24561v1","updated":"2025-10-28T15:55:36Z","published":"2025-10-28T15:55:36Z","title":"LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via\n  Asymptotic Analysis","summary":"  With the widespread adoption of LLMs, LoRA has become a dominant method for\nPEFT, and its initialization methods have attracted increasing attention.\nHowever, existing methods have notable limitations: many methods do not\nincorporate target-domain data, while gradient-based methods exploit data only\nat a shallow level by relying on one-step gradient decomposition, which remains\nunsatisfactory due to the weak empirical performance of the one-step\nfine-tuning model that serves as their basis, as well as the fact that these\nmethods either lack a rigorous theoretical foundation or depend heavily on\nrestrictive isotropic assumptions. In this paper, we establish a theoretical\nframework for data-aware LoRA initialization based on asymptotic analysis.\nStarting from a general optimization objective that minimizes the expectation\nof the parameter discrepancy between the fine-tuned and target models, we\nderive an optimization problem with two components: a bias term, which is\nrelated to the parameter distance between the fine-tuned and target models, and\nis approximated using a Fisher-gradient formulation to preserve anisotropy; and\na variance term, which accounts for the uncertainty introduced by sampling\nstochasticity through the Fisher information. By solving this problem, we\nobtain an optimal initialization strategy for LoRA. Building on this\ntheoretical framework, we develop an efficient algorithm, LoRA-DA, which\nestimates the terms in the optimization problem from a small set of target\ndomain samples and obtains the optimal LoRA initialization. Empirical results\nacross multiple benchmarks demonstrate that LoRA-DA consistently improves final\naccuracy over existing initialization methods. Additional studies show faster,\nmore stable convergence, robustness across ranks, and only a small\ninitialization overhead for LoRA-DA. The source code will be released upon\npublication.\n","authors":["Qingyue Zhang","Chang Chu","Tianren Peng","Qi Li","Xiangyang Luo","Zhihao Jiang","Shao-Lun Huang"],"pdf_url":"https://arxiv.org/pdf/2510.24561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24557v1","updated":"2025-10-28T15:51:48Z","published":"2025-10-28T15:51:48Z","title":"Enforcing boundary conditions for physics-informed neural operators","summary":"  Machine-learning based methods like physics-informed neural networks and\nphysics-informed neural operators are becoming increasingly adept at solving\neven complex systems of partial differential equations. Boundary conditions can\nbe enforced either weakly by penalizing deviations in the loss function or\nstrongly by training a solution structure that inherently matches the\nprescribed values and derivatives. The former approach is easy to implement but\nthe latter can provide benefits with respect to accuracy and training times.\nHowever, previous approaches to strongly enforcing Neumann or Robin boundary\nconditions require a domain with a fully $C^1$ boundary and, as we demonstrate,\ncan lead to instability if those boundary conditions are posed on a segment of\nthe boundary that is piecewise $C^1$ but only $C^0$ globally. We introduce a\ngeneralization of the approach by Sukumar \\& Srivastava (doi:\n10.1016/j.cma.2021.114333), and a new approach based on orthogonal projections\nthat overcome this limitation. The performance of these new techniques is\ncompared against weakly and semi-weakly enforced boundary conditions for the\nscalar Darcy flow equation and the stationary Navier-Stokes equations.\n","authors":["Niklas GÃ¶schel","Sebastian GÃ¶tschel","Daniel Ruprecht"],"pdf_url":"https://arxiv.org/pdf/2510.24557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22931v2","updated":"2025-10-28T15:51:13Z","published":"2025-10-27T02:15:51Z","title":"Robust Uncertainty Quantification for Self-Evolving Large Language\n  Models via Continual Domain Pretraining","summary":"  Continual Learning (CL) is essential for enabling self-evolving large\nlanguage models (LLMs) to adapt and remain effective amid rapid knowledge\ngrowth. Yet, despite its importance, little attention has been given to\nestablishing statistical reliability guarantees for LLMs under CL, particularly\nin the setting of continual domain pretraining (CDP). Conformal Prediction (CP)\nhas shown promise in offering correctness guarantees for LLMs, but it faces\nmajor challenges in CDP: testing data often stems from unknown or shifting\ndomain distributions, under which CP may no longer provide valid guarantees.\nMoreover, when high coverage is required, CP can yield excessively large\nprediction sets for unanswerable queries, reducing informativeness. To address\nthese challenges, we introduce an adaptive rejection and non-exchangeable CP\nframework. Our method first estimates the distribution of questions across\ndomains in the test set using transformer-based clustering, then reweights or\nresamples the calibration data accordingly. Building on this, adaptive\nrejection CP allows the LLM to selectively abstain from answering when its\nconfidence or competence shifts significantly. Extensive experiments\ndemonstrate that our framework enhances both the effectiveness and reliability\nof CP under CDP scenarios. Our code is available at:\nhttps://anonymous.4open.science/r/CPCL-8C12/\n","authors":["Xiaofan Zhou","Lu Cheng"],"pdf_url":"https://arxiv.org/pdf/2510.22931v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18976v3","updated":"2025-10-28T15:46:13Z","published":"2025-05-25T04:58:57Z","title":"GraSS: Scalable Data Attribution with Gradient Sparsification and Sparse\n  Projection","summary":"  Gradient-based data attribution methods, such as influence functions, are\ncritical for understanding the impact of individual training samples without\nrequiring repeated model retraining. However, their scalability is often\nlimited by the high computational and memory costs associated with per-sample\ngradient computation. In this work, we propose GraSS, a novel gradient\ncompression algorithm and its variants FactGraSS for linear layers\nspecifically, that explicitly leverage the inherent sparsity of per-sample\ngradients to achieve sub-linear space and time complexity. Extensive\nexperiments demonstrate the effectiveness of our approach, achieving\nsubstantial speedups while preserving data influence fidelity. In particular,\nFactGraSS achieves up to 165% faster throughput on billion-scale models\ncompared to the previous state-of-the-art baselines. Our code is publicly\navailable at https://github.com/TRAIS-Lab/GraSS.\n","authors":["Pingbang Hu","Joseph Melkonian","Weijing Tang","Han Zhao","Jiaqi W. Ma"],"pdf_url":"https://arxiv.org/pdf/2505.18976v3.pdf","comment":"Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.24546v1","updated":"2025-10-28T15:45:15Z","published":"2025-10-28T15:45:15Z","title":"Dual-Mind World Models: A General Framework for Learning in Dynamic\n  Wireless Networks","summary":"  Despite the popularity of reinforcement learning (RL) in wireless networks,\nexisting approaches that rely on model-free RL (MFRL) and model-based RL (MBRL)\nare data inefficient and short-sighted. Such RL-based solutions cannot\ngeneralize to novel network states since they capture only statistical patterns\nrather than the underlying physics and logic from wireless data. These\nlimitations become particularly challenging in complex wireless networks with\nhigh dynamics and long-term planning requirements. To address these\nlimitations, in this paper, a novel dual-mind world model-based learning\nframework is proposed with the goal of optimizing completeness-weighted age of\ninformation (CAoI) in a challenging mmWave V2X scenario. Inspired by cognitive\npsychology, the proposed dual-mind world model encompasses a pattern-driven\nSystem 1 component and a logic-driven System 2 component to learn dynamics and\nlogic of the wireless network, and to provide long-term link scheduling over\nreliable imagined trajectories. Link scheduling is learned through end-to-end\ndifferentiable imagined trajectories with logical consistency over an extended\nhorizon rather than relying on wireless data obtained from environment\ninteractions. Moreover, through imagination rollouts, the proposed world model\ncan jointly reason network states and plan link scheduling. During intervals\nwithout observations, the proposed method remains capable of making efficient\ndecisions. Extensive experiments are conducted on a realistic simulator based\non Sionna with real-world physical channel, ray-tracing, and scene objects with\nmaterial properties. Simulation results show that the proposed world model\nachieves a significant improvement in data efficiency and achieves strong\ngeneralization and adaptation to unseen environments, compared to the\nstate-of-the-art RL baselines, and the world model approach with only System 1.\n","authors":["Lingyi Wang","Rashed Shelim","Walid Saad","Naren Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2510.24546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.07530v3","updated":"2025-10-28T15:36:36Z","published":"2023-01-18T13:48:20Z","title":"Online (Non-)Convex Learning via Tempered Optimism","summary":"  Optimistic Online Learning aims to exploit experts conveying reliable\ninformation to predict the future. However, such implicit optimism may be\nchallenged when it comes to practical crafting of such experts. A fundamental\nexample consists in approximating a minimiser of the current problem and use it\nas expert. In the context of dynamic environments, such an expert only conveys\npartially relevant information as it may lead to overfitting. To tackle this\nissue, we introduce in this work the \\emph{optimistically tempered} (OT) online\nlearning framework designed to handle such imperfect experts. As a first\ncontribution, we show that tempered optimism is a fruitful paradigm for Online\nNon-Convex Learning by proposing simple, yet powerful modification of Online\nGradient and Mirror Descent. Second, we derive a second OT algorithm for convex\nlosses and third, evaluate the practical efficiency of tempered optimism on\nreal-life datasets and a toy experiment.\n","authors":["Maxime Haddouche","Olivier Wintenberger","Benjamin Guedj"],"pdf_url":"https://arxiv.org/pdf/2301.07530v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24523v1","updated":"2025-10-28T15:34:23Z","published":"2025-10-28T15:34:23Z","title":"Unsupervised Machine-Learning Pipeline for Data-Driven Defect Detection\n  and Characterisation: Application to Displacement Cascades","summary":"  Neutron irradiation produces, within a few picoseconds, displacement cascades\nthat are sequences of atomic collisions generating point and extended defects\nwhich subsequently affects the long-term evolution of materials. The diversity\nof these defects, characterized morphologically and statistically, defines what\nis called the \"primary damage\". In this work, we present a fully unsupervised\nmachine learning (ML) workflow that detects and classifies these defects\ndirectly from molecular dynamics data. Local environments are encoded by the\nSmooth Overlap of Atomic Positions (SOAP) vector, anomalous atoms are isolated\nwith autoencoder neural networks (AE), embedded with Uniform Manifold\nApproximation and Projection (UMAP) and clustered using Hierarchical\nDensity-Based Spatial Clustering of Applications with Noise (HDBSCAN). Applied\nto 80 keV displacement cascades in Ni, Fe$_7$0Ni$_{10}$Cr$_{20}$, and Zr, the\nAE successfully identify the small fraction of outlier atoms that participate\nin defect formation. HDBSCAN then partitions the UMAP latent space of\nAE-flagged SOAP descriptors into well defined groups representing vacancy- and\ninterstitial-dominated regions and, within each, separates small from large\naggregates, assigning 99.7 % of outliers to compact physical motifs. A signed\ncluster-identification score confirms this separation, and cluster size scales\nwith net defect counts (R2 > 0.89). Statistical cross analyses between the ML\noutlier map and several conventional detectors (centrosymmetry, dislocation\nextraction, etc.) reveal strong overlap and complementary coverage, all\nachieved without template or threshold tuning. This ML workflow thus provides\nan efficient tool for the quantitative mapping of structural anomalies in\nmaterials, particularly those arising from irradiation damage in displacement\ncascades.\n","authors":["Samuel Del FrÃ©","AndrÃ©e de Backer","Christophe Domain","Ludovic Thuinet","Charlotte S. Becquart"],"pdf_url":"https://arxiv.org/pdf/2510.24523v1.pdf","comment":"22 pages, 1 graphical abstract, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2508.02293v2","updated":"2025-10-28T15:28:13Z","published":"2025-08-04T11:03:12Z","title":"Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning","summary":"  So-called unsupervised anomaly detection is better described as\nsemi-supervised, as it assumes all training data are nominal. This assumption\nsimplifies training but requires manual data curation, introducing bias and\nlimiting adaptability. We propose Confident Meta-learning (CoMet), a novel\ntraining strategy that enables deep anomaly detection models to learn from\nuncurated datasets where nominal and anomalous samples coexist, eliminating the\nneed for explicit filtering. Our approach integrates Soft Confident Learning,\nwhich assigns lower weights to low-confidence samples, and Meta-Learning, which\nstabilizes training by regularizing updates based on training validation loss\ncovariance. This prevents overfitting and enhances robustness to noisy data.\nCoMet is model-agnostic and can be applied to any anomaly detection method\ntrainable via gradient descent. Experiments on MVTec-AD, VIADUCT, and KSDD2\nwith two state-of-the-art models demonstrate the effectiveness of our approach,\nconsistently improving over the baseline methods, remaining insensitive to\nanomalies in the training set, and setting a new state-of-the-art across all\ndatasets. Code is available at https://github.com/aqeeelmirza/CoMet\n","authors":["Muhammad Aqeel","Shakiba Sharifi","Marco Cristani","Francesco Setti"],"pdf_url":"https://arxiv.org/pdf/2508.02293v2.pdf","comment":"Accepted to IEEE/CVF International Conference on Computer Vision\n  (ICCV2025)"},{"id":"http://arxiv.org/abs/2501.00913v2","updated":"2025-10-28T15:26:34Z","published":"2025-01-01T18:12:18Z","title":"$Î²$-DQN: Improving Deep Q-Learning By Evolving the Behavior","summary":"  While many sophisticated exploration methods have been proposed, their lack\nof generality and high computational cost often lead researchers to favor\nsimpler methods like $\\epsilon$-greedy. Motivated by this, we introduce\n$\\beta$-DQN, a simple and efficient exploration method that augments the\nstandard DQN with a behavior function $\\beta$. This function estimates the\nprobability that each action has been taken at each state. By leveraging\n$\\beta$, we generate a population of diverse policies that balance exploration\nbetween state-action coverage and overestimation bias correction. An adaptive\nmeta-controller is designed to select an effective policy for each episode,\nenabling flexible and explainable exploration. $\\beta$-DQN is straightforward\nto implement and adds minimal computational overhead to the standard DQN.\nExperiments on both simple and challenging exploration domains show that\n$\\beta$-DQN outperforms existing baseline methods across a wide range of tasks,\nproviding an effective solution for improving exploration in deep reinforcement\nlearning.\n","authors":["Hongming Zhang","Fengshuo Bai","Chenjun Xiao","Chao Gao","Bo Xu","Martin MÃ¼ller"],"pdf_url":"https://arxiv.org/pdf/2501.00913v2.pdf","comment":"aamas 2025"},{"id":"http://arxiv.org/abs/2506.00799v3","updated":"2025-10-28T15:20:47Z","published":"2025-06-01T03:00:09Z","title":"Uni-LoRA: One Vector is All You Need","summary":"  Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient\nfine-tuning (PEFT) method for large language models (LLMs) by constraining\nweight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and\nVB-LoRA push efficiency further by introducing additional constraints to reduce\nthe trainable parameter space. In this paper, we show that the parameter space\nreduction strategies employed by these LoRA variants can be formulated within a\nunified framework, Uni-LoRA, where the LoRA parameter space, flattened as a\nhigh-dimensional vector space $R^D$, can be reconstructed through a projection\nfrom a subspace R^d, with $d \\ll D$. We demonstrate that the fundamental\ndifference among various LoRA methods lies in the choice of the projection\nmatrix, $P \\in R^{D \\times d}$.Most existing LoRA variants rely on layer-wise\nor structure-specific projections that limit cross-layer parameter sharing,\nthereby compromising parameter efficiency. In light of this, we introduce an\nefficient and theoretically grounded projection matrix that is isometric,\nenabling global parameter sharing and reducing computation overhead.\nFurthermore, under the unified view of Uni-LoRA, this design requires only a\nsingle trainable vector to reconstruct LoRA parameters for the entire LLM -\nmaking Uni-LoRA both a unified framework and a \"one-vector-only\" solution.\nExtensive experiments on GLUE, mathematical reasoning, and instruction tuning\nbenchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter\nefficiency while outperforming or matching prior approaches in predictive\nperformance. Our code is available at\nhttps://github.com/KaiyangLi1992/Uni-LoRA.\n","authors":["Kaiyang Li","Shaobo Han","Qing Su","Wei Li","Zhipeng Cai","Shihao Ji"],"pdf_url":"https://arxiv.org/pdf/2506.00799v3.pdf","comment":"NeurIPS 2025 Spotlight"},{"id":"http://arxiv.org/abs/2510.24503v1","updated":"2025-10-28T15:15:14Z","published":"2025-10-28T15:15:14Z","title":"Local Performance vs. Out-of-Distribution Generalization: An Empirical\n  Analysis of Personalized Federated Learning in Heterogeneous Data\n  Environments","summary":"  In the context of Federated Learning with heterogeneous data environments,\nlocal models tend to converge to their own local model optima during local\ntraining steps, deviating from the overall data distributions. Aggregation of\nthese local updates, e.g., with FedAvg, often does not align with the global\nmodel optimum (client drift), resulting in an update that is suboptimal for\nmost clients. Personalized Federated Learning approaches address this challenge\nby exclusively focusing on the average local performances of clients' models on\ntheir own data distribution. Generalization to out-of-distribution samples,\nwhich is a substantial benefit of FedAvg and represents a significant component\nof robustness, appears to be inadequately incorporated into the assessment and\nevaluation processes. This study involves a thorough evaluation of Federated\nLearning approaches, encompassing both their local performance and their\ngeneralization capabilities. Therefore, we examine different stages within a\nsingle communication round to enable a more nuanced understanding of the\nconsidered metrics. Furthermore, we propose and incorporate a modified approach\nof FedAvg, designated as Federated Learning with Individualized Updates (FLIU),\nextending the algorithm by a straightforward individualization step with an\nadaptive personalization factor. We evaluate and compare the approaches\nempirically using MNIST and CIFAR-10 under various distributional conditions,\nincluding benchmark IID and pathological non-IID, as well as additional novel\ntest environments with Dirichlet distribution specifically developed to stress\nthe algorithms on complex data heterogeneity.\n","authors":["Mortesa Hussaini","Jan TheiÃ","Anthony Stein"],"pdf_url":"https://arxiv.org/pdf/2510.24503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24500v1","updated":"2025-10-28T15:13:38Z","published":"2025-10-28T15:13:38Z","title":"MIMIC-Sepsis: A Curated Benchmark for Modeling and Learning from Sepsis\n  Trajectories in the ICU","summary":"  Sepsis is a leading cause of mortality in intensive care units (ICUs), yet\nexisting research often relies on outdated datasets, non-reproducible\npreprocessing pipelines, and limited coverage of clinical interventions. We\nintroduce MIMIC-Sepsis, a curated cohort and benchmark framework derived from\nthe MIMIC-IV database, designed to support reproducible modeling of sepsis\ntrajectories. Our cohort includes 35,239 ICU patients with time-aligned\nclinical variables and standardized treatment data, including vasopressors,\nfluids, mechanical ventilation and antibiotics. We describe a transparent\npreprocessing pipeline-based on Sepsis-3 criteria, structured imputation\nstrategies, and treatment inclusion-and release it alongside benchmark tasks\nfocused on early mortality prediction, length-of-stay estimation, and shock\nonset classification. Empirical results demonstrate that incorporating\ntreatment variables substantially improves model performance, particularly for\nTransformer-based architectures. MIMIC-Sepsis serves as a robust platform for\nevaluating predictive and sequential models in critical care research.\n","authors":["Yong Huang","Zhongqi Yang","Amir Rahmani"],"pdf_url":"https://arxiv.org/pdf/2510.24500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10978v3","updated":"2025-10-28T15:11:36Z","published":"2025-05-16T08:26:59Z","title":"Group-in-Group Policy Optimization for LLM Agent Training","summary":"  Recent advances in group-based reinforcement learning (RL) have driven\nfrontier large language models (LLMs) in single-turn tasks like mathematical\nreasoning. However, their scalability to multi-turn LLM agent training remains\nlimited. Unlike static tasks, agent-environment interactions unfold over many\nsteps and often yield sparse or delayed rewards, making credit assignment\nacross individual steps significantly more challenging. In this work, we\npropose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that\nachieves fine-grained credit assignment for LLM agents while preserving the\nappealing properties of group-based RL: critic-free, low memory, and stable\nconvergence. GiGPO introduces a two-level structure for estimating relative\nadvantage: (i) At the episode-level, GiGPO computes macro relative advantages\nbased on groups of complete trajectories; (ii) At the step-level, GiGPO\nintroduces an anchor state grouping mechanism that retroactively constructs\nstep-level groups by identifying repeated environment states across\ntrajectories. Actions stemming from the same state are grouped together,\nenabling micro relative advantage estimation. This hierarchical structure\neffectively captures both global trajectory quality and local step\neffectiveness without relying on auxiliary models or additional rollouts. We\nevaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop,\nas well as tool-integrated reasoning on search-augmented QA tasks, using\nQwen2.5-1.5B/3B/7B-Instruct. Crucially, GiGPO delivers fine-grained per-step\ncredit signals, achieves performance gains of > 12% on ALFWorld and > 9% on\nWebShop over GRPO, and obtains superior performance on QA tasks (42.1% on 3B\nand 47.2% on 7B): all while maintaining the same GPU memory overhead, identical\nLLM rollout, and incurring little to no additional time cost.\n","authors":["Lang Feng","Zhenghai Xue","Tingcong Liu","Bo An"],"pdf_url":"https://arxiv.org/pdf/2505.10978v3.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2406.04378v3","updated":"2025-10-28T15:03:25Z","published":"2024-06-05T22:18:36Z","title":"TIDMAD: Time Series Dataset for Discovering Dark Matter with AI\n  Denoising","summary":"  Dark matter makes up approximately 85% of total matter in our universe, yet\nit has never been directly observed in any laboratory on Earth. The origin of\ndark matter is one of the most important questions in contemporary physics, and\na convincing detection of dark matter would be a Nobel-Prize-level breakthrough\nin fundamental science. The ABRACADABRA experiment was specifically designed to\nsearch for dark matter. Although it has not yet made a discovery, ABRACADABRA\nhas produced several dark matter search results widely endorsed by the physics\ncommunity. The experiment generates ultra-long time-series data at a rate of 10\nmillion samples per second, where the dark matter signal would manifest itself\nas a sinusoidal oscillation mode within the ultra-long time series. In this\npaper, we present the TIDMAD -- a comprehensive data release from the\nABRACADABRA experiment including three key components: an ultra-long time\nseries dataset divided into training, validation, and science subsets; a\ncarefully-designed denoising score for direct model benchmarking; and a\ncomplete analysis framework which produces a community-standard dark matter\nsearch result suitable for publication as a physics paper. This data release\nenables core AI algorithms to extract the dark matter signal and produce real\nphysics results thereby advancing fundamental science. The data downloading and\nassociated analysis scripts are available at\nhttps://github.com/jessicafry/TIDMAD\n","authors":["J. T. Fry","Xinyi Hope Fu","Zhenghao Fu","Kaliroe M. W. Pappas","Lindley Winslow","Aobo Li"],"pdf_url":"https://arxiv.org/pdf/2406.04378v3.pdf","comment":"Accepted by NeurIPS 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2503.08748v4","updated":"2025-10-28T15:01:16Z","published":"2025-03-11T10:50:07Z","title":"Mirror Descent and Novel Exponentiated Gradient Algorithms Using\n  Trace-Form Entropies and Deformed Logarithms","summary":"  This paper introduces a broad class of Mirror Descent (MD) and Generalized\nExponentiated Gradient (GEG) algorithms derived from trace-form entropies\ndefined via deformed logarithms. Leveraging these generalized entropies yields\nMD \\& GEG algorithms with improved convergence behavior, robustness to\nvanishing and exploding gradients, and inherent adaptability to non-Euclidean\ngeometries through mirror maps. We establish deep connections between these\nmethods and Amari's natural gradient, revealing a unified geometric foundation\nfor additive, multiplicative, and natural gradient updates. Focusing on the\nTsallis, Kaniadakis, Sharma--Taneja--Mittal, and Kaniadakis--Lissia--Scarfone\nentropy families, we show that each entropy induces a distinct Riemannian\nmetric on the parameter space, leading to GEG algorithms that preserve the\nnatural statistical geometry. The tunable parameters of deformed logarithms\nenable adaptive geometric selection, providing enhanced robustness and\nconvergence over classical Euclidean optimization. Overall, our framework\nunifies key first-order MD optimization methods under a single\ninformation-geometric perspective based on generalized Bregman divergences,\nwhere the choice of entropy determines the underlying metric and dual geometric\nstructure.\n","authors":["Andrzej Cichocki","Toshihisa Tanaka","Frank Nielsen","Sergio Cruces"],"pdf_url":"https://arxiv.org/pdf/2503.08748v4.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.24482v1","updated":"2025-10-28T14:54:12Z","published":"2025-10-28T14:54:12Z","title":"Sample-efficient and Scalable Exploration in Continuous-Time RL","summary":"  Reinforcement learning algorithms are typically designed for discrete-time\ndynamics, even though the underlying real-world control systems are often\ncontinuous in time. In this paper, we study the problem of continuous-time\nreinforcement learning, where the unknown system dynamics are represented using\nnonlinear ordinary differential equations (ODEs). We leverage probabilistic\nmodels, such as Gaussian processes and Bayesian neural networks, to learn an\nuncertainty-aware model of the underlying ODE. Our algorithm, COMBRL, greedily\nmaximizes a weighted sum of the extrinsic reward and model epistemic\nuncertainty. This yields a scalable and sample-efficient approach to\ncontinuous-time model-based RL. We show that COMBRL achieves sublinear regret\nin the reward-driven setting, and in the unsupervised RL setting (i.e., without\nextrinsic rewards), we provide a sample complexity bound. In our experiments,\nwe evaluate COMBRL in both standard and unsupervised RL settings and\ndemonstrate that it scales better, is more sample-efficient than prior methods,\nand outperforms baselines across several deep RL tasks.\n","authors":["Klemens Iten","Lenart Treven","Bhavya Sukhija","Florian DÃ¶rfler","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2510.24482v1.pdf","comment":"26 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.21142v2","updated":"2025-10-28T14:49:07Z","published":"2025-02-28T15:24:17Z","title":"Multimodal Dreaming: A Global Workspace Approach to World Model-Based\n  Reinforcement Learning","summary":"  Humans leverage rich internal models of the world to reason about the future,\nimagine counterfactuals, and adapt flexibly to new situations. In Reinforcement\nLearning (RL), world models aim to capture how the environment evolves in\nresponse to the agent's actions, facilitating planning and generalization.\nHowever, typical world models directly operate on the environment variables\n(e.g. pixels, physical attributes), which can make their training slow and\ncumbersome; instead, it may be advantageous to rely on high-level latent\ndimensions that capture relevant multimodal variables. Global Workspace (GW)\nTheory offers a cognitive framework for multimodal integration and information\nbroadcasting in the brain, and recent studies have begun to introduce efficient\ndeep learning implementations of GW. Here, we evaluate the capabilities of an\nRL system combining GW with a world model. We compare our GW-Dreamer with\nvarious versions of the standard PPO and the original Dreamer algorithms. We\nshow that performing the dreaming process (i.e., mental simulation) inside the\nGW latent space allows for training with fewer environment steps. As an\nadditional emergent property, the resulting model (but not its comparison\nbaselines) displays strong robustness to the absence of one of its observation\nmodalities (images or simulation attributes). We conclude that the combination\nof GW with World Models holds great potential for improving decision-making in\nRL agents.\n","authors":["LÃ©opold MaytiÃ©","Roland Bertin Johannet","Rufin VanRullen"],"pdf_url":"https://arxiv.org/pdf/2502.21142v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2510.03534v2","updated":"2025-10-28T14:48:21Z","published":"2025-10-03T22:08:08Z","title":"Long-Term Mapping of the Douro River Plume with Multi-Agent\n  Reinforcement Learning","summary":"  We study the problem of long-term (multiple days) mapping of a river plume\nusing multiple autonomous underwater vehicles (AUVs), focusing on the Douro\nriver representative use-case. We propose an energy - and communication -\nefficient multi-agent reinforcement learning approach in which a central\ncoordinator intermittently communicates with the AUVs, collecting measurements\nand issuing commands. Our approach integrates spatiotemporal Gaussian process\nregression (GPR) with a multi-head Q-network controller that regulates\ndirection and speed for each AUV. Simulations using the Delft3D ocean model\ndemonstrate that our method consistently outperforms both single- and\nmulti-agent benchmarks, with scaling the number of agents both improving mean\nsquared error (MSE) and operational endurance. In some instances, our algorithm\ndemonstrates that doubling the number of AUVs can more than double endurance\nwhile maintaining or improving accuracy, underscoring the benefits of\nmulti-agent coordination. Our learned policies generalize across unseen\nseasonal regimes over different months and years, demonstrating promise for\nfuture developments of data-driven long-term monitoring of dynamic plume\nenvironments.\n","authors":["NicolÃ² Dal Fabbro","Milad Mesbahi","Renato Mendes","JoÃ£o Borges de Sousa","George J. Pappas"],"pdf_url":"https://arxiv.org/pdf/2510.03534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22830v2","updated":"2025-10-28T14:43:58Z","published":"2025-10-26T20:59:22Z","title":"Exploration of Summarization by Generative Language Models for Automated\n  Scoring of Long Essays","summary":"  BERT and its variants are extensively explored for automated scoring.\nHowever, a limit of 512 tokens for these encoder-based models showed the\ndeficiency in automated scoring of long essays. Thus, this research explores\ngenerative language models for automated scoring of long essays via\nsummarization and prompting. The results revealed great improvement of scoring\naccuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab\nAutomated Essay Scoring 2.0 dataset.\n","authors":["Haowei Hua","Hong Jiao","Xinyi Wang"],"pdf_url":"https://arxiv.org/pdf/2510.22830v2.pdf","comment":"19 pages, 5 Tables 7 Figures, Presentation at Artificial Intelligence\n  in Measurement and Education Conference (AIME-Con)"},{"id":"http://arxiv.org/abs/2510.24473v1","updated":"2025-10-28T14:42:28Z","published":"2025-10-28T14:42:28Z","title":"Methodology for Comparing Machine Learning Algorithms for Survival\n  Analysis","summary":"  This study presents a comparative methodological analysis of six machine\nlearning models for survival analysis (MLSA). Using data from nearly 45,000\ncolorectal cancer patients in the Hospital-Based Cancer Registries of S\\~ao\nPaulo, we evaluated Random Survival Forest (RSF), Gradient Boosting for\nSurvival Analysis (GBSA), Survival SVM (SSVM), XGBoost-Cox (XGB-Cox),\nXGBoost-AFT (XGB-AFT), and LightGBM (LGBM), capable of predicting survival\nconsidering censored data. Hyperparameter optimization was performed with\ndifferent samplers, and model performance was assessed using the Concordance\nIndex (C-Index), C-Index IPCW, time-dependent AUC, and Integrated Brier Score\n(IBS). Survival curves produced by the models were compared with predictions\nfrom classification algorithms, and predictor interpretation was conducted\nusing SHAP and permutation importance. XGB-AFT achieved the best performance\n(C-Index = 0.7618; IPCW = 0.7532), followed by GBSA and RSF. The results\nhighlight the potential and applicability of MLSA to improve survival\nprediction and support decision making.\n","authors":["Lucas Buk Cardoso","Simone Aldrey Angelo","Yasmin Pacheco Gil Bonilha","Fernando Maia","Adeylson GuimarÃ£es Ribeiro","Maria Paula Curado","Gisele Aparecida Fernandes","Vanderlei Cunha Parro","FlÃ¡vio Almeida de MagalhÃ£es Cipparrone","Alexandre Dias Porto Chiavegatto Filho","Tatiana Natasha Toporcov"],"pdf_url":"https://arxiv.org/pdf/2510.24473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.20039v3","updated":"2025-10-28T14:35:32Z","published":"2025-04-28T17:59:28Z","title":"AutoJudge: Judge Decoding Without Manual Annotation","summary":"  We introduce AutoJudge, a method that accelerates large language model (LLM)\ninference with task-specific lossy speculative decoding. Instead of matching\nthe original model output distribution token-by-token, we identify which of the\ngenerated tokens affect the downstream quality of the response, relaxing the\ndistribution match guarantee so that the \"unimportant\" tokens can be generated\nfaster. Our approach relies on a semi-greedy search algorithm to test which of\nthe mismatches between target and draft models should be corrected to preserve\nquality and which ones may be skipped. We then train a lightweight classifier\nbased on existing LLM embeddings to predict, at inference time, which\nmismatching tokens can be safely accepted without compromising the final answer\nquality. We evaluate the effectiveness of AutoJudge with multiple draft/target\nmodel pairs on mathematical reasoning and programming benchmarks, achieving\nsignificant speedups at the cost of a minor accuracy reduction. Notably, on\nGSM8k with the Llama 3.1 70B target model, our approach achieves up to\n$\\approx2\\times$ speedup over speculative decoding at the cost of $\\le 1\\%$\ndrop in accuracy. When applied to the LiveCodeBench benchmark, AutoJudge\nautomatically detects programming-specific important tokens, accepting $\\ge 25$\ntokens per speculation cycle at $2\\%$ drop in Pass@1. Our approach requires no\nhuman annotation and is easy to integrate with modern LLM inference frameworks.\n","authors":["Roman Garipov","Fedor Velikonivtsev","Ivan Ermakov","Ruslan Svirschevski","Vage Egiazarian","Max Ryabinin"],"pdf_url":"https://arxiv.org/pdf/2504.20039v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24466v1","updated":"2025-10-28T14:34:33Z","published":"2025-10-28T14:34:33Z","title":"Non-Singularity of the Gradient Descent map for Neural Networks with\n  Piecewise Analytic Activations","summary":"  The theory of training deep networks has become a central question of modern\nmachine learning and has inspired many practical advancements. In particular,\nthe gradient descent (GD) optimization algorithm has been extensively studied\nin recent years. A key assumption about GD has appeared in several recent\nworks: the \\emph{GD map is non-singular} -- it preserves sets of measure zero\nunder preimages. Crucially, this assumption has been used to prove that GD\navoids saddle points and maxima, and to establish the existence of a computable\nquantity that determines the convergence to global minima (both for GD and\nstochastic GD). However, the current literature either assumes the\nnon-singularity of the GD map or imposes restrictive assumptions, such as\nLipschitz smoothness of the loss (for example, Lipschitzness does not hold for\ndeep ReLU networks with the cross-entropy loss) and restricts the analysis to\nGD with small step-sizes. In this paper, we investigate the neural network map\nas a function on the space of weights and biases. We also prove, for the first\ntime, the non-singularity of the gradient descent (GD) map on the loss\nlandscape of realistic neural network architectures (with fully connected,\nconvolutional, or softmax attention layers) and piecewise analytic activations\n(which includes sigmoid, ReLU, leaky ReLU, etc.) for almost all step-sizes. Our\nwork significantly extends the existing results on the convergence of GD and\nSGD by guaranteeing that they apply to practical neural network settings and\nhas the potential to unlock further exploration of learning dynamics.\n","authors":["Alexandru CrÄciun","Debarghya Ghoshdastidar"],"pdf_url":"https://arxiv.org/pdf/2510.24466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22911v2","updated":"2025-10-28T14:33:37Z","published":"2025-10-27T01:28:57Z","title":"Towards Personalized Treatment Plan: Geometrical Model-Agnostic Approach\n  to Counterfactual Explanations","summary":"  In our article, we describe a method for generating counterfactual\nexplanations in high-dimensional spaces using four steps that involve fitting\nour dataset to a model, finding the decision boundary, determining constraints\non the problem, and computing the closest point (counterfactual explanation)\nfrom that boundary. We propose a discretized approach where we find many\ndiscrete points on the boundary and then identify the closest feasible\ncounterfactual explanation. This method, which we later call $\\textit{Segmented\nSampling for Boundary Approximation}$ (SSBA), applies binary search to find\ndecision boundary points and then searches for the closest boundary point.\nAcross four datasets of varying dimensionality, we show that our method can\noutperform current methods for counterfactual generation with reductions in\ndistance between $5\\%$ to $50\\%$ in terms of the $L_2$ norm. Our method can\nalso handle real-world constraints by restricting changes to immutable and\ncategorical features, such as age, gender, sex, height, and other related\ncharacteristics such as the case for a health-based dataset. In terms of\nruntime, the SSBA algorithm generates decision boundary points on multiple\norders of magnitude in the same given time when we compare to a grid-based\napproach. In general, our method provides a simple and effective model-agnostic\nmethod that can compute nearest feasible (i.e. realistic with constraints)\ncounterfactual explanations. All of our results and code are available at:\nhttps://github.com/dsin85691/SSBA_For_Counterfactuals\n","authors":["Daniel Sin","Milad Toutounchian"],"pdf_url":"https://arxiv.org/pdf/2510.22911v2.pdf","comment":"This paper is 15 pages long consisting of multiple sections including\n  an abstract, introduction, related works, methodology, results, ablation\n  studies, conclusion, future works, and an appendix section. There are 10\n  figures and 5 tables in total"},{"id":"http://arxiv.org/abs/2508.20072v2","updated":"2025-10-28T14:22:20Z","published":"2025-08-27T17:39:11Z","title":"Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding\n  in Vision-Language-Action Policies","summary":"  Vision-Language-Action (VLA) models adapt large vision-language backbones to\nmap images and instructions into robot actions. However, prevailing VLAs either\ngenerate actions auto-regressively in a fixed left-to-right order or attach\nseparate MLP or diffusion heads outside the backbone, leading to fragmented\ninformation pathways and specialized training requirements that hinder a\nunified, scalable architecture. We present Discrete Diffusion VLA, a\nunified-transformer policy that models discretized action chunks with discrete\ndiffusion. The design retains diffusion's progressive refinement paradigm while\nremaining natively compatible with the discrete token interface of VLMs. Our\nmethod achieves an adaptive decoding order that resolves easy action elements\nbefore harder ones and uses secondary re-masking to revisit uncertain\npredictions across refinement rounds, which improves consistency and enables\nrobust error correction. This unified decoder preserves pre-trained\nvision-language priors, supports parallel decoding, breaks the autoregressive\nbottleneck, and reduces the number of function evaluations. Discrete Diffusion\nVLA achieves 96.3% avg. success rates on LIBERO, 71.2% visual matching on\nSimplerEnv-Fractal and 54.2% overall on SimplerEnv-Bridge, improving over\nautoregressive, MLP decoder and continuous diffusion baselines. These findings\nindicate that discrete-diffusion VLA supports precise action modeling and\nconsistent training, laying groundwork for scaling VLA to larger models and\ndatasets. Our project page is https://github.com/Liang-ZX/DiscreteDiffusionVLA\n","authors":["Zhixuan Liang","Yizhuo Li","Tianshuo Yang","Chengyue Wu","Sitong Mao","Tian Nian","Liuao Pei","Shunbo Zhou","Xiaokang Yang","Jiangmiao Pang","Yao Mu","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2508.20072v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2504.06923v4","updated":"2025-10-28T14:21:34Z","published":"2025-04-09T14:30:30Z","title":"The Importance of Being Discrete: Measuring the Impact of Discretization\n  in End-to-End Differentially Private Synthetic Data","summary":"  Differentially Private (DP) generative marginal models are often used in the\nwild to release synthetic tabular datasets in lieu of sensitive data while\nproviding formal privacy guarantees. These models approximate low-dimensional\nmarginals or query workloads; crucially, they require the training data to be\npre-discretized, i.e., continuous values need to first be partitioned into\nbins. However, as the range of values (or their domain) is often inferred\ndirectly from the training data, with the number of bins and bin edges\ntypically defined arbitrarily, this approach can ultimately break end-to-end DP\nguarantees and may not always yield optimal utility.\n  In this paper, we present an extensive measurement study of four\ndiscretization strategies in the context of DP marginal generative models. More\nprecisely, we design DP versions of three discretizers (uniform, quantile, and\nk-means) and reimplement the PrivTree algorithm. We find that optimizing both\nthe choice of discretizer and bin count can improve utility, on average, by\nalmost 30% across six DP marginal models, compared to the default strategy and\nnumber of bins, with PrivTree being the best-performing discretizer in the\nmajority of cases. We demonstrate that, while DP generative models with\nnon-private discretization remain vulnerable to membership inference attacks,\napplying DP during discretization effectively mitigates this risk. Finally, we\nimprove on an existing approach for automatically selecting the optimal number\nof bins, and achieve high utility while reducing both privacy budget\nconsumption and computational overhead.\n","authors":["Georgi Ganev","Meenatchi Sundaram Muthu Selva Annamalai","Sofiane Mahiou","Emiliano De Cristofaro"],"pdf_url":"https://arxiv.org/pdf/2504.06923v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24452v1","updated":"2025-10-28T14:18:50Z","published":"2025-10-28T14:18:50Z","title":"ARIMA_PLUS: Large-scale, Accurate, Automatic and Interpretable\n  In-Database Time Series Forecasting and Anomaly Detection in Google BigQuery","summary":"  Time series forecasting and anomaly detection are common tasks for\npractitioners in industries such as retail, manufacturing, advertising and\nenergy. Two unique challenges stand out: (1) efficiently and accurately\nforecasting time series or detecting anomalies in large volumes automatically;\nand (2) ensuring interpretability of results to effectively incorporate\nbusiness insights. We present ARIMA_PLUS, a novel framework to overcome these\ntwo challenges by a unique combination of (a) accurate and interpretable time\nseries models and (b) scalable and fully managed system infrastructure. The\nmodel has a sequential and modular structure to handle different components of\nthe time series, including holiday effects, seasonality, trend, and anomalies,\nwhich enables high interpretability of the results. Novel enhancements are made\nto each module, and a unified framework is established to address both\nforecasting and anomaly detection tasks simultaneously. In terms of accuracy,\nits comprehensive benchmark on the 42 public datasets in the Monash forecasting\nrepository shows superior performance over not only well-established\nstatistical alternatives (such as ETS, ARIMA, TBATS, Prophet) but also newer\nneural network models (such as DeepAR, N-BEATS, PatchTST, TimeMixer). In terms\nof infrastructure, it is directly built into the query engine of BigQuery in\nGoogle Cloud. It uses a simple SQL interface and automates tedious\ntechnicalities such as data cleaning and model selection. It automatically\nscales with managed cloud computational and storage resources, making it\npossible to forecast 100 million time series using only 1.5 hours with a\nthroughput of more than 18000 time series per second. In terms of\ninterpretability, we present several case studies to demonstrate time series\ninsights it generates and customizability it offers.\n","authors":["Xi Cheng","Weijie Shen","Haoming Chen","Chaoyi Shen","Jean Ortega","Jiashang Liu","Steve Thomas","Honglin Zheng","Haoyun Wu","Yuxiang Li","Casey Lichtendahl","Jenny Ortiz","Gang Liu","Haiyang Qi","Omid Fatemieh","Chris Fry","Jing Jing Long"],"pdf_url":"https://arxiv.org/pdf/2510.24452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.16175v2","updated":"2025-10-28T14:06:41Z","published":"2025-10-17T19:35:54Z","title":"The Formalism-Implementation Gap in Reinforcement Learning Research","summary":"  The last decade has seen an upswing in interest and adoption of reinforcement\nlearning (RL) techniques, in large part due to its demonstrated capabilities at\nperforming certain tasks at \"super-human levels\". This has incentivized the\ncommunity to prioritize research that demonstrates RL agent performance, often\nat the expense of research aimed at understanding their learning dynamics.\nPerformance-focused research runs the risk of overfitting on academic\nbenchmarks -- thereby rendering them less useful -- which can make it difficult\nto transfer proposed techniques to novel problems. Further, it implicitly\ndiminishes work that does not push the performance-frontier, but aims at\nimproving our understanding of these techniques. This paper argues two points:\n(i) RL research should stop focusing solely on demonstrating agent\ncapabilities, and focus more on advancing the science and understanding of\nreinforcement learning; and (ii) we need to be more precise on how our\nbenchmarks map to the underlying mathematical formalisms. We use the popular\nArcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a\nbenchmark that, despite being increasingly considered \"saturated\", can be\neffectively used for developing this understanding, and facilitating the\ndeployment of RL techniques in impactful real-world problems.\n","authors":["Pablo Samuel Castro"],"pdf_url":"https://arxiv.org/pdf/2510.16175v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24433v1","updated":"2025-10-28T14:01:51Z","published":"2025-10-28T14:01:51Z","title":"Nearest Neighbor Matching as Least Squares Density Ratio Estimation and\n  Riesz Regression","summary":"  This study proves that Nearest Neighbor (NN) matching can be interpreted as\nan instance of Riesz regression for automatic debiased machine learning. Lin et\nal. (2023) shows that NN matching is an instance of density-ratio estimation\nwith their new density-ratio estimator. Chernozhukov et al. (2024) develops\nRiesz regression for automatic debiased machine learning, which directly\nestimates the Riesz representer (or equivalently, the bias-correction term) by\nminimizing the mean squared error. In this study, we first prove that the\ndensity-ratio estimation method proposed in Lin et al. (2023) is essentially\nequivalent to Least-Squares Importance Fitting (LSIF) proposed in Kanamori et\nal. (2009) for direct density-ratio estimation. Furthermore, we derive Riesz\nregression using the LSIF framework. Based on these results, we derive NN\nmatching from Riesz regression. This study is based on our work Kato (2025a)\nand Kato (2025b).\n","authors":["Masahiro Kato"],"pdf_url":"https://arxiv.org/pdf/2510.24433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24432v1","updated":"2025-10-28T14:01:13Z","published":"2025-10-28T14:01:13Z","title":"Fill in the Blanks: Accelerating Q-Learning with a Handful of\n  Demonstrations in Sparse Reward Settings","summary":"  Reinforcement learning (RL) in sparse-reward environments remains a\nsignificant challenge due to the lack of informative feedback. We propose a\nsimple yet effective method that uses a small number of successful\ndemonstrations to initialize the value function of an RL agent. By precomputing\nvalue estimates from offline demonstrations and using them as targets for early\nlearning, our approach provides the agent with a useful prior over promising\nactions. The agent then refines these estimates through standard online\ninteraction. This hybrid offline-to-online paradigm significantly reduces the\nexploration burden and improves sample efficiency in sparse-reward settings.\nExperiments on benchmark tasks demonstrate that our method accelerates\nconvergence and outperforms standard baselines, even with minimal or suboptimal\ndemonstration data.\n","authors":["Seyed Mahdi Basiri Azad","Joschka Boedecker"],"pdf_url":"https://arxiv.org/pdf/2510.24432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17638v2","updated":"2025-10-28T13:54:07Z","published":"2025-05-23T08:58:47Z","title":"Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical\n  Regularization in Training","summary":"  Diffusion models have achieved remarkable success across a wide range of\ngenerative tasks. A key challenge is understanding the mechanisms that prevent\ntheir memorization of training data and allow generalization. In this work, we\ninvestigate the role of the training dynamics in the transition from\ngeneralization to memorization. Through extensive experiments and theoretical\nanalysis, we identify two distinct timescales: an early time\n$\\tau_\\mathrm{gen}$ at which models begin to generate high-quality samples, and\na later time $\\tau_\\mathrm{mem}$ beyond which memorization emerges. Crucially,\nwe find that $\\tau_\\mathrm{mem}$ increases linearly with the training set size\n$n$, while $\\tau_\\mathrm{gen}$ remains constant. This creates a growing window\nof training times with $n$ where models generalize effectively, despite showing\nstrong memorization if training continues beyond it. It is only when $n$\nbecomes larger than a model-dependent threshold that overfitting disappears at\ninfinite training times. These findings reveal a form of implicit dynamical\nregularization in the training dynamics, which allow to avoid memorization even\nin highly overparameterized settings. Our results are supported by numerical\nexperiments with standard U-Net architectures on realistic and synthetic\ndatasets, and by a theoretical analysis using a tractable random features model\nstudied in the high-dimensional limit.\n","authors":["Tony Bonnaire","RaphaÃ«l Urfin","Giulio Biroli","Marc MÃ©zard"],"pdf_url":"https://arxiv.org/pdf/2505.17638v2.pdf","comment":"Accepted as an oral at Neurips 2025. 40 pages, 15 figures"},{"id":"http://arxiv.org/abs/2412.10856v4","updated":"2025-10-28T13:45:25Z","published":"2024-12-14T15:11:07Z","title":"RWKV-edge: Deeply Compressed RWKV for Resource-Constrained Devices","summary":"  To deploy LLMs on resource-contained platforms such as mobile robots and\nsmartphones, non-transformers LLMs have achieved major breakthroughs. Recently,\na novel RNN-based LLM family, Repentance Weighted Key Value (RWKV) has shown\nstrong computational efficiency; nevertheless, RWKV models still have high\nparameter counts which limited their deployment. In this paper, we propose a\nsuite of compression techniques, ranging from model architecture optimizations\nto post-training compression, tailored to the RWKV architecture. Combined, our\ntechniques reduce the memory footprint of RWKV models by 3.4x -- 5x with only\nnegligible degradation in accuracy; compared to transformer LLMs with similar\naccuracy, our models require 4x less memory footprint.\n","authors":["Wonkyo Choe","Yangfeng Ji","Felix Xiaozhu Lin"],"pdf_url":"https://arxiv.org/pdf/2412.10856v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24422v1","updated":"2025-10-28T13:43:00Z","published":"2025-10-28T13:43:00Z","title":"Attack on a PUF-based Secure Binary Neural Network","summary":"  Binarized Neural Networks (BNNs) deployed on memristive crossbar arrays\nprovide energy-efficient solutions for edge computing but are susceptible to\nphysical attacks due to memristor nonvolatility. Recently, Rajendran et al.\n(IEEE Embedded Systems Letter 2025) proposed a Physical Unclonable Function\n(PUF)-based scheme to secure BNNs against theft attacks. Specifically, the\nweight and bias matrices of the BNN layers were secured by swapping columns\nbased on device's PUF key bits.\n  In this paper, we demonstrate that this scheme to secure BNNs is vulnerable\nto PUF-key recovery attack. As a consequence of our attack, we recover the\nsecret weight and bias matrices of the BNN. Our approach is motivated by\ndifferential cryptanalysis and reconstructs the PUF key bit-by-bit by observing\nthe change in model accuracy, and eventually recovering the BNN model\nparameters. Evaluated on a BNN trained on the MNIST dataset, our attack could\nrecover 85% of the PUF key, and recover the BNN model up to 93% classification\naccuracy compared to the original model's 96% accuracy. Our attack is very\nefficient and it takes a couple of minutes to recovery the PUF key and the\nmodel parameters.\n","authors":["Bijeet Basak","Nupur Patil","Kurian Polachan","Srinivas Vivek"],"pdf_url":"https://arxiv.org/pdf/2510.24422v1.pdf","comment":"Accepted at VLSID 2026. To be published in IEEE Xplore"},{"id":"http://arxiv.org/abs/2504.07297v2","updated":"2025-10-28T13:27:06Z","published":"2025-04-09T21:40:15Z","title":"Data Fusion of Deep Learned Molecular Embeddings for Property Prediction","summary":"  Data-driven approaches such as deep learning can result in predictive models\nfor material properties with exceptional accuracy and efficiency. However, in\nmany applications, data is sparse, severely limiting their accuracy and\napplicability. To improve predictions, techniques such as transfer learning and\nmultitask learning have been used. The performance of multitask learning models\ndepends on the strength of the underlying correlations between tasks and the\ncompleteness of the data set. Standard multitask models tend to underperform\nwhen trained on sparse data sets with weakly correlated properties. To address\nthis gap, we fuse deep-learned embeddings generated by independent pretrained\nsingle-task models, resulting in a multitask model that inherits rich,\nproperty-specific representations. By reusing (rather than retraining) these\nembeddings, the resulting fused model outperforms standard multitask models and\ncan be extended with fewer trainable parameters. We demonstrate this technique\non a widely used benchmark data set of quantum chemistry data for small\nmolecules as well as a newly compiled sparse data set of experimental data\ncollected from literature and our own quantum chemistry and thermochemical\ncalculations.\n","authors":["Robert J Appleton","Brian C Barnes","Alejandro Strachan"],"pdf_url":"https://arxiv.org/pdf/2504.07297v2.pdf","comment":"J. Chem. Inf. Model. 2025"},{"id":"http://arxiv.org/abs/2310.06328v5","updated":"2025-10-28T13:10:33Z","published":"2023-10-10T05:54:00Z","title":"UniCrossFi: A Unified Framework For Cross-Domain Wi-Fi-based Gesture\n  Recognition","summary":"  Wi-Fi sensing systems are severely hindered by cross domain problem when\ndeployed in unseen real-world environments. Existing methods typically design\nseparate frameworks for either domain adaptation or domain generalization,\noften relying on extensive labeled data. Existing methods that designed for\ndomain generalization is often relying on extensive labeled data. However,\nreal-world scenarios are far more complex, where the deployed model must be\ncapable of handling generalization under limited labeled source data. To this\nend, we propose UniCrossFi, a unified framework designed to mitigate\nperformance drop in CSI-based sensing across diverse deployment settings. Our\nframework not only extends conventional Domain Generalization (DG) to a more\npractical Semi-Supervised Domain Generalization (SSDG) setting, where only\npartially labeled source data are available, but also introduces a\nphysics-informed data augmentation strategy, Antenna Response Consistency\n(ARC). ARC mitigates the risk of learning superficial shortcuts by exploiting\nthe intrinsic spatial diversity of multi-antenna systems, treating signals from\ndifferent antennas as naturally augmented views of the same event. In addition,\nwe design a Unified Contrastive Objective to prevent conventional contrastive\nlearning from pushing apart samples from different domains that share the same\nclass. We conduct extensive experiments on the public Widar and CSIDA datasets.\nThe results demonstrate that UniCrossFi consistently establishes a new\nstate-of-the-art, significantly outperforming existing methods across all\nunsupervised domain adaptation, DG, and SSDG benchmarks. UniCrossFi provides a\nprincipled and practical solution to the domain shift challenge, advancing the\nfeasibility of robust, real-world Wi-Fi sensing systems that can operate\neffectively with limited labeled data.\n","authors":["Ke Xu","Zhiyong Zheng","Hongyuan Zhu","Lei Wang","Jiangtao Wang"],"pdf_url":"https://arxiv.org/pdf/2310.06328v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24380v1","updated":"2025-10-28T12:57:59Z","published":"2025-10-28T12:57:59Z","title":"APEX: Approximate-but-exhaustive search for ultra-large combinatorial\n  synthesis libraries","summary":"  Make-on-demand combinatorial synthesis libraries (CSLs) like Enamine REAL\nhave significantly enabled drug discovery efforts. However, their large size\npresents a challenge for virtual screening, where the goal is to identify the\ntop compounds in a library according to a computational objective (e.g.,\noptimizing docking score) subject to computational constraints under a limited\ncomputational budget. For current library sizes -- numbering in the tens of\nbillions of compounds -- and scoring functions of interest, a routine virtual\nscreening campaign may be limited to scoring fewer than 0.1% of the available\ncompounds, leaving potentially many high scoring compounds undiscovered.\nFurthermore, as constraints (and sometimes objectives) change during the course\nof a virtual screening campaign, existing virtual screening algorithms\ntypically offer little room for amortization. We propose the\napproximate-but-exhaustive search protocol for CSLs, or APEX. APEX utilizes a\nneural network surrogate that exploits the structure of CSLs in the prediction\nof objectives and constraints to make full enumeration on a consumer GPU\npossible in under a minute, allowing for exact retrieval of approximate top-$k$\nsets. To demonstrate APEX's capabilities, we develop a benchmark CSL comprised\nof more than 10 million compounds, all of which have been annotated with their\ndocking scores on five medically relevant targets along with physicohemical\nproperties measured with RDKit such that, for any objective and set of\nconstraints, the ground truth top-$k$ compounds can be identified and compared\nagainst the retrievals from any virtual screening algorithm. We show APEX's\nconsistently strong performance both in retrieval accuracy and runtime compared\nto alternative methods.\n","authors":["Aryan Pedawi","Jordi Silvestre-Ryan","Bradley Worley","Darren J Hsu","Kushal S Shah","Elias Stehle","Jingrong Zhang","Izhar Wallach"],"pdf_url":"https://arxiv.org/pdf/2510.24380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.20641v4","updated":"2025-10-28T12:57:51Z","published":"2025-06-25T17:37:21Z","title":"Telegrapher's Generative Model via Kac Flows","summary":"  We break the mold in flow-based generative modeling by proposing a new model\nbased on the damped wave equation, also known as telegrapher's equation.\nSimilar to the diffusion equation and Brownian motion, there is a Feynman-Kac\ntype relation between the telegrapher's equation and the stochastic Kac process\nin 1D. The Kac flow evolves stepwise linearly in time, so that the probability\nflow is Lipschitz continuous in the Wasserstein distance and, in contrast to\ndiffusion flows, the norm of the velocity is globally bounded. Furthermore, the\nKac model has the diffusion model as its asymptotic limit. We extend these\nconsiderations to a multi-dimensional stochastic process which consists of\nindependent 1D Kac processes in each spatial component. We show that this\nprocess gives rise to an absolutely continuous curve in the Wasserstein space\nand compute the conditional velocity field starting in a Dirac point\nanalytically. Using the framework of flow matching, we train a neural network\nthat approximates the velocity field and use it for sample generation. Our\nnumerical experiments demonstrate the scalability of our approach, and show its\nadvantages over diffusion models.\n","authors":["Richard Duong","Jannis Chemseddine","Peter K. Friz","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2506.20641v4.pdf","comment":"Update V2: We added CIFAR experiments. Update V3: The old FID scores\n  & CIFAR images of the Kac model corresponded to the schedule g(t) = t. We now\n  updated them with both schedules t and t^2. Update V4: We corrected a minor\n  implementation error and updated the CIFAR images/table"},{"id":"http://arxiv.org/abs/2502.17500v2","updated":"2025-10-28T12:53:44Z","published":"2025-02-21T11:05:04Z","title":"Generalized Exponentiated Gradient Algorithms Using the Euler\n  Two-Parameter Logarithm","summary":"  IIn this paper we propose and investigate a new class of Generalized\nExponentiated Gradient (GEG) algorithms using Mirror Descent (MD) updates, and\napplying the Bregman divergence with a two--parameter\n  deformation of the logarithm as a link function. This link function (referred\nhere to as the Euler logarithm) is associated with a relatively wide class of\ntrace--form entropies. In order to derive novel GEG/MD updates, we estimate a\ndeformed exponential function, which closely approximates the inverse of the\nEuler two--parameter deformed logarithm. The characteristic shape and\nproperties of the Euler logarithm and its inverse--deformed exponential\nfunctions, are tuned by two hyperparameters. By learning these hyperparameters,\nwe can adapt to the distribution of training data and adjust them to achieve\ndesired properties of gradient descent algorithms. In the literature, there\nexist nowadays more than fifty mathematically well-established entropic\nfunctionals and associated deformed logarithms, so it is impossible to\ninvestigate all of them in one research paper. Therefore, we focus here on a\nclass of trace-form entropies and the associated deformed two--parameters\nlogarithms.\n","authors":["Andrzej Cichocki"],"pdf_url":"https://arxiv.org/pdf/2502.17500v2.pdf","comment":"10 pages, preprint of Journal paper"},{"id":"http://arxiv.org/abs/2510.24375v1","updated":"2025-10-28T12:52:47Z","published":"2025-10-28T12:52:47Z","title":"A Comprehensive Evaluation Framework for Synthetic Trip Data Generation\n  in Public Transport","summary":"  Synthetic data offers a promising solution to the privacy and accessibility\nchallenges of using smart card data in public transport research. Despite rapid\nprogress in generative modeling, there is limited attention to comprehensive\nevaluation, leaving unclear how reliable, safe, and useful synthetic data truly\nare. Existing evaluations remain fragmented, typically limited to\npopulation-level representativeness or record-level privacy, without\nconsidering group-level variations or task-specific utility. To address this\ngap, we propose a Representativeness-Privacy-Utility (RPU) framework that\nsystematically evaluates synthetic trip data across three complementary\ndimensions and three hierarchical levels (record, group, population). The\nframework integrates a consistent set of metrics to quantify similarity,\ndisclosure risk, and practical usefulness, enabling transparent and balanced\nassessment of synthetic data quality. We apply the framework to benchmark\ntwelve representative generation methods, spanning conventional statistical\nmodels, deep generative networks, and privacy-enhanced variants. Results show\nthat synthetic data do not inherently guarantee privacy and there is no\n\"one-size-fits-all\" model, the trade-off between privacy and\nrepresentativeness/utility is obvious. Conditional Tabular generative\nadversarial network (CTGAN) provide the most balanced trade-off and is\nsuggested for practical applications. The RPU framework provides a systematic\nand reproducible basis for researchers and practitioners to compare synthetic\ndata generation techniques and select appropriate methods in public transport\napplications.\n","authors":["Yuanyuan Wu","Zhenlin Qin","Zhenliang Ma"],"pdf_url":"https://arxiv.org/pdf/2510.24375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01143v2","updated":"2025-10-28T12:49:08Z","published":"2025-06-01T19:55:31Z","title":"Linear regression with overparameterized linear neural networks: Tight\n  upper and lower bounds for implicit $\\ell^1$-regularization","summary":"  Modern machine learning models are often trained in a setting where the\nnumber of parameters exceeds the number of training samples. To understand the\nimplicit bias of gradient descent in such overparameterized models, prior work\nhas studied diagonal linear neural networks in the regression setting. These\nstudies have shown that, when initialized with small weights, gradient descent\ntends to favor solutions with minimal $\\ell^1$-norm - an effect known as\nimplicit regularization. In this paper, we investigate implicit regularization\nin diagonal linear neural networks of depth $D\\ge 2$ for overparameterized\nlinear regression problems. We focus on analyzing the approximation error\nbetween the limit point of gradient flow trajectories and the solution to the\n$\\ell^1$-minimization problem. By deriving tight upper and lower bounds on the\napproximation error, we precisely characterize how the approximation error\ndepends on the scale of initialization $\\alpha$. Our results reveal a\nqualitative difference between depths: for $D \\ge 3$, the error decreases\nlinearly with $\\alpha$, whereas for $D=2$, it decreases at rate\n$\\alpha^{1-\\varrho}$, where the parameter $\\varrho \\in [0,1)$ can be explicitly\ncharacterized. Interestingly, this parameter is closely linked to so-called\nnull space property constants studied in the sparse recovery literature. We\ndemonstrate the asymptotic tightness of our bounds through explicit examples.\nNumerical experiments corroborate our theoretical findings and suggest that\ndeeper networks, i.e., $D \\ge 3$, may lead to better generalization,\nparticularly for realistic initialization scales.\n","authors":["Hannes Matt","Dominik StÃ¶ger"],"pdf_url":"https://arxiv.org/pdf/2506.01143v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.13397v2","updated":"2025-10-28T12:46:53Z","published":"2025-10-15T10:51:17Z","title":"Assessing the robustness of heterogeneous treatment effects in survival\n  analysis under informative censoring","summary":"  Dropout is common in clinical studies, with up to half of patients leaving\nearly due to side effects or other reasons. When dropout is informative (i.e.,\ndependent on survival time), it introduces censoring bias, because of which\ntreatment effect estimates are also biased. In this paper, we propose an\nassumption-lean framework to assess the robustness of conditional average\ntreatment effect (CATE) estimates in survival analysis when facing censoring\nbias. Unlike existing works that rely on strong assumptions, such as\nnon-informative censoring, to obtain point estimation, we use partial\nidentification to derive informative bounds on the CATE. Thereby, our framework\nhelps to identify patient subgroups where treatment is effective despite\ninformative censoring. We further develop a novel meta-learner that estimates\nthe bounds using arbitrary machine learning models and with favorable\ntheoretical properties, including double robustness and quasi-oracle\nefficiency. We demonstrate the practical value of our meta-learner through\nnumerical experiments and in an application to a cancer drug trial. Together,\nour framework offers a practical tool for assessing the robustness of estimated\ntreatment effects in the presence of censoring and thus promotes the reliable\nuse of survival data for evidence generation in medicine and epidemiology.\n","authors":["Yuxin Wang","Dennis Frauen","Jonas Schweisthal","Maresa SchrÃ¶der","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2510.13397v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24368v1","updated":"2025-10-28T12:45:20Z","published":"2025-10-28T12:45:20Z","title":"Filtering instances and rejecting predictions to obtain reliable models\n  in healthcare","summary":"  Machine Learning (ML) models are widely used in high-stakes domains such as\nhealthcare, where the reliability of predictions is critical. However, these\nmodels often fail to account for uncertainty, providing predictions even with\nlow confidence. This work proposes a novel two-step data-centric approach to\nenhance the performance of ML models by improving data quality and filtering\nlow-confidence predictions. The first step involves leveraging Instance\nHardness (IH) to filter problematic instances during training, thereby refining\nthe dataset. The second step introduces a confidence-based rejection mechanism\nduring inference, ensuring that only reliable predictions are retained. We\nevaluate our approach using three real-world healthcare datasets, demonstrating\nits effectiveness at improving model reliability while balancing predictive\nperformance and rejection rate. Additionally, we use alternative criteria -\ninfluence values for filtering and uncertainty for rejection - as baselines to\nevaluate the efficiency of the proposed method. The results demonstrate that\nintegrating IH filtering with confidence-based rejection effectively enhances\nmodel performance while preserving a large proportion of instances. This\napproach provides a practical method for deploying ML systems in\nsafety-critical applications.\n","authors":["Maria Gabriela Valeriano","David Kohan MarzagÃ£o","Alfredo Montelongo","Carlos Roberto Veiga Kiffer","Natan Katz","Ana Carolina Lorena"],"pdf_url":"https://arxiv.org/pdf/2510.24368v1.pdf","comment":"This paper is under review at Machine Learning (Springer)"},{"id":"http://arxiv.org/abs/2402.10028v3","updated":"2025-10-28T12:23:40Z","published":"2024-02-15T15:48:55Z","title":"Diffusion Models Meet Contextual Bandits","summary":"  Efficient online decision-making in contextual bandits is challenging, as\nmethods without informative priors often suffer from computational or\nstatistical inefficiencies. In this work, we leverage pre-trained diffusion\nmodels as expressive priors to capture complex action dependencies and develop\na practical algorithm that efficiently approximates posteriors under such\npriors, enabling both fast updates and sampling. Empirical results demonstrate\nthe effectiveness and versatility of our approach across diverse contextual\nbandit settings.\n","authors":["Imad Aouali"],"pdf_url":"https://arxiv.org/pdf/2402.10028v3.pdf","comment":"Neurips 2025"},{"id":"http://arxiv.org/abs/2510.24356v1","updated":"2025-10-28T12:19:49Z","published":"2025-10-28T12:19:49Z","title":"Perception Learning: A Formal Separation of Sensory Representation\n  Learning from Decision Learning","summary":"  We introduce Perception Learning (PeL), a paradigm that optimizes an agent's\nsensory interface $f_\\phi:\\mathcal{X}\\to\\mathcal{Z}$ using task-agnostic\nsignals, decoupled from downstream decision learning\n$g_\\theta:\\mathcal{Z}\\to\\mathcal{Y}$. PeL directly targets label-free\nperceptual properties, such as stability to nuisances, informativeness without\ncollapse, and controlled geometry, assessed via objective\nrepresentation-invariant metrics. We formalize the separation of perception and\ndecision, define perceptual properties independent of objectives or\nreparameterizations, and prove that PeL updates preserving sufficient\ninvariants are orthogonal to Bayes task-risk gradients. Additionally, we\nprovide a suite of task-agnostic evaluation metrics to certify perceptual\nquality.\n","authors":["Suman Sanyal"],"pdf_url":"https://arxiv.org/pdf/2510.24356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.24424v2","updated":"2025-10-28T12:08:40Z","published":"2025-05-30T10:04:00Z","title":"Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning","summary":"  Vision-language models like CLIP have demonstrated remarkable zero-shot\ncapabilities in classification and retrieval. However, these models often\nstruggle with compositional reasoning - the ability to understand the\nrelationships between concepts. A recent benchmark, SugarCrepe++, reveals that\nprevious works on improving compositionality have mainly improved lexical\nsensitivity but neglected semantic understanding. In addition, downstream\nretrieval performance often deteriorates, although one would expect that\nimproving compositionality should enhance retrieval. In this work, we introduce\nCLIC (Compositionally-aware Learning in CLIP), a fine-tuning method based on a\nnovel training technique combining multiple images and their associated\ncaptions. CLIC improves compositionality across architectures as well as\ndifferently pre-trained CLIP models, both in terms of lexical and semantic\nunderstanding, and achieves consistent gains in retrieval performance. This\neven applies to the recent CLIPS, which achieves SOTA retrieval performance.\nNevertheless, the short fine-tuning with CLIC leads to an improvement in\nretrieval and to the best compositional CLIP model on SugarCrepe++. All our\nmodels and code are available at https://clic-compositional-clip.github.io\n","authors":["Amit Peleg","Naman Deep Singh","Matthias Hein"],"pdf_url":"https://arxiv.org/pdf/2505.24424v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2411.19477v5","updated":"2025-10-28T11:59:43Z","published":"2024-11-29T05:29:47Z","title":"Provable Scaling Laws for the Test-Time Compute of Large Language Models","summary":"  We propose two simple, principled and practical algorithms that enjoy\nprovable scaling laws for the test-time compute of large language models\n(LLMs). The first one is a two-stage knockout-style algorithm: given an input\nproblem, it first generates multiple candidate solutions, and then aggregate\nthem via a knockout tournament for the final output. Assuming that the LLM can\ngenerate a correct solution with non-zero probability and do better than a\nrandom guess in comparing a pair of correct and incorrect solutions, we prove\ntheoretically that the failure probability of this algorithm decays to zero\nexponentially or by a power law (depending on the specific way of scaling) as\nits test-time compute grows. The second one is a two-stage league-style\nalgorithm, where each candidate is evaluated by its average win rate against\nmultiple opponents, rather than eliminated upon loss to a single opponent.\nUnder analogous but more robust assumptions, we prove that its failure\nprobability also decays to zero exponentially with more test-time compute. Both\nalgorithms require a black-box LLM and nothing else (e.g., no verifier or\nreward model) for a minimalistic implementation, which makes them appealing for\npractical applications and easy to adapt for different tasks. Through extensive\nexperiments with diverse models and datasets, we validate the proposed theories\nand demonstrate the outstanding scaling properties of both algorithms.\n","authors":["Yanxi Chen","Xuchen Pan","Yaliang Li","Bolin Ding","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.19477v5.pdf","comment":"NeurIPS 2025 camera-ready version"},{"id":"http://arxiv.org/abs/2510.20974v2","updated":"2025-10-28T11:58:54Z","published":"2025-10-23T20:06:29Z","title":"Robust Point Cloud Reinforcement Learning via PCA-Based Canonicalization","summary":"  Reinforcement Learning (RL) from raw visual input has achieved impressive\nsuccesses in recent years, yet it remains fragile to out-of-distribution\nvariations such as changes in lighting, color, and viewpoint. Point Cloud\nReinforcement Learning (PC-RL) offers a promising alternative by mitigating\nappearance-based brittleness, but its sensitivity to camera pose mismatches\ncontinues to undermine reliability in realistic settings. To address this\nchallenge, we propose PCA Point Cloud (PPC), a canonicalization framework\nspecifically tailored for downstream robotic control. PPC maps point clouds\nunder arbitrary rigid-body transformations to a unique canonical pose, aligning\nobservations to a consistent frame, thereby substantially decreasing\nviewpoint-induced inconsistencies. In our experiments, we show that PPC\nimproves robustness to unseen camera poses across challenging robotic tasks,\nproviding a principled alternative to domain randomization.\n","authors":["Michael Bezick","Vittorio Giammarino","Ahmed H. Qureshi"],"pdf_url":"https://arxiv.org/pdf/2510.20974v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24331v1","updated":"2025-10-28T11:55:24Z","published":"2025-10-28T11:55:24Z","title":"What do vision-language models see in the context? Investigating\n  multimodal in-context learning","summary":"  In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks\nfrom demonstration examples without parameter updates. Although it has been\nextensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)\nremains underexplored. In this work, we present a systematic study of ICL in\nVLMs, evaluating seven models spanning four architectures on three image\ncaptioning benchmarks. We analyze how prompt design, architectural choices, and\ntraining strategies influence multimodal ICL. To our knowledge, we are the\nfirst to analyze how attention patterns in VLMs vary with an increasing number\nof in-context demonstrations. Our results reveal that training on imag-text\ninterleaved data enhances ICL performance but does not imply effective\nintegration of visual and textual information from demonstration examples. In\ncontrast, instruction tuning improves instruction-following but can reduce\nreliance on in-context demonstrations, suggesting a trade-off between\ninstruction alignment and in-context adaptation. Attention analyses further\nshow that current VLMs primarily focus on textual cues and fail to leverage\nvisual information, suggesting a limited capacity for multimodal integration.\nThese findings highlight key limitations in the ICL abilities of current VLMs\nand provide insights for enhancing their ability to learn from multimodal\nin-context examples.\n","authors":["Gabriel O. dos Santos","Esther Colombini","Sandra Avila"],"pdf_url":"https://arxiv.org/pdf/2510.24331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11364v4","updated":"2025-10-28T11:36:51Z","published":"2025-04-15T16:30:02Z","title":"Offline Learning and Forgetting for Reasoning with Large Language Models","summary":"  Leveraging inference-time search in large language models has proven\neffective in further enhancing a trained model's capability to solve complex\nmathematical and reasoning problems. However, this approach significantly\nincreases computational costs and inference time, as the model must generate\nand evaluate multiple candidate solutions to identify a viable reasoning path.\nTo address this, we propose an effective approach that integrates search\ncapabilities directly into the model by fine-tuning it on unpaired successful\n(learning) and failed reasoning paths (forgetting) derived from diverse search\nmethods. A key challenge we identify is that naive fine-tuning can degrade the\nmodel's search capability; we show this can be mitigated with a smaller\nlearning rate. Extensive experiments on the challenging Game-of-24 and\nCountdown arithmetic puzzles show that, replacing CoT-generated data with\nsearch-generated data for offline fine-tuning improves success rates by around\n23% over inference-time search baselines, while reducing inference time by\n180$\\times$. On top of this, our learning and forgetting objective consistently\noutperforms both supervised fine-tuning and preference-based methods.\n","authors":["Tianwei Ni","Allen Nie","Sapana Chaudhary","Yao Liu","Huzefa Rangwala","Rasool Fakoor"],"pdf_url":"https://arxiv.org/pdf/2504.11364v4.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR), 2025.\n  Code: https://github.com/twni2016/llm-reasoning-uft"},{"id":"http://arxiv.org/abs/2510.24318v1","updated":"2025-10-28T11:36:31Z","published":"2025-10-28T11:36:31Z","title":"Transformers can do Bayesian Clustering","summary":"  Bayesian clustering accounts for uncertainty but is computationally demanding\nat scale. Furthermore, real-world datasets often contain missing values, and\nsimple imputation ignores the associated uncertainty, resulting in suboptimal\nresults. We present Cluster-PFN, a Transformer-based model that extends\nPrior-Data Fitted Networks (PFNs) to unsupervised Bayesian clustering. Trained\nentirely on synthetic datasets generated from a finite Gaussian Mixture Model\n(GMM) prior, Cluster-PFN learns to estimate the posterior distribution over\nboth the number of clusters and the cluster assignments. Our method estimates\nthe number of clusters more accurately than handcrafted model selection\nprocedures such as AIC, BIC and Variational Inference (VI), and achieves\nclustering quality competitive with VI while being orders of magnitude faster.\nCluster-PFN can be trained on complex priors that include missing data,\noutperforming imputation-based baselines on real-world genomic datasets, at\nhigh missingness. These results show that the Cluster-PFN can provide scalable\nand flexible Bayesian clustering.\n","authors":["Prajit Bhaskaran","Tom Viering"],"pdf_url":"https://arxiv.org/pdf/2510.24318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.23712v2","updated":"2025-10-28T11:34:23Z","published":"2025-09-28T07:53:41Z","title":"FraudTransformer: Time-Aware GPT for Transaction Fraud Detection","summary":"  Detecting payment fraud in real-world banking streams requires models that\ncan exploit both the order of events and the irregular time gaps between them.\nWe introduce FraudTransformer, a sequence model that augments a vanilla\nGPT-style architecture with (i) a dedicated time encoder that embeds either\nabsolute timestamps or inter-event values, and (ii) a learned positional\nencoder that preserves relative order. Experiments on a large industrial\ndataset -- tens of millions of transactions and auxiliary events -- show that\nFraudTransformer surpasses four strong classical baselines (Logistic\nRegression, XGBoost and LightGBM) as well as transformer ablations that omit\neither the time or positional component. On the held-out test set it delivers\nthe highest AUROC and PRAUC.\n","authors":["Gholamali Aminian","Andrew Elliott","Tiger Li","Timothy Cheuk Hin Wong","Victor Claude Dehon","Lukasz Szpruch","Carsten Maple","Christopher Read","Martin Brown","Gesine Reinert","Mo Mamouei"],"pdf_url":"https://arxiv.org/pdf/2509.23712v2.pdf","comment":"Accepted in AI-FIND ICAIF'25\n  (https://sites.google.com/view/icaif-fraud-detection-workshop/home)"},{"id":"http://arxiv.org/abs/2509.20234v3","updated":"2025-10-28T11:26:53Z","published":"2025-09-24T15:24:43Z","title":"ImageNet-trained CNNs are not biased towards texture: Revisiting feature\n  reliance through controlled suppression","summary":"  The hypothesis that Convolutional Neural Networks (CNNs) are inherently\ntexture-biased has shaped much of the discourse on feature use in deep\nlearning. We revisit this hypothesis by examining limitations in the\ncue-conflict experiment by Geirhos et al. To address these limitations, we\npropose a domain-agnostic framework that quantifies feature reliance through\nsystematic suppression of shape, texture, and color cues, avoiding the\nconfounds of forced-choice conflicts. By evaluating humans and neural networks\nunder controlled suppression conditions, we find that CNNs are not inherently\ntexture-biased but predominantly rely on local shape features. Nonetheless,\nthis reliance can be substantially mitigated through modern training strategies\nor architectures (ConvNeXt, ViTs). We further extend the analysis across\ncomputer vision, medical imaging, and remote sensing, revealing that reliance\npatterns differ systematically: computer vision models prioritize shape,\nmedical imaging models emphasize color, and remote sensing models exhibit a\nstronger reliance on texture. Code is available at\nhttps://github.com/tomburgert/feature-reliance.\n","authors":["Tom Burgert","Oliver Stoll","Paolo Rota","BegÃ¼m Demir"],"pdf_url":"https://arxiv.org/pdf/2509.20234v3.pdf","comment":"Accepted at NeurIPS 2025 (oral)"},{"id":"http://arxiv.org/abs/2510.24310v1","updated":"2025-10-28T11:20:06Z","published":"2025-10-28T11:20:06Z","title":"EDC: Equation Discovery for Classification","summary":"  Equation Discovery techniques have shown considerable success in regression\ntasks, where they are used to discover concise and interpretable models\n(\\textit{Symbolic Regression}). In this paper, we propose a new ED-based binary\nclassification framework. Our proposed method EDC finds analytical functions of\nmanageable size that specify the location and shape of the decision boundary.\nIn extensive experiments on artificial and real-life data, we demonstrate how\nEDC is able to discover both the structure of the target equation as well as\nthe value of its parameters, outperforming the current state-of-the-art\nED-based classification methods in binary classification and achieving\nperformance comparable to the state of the art in binary classification. We\nsuggest a grammar of modest complexity that appears to work well on the tested\ndatasets but argue that the exact grammar -- and thus the complexity of the\nmodels -- is configurable, and especially domain-specific expressions can be\nincluded in the pattern language, where that is required. The presented grammar\nconsists of a series of summands (additive terms) that include linear,\nquadratic and exponential terms, as well as products of two features (producing\nhyperbolic curves ideal for capturing XOR-like dependencies). The experiments\ndemonstrate that this grammar allows fairly flexible decision boundaries while\nnot so rich to cause overfitting.\n","authors":["Guus Toussaint","Arno Knobbe"],"pdf_url":"https://arxiv.org/pdf/2510.24310v1.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Lecture Notes in Computer Science, and is available online at\n  https://doi.org/10.1007/978-3-032-05461-6_9"},{"id":"http://arxiv.org/abs/2510.08146v3","updated":"2025-10-28T10:58:14Z","published":"2025-10-09T12:33:16Z","title":"Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM\n  Reasoning","summary":"  We introduce a simple, yet novel entropy-based framework to drive token\nefficiency in large language models during reasoning tasks. Our approach uses\nShannon entropy from token-level logprobs as a confidence signal to enable\nearly stopping, achieving 25-50% computational savings while maintaining task\naccuracy. Crucially, we demonstrate that entropy-based confidence calibration\nrepresents an emergent property of advanced post-training optimization present\nin modern reasoning models but notably absent in standard instruction-tuned and\npre-trained models (Llama 3.3 70B). We show that the entropy threshold to stop\nreasoning varies from model to model but can be calculated easily in one shot\nusing only a few examples from existing reasoning datasets. Our results\nindicate that advanced reasoning models often know that they've gotten a\ncorrect answer early on, and that this emergent confidence awareness can be\nexploited to save tokens and reduce latency. The framework demonstrates\nconsistent performance across reasoning-optimized model families with 25-50%\ncomputational cost reduction while preserving accuracy, revealing that\nconfidence mechanisms represent a distinguishing characteristic of modern\npost-trained reasoning systems versus their predecessors.\n","authors":["Aman Sharma","Paras Chopra"],"pdf_url":"https://arxiv.org/pdf/2510.08146v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13771v2","updated":"2025-10-28T10:57:14Z","published":"2025-05-30T06:43:03Z","title":"LittleBit: Ultra Low-Bit Quantization via Latent Factorization","summary":"  Deploying large language models (LLMs) often faces challenges from\nsubstantial memory and computational costs. Quantization offers a solution, yet\nperformance degradation in the sub-1-bit regime remains particularly difficult.\nThis paper introduces LittleBit, a novel method for extreme LLM compression. It\ntargets levels like 0.1 bits per weight (BPW), achieving nearly 31$\\times$\nmemory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents\nweights in a low-rank form using latent matrix factorization, subsequently\nbinarizing these factors. To counteract information loss from this extreme\nprecision, it integrates a multi-scale compensation mechanism. This includes\nrow, column, and an additional latent dimension that learns per-rank\nimportance. Two key contributions enable effective training: Dual\nSign-Value-Independent Decomposition (Dual-SVID) for quantization-aware\ntraining (QAT) initialization, and integrated Residual Compensation to mitigate\nerrors. Extensive experiments confirm LittleBit's superiority in sub-1-bit\nquantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading\nmethod's 0.7 BPW. LittleBit establishes a new, viable size-performance\ntrade-off--unlocking a potential 11.6$\\times$ speedup over FP16 at the kernel\nlevel--and makes powerful LLMs practical for resource-constrained environments.\n","authors":["Banseok Lee","Dongkyu Kim","Youngcheon You","Youngmin Kim"],"pdf_url":"https://arxiv.org/pdf/2506.13771v2.pdf","comment":"Accepted to NeurIPS 2025. Banseok Lee and Dongkyu Kim contributed\n  equally"},{"id":"http://arxiv.org/abs/2506.00129v2","updated":"2025-10-28T10:56:55Z","published":"2025-05-30T18:05:33Z","title":"Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically Aware\n  Sign Language Translation","summary":"  Recent progress in Sign Language Translation (SLT) has focussed primarily on\nimproving the representational capacity of large language models to incorporate\nSign Language features. This work explores an alternative direction: enhancing\nthe geometric properties of skeletal representations themselves. We propose\nGeo-Sign, a method that leverages the properties of hyperbolic geometry to\nmodel the hierarchical structure inherent in sign language kinematics. By\nprojecting skeletal features derived from Spatio-Temporal Graph Convolutional\nNetworks (ST-GCNs) into the Poincar\\'e ball model, we aim to create more\ndiscriminative embeddings, particularly for fine-grained motions like finger\narticulations. We introduce a hyperbolic projection layer, a weighted Fr\\'echet\nmean aggregation scheme, and a geometric contrastive loss operating directly in\nhyperbolic space. These components are integrated into an end-to-end\ntranslation framework as a regularisation function, to enhance the\nrepresentations within the language model. This work demonstrates the potential\nof hyperbolic geometry to improve skeletal representations for Sign Language\nTranslation, improving on SOTA RGB methods while preserving privacy and\nimproving computational efficiency. Code available here:\nhttps://github.com/ed-fish/geo-sign.\n","authors":["Edward Fish","Richard Bowden"],"pdf_url":"https://arxiv.org/pdf/2506.00129v2.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.01855v2","updated":"2025-10-28T10:53:56Z","published":"2025-06-02T16:41:49Z","title":"Trade-offs in Data Memorization via Strong Data Processing Inequalities","summary":"  Recent research demonstrated that training large language models involves\nmemorization of a significant fraction of training data. Such memorization can\nlead to privacy violations when training on sensitive user data and thus\nmotivates the study of data memorization's role in learning. In this work, we\ndevelop a general approach for proving lower bounds on excess data\nmemorization, that relies on a new connection between strong data processing\ninequalities and data memorization. We then demonstrate that several simple and\nnatural binary classification problems exhibit a trade-off between the number\nof samples available to a learning algorithm, and the amount of information\nabout the training data that a learning algorithm needs to memorize to be\naccurate. In particular, $\\Omega(d)$ bits of information about the training\ndata need to be memorized when $O(1)$ $d$-dimensional examples are available,\nwhich then decays as the number of examples grows at a problem-specific rate.\nFurther, our lower bounds are generally matched (up to logarithmic factors) by\nsimple learning algorithms. We also extend our lower bounds to more general\nmixture-of-clusters models. Our definitions and results build on the work of\nBrown et al. (2021) and address several limitations of the lower bounds in\ntheir work.\n","authors":["Vitaly Feldman","Guy Kornowski","Xin Lyu"],"pdf_url":"https://arxiv.org/pdf/2506.01855v2.pdf","comment":"Appeared in COLT 2025; this revision includes an improved upper bound\n  in Theorem 3.1, as well as several minor clarifications and modifications"},{"id":"http://arxiv.org/abs/2510.24288v1","updated":"2025-10-28T10:50:04Z","published":"2025-10-28T10:50:04Z","title":"Problem-Parameter-Free Decentralized Bilevel Optimization","summary":"  Decentralized bilevel optimization has garnered significant attention due to\nits critical role in solving large-scale machine learning problems. However,\nexisting methods often rely on prior knowledge of problem parameters-such as\nsmoothness, convexity, or communication network topologies-to determine\nappropriate stepsizes. In practice, these problem parameters are typically\nunavailable, leading to substantial manual effort for hyperparameter tuning. In\nthis paper, we propose AdaSDBO, a fully problem-parameter-free algorithm for\ndecentralized bilevel optimization with a single-loop structure. AdaSDBO\nleverages adaptive stepsizes based on cumulative gradient norms to update all\nvariables simultaneously, dynamically adjusting its progress and eliminating\nthe need for problem-specific hyperparameter tuning. Through rigorous\ntheoretical analysis, we establish that AdaSDBO achieves a convergence rate of\n$\\widetilde{\\mathcal{O}}\\left(\\frac{1}{T}\\right)$, matching the performance of\nwell-tuned state-of-the-art methods up to polylogarithmic factors. Extensive\nnumerical experiments demonstrate that AdaSDBO delivers competitive performance\ncompared to existing decentralized bilevel optimization methods while\nexhibiting remarkable robustness across diverse stepsize configurations.\n","authors":["Zhiwei Zhai","Wenjing Yan","Ying-Jun Angela Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.24288v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24287v1","updated":"2025-10-28T10:49:42Z","published":"2025-10-28T10:49:42Z","title":"Towards actionable hypotension prediction -- predicting catecholamine\n  therapy initiation in the intensive care unit","summary":"  Hypotension in critically ill ICU patients is common and life-threatening.\nEscalation to catecholamine therapy marks a key management step, with both\nundertreatment and overtreatment posing risks. Most machine learning (ML)\nmodels predict hypotension using fixed MAP thresholds or MAP forecasting,\noverlooking the clinical decision behind treatment escalation. Predicting\ncatecholamine initiation, the start of vasoactive or inotropic agent\nadministration offers a more clinically actionable target reflecting real\ndecision-making. Using the MIMIC-III database, we modeled catecholamine\ninitiation as a binary event within a 15-minute prediction window. Input\nfeatures included statistical descriptors from a two-hour sliding MAP context\nwindow, along with demographics, biometrics, comorbidities, and ongoing\ntreatments. An Extreme Gradient Boosting (XGBoost) model was trained and\ninterpreted via SHapley Additive exPlanations (SHAP). The model achieved an\nAUROC of 0.822 (0.813-0.830), outperforming the hypotension baseline (MAP < 65,\nAUROC 0.686 [0.675-0.699]). SHAP analysis highlighted recent MAP values, MAP\ntrends, and ongoing treatments (e.g., sedatives, electrolytes) as dominant\npredictors. Subgroup analysis showed higher performance in males, younger\npatients (<53 years), those with higher BMI (>32), and patients without\ncomorbidities or concurrent medications. Predicting catecholamine initiation\nbased on MAP dynamics, treatment context, and patient characteristics supports\nthe critical decision of when to escalate therapy, shifting focus from\nthreshold-based alarms to actionable decision support. This approach is\nfeasible across a broad ICU cohort under natural event imbalance. Future work\nshould enrich temporal and physiological context, extend label definitions to\ninclude therapy escalation, and benchmark against existing hypotension\nprediction systems.\n","authors":["Richard Koebe","Noah Saibel","Juan Miguel Lopez Alcaraz","Simon SchÃ¤fer","Nils Strodthoff"],"pdf_url":"https://arxiv.org/pdf/2510.24287v1.pdf","comment":"27 pages, 8 figures, source code under\n  https://github.com/AI4HealthUOL/actionable-hypotension"},{"id":"http://arxiv.org/abs/2510.24279v1","updated":"2025-10-28T10:39:10Z","published":"2025-10-28T10:39:10Z","title":"HergNet: a Fast Neural Surrogate Model for Sound Field Predictions via\n  Superposition of Plane Waves","summary":"  We present a novel neural network architecture for the efficient prediction\nof sound fields in two and three dimensions. The network is designed to\nautomatically satisfy the Helmholtz equation, ensuring that the outputs are\nphysically valid. Therefore, the method can effectively learn solutions to\nboundary-value problems in various wave phenomena, such as acoustics, optics,\nand electromagnetism. Numerical experiments show that the proposed strategy can\npotentially outperform state-of-the-art methods in room acoustics simulation,\nin particular in the range of mid to high frequencies.\n","authors":["Matteo CalafÃ ","Yuanxin Xia","Cheol-Ho Jeong"],"pdf_url":"https://arxiv.org/pdf/2510.24279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24273v1","updated":"2025-10-28T10:32:52Z","published":"2025-10-28T10:32:52Z","title":"SALS: Sparse Attention in Latent Space for KV cache Compression","summary":"  Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively.\n","authors":["Junlin Mu","Hantao Huang","Jihang Zhang","Minghui Yu","Tao Wang","Yidong Li"],"pdf_url":"https://arxiv.org/pdf/2510.24273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24262v1","updated":"2025-10-28T10:17:11Z","published":"2025-10-28T10:17:11Z","title":"UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level\n  Task Adaptation","summary":"  Data augmentation using generative models has emerged as a powerful paradigm\nfor enhancing performance in computer vision tasks. However, most existing\naugmentation approaches primarily focus on optimizing intrinsic data attributes\n-- such as fidelity and diversity -- to generate visually high-quality\nsynthetic data, while often neglecting task-specific requirements. Yet, it is\nessential for data generators to account for the needs of downstream tasks, as\ntraining data requirements can vary significantly across different tasks and\nnetwork architectures. To address these limitations, we propose UtilGen, a\nnovel utility-centric data augmentation framework that adaptively optimizes the\ndata generation process to produce task-specific, high-utility training data\nvia downstream task feedback. Specifically, we first introduce a weight\nallocation network to evaluate the task-specific utility of each synthetic\nsample. Guided by these evaluations, UtilGen iteratively refines the data\ngeneration process using a dual-level optimization strategy to maximize the\nsynthetic data utility: (1) model-level optimization tailors the generative\nmodel to the downstream task, and (2) instance-level optimization adjusts\ngeneration policies -- such as prompt embeddings and initial noise -- at each\ngeneration round. Extensive experiments on eight benchmark datasets of varying\ncomplexity and granularity demonstrate that UtilGen consistently achieves\nsuperior performance, with an average accuracy improvement of 3.87% over\nprevious SOTA. Further analysis of data influence and distribution reveals that\nUtilGen produces more impactful and task-relevant synthetic data, validating\nthe effectiveness of the paradigm shift from visual characteristics-centric to\ntask utility-centric data augmentation.\n","authors":["Jiyu Guo","Shuo Yang","Yiming Huang","Yancheng Long","Xiaobo Xia","Xiu Su","Bo Zhao","Zeke Xie","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2510.24262v1.pdf","comment":"39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)"},{"id":"http://arxiv.org/abs/2510.24256v1","updated":"2025-10-28T10:09:35Z","published":"2025-10-28T10:09:35Z","title":"From Memorization to Reasoning in the Spectrum of Loss Curvature","summary":"  We characterize how memorization is represented in transformer models and\nshow that it can be disentangled in the weights of both language models (LMs)\nand vision transformers (ViTs) using a decomposition based on the loss\nlandscape curvature. This insight is based on prior theoretical and empirical\nwork showing that the curvature for memorized training points is much sharper\nthan non memorized, meaning ordering weight components from high to low\ncurvature can reveal a distinction without explicit labels. This motivates a\nweight editing procedure that suppresses far more recitation of untargeted\nmemorized data more effectively than a recent unlearning method\n(BalancedSubnet), while maintaining lower perplexity. Since the basis of\ncurvature has a natural interpretation for shared structure in model weights,\nwe analyze the editing procedure extensively on its effect on downstream tasks\nin LMs, and find that fact retrieval and arithmetic are specifically and\nconsistently negatively affected, even though open book fact retrieval and\ngeneral logical reasoning is conserved. We posit these tasks rely heavily on\nspecialized directions in weight space rather than general purpose mechanisms,\nregardless of whether those individual datapoints are memorized. We support\nthis by showing a correspondence between task data's activation strength with\nlow curvature components that we edit out, and the drop in task performance\nafter the edit. Our work enhances the understanding of memorization in neural\nnetworks with practical applications towards removing it, and provides evidence\nfor idiosyncratic, narrowly-used structures involved in solving tasks like math\nand fact retrieval.\n","authors":["Jack Merullo","Srihita Vatsavaya","Lucius Bushnaq","Owen Lewis"],"pdf_url":"https://arxiv.org/pdf/2510.24256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24254v1","updated":"2025-10-28T10:05:34Z","published":"2025-10-28T10:05:34Z","title":"Forecasting precipitation in the Arctic using probabilistic machine\n  learning informed by causal climate drivers","summary":"  Understanding and forecasting precipitation events in the Arctic maritime\nenvironments, such as Bear Island and Ny-{\\AA}lesund, is crucial for assessing\nclimate risk and developing early warning systems in vulnerable marine regions.\nThis study proposes a probabilistic machine learning framework for modeling and\npredicting the dynamics and severity of precipitation. We begin by analyzing\nthe scale-dependent relationships between precipitation and key atmospheric\ndrivers (e.g., temperature, relative humidity, cloud cover, and air pressure)\nusing wavelet coherence, which captures localized dependencies across time and\nfrequency domains. To assess joint causal influences, we employ\nSynergistic-Unique-Redundant Decomposition, which quantifies the impact of\ninteraction effects among each variable on future precipitation dynamics. These\ninsights inform the development of data-driven forecasting models that\nincorporate both historical precipitation and causal climate drivers. To\naccount for uncertainty, we employ the conformal prediction method, which\nenables the generation of calibrated non-parametric prediction intervals. Our\nresults underscore the importance of utilizing a comprehensive framework that\ncombines causal analysis with probabilistic forecasting to enhance the\nreliability and interpretability of precipitation predictions in Arctic marine\nenvironments.\n","authors":["Madhurima Panja","Dhiman Das","Tanujit Chakraborty","Arnob Ray","R. Athulya","Chittaranjan Hens","Syamal K. Dana","Nuncio Murukesh","Dibakar Ghosh"],"pdf_url":"https://arxiv.org/pdf/2510.24254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18195v2","updated":"2025-10-28T10:02:13Z","published":"2025-05-20T09:05:30Z","title":"Acoustic and Machine Learning Methods for Speech-Based Suicide Risk\n  Assessment: A Systematic Review","summary":"  Suicide remains a public health challenge, necessitating improved detection\nmethods to facilitate timely intervention and treatment. This systematic review\nevaluates the role of Artificial Intelligence (AI) and Machine Learning (ML) in\nassessing suicide risk through acoustic analysis of speech. Following PRISMA\nguidelines, we analyzed 33 articles selected from PubMed, Cochrane, Scopus, and\nWeb of Science databases. The last search was conducted in February 2025. Risk\nof bias was assessed using the PROBAST tool. Studies analyzing acoustic\nfeatures between individuals at risk of suicide (RS) and those not at risk\n(NRS) were included, while studies lacking acoustic data, a suicide-related\nfocus, or sufficient methodological details were excluded. Sample sizes varied\nwidely and were reported in terms of participants or speech segments, depending\non the study. Results were synthesized narratively based on acoustic features\nand classifier performance. Findings consistently showed significant acoustic\nfeature variations between RS and NRS populations, particularly involving\njitter, fundamental frequency (F0), Mel-frequency cepstral coefficients (MFCC),\nand power spectral density (PSD). Classifier performance varied based on\nalgorithms, modalities, and speech elicitation methods, with multimodal\napproaches integrating acoustic, linguistic, and metadata features\ndemonstrating superior performance. Among the 29 classifier-based studies,\nreported AUC values ranged from 0.62 to 0.985 and accuracies from 60% to\n99.85%. Most datasets were imbalanced in favor of NRS, and performance metrics\nwere rarely reported separately by group, limiting clear identification of\ndirection of effect.\n","authors":["Ambre Marie","Marine Garnier","Thomas Bertin","Laura Machart","Guillaume Dardenne","GwenolÃ© Quellec","Sofian Berrouiguet"],"pdf_url":"https://arxiv.org/pdf/2505.18195v2.pdf","comment":"Preprint version of a manuscript submitted to the Journal of\n  Affective Disorders"},{"id":"http://arxiv.org/abs/2507.20362v3","updated":"2025-10-28T09:58:09Z","published":"2025-07-27T17:31:47Z","title":"MH-GIN: Multi-scale Heterogeneous Graph-based Imputation Network for AIS\n  Data (Extended Version)","summary":"  Location-tracking data from the Automatic Identification System, much of\nwhich is publicly available, plays a key role in a range of maritime safety and\nmonitoring applications. However, the data suffers from missing values that\nhamper downstream applications. Imputing the missing values is challenging\nbecause the values of different heterogeneous attributes are updated at diverse\nrates, resulting in the occurrence of multi-scale dependencies among\nattributes. Existing imputation methods that assume similar update rates across\nattributes are unable to capture and exploit such dependencies, limiting their\nimputation accuracy. We propose MH-GIN, a Multi-scale Heterogeneous Graph-based\nImputation Network that aims improve imputation accuracy by capturing\nmulti-scale dependencies. Specifically, MH-GIN first extracts multi-scale\ntemporal features for each attribute while preserving their intrinsic\nheterogeneous characteristics. Then, it constructs a multi-scale heterogeneous\ngraph to explicitly model dependencies between heterogeneous attributes to\nenable more accurate imputation of missing values through graph propagation.\nExperimental results on two real-world datasets find that MH-GIN is capable of\nan average 57% reduction in imputation errors compared to state-of-the-art\nmethods, while maintaining computational efficiency. The source code and\nimplementation details of MH-GIN are publicly available\nhttps://github.com/hyLiu1994/MH-GIN.\n","authors":["Hengyu Liu","Tianyi Li","Yuqiang He","Kristian Torp","Yushuai Li","Christian S. Jensen"],"pdf_url":"https://arxiv.org/pdf/2507.20362v3.pdf","comment":"18 pages, 4 figures; This paper is accepted by PVLDB 2026"},{"id":"http://arxiv.org/abs/2510.23216v2","updated":"2025-10-28T09:50:12Z","published":"2025-10-27T11:06:00Z","title":"Human-Like Goalkeeping in a Realistic Football Simulation: a\n  Sample-Efficient Reinforcement Learning Approach","summary":"  While several high profile video games have served as testbeds for Deep\nReinforcement Learning (DRL), this technique has rarely been employed by the\ngame industry for crafting authentic AI behaviors. Previous research focuses on\ntraining super-human agents with large models, which is impractical for game\nstudios with limited resources aiming for human-like agents. This paper\nproposes a sample-efficient DRL method tailored for training and fine-tuning\nagents in industrial settings such as the video game industry. Our method\nimproves sample efficiency of value-based DRL by leveraging pre-collected data\nand increasing network plasticity. We evaluate our method training a goalkeeper\nagent in EA SPORTS FC 25, one of the best-selling football simulations today.\nOur agent outperforms the game's built-in AI by 10% in ball saving rate.\nAblation studies show that our method trains agents 50% faster compared to\nstandard DRL methods. Finally, qualitative evaluation from domain experts\nindicates that our approach creates more human-like gameplay compared to\nhand-crafted agents. As a testimony of the impact of the approach, the method\nis intended to replace the hand-crafted counterpart in next iterations of the\nseries.\n","authors":["Alessandro Sestini","Joakim Bergdahl","Jean-Philippe Barrette-LaPierre","Florian Fuchs","Brady Chen","Michael Jones","Linus GisslÃ©n"],"pdf_url":"https://arxiv.org/pdf/2510.23216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24242v1","updated":"2025-10-28T09:48:26Z","published":"2025-10-28T09:48:26Z","title":"Enabling Near-realtime Remote Sensing via Satellite-Ground Collaboration\n  of Large Vision-Language Models","summary":"  Large vision-language models (LVLMs) have recently demonstrated great\npotential in remote sensing (RS) tasks (e.g., disaster monitoring) conducted by\nlow Earth orbit (LEO) satellites. However, their deployment in real-world LEO\nsatellite systems remains largely unexplored, hindered by limited onboard\ncomputing resources and brief satellite-ground contacts. We propose Grace, a\nsatellite-ground collaborative system designed for near-realtime LVLM inference\nin RS tasks. Accordingly, we deploy compact LVLM on satellites for realtime\ninference, but larger ones on ground stations (GSs) to guarantee end-to-end\nperformance. Grace is comprised of two main phases that are asynchronous\nsatellite-GS Retrieval-Augmented Generation (RAG), and a task dispatch\nalgorithm. Firstly, we still the knowledge archive of GS RAG to satellite\narchive with tailored adaptive update algorithm during limited satellite-ground\ndata exchange period. Secondly, propose a confidence-based test algorithm that\neither processes the task onboard the satellite or offloads it to the GS.\nExtensive experiments based on real-world satellite orbital data show that\nGrace reduces the average latency by 76-95% compared to state-of-the-art\nmethods, without compromising inference accuracy.\n","authors":["Zihan Li","Jiahao Yang","Yuxin Zhang","Zhe Chen","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2510.24242v1.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2510.24240v1","updated":"2025-10-28T09:47:38Z","published":"2025-10-28T09:47:38Z","title":"Temporal Knowledge Graph Hyperedge Forecasting: Exploring\n  Entity-to-Category Link Prediction","summary":"  Temporal Knowledge Graphs have emerged as a powerful way of not only modeling\nstatic relationships between entities but also the dynamics of how relations\nevolve over time. As these informational structures can be used to store\ninformation from a real-world setting, such as a news flow, predicting future\ngraph components to a certain extent equates predicting real-world events. Most\nof the research in this field focuses on embedding-based methods, often\nleveraging convolutional neural net architectures. These solutions act as black\nboxes, limiting insight. In this paper, we explore an extension to an\nestablished rule-based framework, TLogic, that yields a high accuracy in\ncombination with explainable predictions. This offers transparency and allows\nthe end-user to critically evaluate the rules applied at the end of the\nprediction stage. The new rule format incorporates entity category as a key\ncomponent with the purpose of limiting rule application only to relevant\nentities. When categories are unknown for building the graph, we propose a\ndata-driven method to generate them with an LLM-based approach. Additionally,\nwe investigate the choice of aggregation method for scores of retrieved\nentities when performing category prediction.\n","authors":["Edward Markai","Sina Molavipour"],"pdf_url":"https://arxiv.org/pdf/2510.24240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24235v1","updated":"2025-10-28T09:43:47Z","published":"2025-10-28T09:43:47Z","title":"PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware\n  Task-Adaptive Reward Modeling","summary":"  Reward models (RMs) are central to reinforcement learning from human feedback\n(RLHF), providing the critical supervision signals that align large language\nmodels (LLMs) with human preferences. While generative reward models (GRMs)\noffer greater interpretability than traditional scalar RMs, current training\nparadigms remain limited. Pair-wise methods rely on binary good-versus-bad\nlabels, which cause mismatches for point-wise inference and necessitate complex\npairing strategies for effective application in RLHF. On the other hand,\npoint-wise methods require more elaborate absolute labeling with rubric-driven\ncriteria, resulting in poor adaptability and high annotation costs. In this\nwork, we propose the Preference-Aware Task-Adaptive Reward Model (PaTaRM), a\nunified framework that integrates a preference-aware reward (PAR) mechanism\nwith dynamic rubric adaptation. PaTaRM leverages relative preference\ninformation from pairwise data to construct robust point-wise training signals,\neliminating the need for explicit point-wise labels. Simultaneously, it employs\na task-adaptive rubric system that flexibly generates evaluation criteria for\nboth global task consistency and instance-specific fine-grained reasoning. This\ndesign enables efficient, generalizable, and interpretable reward modeling for\nRLHF. Extensive experiments show that PaTaRM achieves an average relative\nimprovement of 4.7% on RewardBench and RMBench across Qwen3-8B and Qwen3-14B\nmodels. Furthermore, PaTaRM boosts downstream RLHF performance, with an average\nimprovement of 13.6% across IFEval and InFoBench benchmarks, confirming its\neffectiveness and robustness. Our code is available at\nhttps://github.com/JaneEyre0530/PaTaRM.\n","authors":["Ai Jian","Jingqing Ruan","Xing Ma","Dailin Li","QianLin Zhou","Ke Zeng","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2510.24235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11930v2","updated":"2025-10-28T09:43:41Z","published":"2025-05-17T09:34:57Z","title":"The Logical Expressiveness of Temporal GNNs via Two-Dimensional Product\n  Logics","summary":"  In recent years, the expressive power of various neural architectures --\nincluding graph neural networks (GNNs), transformers, and recurrent neural\nnetworks -- has been characterised using tools from logic and formal language\ntheory. As the capabilities of basic architectures are becoming well\nunderstood, increasing attention is turning to models that combine multiple\narchitectural paradigms. Among them particularly important, and challenging to\nanalyse, are temporal extensions of GNNs, which integrate both spatial\n(graph-structure) and temporal (evolution over time) dimensions. In this paper,\nwe initiate the study of logical characterisation of temporal GNNs by\nconnecting them to two-dimensional product logics. We show that the expressive\npower of temporal GNNs depends on how graph and temporal components are\ncombined. In particular, temporal GNNs that apply static GNNs recursively over\ntime can capture all properties definable in the product logic of (past)\npropositional temporal logic PTL and the modal logic K. In contrast,\narchitectures such as graph-and-time TGNNs and global TGNNs can only express\nrestricted fragments of this logic, where the interaction between temporal and\nspatial operators is syntactically constrained. These provide us with the first\nresults on the logical expressiveness of temporal GNNs.\n","authors":["Marco SÃ¤lzer","PrzemysÅaw Andrzej WaÅÄga","Martin Lange"],"pdf_url":"https://arxiv.org/pdf/2505.11930v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24234v1","updated":"2025-10-28T09:42:15Z","published":"2025-10-28T09:42:15Z","title":"Sparse Optimistic Information Directed Sampling","summary":"  Many high-dimensional online decision-making problems can be modeled as\nstochastic sparse linear bandits. Most existing algorithms are designed to\nachieve optimal worst-case regret in either the data-rich regime, where\npolynomial dependence on the ambient dimension is unavoidable, or the data-poor\nregime, where dimension-independence is possible at the cost of worse\ndependence on the number of rounds. In contrast, the sparse Information\nDirected Sampling (IDS) algorithm satisfies a Bayesian regret bound that has\nthe optimal rate in both regimes simultaneously. In this work, we explore the\nuse of Sparse Optimistic Information Directed Sampling (SOIDS) to achieve the\nsame adaptivity in the worst-case setting, without Bayesian assumptions.\nThrough a novel analysis that enables the use of a time-dependent learning\nrate, we show that SOIDS can optimally balance information and regret. Our\nresults extend the theoretical guarantees of IDS, providing the first algorithm\nthat simultaneously achieves optimal worst-case regret in both the data-rich\nand data-poor regimes. We empirically demonstrate the good performance of\nSOIDS.\n","authors":["Ludovic Schwartz","Hamish Flynn","Gergely Neu"],"pdf_url":"https://arxiv.org/pdf/2510.24234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24233v1","updated":"2025-10-28T09:42:03Z","published":"2025-10-28T09:42:03Z","title":"PRIVET: Privacy Metric Based on Extreme Value Theory","summary":"  Deep generative models are often trained on sensitive data, such as genetic\nsequences, health data, or more broadly, any copyrighted, licensed or protected\ncontent. This raises critical concerns around privacy-preserving synthetic\ndata, and more specifically around privacy leakage, an issue closely tied to\noverfitting. Existing methods almost exclusively rely on global criteria to\nestimate the risk of privacy failure associated to a model, offering only\nquantitative non interpretable insights. The absence of rigorous evaluation\nmethods for data privacy at the sample-level may hinder the practical\ndeployment of synthetic data in real-world applications. Using extreme value\nstatistics on nearest-neighbor distances, we propose PRIVET, a generic\nsample-based, modality-agnostic algorithm that assigns an individual privacy\nleak score to each synthetic sample. We empirically demonstrate that PRIVET\nreliably detects instances of memorization and privacy leakage across diverse\ndata modalities, including settings with very high dimensionality, limited\nsample sizes such as genetic data and even under underfitting regimes. We\ncompare our method to existing approaches under controlled settings and show\nits advantage in providing both dataset level and sample level assessments\nthrough qualitative and quantitative outputs. Additionally, our analysis\nreveals limitations in existing computer vision embeddings to yield\nperceptually meaningful distances when identifying near-duplicate samples.\n","authors":["Antoine Szatkownik","AurÃ©lien Decelle","Beatriz Seoane","Nicolas Bereux","LÃ©o Planche","Guillaume Charpiat","Burak Yelmen","Flora Jay","Cyril Furtlehner"],"pdf_url":"https://arxiv.org/pdf/2510.24233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16947v2","updated":"2025-10-28T09:41:22Z","published":"2025-05-22T17:32:50Z","title":"MixAT: Combining Continuous and Discrete Adversarial Training for LLMs","summary":"  Despite recent efforts in Large Language Model (LLM) safety and alignment,\ncurrent adversarial attacks on frontier LLMs can still consistently force\nharmful generations. Although adversarial training has been widely studied and\nshown to significantly improve the robustness of traditional machine learning\nmodels, its strengths and weaknesses in the context of LLMs are less\nunderstood. Specifically, while existing discrete adversarial attacks are\neffective at producing harmful content, training LLMs with concrete adversarial\nprompts is often computationally expensive, leading to reliance on continuous\nrelaxations. At the same time, despite their effectiveness and generalization\ncapabilities, training with continuous perturbations does not always capture\nthe full spectrum of vulnerabilities exploited by discrete attacks. In this\nwork, we aim to bridge this gap by introducing MixAT, a novel method that\ncombines stronger discrete and faster continuous attacks during training. We\nrigorously evaluate MixAT across a wide spectrum of state-of-the-art attacks,\nproposing the At Least One Attack Success Rate (ALO-ASR) metric to capture the\nworst-case vulnerability of models. We show MixAT achieves substantially better\nrobustness (ALO-ASR < 20%) compared to prior defenses (ALO-ASR > 50%), while\nmaintaining a runtime comparable to methods based on continuous relaxations. We\nfurther analyze MixAT in realistic deployment settings, exploring how chat\ntemplates, quantization, low-rank adapters, and temperature affect both\nadversarial training and evaluation, revealing additional blind spots in\ncurrent methodologies. Our results demonstrate that MixAT's discrete-continuous\ndefense offers a principled and superior robustness-accuracy tradeoff with\nminimal computational overhead, highlighting its promise for building safer\nLLMs. We provide our code and models at\nhttps://github.com/insait-institute/MixAT.\n","authors":["Csaba DÃ©kÃ¡ny","Stefan Balauca","Robin Staab","Dimitar I. Dimitrov","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2505.16947v2.pdf","comment":"Published at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.22777v2","updated":"2025-10-28T09:39:42Z","published":"2025-10-26T18:01:32Z","title":"SeeDNorm: Self-Rescaled Dynamic Normalization","summary":"  Normalization layer constitutes an essential component in neural networks. In\ntransformers, the predominantly used RMSNorm constrains vectors to a unit\nhypersphere, followed by dimension-wise rescaling through a learnable scaling\ncoefficient $\\gamma$ to maintain the representational capacity of the model.\nHowever, RMSNorm discards the input norm information in forward pass and a\nstatic scaling factor $\\gamma$ may be insufficient to accommodate the wide\nvariability of input data and distributional shifts, thereby limiting further\nperformance improvements, particularly in zero-shot scenarios that large\nlanguage models routinely encounter. To address this limitation, we propose\nSeeDNorm, which enhances the representational capability of the model by\ndynamically adjusting the scaling coefficient based on the current input,\nthereby preserving the input norm information and enabling data-dependent,\nself-rescaled dynamic normalization. During backpropagation, SeeDNorm retains\nthe ability of RMSNorm to dynamically adjust gradient according to the input\nnorm. We provide a detailed analysis of the training optimization for SeedNorm\nand proposed corresponding solutions to address potential instability issues\nthat may arise when applying SeeDNorm. We validate the effectiveness of\nSeeDNorm across models of varying sizes in large language model pre-training as\nwell as supervised and unsupervised computer vision tasks. By introducing a\nminimal number of parameters and with neglligible impact on model efficiency,\nSeeDNorm achieves consistently superior performance compared to previously\ncommonly used normalization layers such as RMSNorm and LayerNorm, as well as\nelement-wise activation alternatives to normalization layers like DyT.\n","authors":["Wenrui Cai","Defa Zhu","Qingjie Liu","Qiyang Min"],"pdf_url":"https://arxiv.org/pdf/2510.22777v2.pdf","comment":"31 pages, 14 figures, 18 tables"},{"id":"http://arxiv.org/abs/2510.24228v1","updated":"2025-10-28T09:39:41Z","published":"2025-10-28T09:39:41Z","title":"A comparison between joint and dual UKF implementations for state\n  estimation and leak localization in water distribution networks","summary":"  The sustainability of modern cities highly depends on efficient water\ndistribution management, including effective pressure control and leak\ndetection and localization. Accurate information about the network hydraulic\nstate is therefore essential. This article presents a comparison between two\ndata-driven state estimation methods based on the Unscented Kalman Filter\n(UKF), fusing pressure, demand and flow data for head and flow estimation. One\napproach uses a joint state vector with a single estimator, while the other\nuses a dual-estimator scheme. We analyse their main characteristics, discussing\ndifferences, advantages and limitations, and compare them theoretically in\nterms of accuracy and complexity. Finally, we show several estimation results\nfor the L-TOWN benchmark, allowing to discuss their properties in a real\nimplementation.\n","authors":["Luis Romero-Ben","Paul Irofti","Florin Stoican","VicenÃ§ Puig"],"pdf_url":"https://arxiv.org/pdf/2510.24228v1.pdf","comment":"This work has been submitted to ECC2026 for review. It has 7 pages\n  and 2 figures"},{"id":"http://arxiv.org/abs/2508.06041v3","updated":"2025-10-28T09:34:36Z","published":"2025-08-08T05:57:04Z","title":"DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision\n  Assignment","summary":"  How can we effectively handle queries for on-device large language models\n(LLMs) with varying runtime constraints, such as latency and accuracy?\nMulti-scale quantization addresses this challenge by enabling memory-efficient\nruntime model adaptation of LLMs through the overlaying of multiple model\nvariants quantized to different bitwidths. Meanwhile, an important question\nstill remains open-ended: how can models be properly configured to match a\ntarget precision or latency? While mixed-precision offers a promising solution,\nwe take this further by leveraging the key observation that the sensitivity of\neach layer dynamically changes across decoding steps. Building on this insight,\nwe introduce DP-LLM, a novel mechanism that dynamically assigns precision to\neach layer based on input values. Experimental results across multiple models\nand benchmarks demonstrate that DP-LLM achieves a superior performance-latency\ntrade-off, outperforming prior approaches.\n","authors":["Sangwoo Kwon","Seong Hoon Seo","Jae W. Lee","Yeonhong Park"],"pdf_url":"https://arxiv.org/pdf/2508.06041v3.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2310.13966v2","updated":"2025-10-28T09:32:53Z","published":"2023-10-21T10:55:31Z","title":"Minimax Optimal Transfer Learning for Kernel-based Nonparametric\n  Regression","summary":"  In recent years, transfer learning has garnered significant attention in the\nmachine learning community. Its ability to leverage knowledge from related\nstudies to improve generalization performance in a target study has made it\nhighly appealing. This paper focuses on investigating the transfer learning\nproblem within the context of nonparametric regression over a reproducing\nkernel Hilbert space. The aim is to bridge the gap between practical\neffectiveness and theoretical guarantees. We specifically consider two\nscenarios: one where the transferable sources are known and another where they\nare unknown. For the known transferable source case, we propose a two-step\nkernel-based estimator by solely using kernel ridge regression. For the unknown\ncase, we develop a novel method based on an efficient aggregation algorithm,\nwhich can automatically detect and alleviate the effects of negative sources.\nThis paper provides the statistical properties of the desired estimators and\nestablishes the minimax optimal rate. Through extensive numerical experiments\non synthetic data and real examples, we validate our theoretical findings and\ndemonstrate the effectiveness of our proposed method.\n","authors":["Chao Wang","Caixing Wang","Xin He","Xingdong Feng"],"pdf_url":"https://arxiv.org/pdf/2310.13966v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.17550v3","updated":"2025-10-28T09:32:18Z","published":"2025-09-22T09:09:13Z","title":"Is It Certainly a Deepfake? Reliability Analysis in Detection &\n  Generation Ecosystem","summary":"  As generative models are advancing in quality and quantity for creating\nsynthetic content, deepfakes begin to cause online mistrust. Deepfake detectors\nare proposed to counter this effect, however, misuse of detectors claiming fake\ncontent as real or vice versa further fuels this misinformation problem. We\npresent the first comprehensive uncertainty analysis of deepfake detectors,\nsystematically investigating how generative artifacts influence prediction\nconfidence. As reflected in detectors' responses, deepfake generators also\ncontribute to this uncertainty as their generative residues vary, so we cross\nthe uncertainty analysis of deepfake detectors and generators. Based on our\nobservations, the uncertainty manifold holds enough consistent information to\nleverage uncertainty for deepfake source detection. Our approach leverages\nBayesian Neural Networks and Monte Carlo dropout to quantify both aleatoric and\nepistemic uncertainties across diverse detector architectures. We evaluate\nuncertainty on two datasets with nine generators, with four blind and two\nbiological detectors, compare different uncertainty methods, explore region-\nand pixel-based uncertainty, and conduct ablation studies. We conduct and\nanalyze binary real/fake, multi-class real/fake, source detection, and\nleave-one-out experiments between the generator/detector combinations to share\ntheir generalization capability, model calibration, uncertainty, and robustness\nagainst adversarial attacks. We further introduce uncertainty maps that\nlocalize prediction confidence at the pixel level, revealing distinct patterns\ncorrelated with generator-specific artifacts. Our analysis provides critical\ninsights for deploying reliable deepfake detection systems and establishes\nuncertainty quantification as a fundamental requirement for trustworthy\nsynthetic media detection.\n","authors":["Neslihan Kose","Anthony Rhodes","Umur Aybars Ciftci","Ilke Demir"],"pdf_url":"https://arxiv.org/pdf/2509.17550v3.pdf","comment":"Accepted for publication at the ICCV 2025 workshop - STREAM"},{"id":"http://arxiv.org/abs/2505.08256v2","updated":"2025-10-28T09:31:26Z","published":"2025-05-13T06:10:05Z","title":"Clustering-Based Low-Rank Matrix Approximation for Medical Image\n  Compression","summary":"  Medical images are inherently high-resolution and contain locally varying\nstructures crucial for diagnosis. Efficient compression must preserve\ndiagnostic fidelity while minimizing redundancy. Low-rank matrix approximation\n(LoRMA) techniques have shown strong potential for image compression by\ncapturing global correlations; however, they often fail to adapt to local\nstructural variations across regions of interest. To address this, we introduce\nan adaptive LoRMA, which partitions a medical image into overlapping patches,\ngroups structurally similar patches into clusters using k-means, and performs\nSVD within each cluster. We derive the overall compression factor accounting\nfor patch overlap and analyze how patch size influences compression efficiency\nand computational cost. While applicable to any data with high local variation,\nwe focus on medical imaging due to its pronounced local variability. We\nevaluate and compare our adaptive LoRMA against global SVD across four imaging\nmodalities: MRI, ultrasound, CT scan, and chest X-ray. Results demonstrate that\nadaptive LoRMA effectively preserves structural integrity, edge details, and\ndiagnostic relevance, measured by PSNR, SSIM, MSE, IoU, and EPI. Adaptive LoRMA\nminimizes block artifacts and residual errors, particularly in pathological\nregions, consistently outperforming global SVD in PSNR, SSIM, IoU, EPI, and\nachieving lower MSE. It prioritizes clinically salient regions while allowing\naggressive compression in non-critical regions, optimizing storage efficiency.\nAlthough adaptive LoRMA requires higher processing time, its diagnostic\nfidelity justifies the overhead for high-compression applications.\n","authors":["Sisipho Hamlomo","Marcellin Atemkeng"],"pdf_url":"https://arxiv.org/pdf/2505.08256v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24217v1","updated":"2025-10-28T09:30:52Z","published":"2025-10-28T09:30:52Z","title":"Closing Gaps: An Imputation Analysis of ICU Vital Signs","summary":"  As more Intensive Care Unit (ICU) data becomes available, the interest in\ndeveloping clinical prediction models to improve healthcare protocols\nincreases. However, the lack of data quality still hinders clinical prediction\nusing Machine Learning (ML). Many vital sign measurements, such as heart rate,\ncontain sizeable missing segments, leaving gaps in the data that could\nnegatively impact prediction performance. Previous works have introduced\nnumerous time-series imputation techniques. Nevertheless, more comprehensive\nwork is needed to compare a representative set of methods for imputing ICU\nvital signs and determine the best practice. In reality, ad-hoc imputation\ntechniques that could decrease prediction accuracy, like zero imputation, are\nstill used. In this work, we compare established imputation techniques to guide\nresearchers in improving the performance of clinical prediction models by\nselecting the most accurate imputation technique. We introduce an extensible\nand reusable benchmark with currently 15 imputation and 4 amputation methods,\ncreated for benchmarking on major ICU datasets. We hope to provide a\ncomparative basis and facilitate further ML development to bring more models\ninto clinical practice.\n","authors":["Alisher Turubayev","Anna Shopova","Fabian Lange","Mahmut Kamalak","Paul Mattes","Victoria Ayvasky","Bert Arnrich","Bjarne Pfitzner","Robin P. van de Water"],"pdf_url":"https://arxiv.org/pdf/2510.24217v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2510.24216v1","updated":"2025-10-28T09:30:35Z","published":"2025-10-28T09:30:35Z","title":"Unlocking Out-of-Distribution Generalization in Dynamics through\n  Physics-Guided Augmentation","summary":"  In dynamical system modeling, traditional numerical methods are limited by\nhigh computational costs, while modern data-driven approaches struggle with\ndata scarcity and distribution shifts. To address these fundamental\nlimitations, we first propose SPARK, a physics-guided quantitative augmentation\nplugin. Specifically, SPARK utilizes a reconstruction autoencoder to integrate\nphysical parameters into a physics-rich discrete state dictionary. This state\ndictionary then acts as a structured dictionary of physical states, enabling\nthe creation of new, physically-plausible training samples via principled\ninterpolation in the latent space. Further, for downstream prediction, these\naugmented representations are seamlessly integrated with a Fourier-enhanced\nGraph ODE, a combination designed to robustly model the enriched data\ndistribution while capturing long-term temporal dependencies. Extensive\nexperiments on diverse benchmarks demonstrate that SPARK significantly\noutperforms state-of-the-art baselines, particularly in challenging\nout-of-distribution scenarios and data-scarce regimes, proving the efficacy of\nour physics-guided augmentation paradigm.\n","authors":["Fan Xu","Hao Wu","Kun Wang","Nan Wang","Qingsong Wen","Xian Wu","Wei Gong","Xibin Zhao"],"pdf_url":"https://arxiv.org/pdf/2510.24216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24215v1","updated":"2025-10-28T09:29:46Z","published":"2025-10-28T09:29:46Z","title":"What Can Be Recovered Under Sparse Adversarial Corruption?\n  Assumption-Free Theory for Linear Measurements","summary":"  Let \\(\\bm{A} \\in \\mathbb{R}^{m \\times n}\\) be an arbitrary, known matrix and\n\\(\\bm{e}\\) a \\(q\\)-sparse adversarial vector. Given \\(\\bm{y} = \\bm{A} x^* +\n\\bm{e}\\) and \\(q\\), we seek the smallest set containing \\(x^*\\)-hence the one\nconveying maximal information about \\(x^*\\)-that is uniformly recoverable from\n\\(\\bm{y}\\) without knowing \\(\\bm{e}\\). While exact recovery of \\(x^*\\) via\nstrong (and often impractical) structural assumptions on \\(\\bm{A}\\) or \\(x^*\\)\n(for example, restricted isometry, sparsity) is well studied, recoverability\nfor arbitrary \\(\\bm{A}\\) and \\(x^*\\) remains open. Our main result shows that\nthe best that one can hope to recover is \\(x^* + \\ker(\\bm{U})\\), where\n\\(\\bm{U}\\) is the unique projection matrix onto the intersection of rowspaces\nof all possible submatrices of \\(\\bm{A}\\) obtained by deleting \\(2q\\) rows.\nMoreover, we prove that every \\(x\\) that minimizes the \\(\\ell\\_0\\)-norm of\n\\(\\bm{y} - \\bm{A} x\\) lies in \\(x^* + \\ker(\\bm{U})\\), which then gives a\nconstructive approach to recover this set.\n","authors":["Vishal Halder","Alexandre Reiffers-Masson","Abdeldjalil AÃ¯ssa-El-Bey","Gugan Thoppe"],"pdf_url":"https://arxiv.org/pdf/2510.24215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21800v2","updated":"2025-10-28T09:27:41Z","published":"2025-10-20T16:49:22Z","title":"MARS-M: When Variance Reduction Meets Matrices","summary":"  Matrix-based preconditioned optimizers, such as Muon, have recently been\nshown to be more efficient than scalar-based optimizers for training\nlarge-scale neural networks, including large language models (LLMs). On the\nother hand, recent benchmarks on optimizers for LLM pre-training have\ndemonstrated that variance-reduction techniques such as MARS can achieve\nsubstantial speedups over standard optimizers that do not employ variance\nreduction. In this paper, to achieve the best of both worlds, we introduce\nMARS-M, a new optimizer that integrates the variance reduction technique in\nMARS with Muon. Under standard regularity conditions, we prove that Muon-M\nconverges to a first-order stationary point at a rate of\n$\\tilde{\\mathcal{O}}(T^{-1/3})$, which improves upon\n$\\tilde{\\mathcal{O}}(T^{-1/4})$ rate attained by Muon. Our empirical results on\nlanguage modeling and computer vision tasks demonstrate that MARS-M\nconsistently yields lower losses and improved performance across various\ndownstream benchmarks. The implementation of MARS-M is available at\nhttps://github.com/AGI-Arena/MARS/tree/main/MARS_M.\n","authors":["Yifeng Liu","Angela Yuan","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2510.21800v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24208v1","updated":"2025-10-28T09:25:40Z","published":"2025-10-28T09:25:40Z","title":"Beyond Neural Incompatibility: Easing Cross-Scale Knowledge Transfer in\n  Large Language Models through Latent Semantic Alignment","summary":"  Large Language Models (LLMs) encode vast amounts of knowledge in their\nmassive parameters, which is accessible to locate, trace, and analyze. Despite\nadvances in neural interpretability, it is still not clear how to transfer\nknowledge in a fine-grained manner, namely parametric knowledge transfer (PKT).\nA key problem is enabling effective and efficient knowledge transfer across\nLLMs of different scales, which is essential for achieving greater flexibility\nand broader applicability in transferring knowledge between LLMs. Due to neural\nincompatibility, referring to the architectural and parametric differences\nbetween LLMs of varying scales, existing methods that directly reuse layer\nparameters are severely limited. In this paper, we identify the semantic\nalignment in latent space as the fundamental prerequisite for LLM cross-scale\nknowledge transfer. Instead of directly using the layer parameters, our\napproach takes activations as the medium of layer-wise knowledge transfer.\nLeveraging the semantics in latent space, our approach is simple and\noutperforms prior work, better aligning model behaviors across varying scales.\nEvaluations on four benchmarks demonstrate the efficacy of our method. Further\nanalysis reveals the key factors easing cross-scale knowledge transfer and\nprovides insights into the nature of latent semantic alignment.\n","authors":["Jian Gu","Aldeida Aleti","Chunyang Chen","Hongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.24208v1.pdf","comment":"an early-stage version"},{"id":"http://arxiv.org/abs/2510.15403v2","updated":"2025-10-28T09:19:23Z","published":"2025-10-17T07:56:15Z","title":"Geometric Mixture Models for Electrolyte Conductivity Prediction","summary":"  Accurate prediction of ionic conductivity in electrolyte systems is crucial\nfor advancing numerous scientific and technological applications. While\nsignificant progress has been made, current research faces two fundamental\nchallenges: (1) the lack of high-quality standardized benchmarks, and (2)\ninadequate modeling of geometric structure and intermolecular interactions in\nmixture systems. To address these limitations, we first reorganize and enhance\nthe CALiSol and DiffMix electrolyte datasets by incorporating geometric graph\nrepresentations of molecules. We then propose GeoMix, a novel geometry-aware\nframework that preserves Set-SE(3) equivariance-an essential but challenging\nproperty for mixture systems. At the heart of GeoMix lies the Geometric\nInteraction Network (GIN), an equivariant module specifically designed for\nintermolecular geometric message passing. Comprehensive experiments demonstrate\nthat GeoMix consistently outperforms diverse baselines (including MLPs, GNNs,\nand geometric GNNs) across both datasets, validating the importance of\ncross-molecular geometric interactions and equivariant message passing for\naccurate property prediction. This work not only establishes new benchmarks for\nelectrolyte research but also provides a general geometric learning framework\nthat advances modeling of mixture systems in energy materials, pharmaceutical\ndevelopment, and beyond.\n","authors":["Anyi Li","Jiacheng Cen","Songyou Li","Mingze Li","Yang Yu","Wenbing Huang"],"pdf_url":"https://arxiv.org/pdf/2510.15403v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21758v2","updated":"2025-10-28T09:14:57Z","published":"2025-10-11T20:16:32Z","title":"Taxonomy and Trends in Reinforcement Learning for Robotics and Control\n  Systems: A Structured Review","summary":"  Reinforcement learning (RL) has become a foundational approach for enabling\nintelligent robotic behavior in dynamic and uncertain environments. This work\npresents an in-depth review of RL principles, advanced deep reinforcement\nlearning (DRL) algorithms, and their integration into robotic and control\nsystems. Beginning with the formalism of Markov Decision Processes (MDPs), the\nstudy outlines essential elements of the agent-environment interaction and\nexplores core algorithmic strategies including actor-critic methods,\nvalue-based learning, and policy gradients. Emphasis is placed on modern DRL\ntechniques such as DDPG, TD3, PPO, and SAC, which have shown promise in solving\nhigh-dimensional, continuous control tasks. A structured taxonomy is introduced\nto categorize RL applications across domains such as locomotion, manipulation,\nmulti-agent coordination, and human-robot interaction, along with training\nmethodologies and deployment readiness levels. The review synthesizes recent\nresearch efforts, highlighting technical trends, design patterns, and the\ngrowing maturity of RL in real-world robotics. Overall, this work aims to\nbridge theoretical advances with practical implementations, providing a\nconsolidated perspective on the evolving role of RL in autonomous robotic\nsystems.\n","authors":["Kumater Ter","Ore-Ofe Ajayi","Daniel Udekwe"],"pdf_url":"https://arxiv.org/pdf/2510.21758v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17734v2","updated":"2025-10-28T09:06:30Z","published":"2025-05-23T10:54:53Z","title":"URB -- Urban Routing Benchmark for RL-equipped Connected Autonomous\n  Vehicles","summary":"  Connected Autonomous Vehicles (CAVs) promise to reduce congestion in future\nurban networks, potentially by optimizing their routing decisions. Unlike for\nhuman drivers, these decisions can be made with collective, data-driven\npolicies, developed using machine learning algorithms. Reinforcement learning\n(RL) can facilitate the development of such collective routing strategies, yet\nstandardized and realistic benchmarks are missing. To that end, we present URB:\nUrban Routing Benchmark for RL-equipped Connected Autonomous Vehicles. URB is a\ncomprehensive benchmarking environment that unifies evaluation across 29\nreal-world traffic networks paired with realistic demand patterns. URB comes\nwith a catalog of predefined tasks, multi-agent RL (MARL) algorithm\nimplementations, three baseline methods, domain-specific performance metrics,\nand a modular configuration scheme. Our results show that, despite the lengthy\nand costly training, state-of-the-art MARL algorithms rarely outperformed\nhumans. The experimental results reported in this paper initiate the first\nleaderboard for MARL in large-scale urban routing optimization. They reveal\nthat current approaches struggle to scale, emphasizing the urgent need for\nadvancements in this domain.\n","authors":["Ahmet Onur Akman","Anastasia Psarou","MichaÅ Hoffmann","Åukasz Gorczyca","Åukasz Kowalski","PaweÅ Gora","Grzegorz JamrÃ³z","RafaÅ Kucharski"],"pdf_url":"https://arxiv.org/pdf/2505.17734v2.pdf","comment":"Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025), Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2510.24200v1","updated":"2025-10-28T09:06:19Z","published":"2025-10-28T09:06:19Z","title":"SPEAR++: Scaling Gradient Inversion via Sparsely-Used Dictionary\n  Learning","summary":"  Federated Learning has seen an increased deployment in real-world scenarios\nrecently, as it enables the distributed training of machine learning models\nwithout explicit data sharing between individual clients. Yet, the introduction\nof the so-called gradient inversion attacks has fundamentally challenged its\nprivacy-preserving properties. Unfortunately, as these attacks mostly rely on\ndirect data optimization without any formal guarantees, the vulnerability of\nreal-world systems remains in dispute and requires tedious testing for each new\nfederated deployment. To overcome these issues, recently the SPEAR attack was\nintroduced, which is based on a theoretical analysis of the gradients of linear\nlayers with ReLU activations. While SPEAR is an important theoretical\nbreakthrough, the attack's practicality was severely limited by its exponential\nruntime in the batch size b. In this work, we fill this gap by applying\nState-of-the-Art techniques from Sparsely-Used Dictionary Learning to make the\nproblem of gradient inversion on linear layers with ReLU activations tractable.\nOur experiments demonstrate that our new attack, SPEAR++, retains all desirable\nproperties of SPEAR, such as robustness to DP noise and FedAvg aggregation,\nwhile being applicable to 10x bigger batch sizes.\n","authors":["Alexander Bakarsky","Dimitar I. Dimitrov","Maximilian Baader","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2510.24200v1.pdf","comment":"Published at the Workshop on Regulatable ML at the 39th Conference on\n  Neural Information Processing Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.24194v1","updated":"2025-10-28T08:57:27Z","published":"2025-10-28T08:57:27Z","title":"Blindfolded Experts Generalize Better: Insights from Robotic\n  Manipulation and Videogames","summary":"  Behavioral cloning is a simple yet effective technique for learning\nsequential decision-making from demonstrations. Recently, it has gained\nprominence as the core of foundation models for the physical world, where\nachieving generalization requires countless demonstrations of a multitude of\ntasks. Typically, a human expert with full information on the task demonstrates\na (nearly) optimal behavior. In this paper, we propose to hide some of the\ntask's information from the demonstrator. This ``blindfolded'' expert is\ncompelled to employ non-trivial exploration to solve the task. We show that\ncloning the blindfolded expert generalizes better to unseen tasks than its\nfully-informed counterpart. We conduct experiments of real-world robot peg\ninsertion tasks with (limited) human demonstrations, alongside videogames from\nthe Procgen benchmark. Additionally, we support our findings with theoretical\nanalysis, which confirms that the generalization error scales with\n$\\sqrt{I/m}$, where $I$ measures the amount of task information available to\nthe demonstrator, and $m$ is the number of demonstrated tasks. Both theory and\npractice indicate that cloning blindfolded experts generalizes better with\nfewer demonstrated tasks. Project page with videos and code:\nhttps://sites.google.com/view/blindfoldedexperts/home\n","authors":["Ev Zisselman","Mirco Mutti","Shelly Francis-Meretzki","Elisei Shafer","Aviv Tamar"],"pdf_url":"https://arxiv.org/pdf/2510.24194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13898v3","updated":"2025-10-28T08:56:58Z","published":"2025-05-20T04:00:56Z","title":"Do Language Models Use Their Depth Efficiently?","summary":"  Modern LLMs are increasingly deep, and depth correlates with performance,\nalbeit with diminishing returns. However, do these models use their depth\nefficiently? Do they compose more features to create higher-order computations\nthat are impossible in shallow models, or do they merely spread the same kinds\nof computation out over more layers? To address these questions, we analyze the\nresidual stream of the Llama 3.1, Qwen 3, and OLMo 2 family of models. We find:\nFirst, comparing the output of the sublayers to the residual stream reveals\nthat layers in the second half contribute much less than those in the first\nhalf, with a clear phase transition between the two halves. Second, skipping\nlayers in the second half has a much smaller effect on future computations and\noutput predictions. Third, for multihop tasks, we are unable to find evidence\nthat models are using increased depth to compose subresults in examples\ninvolving many hops. Fourth, we seek to directly address whether deeper models\nare using their additional layers to perform new kinds of computation. To do\nthis, we train linear maps from the residual stream of a shallow model to a\ndeeper one. We find that layers with the same relative depth map best to each\nother, suggesting that the larger model simply spreads the same computations\nout over its many layers. All this evidence suggests that deeper models are not\nusing their depth to learn new kinds of computation, but only using the greater\ndepth to perform more fine-grained adjustments to the residual. This may help\nexplain why increasing scale leads to diminishing returns for stacked\nTransformer architectures.\n","authors":["RÃ³bert CsordÃ¡s","Christopher D. Manning","Christopher Potts"],"pdf_url":"https://arxiv.org/pdf/2505.13898v3.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2508.12730v2","updated":"2025-10-28T08:47:58Z","published":"2025-08-18T08:53:53Z","title":"Unlearning Comparator: A Visual Analytics System for Comparative\n  Evaluation of Machine Unlearning Methods","summary":"  Machine Unlearning (MU) aims to remove target training data from a trained\nmodel so that the removed data no longer influences the model's behavior,\nfulfilling \"right to be forgotten\" obligations under data privacy laws. Yet, we\nobserve that researchers in this rapidly emerging field face challenges in\nanalyzing and understanding the behavior of different MU methods, especially in\nterms of three fundamental principles in MU: accuracy, efficiency, and privacy.\nConsequently, they often rely on aggregate metrics and ad-hoc evaluations,\nmaking it difficult to accurately assess the trade-offs between methods. To\nfill this gap, we introduce a visual analytics system, Unlearning Comparator,\ndesigned to facilitate the systematic evaluation of MU methods. Our system\nsupports two important tasks in the evaluation process: model comparison and\nattack simulation. First, it allows the user to compare the behaviors of two\nmodels, such as a model generated by a certain method and a retrained baseline,\nat class-, instance-, and layer-levels to better understand the changes made\nafter unlearning. Second, our system simulates membership inference attacks\n(MIAs) to evaluate the privacy of a method, where an attacker attempts to\ndetermine whether specific data samples were part of the original training set.\nWe evaluate our system through a case study visually analyzing prominent MU\nmethods and demonstrate that it helps the user not only understand model\nbehaviors but also gain insights that can inform the improvement of MU methods.\nThe source code is publicly available at\nhttps://github.com/gnueaj/Machine-Unlearning-Comparator.\n","authors":["Jaeung Lee","Suhyeon Yu","Yurim Jang","Simon S. Woo","Jaemin Jo"],"pdf_url":"https://arxiv.org/pdf/2508.12730v2.pdf","comment":"Submitted to IEEE Transactions on Visualization and Computer Graphics\n  (TVCG), under review. 15 pages. This work has been submitted to the IEEE for\n  possible publication"},{"id":"http://arxiv.org/abs/2510.24187v1","updated":"2025-10-28T08:47:15Z","published":"2025-10-28T08:47:15Z","title":"Self-Concordant Perturbations for Linear Bandits","summary":"  We study the adversarial linear bandits problem and present a unified\nalgorithmic framework that bridges Follow-the-Regularized-Leader (FTRL) and\nFollow-the-Perturbed-Leader (FTPL) methods, extending the known connection\nbetween them from the full-information setting. Within this framework, we\nintroduce self-concordant perturbations, a family of probability distributions\nthat mirror the role of self-concordant barriers previously employed in the\nFTRL-based SCRiBLe algorithm. Using this idea, we design a novel FTPL-based\nalgorithm that combines self-concordant regularization with efficient\nstochastic exploration. Our approach achieves a regret of $O(d\\sqrt{n \\ln n})$\non both the $d$-dimensional hypercube and the Euclidean ball. On the Euclidean\nball, this matches the rate attained by existing self-concordant FTRL methods.\nFor the hypercube, this represents a $\\sqrt{d}$ improvement over these methods\nand matches the optimal bound up to logarithmic factors.\n","authors":["Lucas LÃ©vy","Jean-Lou Valeau","Arya Akhavan","Patrick Rebeschini"],"pdf_url":"https://arxiv.org/pdf/2510.24187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01356v2","updated":"2025-10-28T08:42:04Z","published":"2025-06-02T06:20:09Z","title":"Two-Stage Learning of Stabilizing Neural Controllers via Zubov Sampling\n  and Iterative Domain Expansion","summary":"  Learning-based neural network (NN) control policies have shown impressive\nempirical performance. However, obtaining stability guarantees and estimates of\nthe region of attraction of these learned neural controllers is challenging due\nto the lack of stable and scalable training and verification algorithms.\nAlthough previous works in this area have achieved great success, much\nconservatism remains in their frameworks. In this work, we propose a novel\ntwo-stage training framework to jointly synthesize a controller and a Lyapunov\nfunction for continuous-time systems. By leveraging a Zubov-inspired region of\nattraction characterization to directly estimate stability boundaries, we\npropose a novel training-data sampling strategy and a domain-updating mechanism\nthat significantly reduces the conservatism in training. Moreover, unlike\nexisting works on continuous-time systems that rely on an SMT solver to\nformally verify the Lyapunov condition, we extend state-of-the-art neural\nnetwork verifier $\\alpha,\\!\\beta$-CROWN with the capability of performing\nautomatic bound propagation through the Jacobian of dynamical systems and a\nnovel verification scheme that avoids expensive bisection. To demonstrate the\neffectiveness of our approach, we conduct numerical experiments by synthesizing\nand verifying controllers on several challenging nonlinear systems across\nmultiple dimensions. We show that our training can yield region of attractions\nwith volume $5 - 1.5\\cdot 10^{5}$ times larger compared to the baselines, and\nour verification on continuous systems can be up to $40-10{,}000$ times faster\ncompared to the traditional SMT solver dReal. Our code is available at\nhttps://github.com/Verified-Intelligence/Two-Stage_Neural_Controller_Training.\n","authors":["Haoyu Li","Xiangru Zhong","Bin Hu","Huan Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.01356v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24180v1","updated":"2025-10-28T08:34:27Z","published":"2025-10-28T08:34:27Z","title":"V-SAT: Video Subtitle Annotation Tool","summary":"  The surge of audiovisual content on streaming platforms and social media has\nheightened the demand for accurate and accessible subtitles. However, existing\nsubtitle generation methods primarily speech-based transcription or OCR-based\nextraction suffer from several shortcomings, including poor synchronization,\nincorrect or harmful text, inconsistent formatting, inappropriate reading\nspeeds, and the inability to adapt to dynamic audio-visual contexts. Current\napproaches often address isolated issues, leaving post-editing as a\nlabor-intensive and time-consuming process. In this paper, we introduce V-SAT\n(Video Subtitle Annotation Tool), a unified framework that automatically\ndetects and corrects a wide range of subtitle quality issues. By combining\nLarge Language Models(LLMs), Vision-Language Models (VLMs), Image Processing,\nand Automatic Speech Recognition (ASR), V-SAT leverages contextual cues from\nboth audio and video. Subtitle quality improved, with the SUBER score reduced\nfrom 9.6 to 3.54 after resolving all language mode issues and F1-scores of\n~0.80 for image mode issues. Human-in-the-loop validation ensures high-quality\nresults, providing the first comprehensive solution for robust subtitle\nannotation.\n","authors":["Arpita Kundu","Joyita Chakraborty","Anindita Desarkar","Aritra Sen","Srushti Anil Patil","Vishwanathan Raman"],"pdf_url":"https://arxiv.org/pdf/2510.24180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.03188v4","updated":"2025-10-28T08:28:52Z","published":"2025-04-04T05:32:54Z","title":"Pairwise Optimal Transports for Training All-to-All Flow-Based Condition\n  Transfer Model","summary":"  In this paper, we propose a flow-based method for learning all-to-all\ntransfer maps among conditional distributions that approximates pairwise\noptimal transport. The proposed method addresses the challenge of handling the\ncase of continuous conditions, which often involve a large set of conditions\nwith sparse empirical observations per condition. We introduce a novel cost\nfunction that enables simultaneous learning of optimal transports for all pairs\nof conditional distributions. Our method is supported by a theoretical\nguarantee that, in the limit, it converges to the pairwise optimal transports\namong infinite pairs of conditional distributions. The learned transport maps\nare subsequently used to couple data points in conditional flow matching. We\ndemonstrate the effectiveness of this method on synthetic and benchmark\ndatasets, as well as on chemical datasets in which continuous physical\nproperties are defined as conditions. The code for this project can be found at\nhttps://github.com/kotatumuri-room/A2A-FM\n","authors":["Kotaro Ikeda","Masanori Koyama","Jinzhe Zhang","Kohei Hayashi","Kenji Fukumizu"],"pdf_url":"https://arxiv.org/pdf/2504.03188v4.pdf","comment":"Accepted at NeurIPS 2025, 32 pages, 18 figures"},{"id":"http://arxiv.org/abs/2510.24173v1","updated":"2025-10-28T08:27:37Z","published":"2025-10-28T08:27:37Z","title":"EddyFormer: Accelerated Neural Simulations of Three-Dimensional\n  Turbulence at Scale","summary":"  Computationally resolving turbulence remains a central challenge in fluid\ndynamics due to its multi-scale interactions. Fully resolving large-scale\nturbulence through direct numerical simulation (DNS) is computationally\nprohibitive, motivating data-driven machine learning alternatives. In this\nwork, we propose EddyFormer, a Transformer-based spectral-element (SEM)\narchitecture for large-scale turbulence simulation that combines the accuracy\nof spectral methods with the scalability of the attention mechanism. We\nintroduce an SEM tokenization that decomposes the flow into grid-scale and\nsubgrid-scale components, enabling capture of both local and global features.\nWe create a new three-dimensional isotropic turbulence dataset and train\nEddyFormer to achieves DNS-level accuracy at 256^3 resolution, providing a 30x\nspeedup over DNS. When applied to unseen domains up to 4x larger than in\ntraining, EddyFormer preserves accuracy on physics-invariant metrics-energy\nspectra, correlation functions, and structure functions-showing domain\ngeneralization. On The Well benchmark suite of diverse turbulent flows,\nEddyFormer resolves cases where prior ML models fail to converge, accurately\nreproducing complex dynamics across a wide range of physical conditions.\n","authors":["Yiheng Du","Aditi S. Krishnapriyan"],"pdf_url":"https://arxiv.org/pdf/2510.24173v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2407.13195v6","updated":"2025-10-28T08:26:28Z","published":"2024-07-18T06:16:09Z","title":"Scalable Exploration via Ensemble++","summary":"  Thompson Sampling is a principled method for balancing exploration and\nexploitation, but its real-world adoption faces computational challenges in\nlarge-scale or non-conjugate settings. While ensemble-based approaches offer\npartial remedies, they typically require prohibitively large ensemble sizes. We\npropose Ensemble++, a scalable exploration framework using a novel\nshared-factor ensemble architecture with random linear combinations. For linear\nbandits, we provide theoretical guarantees showing that Ensemble++ achieves\nregret comparable to exact Thompson Sampling with only $\\Theta(d \\log T)$\nensemble sizes--significantly outperforming prior methods. Crucially, this\nefficiency holds across both compact and finite action sets with either\ntime-invariant or time-varying contexts without configuration changes. We\nextend this theoretical foundation to nonlinear rewards by replacing fixed\nfeatures with learnable neural representations while preserving the same\nincremental update principle, effectively bridging theory and practice for\nreal-world tasks. Comprehensive experiments across linear, quadratic, neural,\nand GPT-based contextual bandits validate our theoretical findings and\ndemonstrate Ensemble++'s superior regret-computation tradeoff versus\nstate-of-the-art methods.\n","authors":["Yingru Li","Jiawei Xu","Baoxiang Wang","Zhi-Quan Luo"],"pdf_url":"https://arxiv.org/pdf/2407.13195v6.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2502.15805v3","updated":"2025-10-28T08:12:05Z","published":"2025-02-19T07:01:00Z","title":"FragFM: Hierarchical Framework for Efficient Molecule Generation via\n  Fragment-Level Discrete Flow Matching","summary":"  We introduce FragFM, a novel hierarchical framework via fragment-level\ndiscrete flow matching for efficient molecular graph generation. FragFM\ngenerates molecules at the fragment level, leveraging a coarse-to-fine\nautoencoder to reconstruct details at the atom level. Together with a\nstochastic fragment bag strategy to effectively handle an extensive fragment\nspace, our framework enables more efficient and scalable molecular generation.\nWe demonstrate that our fragment-based approach achieves better property\ncontrol than the atom-based method and additional flexibility through\nconditioning the fragment bag. We also propose a Natural Product Generation\nbenchmark (NPGen) to evaluate modern molecular graph generative models' ability\nto generate natural product-like molecules. Since natural products are\nbiologically prevalidated and differ from typical drug-like molecules, our\nbenchmark provides a more challenging yet meaningful evaluation relevant to\ndrug discovery. We conduct a FragFM comparative study against various models on\ndiverse molecular generation benchmarks, including NPGen, demonstrating\nsuperior performance. The results highlight the potential of fragment-based\ngenerative modeling for large-scale, property-aware molecular design, paving\nthe way for more efficient exploration of chemical space.\n","authors":["Joongwon Lee","Seonghwan Kim","Seokhyun Moon","Hyunwoo Kim","Woo Youn Kim"],"pdf_url":"https://arxiv.org/pdf/2502.15805v3.pdf","comment":"49 pages, 29 figures, under review"},{"id":"http://arxiv.org/abs/2507.01271v4","updated":"2025-10-28T08:11:23Z","published":"2025-07-02T01:13:08Z","title":"PULSE: Practical Evaluation Scenarios for Large Multimodal Model\n  Unlearning","summary":"  In recent years, unlearning techniques, which are methods for inducing a\nmodel to \"forget\" previously learned information, have attracted attention as a\nway to address privacy and copyright concerns in large language models (LLMs)\nand large multimodal models (LMMs). While several unlearning benchmarks have\nbeen established for LLMs, a practical evaluation framework for unlearning in\nLMMs has been less explored. Specifically, existing unlearning benchmark for\nLMMs considers only scenarios in which the model is required to unlearn\nfine-tuned knowledge through a single unlearning operation. In this study, we\nintroduce PULSE protocol for realistic unlearning scenarios for LMMs by\nintroducing two critical perspectives: (i) Pre-trained knowledge Unlearning for\nanalyzing the effect across different knowledge acquisition phases and (ii)\nLong-term Sustainability Evaluation to address sequential requests. We then\nevaluate existing unlearning methods along these dimensions. Our results reveal\nthat, although some techniques can successfully unlearn knowledge acquired\nthrough fine-tuning, they struggle to eliminate information learned during\npre-training. Moreover, methods that effectively unlearn a batch of target data\nin a single operation exhibit substantial performance degradation when the same\ndata are split and unlearned sequentially.\n","authors":["Tatsuki Kawakami","Kazuki Egashira","Atsuyuki Miyai","Go Irie","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2507.01271v4.pdf","comment":"Accepted at NeurIPS 2025 Workshop: Evaluating the Evolving LLM\n  Lifecycle"},{"id":"http://arxiv.org/abs/2505.17637v2","updated":"2025-10-28T07:57:56Z","published":"2025-05-23T08:58:38Z","title":"Causal Spatio-Temporal Prediction: An Effective and Efficient\n  Multi-Modal Approach","summary":"  Spatio-temporal prediction plays a crucial role in intelligent\ntransportation, weather forecasting, and urban planning. While integrating\nmulti-modal data has shown potential for enhancing prediction accuracy, key\nchallenges persist: (i) inadequate fusion of multi-modal information, (ii)\nconfounding factors that obscure causal relations, and (iii) high computational\ncomplexity of prediction models. To address these challenges, we propose\nE^2-CSTP, an Effective and Efficient Causal multi-modal Spatio-Temporal\nPrediction framework. E^2-CSTP leverages cross-modal attention and gating\nmechanisms to effectively integrate multi-modal data. Building on this, we\ndesign a dual-branch causal inference approach: the primary branch focuses on\nspatio-temporal prediction, while the auxiliary branch mitigates bias by\nmodeling additional modalities and applying causal interventions to uncover\ntrue causal dependencies. To improve model efficiency, we integrate GCN with\nthe Mamba architecture for accelerated spatio-temporal encoding. Extensive\nexperiments on 4 real-world datasets show that E^2-CSTP significantly\noutperforms 9 state-of-the-art methods, achieving up to 9.66% improvements in\naccuracy as well as 17.37%-56.11% reductions in computational overhead.\n","authors":["Yuting Huang","Ziquan Fang","Zhihao Zeng","Lu Chen","Yunjun Gao"],"pdf_url":"https://arxiv.org/pdf/2505.17637v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.10101v3","updated":"2025-10-28T07:57:39Z","published":"2025-10-11T08:24:07Z","title":"Rademacher Meets Colors: More Expressivity, but at What Cost ?","summary":"  The expressive power of graph neural networks (GNNs) is typically understood\nthrough their correspondence with graph isomorphism tests such as the\nWeisfeiler-Leman (WL) hierarchy. While more expressive GNNs can distinguish a\nricher set of graphs, they are also observed to suffer from higher\ngeneralization error. This work provides a theoretical explanation for this\ntrade-off by linking expressivity and generalization through the lens of\ncoloring algorithms. Specifically, we show that the number of equivalence\nclasses induced by WL colorings directly bounds the GNNs Rademacher complexity\n-- a key data-dependent measure of generalization. Our analysis reveals that\ngreater expressivity leads to higher complexity and thus weaker generalization\nguarantees. Furthermore, we prove that the Rademacher complexity is stable\nunder perturbations in the color counts across different samples, ensuring\nrobustness to sampling variability across datasets. Importantly, our framework\nis not restricted to message-passing GNNs or 1-WL, but extends to arbitrary GNN\narchitectures and expressivity measures that partition graphs into equivalence\nclasses. These results unify the study of expressivity and generalization in\nGNNs, providing a principled understanding of why increasing expressive power\noften comes at the cost of generalization.\n","authors":["Martin Carrasco","Caio F. Deberaldini Netto","Vahan A. Martirosyan","Aneeqa Mehrab","Ehimare Okoyomon","Caterina Graziani"],"pdf_url":"https://arxiv.org/pdf/2510.10101v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24160v1","updated":"2025-10-28T07:57:14Z","published":"2025-10-28T07:57:14Z","title":"Identifiable learning of dissipative dynamics","summary":"  Complex dissipative systems appear across science and engineering, from\npolymers and active matter to learning algorithms. These systems operate far\nfrom equilibrium, where energy dissipation and time irreversibility are key to\ntheir behavior, but are difficult to quantify from data. Learning accurate and\ninterpretable models of such dynamics remains a major challenge: the models\nmust be expressive enough to describe diverse processes, yet constrained enough\nto remain physically meaningful and mathematically identifiable. Here, we\nintroduce I-OnsagerNet, a neural framework that learns dissipative stochastic\ndynamics directly from trajectories while ensuring both interpretability and\nuniqueness. I-OnsagerNet extends the Onsager principle to guarantee that the\nlearned potential is obtained from the stationary density and that the drift\ndecomposes cleanly into time-reversible and time-irreversible components, as\ndictated by the Helmholtz decomposition. Our approach enables us to calculate\nthe entropy production and to quantify irreversibility, offering a principled\nway to detect and quantify deviations from equilibrium. Applications to polymer\nstretching in elongational flow and to stochastic gradient Langevin dynamics\nreveal new insights, including super-linear scaling of barrier heights and\nsub-linear scaling of entropy production rates with the strain rate, and the\nsuppression of irreversibility with increasing batch size. I-OnsagerNet thus\nestablishes a general, data-driven framework for discovering and interpreting\nnon-equilibrium dynamics.\n","authors":["Aiqing Zhu","Beatrice W. Soh","Grigorios A. Pavliotis","Qianxiao Li"],"pdf_url":"https://arxiv.org/pdf/2510.24160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24159v1","updated":"2025-10-28T07:55:34Z","published":"2025-10-28T07:55:34Z","title":"Self-supervised Synthetic Pretraining for Inference of Stellar Mass\n  Embedded in Dense Gas","summary":"  Stellar mass is a fundamental quantity that determines the properties and\nevolution of stars. However, estimating stellar masses in star-forming regions\nis challenging because young stars are obscured by dense gas and the regions\nare highly inhomogeneous, making spherical dynamical estimates unreliable.\nSupervised machine learning could link such complex structures to stellar mass,\nbut it requires large, high-quality labeled datasets from high-resolution\nmagneto-hydrodynamical (MHD) simulations, which are computationally expensive.\nWe address this by pretraining a vision transformer on one million synthetic\nfractal images using the self-supervised framework DINOv2, and then applying\nthe frozen model to limited high-resolution MHD simulations. Our results\ndemonstrate that synthetic pretraining improves frozen-feature regression\nstellar mass predictions, with the pretrained model performing slightly better\nthan a supervised model trained on the same limited simulations. Principal\ncomponent analysis of the extracted features further reveals semantically\nmeaningful structures, suggesting that the model enables unsupervised\nsegmentation of star-forming regions without the need for labeled data or\nfine-tuning.\n","authors":["Keiya Hirashima","Shingo Nozaki","Naoto Harada"],"pdf_url":"https://arxiv.org/pdf/2510.24159v1.pdf","comment":"6 pages, 3 figures, 1 table, accepted for NeurIPS 2025 ML4PS workshop"},{"id":"http://arxiv.org/abs/2509.04415v2","updated":"2025-10-28T07:32:34Z","published":"2025-09-04T17:37:35Z","title":"Interpretable Clustering with Adaptive Heterogeneous Causal Structure\n  Learning in Mixed Observational Data","summary":"  Understanding causal heterogeneity is essential for scientific discovery in\ndomains such as biology and medicine. However, existing methods lack causal\nawareness, with insufficient modeling of heterogeneity, confounding, and\nobservational constraints, leading to poor interpretability and difficulty\ndistinguishing true causal heterogeneity from spurious associations. We propose\nan unsupervised framework, HCL (Interpretable Causal Mechanism-Aware Clustering\nwith Adaptive Heterogeneous Causal Structure Learning), that jointly infers\nlatent clusters and their associated causal structures from mixed-type\nobservational data without requiring temporal ordering, environment labels,\ninterventions or other prior knowledge. HCL relaxes the homogeneity and\nsufficiency assumptions by introducing an equivalent representation that\nencodes both structural heterogeneity and confounding. It further develops a\nbi-directional iterative strategy to alternately refine causal clustering and\nstructure learning, along with a self-supervised regularization that balance\ncross-cluster universality and specificity. Together, these components enable\nconvergence toward interpretable, heterogeneous causal patterns. Theoretically,\nwe show identifiability of heterogeneous causal structures under mild\nconditions. Empirically, HCL achieves superior performance in both clustering\nand structure learning tasks, and recovers biologically meaningful mechanisms\nin real-world single-cell perturbation data, demonstrating its utility for\ndiscovering interpretable, mechanism-level causal heterogeneity.\n","authors":["Wenrui Li","Qinghao Zhang","Xiaowo Wang"],"pdf_url":"https://arxiv.org/pdf/2509.04415v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24135v1","updated":"2025-10-28T07:20:38Z","published":"2025-10-28T07:20:38Z","title":"Fixed Point Neural Acceleration and Inverse Surrogate Model for Battery\n  Parameter Identification","summary":"  The rapid expansion of electric vehicles has intensified the need for\naccurate and efficient diagnosis of lithium-ion batteries. Parameter\nidentification of electrochemical battery models is widely recognized as a\npowerful method for battery health assessment. However, conventional\nmetaheuristic approaches suffer from high computational cost and slow\nconvergence, and recent machine learning methods are limited by their reliance\non constant current data, which may not be available in practice. To overcome\nthese challenges, we propose deep learning-based framework for parameter\nidentification of electrochemical battery models. The proposed framework\ncombines a neural surrogate model of the single particle model with electrolyte\n(NeuralSPMe) and a deep learning-based fixed-point iteration method. NeuralSPMe\nis trained on realistic EV load profiles to accurately predict lithium\nconcentration dynamics under dynamic operating conditions while a parameter\nupdate network (PUNet) performs fixed-point iterative updates to significantly\nreduce both the evaluation time per sample and the overall number of iterations\nrequired for convergence. Experimental evaluations demonstrate that the\nproposed framework accelerates the parameter identification by more than 2000\ntimes, achieves superior sample efficiency and more than 10 times higher\naccuracy compared to conventional metaheuristic algorithms, particularly under\ndynamic load scenarios encountered in practical applications.\n","authors":["Hojin Cheon","Hyeongseok Seo","Jihun Jeon","Wooju Lee","Dohyun Jeong","Hongseok Kim"],"pdf_url":"https://arxiv.org/pdf/2510.24135v1.pdf","comment":"31 pages, 11 figures, submitted to Applied Energy"},{"id":"http://arxiv.org/abs/2507.14111v8","updated":"2025-10-28T07:04:44Z","published":"2025-07-18T17:43:56Z","title":"CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement\n  Learning","summary":"  The exponential growth in demand for GPU computing resources has created an\nurgent need for automated CUDA optimization strategies. While recent advances\nin LLMs show promise for code generation, current SOTA models achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization that employs a\nnovel contrastive RL algorithm.\n  CUDA-L1 achieves significant performance improvements on the CUDA\noptimization task: trained on A100, it delivers an average speedup of x3.12\nwith a median speedup of x1.42 against default baselines over across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x120. In addition to\nthe default baseline provided by KernelBench, CUDA-L1 demonstrates x2.77 over\nTorch Compile, x2.88 over Torch Compile with reduce overhead, x2.81 over CUDA\nGraph implementations, and remarkably x7.72 over cuDNN libraries. Furthermore,\nthe model also demonstrates portability across different GPU architectures.\n  Beyond these benchmark results, CUDA-L1 demonstrates several properties: it\n1) discovers a variety of CUDA optimization techniques and learns to combine\nthem strategically to achieve optimal performance; 2) uncovers fundamental\nprinciples of CUDA optimization, such as the multiplicative nature of\noptimizations; 3) identifies non-obvious performance bottlenecks and rejects\nseemingly beneficial optimizations that actually harm performance. The\ncapabilities demonstrate that, RL can transform an initially poor-performing\nLLM into an effective CUDA optimizer through speedup-based reward signals\nalone, without human expertise or domain knowledge. This paradigm opens\npossibilities for automated optimization of CUDA operations, and holds promise\nto substantially promote GPU efficiency and alleviate the rising pressure on\nGPU computing resources.\n","authors":["Xiaoya Li","Xiaofei Sun","Albert Wang","Jiwei Li","Chris Shum"],"pdf_url":"https://arxiv.org/pdf/2507.14111v8.pdf","comment":"Project Page: https://deepreinforce-ai.github.io/cudal1_blog/"},{"id":"http://arxiv.org/abs/2510.24125v1","updated":"2025-10-28T06:57:14Z","published":"2025-10-28T06:57:14Z","title":"Causal Convolutional Neural Networks as Finite Impulse Response Filters","summary":"  This study investigates the behavior of Causal Convolutional Neural Networks\n(CNNs) with quasi-linear activation functions when applied to time-series data\ncharacterized by multimodal frequency content. We demonstrate that, once\ntrained, such networks exhibit properties analogous to Finite Impulse Response\n(FIR) filters, particularly when the convolutional kernels are of extended\nlength exceeding those typically employed in standard CNN architectures. Causal\nCNNs are shown to capture spectral features both implicitly and explicitly,\noffering enhanced interpretability for tasks involving dynamic systems.\nLeveraging the associative property of convolution, we further show that the\nentire network can be reduced to an equivalent single-layer filter resembling\nan FIR filter optimized via least-squares criteria. This equivalence yields new\ninsights into the spectral learning behavior of CNNs trained on signals with\nsparse frequency content. The approach is validated on both simulated beam\ndynamics and real-world bridge vibration datasets, underlining its relevance\nfor modeling and identifying physical systems governed by dynamic responses.\n","authors":["Kiran Bacsa","Wei Liu","Xudong Jian","Huangbin Liang","Eleni Chatzi"],"pdf_url":"https://arxiv.org/pdf/2510.24125v1.pdf","comment":"14 pages, 19 figures, Under review"},{"id":"http://arxiv.org/abs/2506.05426v3","updated":"2025-10-28T06:55:14Z","published":"2025-06-05T06:29:14Z","title":"Mixture-of-Experts Meets In-Context Reinforcement Learning","summary":"  In-context reinforcement learning (ICRL) has emerged as a promising paradigm\nfor adapting RL agents to downstream tasks through prompt conditioning.\nHowever, two notable challenges remain in fully harnessing in-context learning\nwithin RL domains: the intrinsic multi-modality of the state-action-reward data\nand the diverse, heterogeneous nature of decision tasks. To tackle these\nchallenges, we propose T2MIR (Token- and Task-wise MoE for In-context RL), an\ninnovative framework that introduces architectural advances of\nmixture-of-experts (MoE) into transformer-based decision models. T2MIR\nsubstitutes the feedforward layer with two parallel layers: a token-wise MoE\nthat captures distinct semantics of input tokens across multiple modalities,\nand a task-wise MoE that routes diverse tasks to specialized experts for\nmanaging a broad task distribution with alleviated gradient conflicts. To\nenhance task-wise routing, we introduce a contrastive learning method that\nmaximizes the mutual information between the task and its router\nrepresentation, enabling more precise capture of task-relevant information. The\noutputs of two MoE components are concatenated and fed into the next layer.\nComprehensive experiments show that T2MIR significantly facilitates in-context\nlearning capacity and outperforms various types of baselines. We bring the\npotential and promise of MoE to ICRL, offering a simple and scalable\narchitectural enhancement to advance ICRL one step closer toward achievements\nin language and vision communities. Our code is available at\nhttps://github.com/NJU-RL/T2MIR.\n","authors":["Wenhao Wu","Fuhong Liu","Haoru Li","Zican Hu","Daoyi Dong","Chunlin Chen","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2506.05426v3.pdf","comment":"28 pages, 13 figures, 17 tables"},{"id":"http://arxiv.org/abs/2510.24120v1","updated":"2025-10-28T06:47:30Z","published":"2025-10-28T06:47:30Z","title":"Graph-Guided Concept Selection for Efficient Retrieval-Augmented\n  Generation","summary":"  Graph-based RAG constructs a knowledge graph (KG) from text chunks to enhance\nretrieval in Large Language Model (LLM)-based question answering. It is\nespecially beneficial in domains such as biomedicine, law, and political\nscience, where effective retrieval often involves multi-hop reasoning over\nproprietary documents. However, these methods demand numerous LLM calls to\nextract entities and relations from text chunks, incurring prohibitive costs at\nscale. Through a carefully designed ablation study, we observe that certain\nwords (termed concepts) and their associated documents are more important.\nBased on this insight, we propose Graph-Guided Concept Selection (G2ConS). Its\ncore comprises a chunk selection method and an LLM-independent concept graph.\nThe former selects salient document chunks to reduce KG construction costs; the\nlatter closes knowledge gaps introduced by chunk selection at zero cost.\nEvaluations on multiple real-world datasets show that G2ConS outperforms all\nbaselines in construction cost, retrieval effectiveness, and answering quality.\n","authors":["Ziyu Liu","Yijing Liu","Jianfei Yuan","Minzhi Yan","Le Yue","Honghui Xiong","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2510.24120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24115v1","updated":"2025-10-28T06:38:59Z","published":"2025-10-28T06:38:59Z","title":"HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws\n  in Vision-Language Models for Histopathology","summary":"  For doctors to truly trust artificial intelligence, it can't be a black box.\nThey need to understand its reasoning, almost as if they were consulting a\ncolleague. We created HistoLens1 to be that transparent, collaborative partner.\nIt allows a pathologist to simply ask a question in plain English about a\ntissue slide--just as they would ask a trainee. Our system intelligently\ntranslates this question into a precise query for its AI engine, which then\nprovides a clear, structured report. But it doesn't stop there. If a doctor\never asks, \"Why?\", HistoLens can instantly provide a 'visual proof' for any\nfinding--a heatmap that points to the exact cells and regions the AI used for\nits analysis. We've also ensured the AI focuses only on the patient's tissue,\njust like a trained pathologist would, by teaching it to ignore distracting\nbackground noise. The result is a workflow where the pathologist remains the\nexpert in charge, using a trustworthy AI assistant to verify their insights and\nmake faster, more confident diagnoses.\n","authors":["Sandeep Vissapragada","Vikrant Sahu","Gagan Raj Gupta","Vandita Singh"],"pdf_url":"https://arxiv.org/pdf/2510.24115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21582v2","updated":"2025-10-28T06:37:16Z","published":"2025-10-24T15:50:31Z","title":"An unsupervised tour through the hidden pathways of deep neural networks","summary":"  The goal of this thesis is to improve our understanding of the internal\nmechanisms by which deep artificial neural networks create meaningful\nrepresentations and are able to generalize. We focus on the challenge of\ncharacterizing the semantic content of the hidden representations with\nunsupervised learning tools, partially developed by us and described in this\nthesis, which allow harnessing the low-dimensional structure of the data.\nChapter 2. introduces Gride, a method that allows estimating the intrinsic\ndimension of the data as an explicit function of the scale without performing\nany decimation of the data set. Our approach is based on rigorous\ndistributional results that enable the quantification of uncertainty of the\nestimates. Moreover, our method is simple and computationally efficient since\nit relies only on the distances among nearest data points. In Chapter 3, we\nstudy the evolution of the probability density across the hidden layers in some\nstate-of-the-art deep neural networks. We find that the initial layers generate\na unimodal probability density getting rid of any structure irrelevant to\nclassification. In subsequent layers, density peaks arise in a hierarchical\nfashion that mirrors the semantic hierarchy of the concepts. This process\nleaves a footprint in the probability density of the output layer, where the\ntopography of the peaks allows reconstructing the semantic relationships of the\ncategories. In Chapter 4, we study the problem of generalization in deep neural\nnetworks: adding parameters to a network that interpolates its training data\nwill typically improve its generalization performance, at odds with the\nclassical bias-variance trade-off. We show that wide neural networks learn\nredundant representations instead of overfitting to spurious correlation and\nthat redundant neurons appear only if the network is regularized and the\ntraining error is zero.\n","authors":["Diego Doimo"],"pdf_url":"https://arxiv.org/pdf/2510.21582v2.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2510.24113v1","updated":"2025-10-28T06:36:44Z","published":"2025-10-28T06:36:44Z","title":"Taming the Tail: NoI Topology Synthesis for Mixed DL Workloads on\n  Chiplet-Based Accelerators","summary":"  Heterogeneous chiplet-based systems improve scaling by disag-gregating\nCPUs/GPUs and emerging technologies (HBM/DRAM).However this on-package\ndisaggregation introduces a latency inNetwork-on-Interposer(NoI). We observe\nthat in modern large-modelinference, parameters and activations routinely move\nbackand forth from HBM/DRAM, injecting large, bursty flows into theinterposer.\nThese memory-driven transfers inflate tail latency andviolate Service Level\nAgreements (SLAs) across k-ary n-cube base-line NoI topologies. To address this\ngap we introduce an InterferenceScore (IS) that quantifies worst-case slowdown\nunder contention.We then formulate NoI synthesis as a multi-objective\noptimization(MOO) problem. We develop PARL (Partition-Aware\nReinforcementLearner), a topology generator that balances throughput,\nlatency,and power. PARL-generated topologies reduce contention at the memory\ncut, meet SLAs, and cut worst-case slowdown to 1.2 times while maintaining\ncompetitive mean throughput relative to link-rich meshes. Overall, this\nreframes NoI design for heterogeneouschiplet accelerators with workload-aware\nobjectives.\n","authors":["Arnav Shukla","Harsh Sharma","Srikant Bharadwaj","Vinayak Abrol","Sujay Deb"],"pdf_url":"https://arxiv.org/pdf/2510.24113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12758v4","updated":"2025-10-28T06:27:55Z","published":"2025-05-19T06:35:11Z","title":"Global urban visual perception varies across demographics and\n  personalities","summary":"  Understanding people's preferences is crucial for urban planning, yet current\napproaches often combine responses from multi-cultural populations, obscuring\ndemographic differences and risking amplifying biases. We conducted a\nlargescale urban visual perception survey of streetscapes worldwide using\nstreet view imagery, examining how demographics -- including gender, age,\nincome, education, race and ethnicity, and personality traits -- shape\nperceptions among 1,000 participants with balanced demographics from five\ncountries and 45 nationalities. This dataset, Street Perception Evaluation\nConsidering Socioeconomics (SPECS), reveals demographic- and personality-based\ndifferences across six traditional indicators -- safe, lively, wealthy,\nbeautiful, boring, depressing -- and four new ones -- live nearby, walk, cycle,\ngreen. Location-based sentiments further shape these preferences. Machine\nlearning models trained on existing global datasets tend to overestimate\npositive indicators and underestimate negative ones compared to human\nresponses, underscoring the need for local context. Our study aspires to\nrectify the myopic treatment of street perception, which rarely considers\ndemographics or personality traits.\n","authors":["Matias Quintana","Youlong Gu","Xiucheng Liang","Yujun Hou","Koichi Ito","Yihan Zhu","Mahmoud Abdelrahman","Filip Biljecki"],"pdf_url":"https://arxiv.org/pdf/2505.12758v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24105v1","updated":"2025-10-28T06:21:06Z","published":"2025-10-28T06:21:06Z","title":"Enhancing Pre-trained Representation Classifiability can Boost its\n  Interpretability","summary":"  The visual representation of a pre-trained model prioritizes the\nclassifiability on downstream tasks, while the widespread applications for\npre-trained visual models have posed new requirements for representation\ninterpretability. However, it remains unclear whether the pre-trained\nrepresentations can achieve high interpretability and classifiability\nsimultaneously. To answer this question, we quantify the representation\ninterpretability by leveraging its correlation with the ratio of interpretable\nsemantics within the representations. Given the pre-trained representations,\nonly the interpretable semantics can be captured by interpretations, whereas\nthe uninterpretable part leads to information loss. Based on this fact, we\npropose the Inherent Interpretability Score (IIS) that evaluates the\ninformation loss, measures the ratio of interpretable semantics, and quantifies\nthe representation interpretability. In the evaluation of the representation\ninterpretability with different classifiability, we surprisingly discover that\nthe interpretability and classifiability are positively correlated, i.e.,\nrepresentations with higher classifiability provide more interpretable\nsemantics that can be captured in the interpretations. This observation further\nsupports two benefits to the pre-trained representations. First, the\nclassifiability of representations can be further improved by fine-tuning with\ninterpretability maximization. Second, with the classifiability improvement for\nthe representations, we obtain predictions based on their interpretations with\nless accuracy degradation. The discovered positive correlation and\ncorresponding applications show that practitioners can unify the improvements\nin interpretability and classifiability for pre-trained vision models. Codes\nare available at https://github.com/ssfgunner/IIS.\n","authors":["Shufan Shen","Zhaobo Qi","Junshu Sun","Qingming Huang","Qi Tian","Shuhui Wang"],"pdf_url":"https://arxiv.org/pdf/2510.24105v1.pdf","comment":"ICLR 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2502.04242v4","updated":"2025-10-28T06:15:39Z","published":"2025-02-06T17:32:49Z","title":"A High-Dimensional Statistical Method for Optimizing Transfer Quantities\n  in Multi-Source Transfer Learning","summary":"  Multi-source transfer learning provides an effective solution to data\nscarcity in real-world supervised learning scenarios by leveraging multiple\nsource tasks. In this field, existing works typically use all available samples\nfrom sources in training, which constrains their training efficiency and may\nlead to suboptimal results. To address this, we propose a theoretical framework\nthat answers the question: what is the optimal quantity of source samples\nneeded from each source task to jointly train the target model? Specifically,\nwe introduce a generalization error measure based on K-L divergence, and\nminimize it based on high-dimensional statistical analysis to determine the\noptimal transfer quantity for each source task. Additionally, we develop an\narchitecture-agnostic and data-efficient algorithm OTQMS to implement our\ntheoretical results for target model training in multi-source transfer\nlearning. Experimental studies on diverse architectures and two real-world\nbenchmark datasets show that our proposed algorithm significantly outperforms\nstate-of-the-art approaches in both accuracy and data efficiency. The code and\nsupplementary materials are available in https://github.com/zqy0126/OTQMS.\n","authors":["Qingyue Zhang","Haohao Fu","Guanbo Huang","Yaoyuan Liang","Chang Chu","Tianren Peng","Yanru Wu","Qi Li","Yang Li","Shao-Lun Huang"],"pdf_url":"https://arxiv.org/pdf/2502.04242v4.pdf","comment":"NeurIPS 2025 Poster"},{"id":"http://arxiv.org/abs/2509.16989v2","updated":"2025-10-28T06:14:52Z","published":"2025-09-21T09:07:20Z","title":"PTQTP: Post-Training Quantization to Trit-Planes for Large Language\n  Models","summary":"  Post-training quantization (PTQ) of large language models (LLMs) to extremely\nlow bit-widths remains challenging due to the fundamental trade-off between\ncomputational efficiency and model expressiveness. While existing ultra-low-bit\nPTQ methods rely on binary approximations or complex compensation mechanisms,\nthey suffer from either limited representational capacity or computational\noverhead that undermines their efficiency gains. We introduce PTQ to\nTrit-Planes (PTQTP), the first ternary-weight PTQ framework that decomposes\nweight matrices into structured ternary {-1, 0, 1} trit-planes using 2x1.58-bit\nrepresentation. PTQTP achieves multiplication-free inference, identical to\n1-bit quantization, while maintaining superior expressiveness through its novel\nstructured decomposition. Our approach provides: (1) a theoretically grounded\nprogressive approximation algorithm ensuring global weight consistency; (2)\nmodel-agnostic deployment across diverse modern LLMs without architectural\nmodifications; and (3) uniform ternary operations that eliminate the need for\nmixed-precision or compensation schemes. Comprehensive experiments across\nLLaMA3.x and Qwen3 model families (0.6B-70B parameters) demonstrate that PTQTP\nsignificantly outperforms existing low-bit PTQ methods, achieving 82.4%\nmathematical reasoning retention versus 0% for competing approaches. PTQTP\napproaches and sometimes surpasses 1.58-bit quantization-aware training\nperformance while requiring only single-hour quantization compared to 10-14 GPU\ndays for training-based methods. These results establish PTQTP as a practical\nsolution for efficient LLM deployment in resource-constrained environments. The\ncode will be available at https://github.com/HeXiao-55/PTQTP.\n","authors":["He Xiao","Runming Yang","Qingyao Yang","Wendong Xu","Zhen Li","Yupeng Su","Zhengwu Liu","Hongxia Yang","Ngai Wong"],"pdf_url":"https://arxiv.org/pdf/2509.16989v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2510.24095v1","updated":"2025-10-28T06:08:25Z","published":"2025-10-28T06:08:25Z","title":"Learning Parameterized Skills from Demonstrations","summary":"  We present DEPS, an end-to-end algorithm for discovering parameterized skills\nfrom expert demonstrations. Our method learns parameterized skill policies\njointly with a meta-policy that selects the appropriate discrete skill and\ncontinuous parameters at each timestep. Using a combination of temporal\nvariational inference and information-theoretic regularization methods, we\naddress the challenge of degeneracy common in latent variable models, ensuring\nthat the learned skills are temporally extended, semantically meaningful, and\nadaptable. We empirically show that learning parameterized skills from\nmultitask expert demonstrations significantly improves generalization to unseen\ntasks. Our method outperforms multitask as well as skill learning baselines on\nboth LIBERO and MetaWorld benchmarks. We also demonstrate that DEPS discovers\ninterpretable parameterized skills, such as an object grasping skill whose\ncontinuous arguments define the grasp location.\n","authors":["Vedant Gupta","Haotian Fu","Calvin Luo","Yiding Jiang","George Konidaris"],"pdf_url":"https://arxiv.org/pdf/2510.24095v1.pdf","comment":"Neurips 2025"},{"id":"http://arxiv.org/abs/2510.24088v1","updated":"2025-10-28T05:59:05Z","published":"2025-10-28T05:59:05Z","title":"Information-Theoretic Discrete Diffusion","summary":"  We present an information-theoretic framework for discrete diffusion models\nthat yields principled estimators of log-likelihood using score-matching\nlosses. Inspired by the I-MMSE identity for the Gaussian setup, we derive\nanalogous results for the discrete setting. Specifically, we introduce the\nInformation-Minimum Denoising Score Entropy (I-MDSE) relation, which links\nmutual information between data and its diffused version to the minimum\ndenoising score entropy (DSE) loss. We extend this theory to masked diffusion\nand establish the Information-Minimum Denoising Cross-Entropy (I-MDCE)\nrelation, connecting cross-entropy losses to mutual information in discrete\nmasked processes. These results provide a time-integral decomposition of the\nlog-likelihood of the data in terms of optimal score-based losses, showing that\ncommonly used losses such as DSE and DCE are not merely variational bounds but\ntight and principled estimators of log-likelihood. The I-MDCE decomposition\nfurther enables practical extensions, including time-free formula, conditional\nlikelihood estimation in prompt-response tasks, and coupled Monte Carlo\nestimation of likelihood ratios. Experiments on synthetic and real-world data\nconfirm the accuracy, variance stability, and utility of our estimators. The\ncode is publicly available at https://github.com/Dongjae0324/infodis.\n","authors":["Moongyu Jeon","Sangwoo Shin","Dongjae Jeon","Albert No"],"pdf_url":"https://arxiv.org/pdf/2510.24088v1.pdf","comment":"Accepted at NeurIPS 2025"}],"Multimedia":[{"id":"http://arxiv.org/abs/2509.17336v2","updated":"2025-10-28T14:31:14Z","published":"2025-09-22T03:13:58Z","title":"Mano Technical Report","summary":"  Graphical user interfaces (GUIs) are the primary medium for human-computer\ninteraction, yet automating GUI interactions remains challenging due to the\ncomplexity of visual elements, dynamic environments, and the need for\nmulti-step reasoning. Existing methods based on vision-language models (VLMs)\noften suffer from limited resolution, domain mismatch, and insufficient\nsequential decisionmaking capability. To address these issues, we propose Mano,\na robust GUI agent built upon a multi-modal foundation model pre-trained on\nextensive web and computer system data. Our approach integrates a novel\nsimulated environment for high-fidelity data generation, a three-stage training\npipeline (supervised fine-tuning, offline reinforcement learning, and online\nreinforcement learning), and a verification module for error recovery. Mano\ndemonstrates state-of-the-art performance on multiple GUI benchmarks, including\nMind2Web and OSWorld, achieving significant improvements in success rate and\noperational accuracy. Our work provides new insights into the effective\nintegration of reinforcement learning with VLMs for practical GUI agent\ndeployment, highlighting the importance of domain-specific data, iterative\ntraining, and holistic reward design.\n","authors":["Tianyu Fu","Anyang Su","Chenxu Zhao","Hanning Wang","Minghui Wu","Zhe Yu","Fei Hu","Mingjia Shi","Wei Dong","Jiayao Wang","Yuyang Chen","Ruiyang Yu","Siran Peng","Menglin Li","Nan Huang","Haitian Wei","Jiawei Yu","Yi Xin","Xilin Zhao","Kai Gu","Ping Jiang","Sifan Zhou","Shuo Wang"],"pdf_url":"https://arxiv.org/pdf/2509.17336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05229v2","updated":"2025-10-28T08:05:02Z","published":"2025-05-08T13:21:10Z","title":"Does CLIP perceive art the same way we do?","summary":"  CLIP has emerged as a powerful multimodal model capable of connecting images\nand text through joint embeddings, but to what extent does it 'see' the same\nway humans do - especially when interpreting artworks? In this paper, we\ninvestigate CLIP's ability to extract high-level semantic and stylistic\ninformation from paintings, including both human-created and AI-generated\nimagery. We evaluate its perception across multiple dimensions: content, scene\nunderstanding, artistic style, historical period, and the presence of visual\ndeformations or artifacts. By designing targeted probing tasks and comparing\nCLIP's responses to human annotations and expert benchmarks, we explore its\nalignment with human perceptual and contextual understanding. Our findings\nreveal both strengths and limitations in CLIP's visual representations,\nparticularly in relation to aesthetic cues and artistic intent. We further\ndiscuss the implications of these insights for using CLIP as a guidance\nmechanism during generative processes, such as style transfer or prompt-based\nimage synthesis. Our work highlights the need for deeper interpretability in\nmultimodal systems, especially when applied to creative domains where nuance\nand subjectivity play a central role.\n","authors":["Andrea Asperti","Leonardo DessÃ¬","Maria Chiara Tonetti","Nico Wu"],"pdf_url":"https://arxiv.org/pdf/2505.05229v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24161v1","updated":"2025-10-28T07:58:39Z","published":"2025-10-28T07:58:39Z","title":"BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and\n  Cross-Embodiment Learning","summary":"  Multimodal large language models (MLLMs) have advanced vision-language\nreasoning and are increasingly deployed in embodied agents. However,\nsignificant limitations remain: MLLMs generalize poorly across digital-physical\nspaces and embodiments; vision-language-action models (VLAs) produce low-level\nactions yet lack robust high-level embodied reasoning; and most embodied large\nlanguage models (ELLMs) are constrained to digital-space with poor\ngeneralization to the physical world. Thus, unified models that operate\nseamlessly across digital and physical spaces while generalizing across\nembodiments and tasks remain absent. We introduce the \\textbf{Boundless Large\nModel (BLM$_1$)}, a multimodal spatial foundation model that preserves\ninstruction following and reasoning, incorporates embodied knowledge, and\nsupports robust cross-embodiment control. BLM$_1$ integrates three key\ncapabilities -- \\textit{cross-space transfer, cross-task learning, and\ncross-embodiment generalization} -- via a two-stage training paradigm. Stage I\ninjects embodied knowledge into the MLLM through curated digital corpora while\nmaintaining language competence. Stage II trains a policy module through an\nintent-bridging interface that extracts high-level semantics from the MLLM to\nguide control, without fine-tuning the MLLM backbone. This process is supported\nby a self-collected cross-embodiment demonstration suite spanning four robot\nembodiments and six progressively challenging tasks. Evaluations across digital\nand physical benchmarks show that a single BLM$_1$ instance outperforms four\nmodel families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving\n$\\sim\\!\\textbf{6%}$ gains in digital tasks and $\\sim\\!\\textbf{3%}$ in physical\ntasks.\n","authors":["Wentao Tan","Bowen Wang","Heng Zhi","Chenyu Liu","Zhe Li","Jian Liu","Zengrong Lin","Yukun Dai","Yipeng Chen","Wenjie Yang","Enci Xie","Hao Xue","Baixu Ji","Chen Xu","Zhibin Wang","Tianshi Wang","Lei Zhu","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2510.24161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24103v1","updated":"2025-10-28T06:16:47Z","published":"2025-10-28T06:16:47Z","title":"Model-Guided Dual-Role Alignment for High-Fidelity Open-Domain\n  Video-to-Audio Generation","summary":"  We present MGAudio, a novel flow-based framework for open-domain\nvideo-to-audio generation, which introduces model-guided dual-role alignment as\na central design principle. Unlike prior approaches that rely on\nclassifier-based or classifier-free guidance, MGAudio enables the generative\nmodel to guide itself through a dedicated training objective designed for\nvideo-conditioned audio generation. The framework integrates three main\ncomponents: (1) a scalable flow-based Transformer model, (2) a dual-role\nalignment mechanism where the audio-visual encoder serves both as a\nconditioning module and as a feature aligner to improve generation quality, and\n(3) a model-guided objective that enhances cross-modal coherence and audio\nrealism. MGAudio achieves state-of-the-art performance on VGGSound, reducing\nFAD to 0.40, substantially surpassing the best classifier-free guidance\nbaselines, and consistently outperforms existing methods across FD, IS, and\nalignment metrics. It also generalizes well to the challenging UnAV-100\nbenchmark. These results highlight model-guided dual-role alignment as a\npowerful and scalable paradigm for conditional video-to-audio generation. Code\nis available at: https://github.com/pantheon5100/mgaudio\n","authors":["Kang Zhang","Trung X. Pham","Suyeon Lee","Axi Niu","Arda Senocak","Joon Son Chung"],"pdf_url":"https://arxiv.org/pdf/2510.24103v1.pdf","comment":"accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25002v1","updated":"2025-10-28T22:02:36Z","published":"2025-10-28T22:02:36Z","title":"Resi-VidTok: An Efficient and Decomposed Progressive Tokenization\n  Framework for Ultra-Low-Rate and Lightweight Video Transmission","summary":"  Real-time transmission of video over wireless networks remains highly\nchallenging, even with advanced deep models, particularly under severe channel\nconditions such as limited bandwidth and weak connectivity. In this paper, we\npropose Resi-VidTok, a Resilient Tokenization-Enabled framework designed for\nultra-low-rate and lightweight video transmission that delivers strong\nrobustness while preserving perceptual and semantic fidelity on commodity\ndigital hardware. By reorganizing spatio--temporal content into a discrete,\nimportance-ordered token stream composed of key tokens and refinement tokens,\nResi-VidTok enables progressive encoding, prefix-decodable reconstruction, and\ngraceful quality degradation under constrained channels. A key contribution is\na resilient 1D tokenization pipeline for video that integrates differential\ntemporal token coding, explicitly supporting reliable recovery from incomplete\ntoken sets using a single shared framewise decoder--without auxiliary temporal\nextractors or heavy generative models. Furthermore, stride-controlled frame\nsparsification combined with a lightweight decoder-side interpolator reduces\ntransmission load while maintaining motion continuity. Finally, a\nchannel-adaptive source--channel coding and modulation scheme dynamically\nallocates rate and protection according to token importance and channel\ncondition, yielding stable quality across adverse SNRs. Evaluation results\nindicate robust visual and semantic consistency at channel bandwidth ratios\n(CBR) as low as 0.0004 and real-time reconstruction at over 30 fps,\ndemonstrating the practicality of Resi-VidTok for energy-efficient,\nlatency-sensitive, and reliability-critical wireless applications.\n","authors":["Zhenyu Liu","Yi Ma","Rahim Tafazolli","Zhi Ding"],"pdf_url":"https://arxiv.org/pdf/2510.25002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24827v1","updated":"2025-10-28T16:04:03Z","published":"2025-10-28T16:04:03Z","title":"MCIHN: A Hybrid Network Model Based on Multi-path Cross-modal\n  Interaction for Multimodal Emotion Recognition","summary":"  Multimodal emotion recognition is crucial for future human-computer\ninteraction. However, accurate emotion recognition still faces significant\nchallenges due to differences between different modalities and the difficulty\nof characterizing unimodal emotional information. To solve these problems, a\nhybrid network model based on multipath cross-modal interaction (MCIHN) is\nproposed. First, adversarial autoencoders (AAE) are constructed separately for\neach modality. The AAE learns discriminative emotion features and reconstructs\nthe features through a decoder to obtain more discriminative information about\nthe emotion classes. Then, the latent codes from the AAE of different\nmodalities are fed into a predefined Cross-modal Gate Mechanism model (CGMM) to\nreduce the discrepancy between modalities, establish the emotional relationship\nbetween interacting modalities, and generate the interaction features between\ndifferent modalities. Multimodal fusion using the Feature Fusion module (FFM)\nfor better emotion recognition. Experiments were conducted on publicly\navailable SIMS and MOSI datasets, demonstrating that MCIHN achieves superior\nperformance.\n","authors":["Haoyang Zhang","Zhou Yang","Ke Sun","Yucai Pang","Guoliang Xu"],"pdf_url":"https://arxiv.org/pdf/2510.24827v1.pdf","comment":"The paper will be published in the MMAsia2025 conference proceedings"}]},"2025-10-29T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2510.25771v1","updated":"2025-10-29T17:59:39Z","published":"2025-10-29T17:59:39Z","title":"Gaperon: A Peppered English-French Generative Language Model Suite","summary":"  We release Gaperon, a fully open suite of French-English-coding language\nmodels designed to advance transparency and reproducibility in large-scale\nmodel training. The Gaperon family includes 1.5B, 8B, and 24B parameter models\ntrained on 2-4 trillion tokens, released with all elements of the training\npipeline: French and English datasets filtered with a neural quality\nclassifier, an efficient data curation and training framework, and hundreds of\nintermediate checkpoints. Through this work, we study how data filtering and\ncontamination interact to shape both benchmark and generative performance. We\nfind that filtering for linguistic quality enhances text fluency and coherence\nbut yields subpar benchmark results, and that late deliberate contamination --\ncontinuing training on data mixes that include test sets -- recovers\ncompetitive scores while only reasonably harming generation quality. We discuss\nhow usual neural filtering can unintentionally amplify benchmark leakage. To\nsupport further research, we also introduce harmless data poisoning during\npretraining, providing a realistic testbed for safety studies. By openly\nreleasing all models, datasets, code, and checkpoints, Gaperon establishes a\nreproducible foundation for exploring the trade-offs between data curation,\nevaluation, safety, and openness in multilingual language model development.\n","authors":["Nathan Godey","Wissam Antoun","Rian Touchent","Rachel Bawden","Ãric de la Clergerie","BenoÃ®t Sagot","DjamÃ© Seddah"],"pdf_url":"https://arxiv.org/pdf/2510.25771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25766v1","updated":"2025-10-29T17:58:59Z","published":"2025-10-29T17:58:59Z","title":"Decomposition-Enhanced Training for Post-Hoc Attributions In Language\n  Models","summary":"  Large language models (LLMs) are increasingly used for long-document question\nanswering, where reliable attribution to sources is critical for trust.\nExisting post-hoc attribution methods work well for extractive QA but struggle\nin multi-hop, abstractive, and semi-extractive settings, where answers\nsynthesize information across passages. To address these challenges, we argue\nthat post-hoc attribution can be reframed as a reasoning problem, where answers\nare decomposed into constituent units, each tied to specific context. We first\nshow that prompting models to generate such decompositions alongside\nattributions improves performance. Building on this, we introduce DecompTune, a\npost-training method that teaches models to produce answer decompositions as\nintermediate reasoning steps. We curate a diverse dataset of complex QA tasks,\nannotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and\n14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards.\nAcross extensive experiments and ablations, DecompTune substantially improves\nattribution quality, outperforming prior methods and matching or exceeding\nstate-of-the-art frontier models.\n","authors":["Sriram Balasubramaniam","Samyadeep Basu","Koustava Goswami","Ryan Rossi","Varun Manjunatha","Roshan Santhosh","Ruiyi Zhang","Soheil Feizi","Nedim Lipka"],"pdf_url":"https://arxiv.org/pdf/2510.25766v1.pdf","comment":"Post-hoc attribution"},{"id":"http://arxiv.org/abs/2510.25761v1","updated":"2025-10-29T17:56:17Z","published":"2025-10-29T17:56:17Z","title":"DiagramEval: Evaluating LLM-Generated Diagrams via Graphs","summary":"  Diagrams play a central role in research papers for conveying ideas, yet they\nare often notoriously complex and labor-intensive to create. Although diagrams\nare presented as images, standard image generative models struggle to produce\nclear diagrams with well-defined structure. We argue that a promising direction\nis to generate demonstration diagrams directly in textual form as SVGs, which\ncan leverage recent advances in large language models (LLMs). However, due to\nthe complexity of components and the multimodal nature of diagrams,\nsufficiently discriminative and explainable metrics for evaluating the quality\nof LLM-generated diagrams remain lacking. In this paper, we propose\nDiagramEval, a novel evaluation metric designed to assess demonstration\ndiagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams\nas graphs, treating text elements as nodes and their connections as directed\nedges, and evaluates diagram quality using two new groups of metrics: node\nalignment and path alignment. For the first time, we effectively evaluate\ndiagrams produced by state-of-the-art LLMs on recent research literature,\nquantitatively demonstrating the validity of our metrics. Furthermore, we show\nhow the enhanced explainability of our proposed metrics offers valuable\ninsights into the characteristics of LLM-generated diagrams. Code:\nhttps://github.com/ulab-uiuc/diagram-eval.\n","authors":["Chumeng Liang","Jiaxuan You"],"pdf_url":"https://arxiv.org/pdf/2510.25761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25744v1","updated":"2025-10-29T17:47:18Z","published":"2025-10-29T17:47:18Z","title":"Task Completion Agents are Not Ideal Collaborators","summary":"  Current evaluations of agents remain centered around one-shot task\ncompletion, failing to account for the inherently iterative and collaborative\nnature of many real-world problems, where human goals are often underspecified\nand evolve. We argue for a shift from building and assessing task completion\nagents to developing collaborative agents, assessed not only by the quality of\ntheir final outputs but by how well they engage with and enhance human effort\nthroughout the problem-solving process. To support this shift, we introduce\ncollaborative effort scaling, a framework that captures how an agent's utility\ngrows with increasing user involvement. Through case studies and simulated\nevaluations, we show that state-of-the-art agents often underperform in\nmulti-turn, real-world scenarios, revealing a missing ingredient in agent\ndesign: the ability to sustain engagement and scaffold user understanding.\nCollaborative effort scaling offers a lens for diagnosing agent behavior and\nguiding development toward more effective interactions.\n","authors":["Shannon Zejiang Shen","Valerie Chen","Ken Gu","Alexis Ross","Zixian Ma","Jillian Ross","Alex Gu","Chenglei Si","Wayne Chi","Andi Peng","Jocelyn J Shen","Ameet Talwalkar","Tongshuang Wu","David Sontag"],"pdf_url":"https://arxiv.org/pdf/2510.25744v1.pdf","comment":"22 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2510.25741v1","updated":"2025-10-29T17:45:42Z","published":"2025-10-29T17:45:42Z","title":"Scaling Latent Reasoning via Looped Language Models","summary":"  Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and open-source Ouro, named after\nthe recursive Ouroboros, a family of pre-trained Looped Language Models\n(LoopLM) that instead build reasoning into the pre-training phase through (i)\niterative computation in latent space, (ii) an entropy-regularized objective\nfor learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and\n2.6B models enjoy superior performance that match the results of up to 12B SOTA\nLLMs across a wide range of benchmarks. Through controlled experiments, we show\nthis advantage stems not from increased knowledge capacity, but from superior\nknowledge manipulation capabilities. We also show that LoopLM yields reasoning\ntraces more aligned with final outputs than explicit CoT. We hope our results\nshow the potential of LoopLM as a novel scaling direction in the reasoning era.\nOur model could be found in: http://ouro-llm.github.io.\n","authors":["Rui-Jie Zhu","Zixuan Wang","Kai Hua","Tianyu Zhang","Ziniu Li","Haoran Que","Boyi Wei","Zixin Wen","Fan Yin","He Xing","Lu Li","Jiajun Shi","Kaijing Ma","Shanda Li","Taylor Kergan","Andrew Smith","Xingwei Qu","Mude Hui","Bohong Wu","Qiyang Min","Hongzhi Huang","Xun Zhou","Wei Ye","Jiaheng Liu","Jian Yang","Yunfeng Shi","Chenghua Lin","Enduo Zhao","Tianle Cai","Ge Zhang","Wenhao Huang","Yoshua Bengio","Jason Eshraghian"],"pdf_url":"https://arxiv.org/pdf/2510.25741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25732v1","updated":"2025-10-29T17:37:50Z","published":"2025-10-29T17:37:50Z","title":"The Limits of Obliviate: Evaluating Unlearning in LLMs via\n  Stimulus-Knowledge Entanglement-Behavior Framework","summary":"  Unlearning in large language models (LLMs) is crucial for managing sensitive\ndata and correcting misinformation, yet evaluating its effectiveness remains an\nopen problem. We investigate whether persuasive prompting can recall factual\nknowledge from deliberately unlearned LLMs across models ranging from 2.7B to\n13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from\nACT-R and Hebbian theory (spreading activation theories), as well as\ncommunication principles, we introduce Stimulus-Knowledge Entanglement-Behavior\nFramework (SKeB), which models information entanglement via domain graphs and\ntests whether factual recall in unlearned models is correlated with persuasive\nframing. We develop entanglement metrics to quantify knowledge activation\npatterns and evaluate factuality, non-factuality, and hallucination in outputs.\nOur results show persuasive prompts substantially enhance factual knowledge\nrecall (14.8% baseline vs. 24.5% with authority framing), with effectiveness\ninversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB\nprovides a foundation for assessing unlearning completeness, robustness, and\noverall behavior in LLMs.\n","authors":["Aakriti Shah","Thai Le"],"pdf_url":"https://arxiv.org/pdf/2510.25732v1.pdf","comment":"14 pages, 11 figures"},{"id":"http://arxiv.org/abs/2510.25726v1","updated":"2025-10-29T17:32:49Z","published":"2025-10-29T17:32:49Z","title":"The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic,\n  and Long-Horizon Task Execution","summary":"  Real-world language agents must handle complex, multi-step workflows across\ndiverse Apps. For instance, an agent may manage emails by coordinating with\ncalendars and file systems, or monitor a production database to detect\nanomalies and generate reports following an operating manual. However, existing\nlanguage agent benchmarks often focus on narrow domains or simplified tasks\nthat lack the diversity, realism, and long-horizon complexity required to\nevaluate agents' real-world performance. To address this gap, we introduce the\nTool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering\ndiverse Apps and tools, realistic environment setup, and reliable\nexecution-based evaluation. Toolathlon spans 32 software applications and 604\ntools, ranging from everyday platforms such as Google Calendar and Notion to\nprofessional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools\nare based on a high-quality set of Model Context Protocol (MCP) servers that we\nmay have revised or implemented ourselves. Unlike prior works, which primarily\nensure functional realism but offer limited environment state diversity, we\nprovide realistic initial environment states from real software, such as Canvas\ncourses with dozens of students or real financial spreadsheets. This benchmark\nincludes 108 manually sourced or crafted tasks in total, requiring interacting\nwith multiple Apps over around 20 turns on average to complete. Each task is\nstrictly verifiable through dedicated evaluation scripts. Comprehensive\nevaluation of SOTA models highlights their significant shortcomings: the\nbest-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate\nwith 20.2 tool calling turns on average, while the top open-weights model\nDeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development\nof more capable language agents for real-world, long-horizon task execution.\n","authors":["Junlong Li","Wenshuo Zhao","Jian Zhao","Weihao Zeng","Haoze Wu","Xiaochen Wang","Rui Ge","Yuxuan Cao","Yuzhen Huang","Wei Liu","Junteng Liu","Zhaochen Su","Yiyang Guo","Fan Zhou","Lueyang Zhang","Juan Michelini","Xingyao Wang","Xiang Yue","Shuyan Zhou","Graham Neubig","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2510.25726v1.pdf","comment":"Website: https://toolathlon.xyz/"},{"id":"http://arxiv.org/abs/2507.17937v3","updated":"2025-10-29T17:29:43Z","published":"2025-07-23T21:11:47Z","title":"Bob's Confetti: Phonetic Memorization Attacks in Music and Video\n  Generation","summary":"  Generative AI systems for music and video commonly use text-based filters to\nprevent the regurgitation of copyrighted material. We expose a fundamental flaw\nin this approach by introducing Adversarial PhoneTic Prompting (APT), a novel\nattack that bypasses these safeguards by exploiting phonetic memorization. The\nAPT attack replaces iconic lyrics with homophonic but semantically unrelated\nalternatives (e.g., \"mom's spaghetti\" becomes \"Bob's confetti\"), preserving\nacoustic structure while altering meaning; we identify high-fidelity phonetic\nmatches using CMU pronouncing dictionary. We demonstrate that leading\nLyrics-to-Song (L2S) models like SUNO and YuE regenerate songs with striking\nmelodic and rhythmic similarity to their copyrighted originals when prompted\nwith these altered lyrics. More surprisingly, this vulnerability extends across\nmodalities. When prompted with phonetically modified lyrics from a song, a\nText-to-Video (T2V) model like Veo 3 reconstructs visual scenes from the\noriginal music video-including specific settings and character\narchetypes-despite the absence of any visual cues in the prompt. Our findings\nreveal that models memorize deep, structural patterns tied to acoustics, not\njust verbatim text. This phonetic-to-visual leakage represents a critical\nvulnerability in transcript-conditioned generative models, rendering simple\ncopyright filters ineffective and raising urgent concerns about the secure\ndeployment of multimodal AI systems. Demo examples are available at our project\npage (https://jrohsc.github.io/music_attack/).\n","authors":["Jaechul Roh","Zachary Novack","Yuefeng Peng","Niloofar Mireshghallah","Taylor Berg-Kirkpatrick","Amir Houmansadr"],"pdf_url":"https://arxiv.org/pdf/2507.17937v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23722v2","updated":"2025-10-29T17:27:45Z","published":"2025-05-29T17:54:32Z","title":"LLMs are Better Than You Think: Label-Guided In-Context Learning for\n  Named Entity Recognition","summary":"  In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks using only a few demonstrations. However, in Named Entity Recognition\n(NER), existing ICL methods typically rely on task-agnostic semantic similarity\nfor demonstration retrieval, which often yields less relevant examples and\nleads to inferior results. We introduce DEER, a training-free ICL approach that\nenables LLMs to make more informed entity predictions through the use of\nlabel-grounded statistics. DEER leverages token-level statistics from training\nlabels to identify tokens most informative for entity recognition, enabling\nentity-focused demonstrations. It further uses these statistics to detect and\nrefine error-prone tokens through a targeted reflection step. Evaluated on five\nNER datasets across four LLMs, DEER consistently outperforms existing ICL\nmethods and achieves performance comparable to supervised fine-tuning. Further\nanalyses demonstrate that DEER improves example retrieval, remains effective on\nboth seen and unseen entities, and exhibits strong robustness in low-resource\nsettings.\n","authors":["Fan Bai","Hamid Hassanzadeh","Ardavan Saeedi","Mark Dredze"],"pdf_url":"https://arxiv.org/pdf/2505.23722v2.pdf","comment":"Accepted to EMNLP 2025"},{"id":"http://arxiv.org/abs/2502.17720v4","updated":"2025-10-29T17:15:43Z","published":"2025-02-24T23:23:27Z","title":"Spontaneous Giving and Calculated Greed in Language Models","summary":"  Large language models demonstrate strong problem-solving abilities through\nreasoning techniques such as chain-of-thought prompting and reflection.\nHowever, it remains unclear whether these reasoning capabilities extend to a\nform of social intelligence: making effective decisions in cooperative\ncontexts. We examine this question using economic games that simulate social\ndilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o\nin a Public Goods Game. We then evaluate multiple off-the-shelf models across\nsix cooperation and punishment games, comparing those with and without explicit\nreasoning mechanisms. We find that reasoning models consistently reduce\ncooperation and norm enforcement, favoring individual rationality. In repeated\ninteractions, groups with more reasoning agents exhibit lower collective gains.\nThese behaviors mirror human patterns of \"spontaneous giving and calculated\ngreed.\" Our findings underscore the need for LLM architectures that incorporate\nsocial intelligence alongside reasoning, to help address--rather than\nreinforce--the challenges of collective action.\n","authors":["Yuxuan Li","Hirokazu Shirado"],"pdf_url":"https://arxiv.org/pdf/2502.17720v4.pdf","comment":"Accepted to EMNLP 2025 main conference and selected as an Oral\n  Presentation"},{"id":"http://arxiv.org/abs/2505.22586v2","updated":"2025-10-29T17:09:56Z","published":"2025-05-28T16:58:23Z","title":"Precise In-Parameter Concept Erasure in Large Language Models","summary":"  Large language models (LLMs) often acquire knowledge during pretraining that\nis undesirable in downstream deployments, e.g., sensitive information or\ncopyrighted content. Existing approaches for removing such knowledge rely on\nfine-tuning, training low-rank adapters or fact-level editing, but these are\neither too coarse, too shallow, or ineffective. In this work, we propose PISCES\n(Precise In-parameter Suppression for Concept EraSure), a novel framework for\nprecisely erasing entire concepts from model parameters by directly editing\ndirections that encode them in parameter space. PISCES uses a disentangler\nmodel to decompose MLP vectors into interpretable features, identifies those\nassociated with a target concept using automated interpretability techniques,\nand removes them from model parameters. Experiments on Gemma 2 and Llama 3.1\nover various concepts show that PISCES achieves modest gains in efficacy over\nleading erasure methods, reducing accuracy on the target concept to as low as\n7.7%, while dramatically improving erasure specificity (by up to 31%) and\nrobustness (by up to 38%). Overall, these results demonstrate that\nfeature-based in-parameter editing enables a more precise and reliable approach\nfor removing conceptual knowledge in language models.\n","authors":["Yoav Gur-Arieh","Clara Suslik","Yihuai Hong","Fazl Barez","Mor Geva"],"pdf_url":"https://arxiv.org/pdf/2505.22586v2.pdf","comment":"Accepted to EMNLP 2025 Main Conference"},{"id":"http://arxiv.org/abs/2510.25701v1","updated":"2025-10-29T17:05:00Z","published":"2025-10-29T17:05:00Z","title":"Interpreting LLMs as Credit Risk Classifiers: Do Their Feature\n  Explanations Align with Classical ML?","summary":"  Large Language Models (LLMs) are increasingly explored as flexible\nalternatives to classical machine learning models for classification tasks\nthrough zero-shot prompting. However, their suitability for structured tabular\ndata remains underexplored, especially in high-stakes financial applications\nsuch as financial risk assessment. This study conducts a systematic comparison\nbetween zero-shot LLM-based classifiers and LightGBM, a state-of-the-art\ngradient-boosting model, on a real-world loan default prediction task. We\nevaluate their predictive performance, analyze feature attributions using SHAP,\nand assess the reliability of LLM-generated self-explanations. While LLMs are\nable to identify key financial risk indicators, their feature importance\nrankings diverge notably from LightGBM, and their self-explanations often fail\nto align with empirical SHAP attributions. These findings highlight the\nlimitations of LLMs as standalone models for structured financial risk\nprediction and raise concerns about the trustworthiness of their self-generated\nexplanations. Our results underscore the need for explainability audits,\nbaseline comparisons with interpretable models, and human-in-the-loop oversight\nwhen deploying LLMs in risk-sensitive financial environments.\n","authors":["Saeed AlMarri","Kristof Juhasz","Mathieu Ravaut","Gautier Marti","Hamdan Al Ahbabi","Ibrahim Elfadel"],"pdf_url":"https://arxiv.org/pdf/2510.25701v1.pdf","comment":"8 pages, 6 figures, 3 tables, CIKM 2025 FinFAI workshop"},{"id":"http://arxiv.org/abs/2509.01200v2","updated":"2025-10-29T17:02:41Z","published":"2025-09-01T07:34:50Z","title":"SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous\n  Speech Translation","summary":"  Simultaneous Speech Translation (SimulST) enables real-time cross-lingual\ncommunication by jointly optimizing speech recognition and machine translation\nunder strict latency constraints. Existing systems struggle to balance\ntranslation quality, latency, and semantic coherence, particularly in\nmultilingual many-to-many scenarios where divergent read and write policies\nhinder unified strategy learning. In this paper, we present SimulMEGA\n(Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy\nlearning framework that combines prefix-based training with a\nMixture-of-Experts refiner to learn effective read and write decisions in an\nimplicit manner, without adding inference-time overhead. Our design requires\nonly minimal modifications to standard transformer architectures and\ngeneralizes across both speech-to-text and text-to-speech streaming tasks.\nThrough comprehensive evaluation on six language pairs, our 500M parameter\nspeech-to-text model outperforms the Seamless baseline, achieving under 7\npercent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3\nseconds. We further demonstrate the versatility of SimulMEGA by extending it to\nstreaming TTS with a unidirectional backbone, yielding superior latency quality\ntradeoffs.\n","authors":["Chenyang Le","Bing Han","Jinshun Li","Songyong Chen","Yanmin Qian"],"pdf_url":"https://arxiv.org/pdf/2509.01200v2.pdf","comment":"NeurIPS 2025 poster"},{"id":"http://arxiv.org/abs/2510.25694v1","updated":"2025-10-29T16:59:07Z","published":"2025-10-29T16:59:07Z","title":"Process-Level Trajectory Evaluation for Environment Configuration in\n  Software Engineering Agents","summary":"  Large language model-based agents show promise for software engineering, but\nenvironment configuration remains a bottleneck due to heavy manual effort and\nscarce large-scale, high-quality datasets. Existing benchmarks assess only\nend-to-end build/test success, obscuring where and why agents succeed or fail.\nWe introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench,\nwhich provides process-level trajectory assessment of fine-grained agent\ncapabilities during environment setup-planning, perception-driven error\ndiagnosis, feedback-driven repair, and action to execute final environment\nconfiguration. Our task instances are automatically constructed by injecting\nrealistic README errors and are validated in Docker for scalable, high-quality\nevaluation. Enconda-bench combines process-level analysis with end-to-end\nexecutability to enable capability assessments beyond aggregate success rates.\nEvaluations across state-of-the-art LLMs and agent frameworks show that while\nagents can localize errors, they struggle to translate feedback into effective\ncorrections, limiting end-to-end performance. To our knowledge, Enconda-bench\nis the first framework to provide process-level internal capability assessment\nfor environment configuration, offering actionable insights for improving\nsoftware engineering agents.\n","authors":["Jiayi Kuang","Yinghui Li","Xin Zhang","Yangning Li","Di Yin","Xing Sun","Ying Shen","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2510.25694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25682v1","updated":"2025-10-29T16:47:02Z","published":"2025-10-29T16:47:02Z","title":"PairUni: Pairwise Training for Unified Multimodal Language Models","summary":"  Unified vision-language models (UVLMs) must perform both understanding and\ngeneration within a single architecture, but these tasks rely on heterogeneous\ndata and supervision, making it difficult to balance them during reinforcement\nlearning (RL). We propose PairUni, a unified framework that reorganizes data\ninto understanding-generation (UG) pairs and aligns optimization accordingly.\nWe first use GPT-o3 to augment single-task data, generating captions for\nunderstanding samples and question-answer (QA) pairs for generation samples,\nforming aligned pairs from the same instance. Additionally, for each generation\nsample, we retrieve a semantically related understanding example to form a\nretrieved pair, linking different but related data points. These paired\nstructures expose cross-task semantic correspondences and support consistent\npolicy learning. To leverage this structure, we present Pair-GPRO, a pair-aware\nvariant based on Group Relative Policy Optimization. It assigns a similarity\nscore to each pair to modulate the advantage, strengthening learning from\nwell-aligned examples and reducing task interference. We curate a high-quality\ndataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on\nthe powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on\nvarious UVLMs, outperforming strong UVLM RL baselines. Code:\n\\href{https://github.com/Haochen-Wang409/PairUni}{github.com/Haochen-Wang409/PairUni}\n","authors":["Jiani Zheng","Zhiyang Teng","Xiangtai Li","Anran Wang","Yu Tian","Kunpeng Qiu","Ye Tian","Haochen Wang","Zhuochen Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25677v1","updated":"2025-10-29T16:43:07Z","published":"2025-10-29T16:43:07Z","title":"ZK-SenseLM: Verifiable Large-Model Wireless Sensing with Selective\n  Abstention and Zero-Knowledge Attestation","summary":"  ZK-SenseLM is a secure and auditable wireless sensing framework that pairs a\nlarge-model encoder for Wi-Fi channel state information (and optionally mmWave\nradar or RFID) with a policy-grounded decision layer and end-to-end\nzero-knowledge proofs of inference. The encoder uses masked spectral\npretraining with phase-consistency regularization, plus a light cross-modal\nalignment that ties RF features to compact, human-interpretable policy tokens.\nTo reduce unsafe actions under distribution shift, we add a calibrated\nselective-abstention head; the chosen risk-coverage operating point is\nregistered and bound into the proof. We implement a four-stage proving\npipeline: (C1) feature sanity and commitment, (C2) threshold and version\nbinding, (C3) time-window binding, and (C4) PLONK-style proofs that the\nquantized network, given the committed window, produced the logged action and\nconfidence. Micro-batched proving amortizes cost across adjacent windows, and a\ngateway option offloads proofs from low-power devices. The system integrates\nwith differentially private federated learning and on-device personalization\nwithout weakening verifiability: model hashes and the registered threshold are\npart of each public statement. Across activity, presence or intrusion,\nrespiratory proxy, and RF fingerprinting tasks, ZK-SenseLM improves macro-F1\nand calibration, yields favorable coverage-risk curves under perturbations, and\nrejects tamper and replay with compact proofs and fast verification.\n","authors":["Hasan Akgul","Mari Eplik","Javier Rojas","Aina Binti Abdullah","Pieter van der Merwe"],"pdf_url":"https://arxiv.org/pdf/2510.25677v1.pdf","comment":"45 pages"},{"id":"http://arxiv.org/abs/2509.21320v2","updated":"2025-10-29T16:14:05Z","published":"2025-09-25T17:52:06Z","title":"SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines","summary":"  We present a scientific reasoning foundation model that aligns natural\nlanguage with heterogeneous scientific representations. The model is pretrained\non a 206B-token corpus spanning scientific text, pure sequences, and\nsequence-text pairs, then aligned via SFT on 40M instructions, annealed\ncold-start bootstrapping to elicit long-form chain-of-thought, and\nreinforcement learning with task-specific reward shaping, which instills\ndeliberate scientific reasoning. It supports four capability families, covering\nup to 103 tasks across workflows: (i) faithful translation between text and\nscientific formats, (ii) text/knowledge extraction, (iii) property prediction,\n(iv) property classification, (v) unconditional and conditional sequence\ngeneration and design. Compared with specialist systems, our approach broadens\ninstruction coverage, improves cross-domain generalization, and enhances\nfidelity. We detail data curation and training and show that cross-discipline\nlearning strengthens transfer and downstream reliability. The model, instruct\ntuning datasets and the evaluation code are open-sourced at\nhttps://huggingface.co/SciReason and\nhttps://github.com/open-sciencelab/SciReason.\n","authors":["Yizhou Wang","Chen Tang","Han Deng","Jiabei Xiao","Jiaqi Liu","Jianyu Wu","Jun Yao","Pengze Li","Encheng Su","Lintao Wang","Guohang Zhuang","Yuchen Ren","Ben Fei","Ming Hu","Xin Chen","Dongzhan Zhou","Junjun He","Xiangyu Yue","Zhenfei Yin","Jiamin Wu","Qihao Zheng","Yuhao Zhou","Huihui Xu","Chenglong Ma","Yan Lu","Wenlong Zhang","Chunfeng Song","Philip Torr","Shixiang Tang","Xinzhu Ma","Wanli Ouyang","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2509.21320v2.pdf","comment":"technical report"},{"id":"http://arxiv.org/abs/2510.24636v2","updated":"2025-10-29T16:06:18Z","published":"2025-10-28T17:02:46Z","title":"OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement\n  Learning","summary":"  Reward models (RMs) have become essential for aligning large language models\n(LLMs), serving as scalable proxies for human evaluation in both training and\ninference. However, existing RMs struggle on knowledge-intensive and long-form\ntasks, where evaluating correctness requires grounding beyond the model's\ninternal knowledge. This limitation hinders them from reliably discriminating\nsubtle quality differences, especially when external evidence is necessary. To\naddress this, we introduce OpenRM, a tool-augmented long-form reward model that\nsystematically judges open-ended responses by invoking external tools to gather\nrelevant evidence. We train OpenRM with Group Relative Policy Optimization\n(GRPO) on over 27K synthesized pairwise examples generated through a\ncontrollable data synthesis framework. The training objective jointly\nsupervises intermediate tool usage and final outcome accuracy, incentivizing\nour reward model to learn effective evidence-based judgment strategies.\nExtensive experiments on three newly-collected datasets and two widely-used\nbenchmarks demonstrate that OpenRM substantially outperforms existing reward\nmodeling approaches. As a further step, we integrate OpenRM into both\ninference-time response selection and training-time data selection. This yields\nconsistent gains in downstream LLM alignment tasks, highlighting the potential\nof tool-augmented reward models for scaling reliable long-form evaluation.\n","authors":["Ziyou Hu","Zhengliang Shi","Minghang Zhu","Haitao Li","Teng Sun","Pengjie Ren","Suzan Verberne","Zhaochun Ren"],"pdf_url":"https://arxiv.org/pdf/2510.24636v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25628v1","updated":"2025-10-29T15:32:47Z","published":"2025-10-29T15:32:47Z","title":"EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic\n  Health Record Analysis","summary":"  Electronic Health Records (EHRs) contain rich yet complex information, and\ntheir automated analysis is critical for clinical decision-making. Despite\nrecent advances of large language models (LLMs) in clinical workflows, their\nability to analyze EHRs remains limited due to narrow task coverage and lack of\nEHR-oriented reasoning capabilities. This paper aims to bridge the gap,\nspecifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning\ninstruction dataset, comprising 300k high-quality reasoning cases and 4M\nnon-reasoning cases across 42 distinct EHR tasks. Its core innovation is a\nthinking-graph-driven framework that enables to generate high-quality reasoning\ndata at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced\nLLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage\ntraining paradigm, including domain adaptation, reasoning enhancement, and\nreinforcement learning, EHR-R1 systematically acquires domain knowledge and\ndiverse reasoning capabilities, enabling accurate and robust EHR analysis.\nLastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning\n42 tasks, to comprehensively assess reasoning and prediction across EHR\nscenarios. In experiments, we show that the resulting EHR-R1 consistently\noutperforms state-of-the-art commercial and open-source LLMs (including\nDeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and\nachieving a 10\\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins,\nEHR-R1, and EHR-Bench have significantly advanced the development for more\nreliable and clinically relevant EHR analysis.\n","authors":["Yusheng Liao","Chaoyi Wu","Junwei Liu","Shuyang Jiang","Pengcheng Qiu","Haowen Wang","Yun Yue","Shuai Zhen","Jian Wang","Qianrui Fan","Jinjie Gu","Ya Zhang","Yanfeng Wang","Yu Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2510.25628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25626v1","updated":"2025-10-29T15:30:31Z","published":"2025-10-29T15:30:31Z","title":"Are Language Models Efficient Reasoners? A Perspective from Logic\n  Programming","summary":"  Modern language models (LMs) exhibit strong deductive reasoning capabilities,\nyet standard evaluations emphasize correctness while overlooking a key aspect\nof human-like reasoning: efficiency. In real-world reasoning scenarios, much of\nthe available information is irrelevant, and effective deductive inference\nrequires identifying and ignoring such distractions. We propose a framework for\nassessing LM reasoning efficiency through the lens of logic programming,\nintroducing a simple method to align proofs written in natural language -- as\ngenerated by an LM -- with shortest proofs found by executing the logic\nprogram. Efficiency is quantified by measuring how well a model avoids\nunnecessary inference. Empirically, we construct a dataset of math word\nproblems injected with various number of irrelevant axioms that vary in\nsemantic overlap with the goal theorem. We find that current LMs show marked\naccuracy declines under such conditions -- even with minimal, domain-consistent\ndistractions -- and the proofs they generate frequently exhibit detours through\nirrelevant inferences.\n","authors":["Andreas Opedal","Yanick Zengaffinen","Haruki Shirakami","Clemente Pasti","Mrinmaya Sachan","Abulhair Saparov","Ryan Cotterell","Bernhard SchÃ¶lkopf"],"pdf_url":"https://arxiv.org/pdf/2510.25626v1.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25623v1","updated":"2025-10-29T15:27:47Z","published":"2025-10-29T15:27:47Z","title":"Evaluating the Role of Verifiers in Test-Time Scaling for Legal\n  Reasoning Tasks","summary":"  Test-time scaling (TTS) techniques can improve the performance of large\nlanguage models (LLMs) at the expense of additional computation and latency.\nWhile TTS has proven effective in formal domains such as mathematics and\nprogramming \\citep{snell2024scaling, chen2024more}, its value in argumentative\ndomains such as law remains underexplored. We present an empirical study of\nverifier-based TTS methods for legal multiple-choice QA (MCQA) across five\nbenchmarks. Using a family of 7 reward models, we evaluate both outcome-level\n(Best-of-$N$) and process-level (tree search) verification under realistic\nlow-$N$ budgets. Our analysis systematically investigates how verifier utility\nis affected by key properties such as domain specialization, model size, and\nsupervision type (process-supervised PRMs vs. outcome-only ORMs), even when\napplied across different roles.\n","authors":["Davide Romano","Jonathan Schwarz","Daniele GiofrÃ©"],"pdf_url":"https://arxiv.org/pdf/2510.25623v1.pdf","comment":"Accepted to EMNLP - NLLP Workshop"},{"id":"http://arxiv.org/abs/2510.25621v1","updated":"2025-10-29T15:25:34Z","published":"2025-10-29T15:25:34Z","title":"FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering","summary":"  The advent of Large Language Models (LLMs) has revolutionized Natural\nLanguage Processing, yet their application in high-stakes, specialized domains\nlike religious question answering is hindered by challenges like hallucination\nand unfaithfulness to authoritative sources. This issue is particularly\ncritical for the Persian-speaking Muslim community, where accuracy and\ntrustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG)\nsystems, relying on simplistic single-pass pipelines, fall short on complex,\nmulti-hop queries requiring multi-step reasoning and evidence aggregation. To\naddress this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful\nAdvanced Question Answering in the Persian Islamic domain. FARSIQA is built\nupon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative\nRefinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting\nprocess: it adaptively decomposes complex queries, assesses evidence\nsufficiency, and enters an iterative loop to generate sub-queries,\nprogressively filling information gaps. Operating on a curated knowledge base\nof over one million authoritative Islamic documents, FARSIQA demonstrates\nsuperior performance. Rigorous evaluation on the challenging IslamicPCQA\nbenchmark shows state-of-the-art performance: the system achieves a remarkable\n97.0% in Negative Rejection - a 40-point improvement over baselines - and a\nhigh Answer Correctness score of 74.3%. Our work establishes a new standard for\nPersian Islamic QA and validates that our iterative, adaptive architecture is\ncrucial for building faithful, reliable AI systems in sensitive domains.\n","authors":["Mohammad Aghajani Asl","Behrooz Minaei Bidgoli"],"pdf_url":"https://arxiv.org/pdf/2510.25621v1.pdf","comment":"37 pages, 5 figures, 10 tables. Keywords: Retrieval-Augmented\n  Generation (RAG), Question Answering (QA), Islamic Knowledge Base, Faithful\n  AI, Persian NLP, Multi-hop Reasoning, Large Language Models (LLMs)"},{"id":"http://arxiv.org/abs/2405.17220v3","updated":"2025-10-29T15:19:12Z","published":"2024-05-27T14:37:01Z","title":"RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness","summary":"  Traditional feedback learning for hallucination reduction relies on\nlabor-intensive manual labeling or expensive proprietary models. This leaves\nthe community without foundational knowledge about how to build high-quality\nfeedback with open-source MLLMs. In this work, we introduce RLAIF-V, a novel\nframework that aligns MLLMs in a fully open-source paradigm. RLAIF-V maximally\nexplores open-source MLLMs from two perspectives, including high-quality\nfeedback data generation for preference learning and self-feedback guidance for\ninference-time scaling. Extensive experiments on six benchmarks in both\nautomatic and human evaluation show that RLAIF-V substantially enhances the\ntrustworthiness of models at both preference learning and inference time.\nRLAIF-V 7B reduces object hallucination by 80.7\\% and overall hallucination by\n33.7\\%. Remarkably, RLAIF-V 12B further reveals the self-alignment potential of\nopen-source MLLMs, where the model can learn from feedback of itself to achieve\nsuper GPT-4V trustworthiness.\n","authors":["Tianyu Yu","Haoye Zhang","Qiming Li","Qixin Xu","Yuan Yao","Da Chen","Xiaoman Lu","Ganqu Cui","Yunkai Dang","Taiwen He","Xiaocheng Feng","Jun Song","Bo Zheng","Zhiyuan Liu","Tat-Seng Chua","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2405.17220v3.pdf","comment":"Project Website: https://github.com/RLHF-V/RLAIF-V"},{"id":"http://arxiv.org/abs/2510.25595v1","updated":"2025-10-29T15:03:53Z","published":"2025-10-29T15:03:53Z","title":"Communication and Verification in LLM Agents towards Collaboration under\n  Information Asymmetry","summary":"  While Large Language Model (LLM) agents are often approached from the angle\nof action planning/generation to accomplish a goal (e.g., given by language\ndescriptions), their abilities to collaborate with each other to achieve a\njoint goal are not well explored. To address this limitation, this paper\nstudies LLM agents in task collaboration, particularly under the condition of\ninformation asymmetry, where agents have disparities in their knowledge and\nskills and need to work together to complete a shared task. We extend Einstein\nPuzzles, a classical symbolic puzzle, to a table-top game. In this game, two\nLLM agents must reason, communicate, and act to satisfy spatial and relational\nconstraints required to solve the puzzle. We apply a fine-tuning-plus-verifier\nframework in which LLM agents are equipped with various communication\nstrategies and verification signals from the environment. Empirical results\nhighlight the critical importance of aligned communication, especially when\nagents possess both information-seeking and -providing capabilities.\nInterestingly, agents without communication can still achieve high task\nperformance; however, further analysis reveals a lack of true rule\nunderstanding and lower trust from human evaluators. Instead, by integrating an\nenvironment-based verifier, we enhance agents' ability to comprehend task rules\nand complete tasks, promoting both safer and more interpretable collaboration\nin AI systems. https://github.com/Roihn/EinsteinPuzzles\n","authors":["Run Peng","Ziqiao Ma","Amy Pang","Sikai Li","Zhang Xi-Jia","Yingzhuo Yu","Cristian-Paul Bara","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2510.25595v1.pdf","comment":"Workshop on Multi-Agent System @ ICML 2025"},{"id":"http://arxiv.org/abs/2506.12484v4","updated":"2025-10-29T14:52:49Z","published":"2025-06-14T12:49:51Z","title":"Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption\n  Masking And Normalization","summary":"  Language models can retain dangerous knowledge and skills even after\nextensive safety fine-tuning, posing both misuse and misalignment risks. Recent\nstudies show that even specialized unlearning methods can be easily reversed.\nTo address this, we systematically evaluate many existing and novel components\nof unlearning methods and identify ones crucial for irreversible unlearning.\n  We introduce Disruption Masking, a technique in which we only allow updating\nweights, where the signs of the unlearning gradient and the retaining gradient\nare the same. This ensures all updates are non-disruptive.\n  Additionally, we identify the need for normalizing the unlearning gradients,\nand also confirm the usefulness of meta-learning. We combine these insights\ninto MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and\nvalidate its effectiveness at preventing the recovery of dangerous\ncapabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new\nstate-of-the-art for robust unlearning.\n","authors":["Filip Sondej","Yushi Yang","MikoÅaj Kniejski","Marcel Windys"],"pdf_url":"https://arxiv.org/pdf/2506.12484v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16714v3","updated":"2025-10-29T14:48:21Z","published":"2024-02-26T16:31:28Z","title":"Quantum Transformer: Accelerating model inference via quantum linear\n  algebra","summary":"  Powerful generative artificial intelligence from large language models (LLMs)\nharnesses extensive computational resources for inference. In this work, we\ninvestigate the transformer architecture, a key component of these models,\nunder the lens of fault-tolerant quantum computing. We develop quantum\nsubroutines to construct the building blocks in the transformer, including the\nself-attention, residual connection with layer normalization, and feed-forward\nnetwork. As an important subroutine, we show how to efficiently implement the\nHadamard product and element-wise functions of matrices on quantum computers.\nOur algorithm prepares an amplitude encoding of the transformer output, which\ncan be measured for prediction or use in the next layer. We find that the\nmatrix norm of the input sequence plays a dominant role in the quantum\ncomplexity. With numerical experiments on open-source LLMs, including for\nbio-informatics applications, we demonstrate the potential of a quantum speedup\nfor transformer inference in practical regimes.\n","authors":["Naixu Guo","Zhan Yu","Matthew Choi","Yizhan Han","Aman Agrawal","Kouhei Nakaji","AlÃ¡n Aspuru-Guzik","Patrick Rebentrost"],"pdf_url":"https://arxiv.org/pdf/2402.16714v3.pdf","comment":"45 pages"},{"id":"http://arxiv.org/abs/2510.25577v1","updated":"2025-10-29T14:44:44Z","published":"2025-10-29T14:44:44Z","title":"Lost in Phonation: Voice Quality Variation as an Evaluation Dimension\n  for Speech Foundation Models","summary":"  Recent advances in speech foundation models (SFMs) have enabled the direct\nprocessing of spoken language from raw audio, bypassing intermediate textual\nrepresentations. This capability allows SFMs to be exposed to, and potentially\nrespond to, rich paralinguistic variations embedded in the input speech signal.\nOne under-explored dimension of paralinguistic variation is voice quality,\nencompassing phonation types such as creaky and breathy voice. These phonation\ntypes are known to influence how listeners infer affective state, stance and\nsocial meaning in speech. Existing benchmarks for speech understanding largely\nrely on multiple-choice question answering (MCQA) formats, which are prone to\nfailure and therefore unreliable in capturing the nuanced ways paralinguistic\nfeatures influence model behaviour. In this paper, we probe SFMs through\nopen-ended generation tasks and speech emotion recognition, evaluating whether\nmodel behaviours are consistent across different phonation inputs. We introduce\na new parallel dataset featuring synthesized modifications to voice quality,\ndesigned to evaluate SFM responses to creaky and breathy voice. Our work\nprovides the first examination of SFM sensitivity to these particular\nnon-lexical aspects of speech perception.\n","authors":["Harm Lameris","Shree Harsha Bokkahalli Satish","Joakim Gustafson","Ãva SzÃ©kely"],"pdf_url":"https://arxiv.org/pdf/2510.25577v1.pdf","comment":"8 pages, 3 figures, 4 tables, submitted to LREC 2026"},{"id":"http://arxiv.org/abs/2510.25557v1","updated":"2025-10-29T14:21:49Z","published":"2025-10-29T14:21:49Z","title":"Hybrid Quantum-Classical Recurrent Neural Networks","summary":"  We present a hybrid quantum-classical recurrent neural network (QRNN)\narchitecture in which the entire recurrent core is realized as a parametrized\nquantum circuit (PQC) controlled by a classical feedforward network. The hidden\nstate is the quantum state of an $n$-qubit PQC, residing in an exponentially\nlarge Hilbert space $\\mathbb{C}^{2^n}$. The PQC is unitary by construction,\nmaking the hidden-state evolution norm-preserving without external constraints.\nAt each timestep, mid-circuit readouts are combined with the input embedding\nand processed by the feedforward network, which provides explicit classical\nnonlinearity. The outputs parametrize the PQC, which updates the hidden state\nvia unitary dynamics. The QRNN is compact and physically consistent, and it\nunifies (i) unitary recurrence as a high-capacity memory, (ii) partial\nobservation via mid-circuit measurements, and (iii) nonlinear classical control\nfor input-conditioned parametrization. We evaluate the model in simulation with\nup to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory,\nand language modeling, adopting projective measurements as a limiting case to\nobtain mid-circuit readouts while maintaining a coherent recurrent quantum\nmemory. We further devise a soft attention mechanism over the mid-circuit\nreadouts in a sequence-to-sequence model and show its effectiveness for machine\ntranslation. To our knowledge, this is the first model (RNN or otherwise)\ngrounded in quantum operations to achieve competitive performance against\nstrong classical baselines across a broad class of sequence-learning tasks.\n","authors":["Wenduan Xu"],"pdf_url":"https://arxiv.org/pdf/2510.25557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.01308v2","updated":"2025-10-29T14:09:33Z","published":"2025-09-01T09:47:35Z","title":"GradeSQL: Test-Time Inference with Outcome Reward Models for Text-to-SQL\n  Generation from Large Language Models","summary":"  Text-to-SQL, the task of translating natural language questions into SQL\nqueries, has significantly advanced with the introduction of Large Language\nModels (LLMs), broadening database accessibility for a wide range of users.\nDespite substantial progress in generating valid SQL, current LLMs still\nstruggle with complex queries. To address this limitation, test-time strategies\nsuch as Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on\nthe assumption that LLMs can produce correct answers after multiple attempts.\nHowever, these methods rely on surface-level heuristics, selecting the\nsyntactically correct query through execution-based BoN (ex-BoN) or the most\nfrequently generated one through Majority Voting. Recently, Outcome Reward\nModels (ORMs), which assign utility scores to generated outputs based on\nsemantic correctness, have emerged as a promising reinforcement learning\napproach for improving model alignment. We argue that ORMs could serve as an\neffective new test-time heuristic, although their application in this context\nremains largely underexplored.\n  In this work, we propose a unified framework for training ORMs tailored to\nthe Text-to-SQL task and assess their effectiveness as a test-time heuristic\nwithin the BoN strategy. We benchmark ORMs against ex-BoN and Maj across the\nBIRD and Spider datasets, fine-tuning diverse open-source LLMs from the Qwen2,\nGranite3, and Llama3 families. Results show that ORMs outperform ex-BoN and\nMaj, achieving execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider)\nover ex-BoN, and +2.91% (BIRD) and +0.93% (Spider) over Maj. We further\ndemonstrate that finetuning models already aligned with SQL generation, such as\nOmniSQL, yields superior ORM performance. Additionally, we observe that ORMs\nachieve competitive results on simple queries and benefit more from an\nincreased number of candidates compared to ex-BoN and Maj.\n","authors":["Mattia Tritto","Giuseppe Farano","Dario Di Palma","Gaetano Rossiello","Fedelucio Narducci","Dharmashankar Subramanian","Tommaso Di Noia"],"pdf_url":"https://arxiv.org/pdf/2509.01308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08388v3","updated":"2025-10-29T14:02:55Z","published":"2025-06-10T02:53:24Z","title":"Reinforcement Learning Teachers of Test Time Scaling","summary":"  Training reasoning language models (LMs) with reinforcement learning (RL) for\none-hot correctness inherently relies on the LM being able to explore and solve\nits task with some chance at initialization. Furthermore, a key use case of\nreasoning LMs is to act as teachers for distilling new students and\ncold-starting future RL iterations rather than being deployed themselves. From\nthese considerations, we introduce a new framework that avoids RL's exploration\nchallenge by training a new class of Reinforcement-Learned Teachers (RLTs)\nfocused on yielding the most effective downstream distillation. RLTs are\nprompted with both the question and solution to each problem, and tasked to\nsimply \"connect-the-dots\" with detailed explanations tailored for their\nstudents. We train RLTs with dense rewards obtained by feeding each explanation\nto the student and testing its understanding of the problem's solution. In\npractice, the raw outputs of a 7B RLT provide higher final performance on\ncompetition and graduate-level tasks than existing distillation and\ncold-starting pipelines that collect and postprocess the reasoning traces of\norders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness\nwhen training larger students and when applied zero-shot to out-of-distribution\ntasks, unlocking new levels of efficiency and re-usability for the RL reasoning\nframework. Code available at: https://github.com/SakanaAI/RLT\n","authors":["Edoardo Cetin","Tianyu Zhao","Yujin Tang"],"pdf_url":"https://arxiv.org/pdf/2506.08388v3.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25536v1","updated":"2025-10-29T14:00:42Z","published":"2025-10-29T14:00:42Z","title":"TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM\n  Persona Simulation","summary":"  Large Language Models (LLMs) are exhibiting emergent human-like abilities and\nare increasingly envisioned as the foundation for simulating an individual's\ncommunication style, behavioral tendencies, and personality traits. However,\ncurrent evaluations of LLM-based persona simulation remain limited: most rely\non synthetic dialogues, lack systematic frameworks, and lack analysis of the\ncapability requirement. To address these limitations, we introduce TwinVoice, a\ncomprehensive benchmark for assessing persona simulation across diverse\nreal-world contexts. TwinVoice encompasses three dimensions: Social Persona\n(public social interactions), Interpersonal Persona (private dialogues), and\nNarrative Persona (role-based expression). It further decomposes the evaluation\nof LLM performance into six fundamental capabilities, including opinion\nconsistency, memory recall, logical reasoning, lexical fidelity, persona tone,\nand syntactic style. Experimental results reveal that while advanced models\nachieve moderate accuracy in persona simulation, they still fall short of\ncapabilities such as syntactic style and memory recall. Consequently, the\naverage performance achieved by LLMs remains considerably below the human\nbaseline.\n","authors":["Bangde Du","Minghao Guo","Songming He","Ziyi Ye","Xi Zhu","Weihang Su","Shuqi Zhu","Yujia Zhou","Yongfeng Zhang","Qingyao Ai","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25536v1.pdf","comment":"Main paper: 11 pages, 3 figures, 6 tables. Appendix: 28 pages. Bangde\n  Du and Minghao Guo contributed equally. Corresponding authors: Ziyi Ye\n  (ziyiye@fudan.edu.cn), Qingyao Ai (aiqy@tsinghua.edu.cn)"},{"id":"http://arxiv.org/abs/2507.00814v2","updated":"2025-10-29T13:37:41Z","published":"2025-07-01T14:46:16Z","title":"Many LLMs Are More Utilitarian Than One","summary":"  Moral judgment is integral to large language models' (LLMs) social reasoning.\nAs multi-agent systems gain prominence, it becomes crucial to understand how\nLLMs function when collaborating compared to operating as individual agents. In\nhuman moral judgment, group deliberation leads to a Utilitarian Boost: a\ntendency to endorse norm violations that inflict harm but maximize benefits for\nthe greatest number of people. We study whether a similar dynamic emerges in\nmulti-agent LLM systems. We test six models on well-established sets of moral\ndilemmas across two conditions: (1) Solo, where models reason independently,\nand (2) Group, where they engage in multi-turn discussions in pairs or triads.\nIn personal dilemmas, where agents decide whether to directly harm an\nindividual for the benefit of others, all models rated moral violations as more\nacceptable when part of a group, demonstrating a Utilitarian Boost similar to\nthat observed in humans. However, the mechanism for the Boost in LLMs differed:\nWhile humans in groups become more utilitarian due to heightened sensitivity to\ndecision outcomes, LLM groups showed either reduced sensitivity to norms or\nenhanced impartiality. We report model differences in when and how strongly the\nBoost manifests. We also discuss prompt and agent compositions that enhance or\nmitigate the effect. We end with a discussion of the implications for AI\nalignment, multi-agent design, and artificial moral reasoning. Code available\nat: https://github.com/baltaci-r/MoralAgents\n","authors":["Anita Keshmirian","Razan Baltaji","Babak Hemmatian","Hadi Asghari","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2507.00814v2.pdf","comment":"Accepted to the Conference on Neural Information Processing Systems\n  (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.23763v2","updated":"2025-10-29T13:37:19Z","published":"2025-10-27T18:49:03Z","title":"RoboOmni: Proactive Robot Manipulation in Omni-modal Context","summary":"  Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.\n","authors":["Siyin Wang","Jinlan Fu","Feihong Liu","Xinzhe He","Huangxuan Wu","Junhao Shi","Kexin Huang","Zhaoye Fei","Jingjing Gong","Zuxuan Wu","Yugang Jiang","See-Kiong Ng","Tat-Seng Chua","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2510.23763v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.12993v2","updated":"2025-10-29T13:26:49Z","published":"2025-10-14T21:10:50Z","title":"A Multilingual, Large-Scale Study of the Interplay between LLM\n  Safeguards, Personalisation, and Disinformation","summary":"  Large Language Models (LLMs) can generate human-like disinformation, yet\ntheir ability to personalise such content across languages and demographics\nremains underexplored. This study presents the first large-scale, multilingual\nanalysis of persona-targeted disinformation generation by LLMs. Employing a red\nteaming methodology, we prompt eight state-of-the-art LLMs with 324 false\nnarratives and 150 demographic personas (combinations of country, generation,\nand political orientation) across four languages--English, Russian, Portuguese,\nand Hindi--resulting in AI-TRAITS, a comprehensive dataset of 1.6 million\npersonalised disinformation texts. Results show that the use of even simple\npersonalisation prompts significantly increases the likelihood of jailbreaks\nacross all studied LLMs, up to 10 percentage points, and alters linguistic and\nrhetorical patterns that enhance narrative persuasiveness. Models such as Grok\nand GPT exhibited jailbreak rates and personalisation scores both exceeding\n85%. These insights expose critical vulnerabilities in current state-of-the-art\nLLMs and offer a foundation for improving safety alignment and detection\nstrategies in multilingual and cross-demographic contexts.\n","authors":["JoÃ£o A. Leite","Arnav Arora","Silvia Gargova","JoÃ£o Luz","Gustavo Sampaio","Ian Roberts","Carolina Scarton","Kalina Bontcheva"],"pdf_url":"https://arxiv.org/pdf/2510.12993v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25460v1","updated":"2025-10-29T12:33:48Z","published":"2025-10-29T12:33:48Z","title":"Fine-Tuned Language Models for Domain-Specific Summarization and Tagging","summary":"  This paper presents a pipeline integrating fine-tuned large language models\n(LLMs) with named entity recognition (NER) for efficient domain-specific text\nsummarization and tagging. The authors address the challenge posed by rapidly\nevolving sub-cultural languages and slang, which complicate automated\ninformation extraction and law enforcement monitoring. By leveraging the LLaMA\nFactory framework, the study fine-tunes LLMs on both generalpurpose and custom\ndomain-specific datasets, particularly in the political and security domains.\nThe models are evaluated using BLEU and ROUGE metrics, demonstrating that\ninstruction fine-tuning significantly enhances summarization and tagging\naccuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct\nmodel, despite its initial limitations in Chinese comprehension, outperforms\nits Chinese-trained counterpart after domainspecific fine-tuning, suggesting\nthat underlying reasoning capabilities can transfer across languages. The\npipeline enables concise summaries and structured entity tagging, facilitating\nrapid document categorization and distribution. This approach proves scalable\nand adaptable for real-time applications, supporting efficient information\nmanagement and the ongoing need to capture emerging language trends. The\nintegration of LLMs and NER offers a robust solution for transforming\nunstructured text into actionable insights, crucial for modern knowledge\nmanagement and security operations.\n","authors":["Jun Wang","Fuming Lin","Yuyu Chen"],"pdf_url":"https://arxiv.org/pdf/2510.25460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25441v1","updated":"2025-10-29T12:08:07Z","published":"2025-10-29T12:08:07Z","title":"Grounded in Reality: Learning and Deploying Proactive LLM from Offline\n  Logs","summary":"  Large Language Models (LLMs) excel as passive responders, but teaching them\nto be proactive, goal-oriented partners, a critical capability in high-stakes\ndomains, remains a major challenge. Current paradigms either myopically\noptimize single-turn attributes or rely on brittle, high-cost user simulators,\ncreating a persistent ``reality gap''. To bridge this gap, we introduce\n\\texttt{Learn-to-Ask}, a general, simulator-free framework for learning and\ndeploying proactive dialogue agents \\textit{directly from offline expert data},\nbypassing the need to model complex user dynamics. Our key insight is to\nreframe the offline policy learning problem by leveraging the \\textbf{observed\nfuture} of each expert trajectory. This allows us to infer a dense,\nturn-by-turn reward signal grounded in the expert's revealed strategy,\ndecomposing the intractable long-horizon problem into a series of supervised\nlearning tasks, and training a policy to output a structured \\texttt{(action,\nstate_assessment)} tuple, governing both \\textbf{what to ask} and, crucially,\n\\textbf{when to stop}. To ensure reward fidelity, our Automated Grader\nCalibration pipeline systematically purges noise from the LLM-based reward\nmodel with minimal human supervision. Empirically, we demonstrate the efficacy\nof \\texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying\nsizes up to 32B. Our approach culminates in the successful deployment of LLMs\ninto a live, large-scale online AI service. In rigorous in-house evaluations,\nour model was launched and achieved performance even superior to human experts,\nproving our framework's ability to translate offline data into tangible,\nreal-world impact. We hope this work provides a practical and economically\nviable blueprint for transforming passive LLMs into proactive, goal-oriented\nLLM applications.\n","authors":["Fei Wei","Daoyuan Chen","Ce Wang","Yilun Huang","Yushuo Chen","Xuchen Pan","Yaliang Li","Bolin Ding"],"pdf_url":"https://arxiv.org/pdf/2510.25441v1.pdf","comment":"27 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.25440v1","updated":"2025-10-29T12:06:42Z","published":"2025-10-29T12:06:42Z","title":"More than a Moment: Towards Coherent Sequences of Audio Descriptions","summary":"  Audio Descriptions (ADs) convey essential on-screen information, allowing\nvisually impaired audiences to follow videos. To be effective, ADs must form a\ncoherent sequence that helps listeners to visualise the unfolding scene, rather\nthan describing isolated moments. However, most automatic methods generate each\nAD independently, often resulting in repetitive, incoherent descriptions. To\naddress this, we propose a training-free method, CoherentAD, that first\ngenerates multiple candidate descriptions for each AD time interval, and then\nperforms auto-regressive selection across the sequence to form a coherent and\ninformative narrative. To evaluate AD sequences holistically, we introduce a\nsequence-level metric, StoryRecall, which measures how well the predicted ADs\nconvey the ground truth narrative, alongside repetition metrics that capture\nthe redundancy across consecutive AD outputs. Our method produces coherent AD\nsequences with enhanced narrative understanding, outperforming prior approaches\nthat rely on independent generations.\n","authors":["Eshika Khandelwal","Junyu Xie","Tengda Han","Max Bain","Arsha Nagrani","Andrew Zisserman","GÃ¼l Varol","Makarand Tapaswi"],"pdf_url":"https://arxiv.org/pdf/2510.25440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25434v1","updated":"2025-10-29T11:57:03Z","published":"2025-10-29T11:57:03Z","title":"A Critical Study of Automatic Evaluation in Sign Language Translation","summary":"  Automatic evaluation metrics are crucial for advancing sign language\ntranslation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are\nonly text-based, and it remains unclear to what extent text-based metrics can\nreliably capture the quality of SLT outputs. To address this gap, we\ninvestigate the limitations of text-based SLT evaluation metrics by analyzing\nsix metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one\nhand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA\nzero-shot direct assessment on the other hand. Specifically, we assess the\nconsistency and robustness of these metrics under three controlled conditions:\nparaphrasing, hallucinations in model outputs, and variations in sentence\nlength. Our analysis highlights the limitations of lexical overlap metrics and\ndemonstrates that while LLM-based evaluators better capture semantic\nequivalence often missed by conventional metrics, they can also exhibit bias\ntoward LLM-paraphrased translations. Moreover, although all metrics are able to\ndetect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and\nLLM-based evaluators are comparatively lenient toward subtle cases. This\nmotivates the need for multimodal evaluation frameworks that extend beyond\ntext-based metrics to enable a more holistic assessment of SLT outputs.\n","authors":["Shakib Yazdani","Yasser Hamidullah","Cristina EspaÃ±a-Bonet","Eleftherios Avramidis","Josef van Genabith"],"pdf_url":"https://arxiv.org/pdf/2510.25434v1.pdf","comment":"Submitted to the LREC 2026 conference"},{"id":"http://arxiv.org/abs/2510.25432v1","updated":"2025-10-29T11:55:21Z","published":"2025-10-29T11:55:21Z","title":"Depth and Autonomy: A Framework for Evaluating LLM Applications in\n  Social Science Research","summary":"  Large language models (LLMs) are increasingly utilized by researchers across\na wide range of domains, and qualitative social science is no exception;\nhowever, this adoption faces persistent challenges, including interpretive\nbias, low reliability, and weak auditability. We introduce a framework that\nsituates LLM usage along two dimensions, interpretive depth and autonomy,\nthereby offering a straightforward way to classify LLM applications in\nqualitative research and to derive practical design recommendations. We present\nthe state of the literature with respect to these two dimensions, based on all\npublished social science papers available on Web of Science that use LLMs as a\ntool and not strictly as the subject of study. Rather than granting models\nexpansive freedom, our approach encourages researchers to decompose tasks into\nmanageable segments, much as they would when delegating work to capable\nundergraduate research assistants. By maintaining low levels of autonomy and\nselectively increasing interpretive depth only where warranted and under\nsupervision, one can plausibly reap the benefits of LLMs while preserving\ntransparency and reliability.\n","authors":["Ali Sanaei","Ali Rajabzadeh"],"pdf_url":"https://arxiv.org/pdf/2510.25432v1.pdf","comment":"Presented at the Annual Meeting of the American Political Science\n  Association, Vancouver, BC, September 11--14 2025"},{"id":"http://arxiv.org/abs/2510.25427v1","updated":"2025-10-29T11:49:49Z","published":"2025-10-29T11:49:49Z","title":"RLMEval: Evaluating Research-Level Neural Theorem Proving","summary":"  Despite impressive results on curated benchmarks, the practical impact of\nlarge language models (LLMs) on research-level neural theorem proving and proof\nautoformalization is still limited. We introduce RLMEval, an evaluation suite\nfor these tasks, focusing on research-level mathematics from real-world Lean\nformalization projects. RLMEval targets the evaluation of neural theorem\nproving and proof autoformalization on challenging research-level theorems by\nleveraging real Lean Blueprint formalization projects. Our evaluation of\nstate-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean\nprojects, reveals a significant gap: progress on existing benchmarks does not\nreadily translate to these more realistic settings, with the best model\nachieving only a 10.3 % pass rate. RLMEval provides a new, challenging\nbenchmark designed to guide and accelerate progress in automated reasoning for\nformal mathematics.\n","authors":["Auguste Poiroux","Antoine Bosselut","Viktor KunÄak"],"pdf_url":"https://arxiv.org/pdf/2510.25427v1.pdf","comment":"Accepted to EMNLP 2025 Findings. RLMEval benchmark released:\n  https://github.com/augustepoiroux/RLMEval"},{"id":"http://arxiv.org/abs/2510.25426v1","updated":"2025-10-29T11:49:42Z","published":"2025-10-29T11:49:42Z","title":"Implicature in Interaction: Understanding Implicature Improves Alignment\n  in Human-LLM Interaction","summary":"  The rapid advancement of Large Language Models (LLMs) is positioning language\nat the core of human-computer interaction (HCI). We argue that advancing HCI\nrequires attention to the linguistic foundations of interaction, particularly\nimplicature (meaning conveyed beyond explicit statements through shared\ncontext) which is essential for human-AI (HAI) alignment. This study examines\nLLMs' ability to infer user intent embedded in context-driven prompts and\nwhether understanding implicature improves response generation. Results show\nthat larger models approximate human interpretations more closely, while\nsmaller models struggle with implicature inference. Furthermore,\nimplicature-based prompts significantly enhance the perceived relevance and\nquality of responses across models, with notable gains in smaller models.\nOverall, 67.6% of participants preferred responses with implicature-embedded\nprompts to literal ones, highlighting a clear preference for contextually\nnuanced communication. Our work contributes to understanding how linguistic\ntheory can be used to address the alignment problem by making HAI interaction\nmore natural and contextually grounded.\n","authors":["Asutosh Hota","Jussi P. P. Jokinen"],"pdf_url":"https://arxiv.org/pdf/2510.25426v1.pdf","comment":"The manuscript is approximately 7360 words and contains 12 figures\n  and 6 tables"},{"id":"http://arxiv.org/abs/2406.07222v3","updated":"2025-10-29T11:36:28Z","published":"2024-06-11T13:01:50Z","title":"Reliable Evaluation and Benchmarks for Statement Autoformalization","summary":"  Evaluating statement autoformalization, translating natural language\nmathematics into formal languages like Lean 4, remains a significant challenge,\nwith few metrics, datasets, and standards to robustly measure progress. In this\nwork, we present a comprehensive approach combining improved metrics, robust\nbenchmarks, and systematic evaluation, to fill this gap. First, we introduce\nBEq+, an automated metric that correlates strongly with human judgment, along\nwith ProofNetVerif, a new dataset for assessing the quality of evaluation\nmetrics, containing 3,752 annotated examples. Second, we develop two new\nautoformalization benchmarks: ProofNet#, a corrected version of ProofNet, and\nRLM25, with 619 new pairs of research-level mathematics from six formalization\nprojects. Through systematic experimentation across these benchmarks, we find\nthat current techniques can achieve up to 45.1% accuracy on undergraduate\nmathematics but struggle with research-level content without proper context.\nOur work establishes a reliable foundation for evaluating and advancing\nautoformalization systems.\n","authors":["Auguste Poiroux","Gail Weiss","Viktor KunÄak","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2406.07222v3.pdf","comment":"Accepted to EMNLP 2025. New benchmarks released, see\n  https://github.com/augustepoiroux/RLMEval ,\n  https://huggingface.co/datasets/PAug/ProofNetSharp , and\n  https://huggingface.co/datasets/PAug/ProofNetVerif . For code, see\n  https://github.com/augustepoiroux/LeanInteract"},{"id":"http://arxiv.org/abs/2510.25413v1","updated":"2025-10-29T11:29:56Z","published":"2025-10-29T11:29:56Z","title":"Seeing, Signing, and Saying: A Vision-Language Model-Assisted Pipeline\n  for Sign Language Data Acquisition and Curation from Social Media","summary":"  Most existing sign language translation (SLT) datasets are limited in scale,\nlack multilingual coverage, and are costly to curate due to their reliance on\nexpert annotation and controlled recording setup. Recently, Vision Language\nModels (VLMs) have demonstrated strong capabilities as evaluators and real-time\nassistants. Despite these advancements, their potential remains untapped in the\ncontext of sign language dataset acquisition. To bridge this gap, we introduce\nthe first automated annotation and filtering framework that utilizes VLMs to\nreduce reliance on manual effort while preserving data quality. Our method is\napplied to TikTok videos across eight sign languages and to the already curated\nYouTube-SL-25 dataset in German Sign Language for the purpose of additional\nevaluation. Our VLM-based pipeline includes a face visibility detection, a sign\nactivity recognition, a text extraction from video content, and a judgment step\nto validate alignment between video and text, implementing generic filtering,\nannotation and validation steps. Using the resulting corpus, TikTok-SL-8, we\nassess the performance of two off-the-shelf SLT models on our filtered dataset\nfor German and American Sign Languages, with the goal of establishing baselines\nand evaluating the robustness of recent models on automatically extracted,\nslightly noisy data. Our work enables scalable, weakly supervised pretraining\nfor SLT and facilitates data acquisition from social media.\n","authors":["Shakib Yazdani","Yasser Hamidullah","Cristina EspaÃ±a-Bonet","Josef van Genabith"],"pdf_url":"https://arxiv.org/pdf/2510.25413v1.pdf","comment":"Accepted by RANLP 2025"},{"id":"http://arxiv.org/abs/2510.25412v1","updated":"2025-10-29T11:29:03Z","published":"2025-10-29T11:29:03Z","title":"Serve Programs, Not Prompts","summary":"  Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications.\n","authors":["In Gim","Lin Zhong"],"pdf_url":"https://arxiv.org/pdf/2510.25412v1.pdf","comment":"HotOS 2025. Follow-up implementation work (SOSP 2025) is available at\n  arXiv:2510.24051"},{"id":"http://arxiv.org/abs/2510.25409v1","updated":"2025-10-29T11:27:08Z","published":"2025-10-29T11:27:08Z","title":"BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic\n  Domains","summary":"  The rapid advancement of large language models(LLMs) has intensified the need\nfor domain and culture specific evaluation. Existing benchmarks are largely\nAnglocentric and domain-agnostic, limiting their applicability to India-centric\ncontexts. To address this gap, we introduce BhashaBench V1, the first\ndomain-specific, multi-task, bilingual benchmark focusing on critical Indic\nknowledge systems. BhashaBench V1 contains 74,166 meticulously curated\nquestion-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from\nauthentic government and domain-specific exams. It spans four major domains:\nAgriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and\ncovering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs\nreveals significant domain and language specific performance gaps, with\nespecially large disparities in low-resource domains. For instance, GPT-4o\nachieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models\nconsistently perform better on English content compared to Hindi across all\ndomains. Subdomain-level analysis shows that areas such as Cyber Law,\nInternational Finance perform relatively well, while Panchakarma, Seed Science,\nand Human Rights remain notably weak. BhashaBench V1 provides a comprehensive\ndataset for evaluating large language models across India's diverse knowledge\ndomains. It enables assessment of models' ability to integrate domain-specific\nknowledge with bilingual understanding. All code, benchmarks, and resources are\npublicly available to support open research.\n","authors":["Vijay Devane","Mohd Nauman","Bhargav Patel","Aniket Mahendra Wakchoure","Yogeshkumar Sant","Shyam Pawar","Viraj Thakur","Ananya Godse","Sunil Patra","Neha Maurya","Suraj Racha","Nitish Kamal Singh","Ajay Nagpal","Piyush Sawarkar","Kundeshwar Vijayrao Pundalik","Rohit Saluja","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2510.25409v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21285v2","updated":"2025-10-29T11:06:45Z","published":"2025-10-24T09:32:25Z","title":"When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large\n  Reasoning Models with Chain-of-Guardrails","summary":"  Large Reasoning Models (LRMs) demonstrate remarkable capabilities on complex\nreasoning tasks but remain vulnerable to severe safety risks, including harmful\ncontent generation and jailbreak attacks. Existing mitigation strategies rely\non injecting heuristic safety signals during training, which often suppress\nreasoning ability and fail to resolve the safety-reasoning trade-off. To\nsystematically investigate this issue, we analyze the reasoning trajectories of\ndiverse LRMs and uncover a phenomenon we term Self-Jailbreak, where models\noverride their own risk assessments and justify responding to unsafe prompts.\nThis finding reveals that LRMs inherently possess the ability to reject unsafe\nqueries, but this ability is compromised, resulting in harmful outputs.\nBuilding on these insights, we propose the Chain-of-Guardrail (CoG), a training\nframework that recomposes or backtracks unsafe reasoning steps, steering the\nmodel back onto safe trajectories while preserving valid reasoning chains.\nExtensive experiments across multiple reasoning and safety benchmarks\ndemonstrate that CoG substantially improves the safety of current LRMs while\npreserving comparable reasoning ability, significantly outperforming prior\nmethods that suffer from severe safety-reasoning trade-offs.\n","authors":["Yingzhi Mao","Chunkang Zhang","Junxiang Wang","Xinyan Guan","Boxi Cao","Yaojie Lu","Hongyu Lin","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2510.21285v2.pdf","comment":"First two authors contributed equally. The main text is 10 pages,\n  with an appendix of 19 pages. The paper contains 18 figures and 16 tables"},{"id":"http://arxiv.org/abs/2506.03690v2","updated":"2025-10-29T11:04:12Z","published":"2025-06-04T08:19:37Z","title":"Robust Preference Optimization via Dynamic Target Margins","summary":"  The alignment of Large Language Models (LLMs) is crucial for ensuring their\nsafety and reliability in practical applications. Direct Preference\nOptimization (DPO) has emerged as an efficient method that directly optimizes\nmodels using preference pairs, significantly reducing resource demands.\nHowever, the effectiveness of DPO heavily depends on the data quality, which is\nfrequently compromised by noise. In this work, we propose $\\gamma$-PO, a\ndynamic target margin preference optimization algorithm that adjust reward\nmargins at the pairwise level. By introducing instance-specific margin\ncalibration, $\\gamma$-PO strategically prioritizes high-confidence pairs (those\ndemonstrating higher reward margins) while suppressing potential noise from\nambiguous pairs. Moreover, $\\gamma$-PO is a plug-and-play method, compatible\nwith variants of DPO that rely on reward margin between preference pairs.\nAcross benchmarks such as AlpacaEval2 and Arena-Hard, $\\gamma$-PO achieves an\naverage 4.4\\% improvement over other baselines, setting new benchmarks for\nstate-of-the-art performance. Additionally, $\\gamma$-PO requires minimal code\nchanges and has a negligible impact on training efficiency, making it a robust\nsolution for enhancing LLMs alignment. Our codes are available at\n\\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.\n","authors":["Jie Sun","Junkang Wu","Jiancan Wu","Zhibo Zhu","Xingyu Lu","Jun Zhou","Lintao Ma","Xiang Wang"],"pdf_url":"https://arxiv.org/pdf/2506.03690v2.pdf","comment":"18 pages, 6 figures, accepted to Findings of the 63rd Annual Meeting\n  of the Association for Computational Linguistics (ACL 2025)"},{"id":"http://arxiv.org/abs/2510.25384v1","updated":"2025-10-29T10:55:52Z","published":"2025-10-29T10:55:52Z","title":"Roleplaying with Structure: Synthetic Therapist-Client Conversation\n  Generation from Questionnaires","summary":"  The development of AI for mental health is hindered by a lack of authentic\ntherapy dialogues, due to strict privacy regulations and the fact that clinical\nsessions were historically rarely recorded. We present an LLM-driven pipeline\nthat generates synthetic counseling dialogues based on structured client\nprofiles and psychological questionnaires. Grounded on the principles of\nCognitive Behavioral Therapy (CBT), our method creates synthetic therapeutic\nconversations for clinical disorders such as anxiety and depression. Our\nframework, SQPsych (Structured Questionnaire-based Psychotherapy), converts\nstructured psychological input into natural language dialogues through\ntherapist-client simulations. Due to data governance policies and privacy\nrestrictions prohibiting the transmission of clinical questionnaire data to\nthird-party services, previous methodologies relying on proprietary models are\ninfeasible in our setting. We address this limitation by generating a\nhigh-quality corpus using open-weight LLMs, validated through human expert\nevaluation and LLM-based assessments. Our SQPsychLLM models fine-tuned on\nSQPsychConv achieve strong performance on counseling benchmarks, surpassing\nbaselines in key therapeutic skills. Our findings highlight the potential of\nsynthetic data to enable scalable, data-secure, and clinically informed AI for\nmental health support. We will release our code, models, and corpus at\nhttps://ai-mh.github.io/SQPsych\n","authors":["Doan Nam Long Vu","Rui Tan","Lena Moench","Svenja Jule Francke","Daniel Woiwod","Florian Thomas-Odenthal","Sanna Stroth","Tilo Kircher","Christiane Hermann","Udo Dannlowski","Hamidreza Jamalabadi","Shaoxiong Ji"],"pdf_url":"https://arxiv.org/pdf/2510.25384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25378v1","updated":"2025-10-29T10:51:35Z","published":"2025-10-29T10:51:35Z","title":"Hallucinations in Bibliographic Recommendation: Citation Frequency as a\n  Proxy for Training Data Redundancy","summary":"  Large language models (LLMs) have been increasingly applied to a wide range\nof tasks, from natural language understanding to code generation. While they\nhave also been used to assist in bibliographic recommendation, the\nhallucination of non-existent papers remains a major issue. Building on prior\nstudies, this study hypothesizes that an LLM's ability to correctly produce\nbibliographic information depends on whether the underlying knowledge is\ngenerated or memorized, with highly cited papers (i.e., more frequently appear\nin the training corpus) showing lower hallucination rates. We therefore assume\ncitation count as a proxy for training data redundancy (i.e., the frequency\nwith which a given bibliographic record is repeatedly represented in the\npretraining corpus) and investigate how citation frequency affects hallucinated\nreferences in LLM outputs. Using GPT-4.1, we generated and manually verified\n100 bibliographic records across twenty computer-science domains, and measured\nfactual consistency via cosine similarity between generated and authentic\nmetadata. The results revealed that (i) hallucination rates vary across\nresearch domains, (ii) citation count is strongly correlated with factual\naccuracy, and (iii) bibliographic information becomes almost verbatimly\nmemorized beyond approximately 1,000 citations. These findings suggest that\nhighly cited papers are nearly verbatimly retained in the model, indicating a\nthreshold where generalization shifts into memorization.\n","authors":["Junichiro Niimi"],"pdf_url":"https://arxiv.org/pdf/2510.25378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25370v1","updated":"2025-10-29T10:41:03Z","published":"2025-10-29T10:41:03Z","title":"Monitoring Transformative Technological Convergence Through\n  LLM-Extracted Semantic Entity Triple Graphs","summary":"  Forecasting transformative technologies remains a critical but challenging\ntask, particularly in fast-evolving domains such as Information and\nCommunication Technologies (ICTs). Traditional expert-based methods struggle to\nkeep pace with short innovation cycles and ambiguous early-stage terminology.\nIn this work, we propose a novel, data-driven pipeline to monitor the emergence\nof transformative technologies by identifying patterns of technological\nconvergence.\n  Our approach leverages advances in Large Language Models (LLMs) to extract\nsemantic triples from unstructured text and construct a large-scale graph of\ntechnology-related entities and relations. We introduce a new method for\ngrouping semantically similar technology terms (noun stapling) and develop\ngraph-based metrics to detect convergence signals. The pipeline includes\nmulti-stage filtering, domain-specific keyword clustering, and a temporal trend\nanalysis of topic co-occurence.\n  We validate our methodology on two complementary datasets: 278,625 arXiv\npreprints (2017--2024) to capture early scientific signals, and 9,793 USPTO\npatent applications (2018-2024) to track downstream commercial developments.\nOur results demonstrate that the proposed pipeline can identify both\nestablished and emerging convergence patterns, offering a scalable and\ngeneralizable framework for technology forecasting grounded in full-text\nanalysis.\n","authors":["Alexander Sternfeld","Andrei Kucharavy","Dimitri Percia David","Alain Mermoud","Julian Jang-Jaccard","Nathan Monnet"],"pdf_url":"https://arxiv.org/pdf/2510.25370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25364v1","updated":"2025-10-29T10:36:39Z","published":"2025-10-29T10:36:39Z","title":"CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction\n  Tuning for BabyLMs","summary":"  This work investigates whether small-scale LMs can benefit from instruction\ntuning. We compare conversational and question-answering instruction tuning\ndatasets, applied either in a merged or sequential curriculum, using\ndecoder-only models with 100M and 140M parameters. Evaluation spans both\nfine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and\npsycholinguistic correlation) settings. Results show that instruction tuning\nyields small but consistent gains in fine-tuning scenarios, with sequential\ncurricula outperforming merged data; however, improvements do not consistently\ntransfer to zero-shot tasks, suggesting a trade-off between interaction-focused\nadaptation and broad linguistic generalization. These results highlight both\nthe potential and the constraints of adapting human-inspired learning\nstrategies to low-resource LMs, and point toward hybrid, curriculum-based\napproaches for enhancing generalization under ecological training limits.\n","authors":["Luca Capone","Alessandro Bondielli","Alessandro Lenci"],"pdf_url":"https://arxiv.org/pdf/2510.25364v1.pdf","comment":"Paper accepted for oral presentation at the BabyLM Challange 2025\n  (EMNLP2025)"},{"id":"http://arxiv.org/abs/2510.25356v1","updated":"2025-10-29T10:21:25Z","published":"2025-10-29T10:21:25Z","title":"Not ready for the bench: LLM legal interpretation is unstable and out of\n  step with human judgments","summary":"  Legal interpretation frequently involves assessing how a legal text, as\nunderstood by an 'ordinary' speaker of the language, applies to the set of\nfacts characterizing a legal dispute in the U.S. judicial system. Recent\nscholarship has proposed that legal practitioners add large language models\n(LLMs) to their interpretive toolkit. This work offers an empirical argument\nagainst LLM interpretation as recently practiced by legal scholars and federal\njudges. Our investigation in English shows that models do not provide stable\ninterpretive judgments: varying the question format can lead the model to\nwildly different conclusions. Moreover, the models show weak to moderate\ncorrelation with human judgment, with large variance across model and question\nvariant, suggesting that it is dangerous to give much credence to the\nconclusions produced by generative AI.\n","authors":["Abhishek Purushothama","Junghyun Min","Brandon Waldon","Nathan Schneider"],"pdf_url":"https://arxiv.org/pdf/2510.25356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06204v2","updated":"2025-10-29T10:17:57Z","published":"2025-07-08T17:30:14Z","title":"Differential Mamba","summary":"  Sequence models like Transformers and RNNs often overallocate attention to\nirrelevant context, leading to noisy intermediate representations. This\ndegrades LLM capabilities by promoting hallucinations, weakening long-range and\nretrieval abilities, and reducing robustness. Recent work has shown that\ndifferential design can mitigate this issue in Transformers, improving their\neffectiveness across various applications. In this paper, we explore whether\nthese techniques, originally developed for Transformers, can be applied to\nMamba, a recent architecture based on selective state-space layers that\nachieves Transformer-level performance with greater efficiency. We show that a\nnaive adaptation of differential design to Mamba is insufficient and requires\ncareful architectural modifications. To address this, we introduce a novel\ndifferential mechanism for Mamba, empirically validated on language modeling\nbenchmarks, demonstrating improved retrieval capabilities and superior\nperformance over vanilla Mamba. Finally, we conduct extensive ablation studies\nand empirical analyses to justify our design choices and provide evidence that\nour approach effectively mitigates the overallocation problem in Mamba-based\nmodels. Our code is publicly available: https://github.com/NadavSc/Diff-Mamba\n","authors":["Nadav Schneider","Itamar Zimerman","Eliya Nachmani"],"pdf_url":"https://arxiv.org/pdf/2507.06204v2.pdf","comment":"AACL 2025. We provide the code at\n  https://github.com/NadavSc/Diff-Mamba"},{"id":"http://arxiv.org/abs/2510.25333v1","updated":"2025-10-29T09:47:40Z","published":"2025-10-29T09:47:40Z","title":"CRMWeaver: Building Powerful Business Agent via Agentic RL and Shared\n  Memories","summary":"  Recent years have witnessed the rapid development of LLM-based agents, which\nshed light on using language agents to solve complex real-world problems. A\nprominent application lies in business agents, which interact with databases\nand internal knowledge bases via tool calls to fulfill diverse user\nrequirements. However, this domain is characterized by intricate data\nrelationships and a wide range of heterogeneous tasks, from statistical data\nqueries to knowledge-based question-answering. To address these challenges, we\npropose CRMWeaver, a novel approach that enhances business agents in such\ncomplex settings. To acclimate the agentic model to intricate business\nenvironments, we employ a synthesis data generation and RL-based paradigm\nduring training, which significantly improves the model's ability to handle\ncomplex data and varied tasks. During inference, a shared memories mechanism is\nintroduced, prompting the agent to learn from task guidelines in similar\nproblems, thereby further boosting its effectiveness and generalization,\nespecially in unseen scenarios. We validate the efficacy of our approach on the\nCRMArena-Pro dataset, where our lightweight model achieves competitive results\nin both B2B and B2C business scenarios, underscoring its practical value for\nreal-world applications.\n","authors":["Yilong Lai","Yipin Yang","Jialong Wu","Fengran Mo","Zhenglin Wang","Ting Liang","Jianguo Lin","Keping Yang"],"pdf_url":"https://arxiv.org/pdf/2510.25333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23252v2","updated":"2025-10-29T09:41:26Z","published":"2025-10-27T12:14:52Z","title":"Are ASR foundation models generalized enough to capture features of\n  regional dialects for low-resource languages?","summary":"  Conventional research on speech recognition modeling relies on the canonical\nform for most low-resource languages while automatic speech recognition (ASR)\nfor regional dialects is treated as a fine-tuning task. To investigate the\neffects of dialectal variations on ASR we develop a 78-hour annotated Bengali\nSpeech-to-Text (STT) corpus named Ben-10. Investigation from linguistic and\ndata-driven perspectives shows that speech foundation models struggle heavily\nin regional dialect ASR, both in zero-shot and fine-tuned settings. We observe\nthat all deep learning methods struggle to model speech data under dialectal\nvariations but dialect specific model training alleviates the issue. Our\ndataset also serves as a out of-distribution (OOD) resource for ASR modeling\nunder constrained resources in ASR algorithms. The dataset and code developed\nfor this project are publicly available\n","authors":["Tawsif Tashwar Dipto","Azmol Hossain","Rubayet Sabbir Faruque","Md. Rezuwan Hassan","Kanij Fatema","Tanmoy Shome","Ruwad Naswan","Md. Foriduzzaman Zihad","Mohaymen Ul Anam","Nazia Tasnim","Hasan Mahmud","Md Kamrul Hasan","Md. Mehedi Hasan Shawon","Farig Sadeque","Tahsin Reasat"],"pdf_url":"https://arxiv.org/pdf/2510.23252v2.pdf","comment":"The manuscript has to be withdrawn to address an authorship and\n  intellectual property clarification"},{"id":"http://arxiv.org/abs/2510.25320v1","updated":"2025-10-29T09:35:55Z","published":"2025-10-29T09:35:55Z","title":"GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement\n  Learning","summary":"  Autonomous agents powered by large language models (LLMs) have shown\nimpressive capabilities in tool manipulation for complex task-solving. However,\nexisting paradigms such as ReAct rely on sequential reasoning and execution,\nfailing to exploit the inherent parallelism among independent sub-tasks. This\nsequential bottleneck leads to inefficient tool utilization and suboptimal\nperformance in multi-step reasoning scenarios. We introduce Graph-based Agent\nPlanning (GAP), a novel framework that explicitly models inter-task\ndependencies through graph-based planning to enable adaptive parallel and\nserial tool execution. Our approach trains agent foundation models to decompose\ncomplex tasks into dependency-aware sub-task graphs, autonomously determining\nwhich tools can be executed in parallel and which must follow sequential\ndependencies. This dependency-aware orchestration achieves substantial\nimprovements in both execution efficiency and task accuracy. To train GAP, we\nconstruct a high-quality dataset of graph-based planning traces derived from\nthe Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage\ntraining strategy: supervised fine-tuning (SFT) on the curated dataset,\nfollowed by reinforcement learning (RL) with a correctness-based reward\nfunction on strategically sampled queries where tool-based reasoning provides\nmaximum value. Experimental results on MHQA datasets demonstrate that GAP\nsignificantly outperforms traditional ReAct baselines, particularly on\nmulti-step retrieval tasks, while achieving dramatic improvements in tool\ninvocation efficiency through intelligent parallelization. The project page is\navailable at: https://github.com/WJQ7777/Graph-Agent-Planning.\n","authors":["Jiaqi Wu","Qinlao Zhao","Zefeng Chen","Kai Qin","Yifei Zhao","Xueqian Wang","Yuhang Yao"],"pdf_url":"https://arxiv.org/pdf/2510.25320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25310v1","updated":"2025-10-29T09:23:17Z","published":"2025-10-29T09:23:17Z","title":"Parrot: A Training Pipeline Enhances Both Program CoT and Natural\n  Language CoT for Reasoning","summary":"  Natural language chain-of-thought (N-CoT) and Program chain-of-thought\n(P-CoT) have emerged as two primary paradigms for large language models (LLMs)\nto solve mathematical reasoning problems. Current research typically endeavors\nto achieve unidirectional enhancement: P-CoT enhanced N-CoT or N-CoT enhanced\nP-CoT. In this paper, we seek to fully unleash the two paradigms' strengths for\nmutual enhancement and ultimately achieve simultaneous improvements. We conduct\na detailed analysis of the error types across two paradigms, based on which we\npropose Parrot, a novel training pipeline for mathematical problems: 1) Three\ntarget-designed subtasks integrate sequential P-CoT and N-CoT generation. 2) A\nsubtask hybrid training strategy to facilitate natural language semantic\ntransferability. 3) The converted N-CoT auxiliary reward is designed to\nalleviate the sparse rewards in P-CoT optimization. Extensive experiments\ndemonstrate that Parrot significantly enhances both the performance of N-CoT\nand P-CoT, especially on N-CoT. Using Parrot SFT, the N-CoT performance of\nLLaMA2 and CodeLLaMA achieve gains of +21.87 and +21.48 on MathQA over the RL\nbaseline, which is resource-intensive.\n","authors":["Senjie Jin","Lu Chen","Zhiheng Xi","Yuhui Wang","Sirui Song","Yuhao Zhou","Xinbo Zhang","Peng Sun","Hong Lu","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2510.25310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25303v1","updated":"2025-10-29T09:14:41Z","published":"2025-10-29T09:14:41Z","title":"Teaching Sarcasm: Few-Shot Multimodal Sarcasm Detection via Distillation\n  to a Parameter-Efficient Student","summary":"  Multimodal sarcasm detection is challenging, especially in low-resource\nsettings where subtle image-text contradictions are hard to learn due to scarce\nannotated data, which hinders the model's performance. Parameter-efficient\nfine-tuning (PEFT) methods like adapters, LoRA, and prompt tuning reduce\noverfitting but struggle to reach optimal performance due to limited\nsupervision from few-shot data. We propose PEKD, a unified framework that\nenhances PEFT methods via distillation from an expert model trained on\nlarge-scale sarcasm data, which acts as the teacher. To mitigate unreliable\nsignals from the teacher, we introduce an entropy-aware gating mechanism that\ndynamically adjusts the distillation strength based on teacher confidence.\nExperiments on two public datasets demonstrate that our PEKD framework enables\nPEFT methods to outperform both prior parameter-efficient approaches and large\nmultimodal models, achieving strong results in the few-shot scenario. The\nframework is modular and adaptable to a wide range of multimodal models and\ntasks.\n","authors":["Soumyadeep Jana","Sanasam Ranbir Singh"],"pdf_url":"https://arxiv.org/pdf/2510.25303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25273v1","updated":"2025-10-29T08:32:22Z","published":"2025-10-29T08:32:22Z","title":"Adapting Small Language Models to Low-Resource Domains: A Case Study in\n  Hindi Tourism QA","summary":"  Domain-specific question answering in low-resource languages faces two key\nchallenges: scarcity of annotated datasets and limited domain knowledge in\ngeneral-purpose language models. In this work, we present a multi-stage\nfinetuning strategy to adapt lightweight language models to the Hindi tourism\ndomain by leveraging both original and synthetic training data. Synthetic\nquestion-answer pairs are generated using large LLMs (LLaMA-70B, Phi-14B) and\nused to augment the limited original dataset. We explore several training\nmethodologies and analyse their impact on domain generalisation. Our results\ndemonstrate that large models can efficiently generate synthetic data, while\nsmall models can effectively adapt to it, offering a scalable pathway for\nlow-resource, domain-specific QA.\n","authors":["Sandipan Majhi","Paheli Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2510.25273v1.pdf","comment":"Accepted at the Forum for Information Retrieval Evaluation 2025\n  (VATIKA Track)"},{"id":"http://arxiv.org/abs/2510.00568v2","updated":"2025-10-29T08:22:54Z","published":"2025-10-01T06:44:28Z","title":"ReSeek: A Self-Correcting Framework for Search Agents with Instructive\n  Rewards","summary":"  Search agents powered by Large Language Models (LLMs) have demonstrated\nsignificant potential in tackling knowledge-intensive tasks. Reinforcement\nlearning (RL) has emerged as a powerful paradigm for training these agents to\nperform complex, multi-step reasoning. However, prior RL-based methods often\nrely on sparse or rule-based rewards, which can lead agents to commit to\nsuboptimal or erroneous reasoning paths without the ability to recover. To\naddress these limitations, we propose ReSeek, a novel self-correcting framework\nfor training search agents. Our framework introduces a self-correction\nmechanism that empowers the agent to dynamically identify and recover from\nerroneous search paths during an episode. By invoking a special JUDGE action,\nthe agent can judge the information and re-plan its search strategy. To guide\nthis process, we design a dense, instructive process reward function, which\ndecomposes into a correctness reward for retrieving factual information and a\nutility reward for finding information genuinely useful for the query.\nFurthermore, to mitigate the risk of data contamination in existing datasets,\nwe introduce FictionalHot, a new and challenging benchmark with recently\ncurated questions requiring complex reasoning. Being intuitively reasonable and\npractically simple, extensive experiments show that agents trained with ReSeek\nsignificantly outperform SOTA baselines in task success rate and path\nfaithfulness.\n","authors":["Shiyu Li","Yang Tang","Yifan Wang","Peiming Li","Xi Chen"],"pdf_url":"https://arxiv.org/pdf/2510.00568v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2503.05493v2","updated":"2025-10-29T08:19:03Z","published":"2025-03-07T15:05:23Z","title":"Can LLMs Outshine Conventional Recommenders? A Comparative Evaluation","summary":"  In recent years, integrating large language models (LLMs) into recommender\nsystems has created new opportunities for improving recommendation quality.\nHowever, a comprehensive benchmark is needed to thoroughly evaluate and compare\nthe recommendation capabilities of LLMs with traditional recommender systems.\nIn this paper, we introduce RecBench, which systematically investigates various\nitem representation forms (including unique identifier, text, semantic\nembedding, and semantic identifier) and evaluates two primary recommendation\ntasks, i.e., click-through rate prediction (CTR) and sequential recommendation\n(SeqRec). Our extensive experiments cover up to 17 large models and are\nconducted across five diverse datasets from fashion, news, video, books, and\nmusic domains. Our findings indicate that LLM-based recommenders outperform\nconventional recommenders, achieving up to a 5% AUC improvement in the CTR\nscenario and up to a 170% NDCG@10 improvement in the SeqRec scenario. However,\nthese substantial performance gains come at the expense of significantly\nreduced inference efficiency, rendering the LLM-as-RS paradigm impractical for\nreal-time recommendation environments. We aim for our findings to inspire\nfuture research, including recommendation-specific model acceleration methods.\nWe will release our code, data, configurations, and platform to enable other\nresearchers to reproduce and build upon our experimental results.\n","authors":["Qijiong Liu","Jieming Zhu","Lu Fan","Kun Wang","Hengchang Hu","Wei Guo","Yong Liu","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2503.05493v2.pdf","comment":"NeurIPS 2025 DB Track Accepted Paper"},{"id":"http://arxiv.org/abs/2510.22967v2","updated":"2025-10-29T07:50:03Z","published":"2025-10-27T03:41:32Z","title":"MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality\n  Evaluation in LLMs","summary":"  The widespread adoption of Large Language Models (LLMs) raises critical\nconcerns about the factual accuracy of their outputs, especially in high-risk\ndomains such as biomedicine, law, and education. Existing evaluation methods\nfor short texts often fail on long-form content due to complex reasoning\nchains, intertwined perspectives, and cumulative information. To address this,\nwe propose a systematic approach integrating large-scale long-form datasets,\nmulti-agent verification mechanisms, and weighted evaluation metrics. We\nconstruct LongHalluQA, a Chinese long-form factuality dataset; and develop\nMAD-Fact, a debate-based multi-agent verification system. We introduce a fact\nimportance hierarchy to capture the varying significance of claims in long-form\ntexts. Experiments on two benchmarks show that larger LLMs generally maintain\nhigher factual consistency, while domestic models excel on Chinese content. Our\nwork provides a structured framework for evaluating and enhancing factual\nreliability in long-form LLM outputs, guiding their safe deployment in\nsensitive domains.\n","authors":["Yucheng Ning","Xixun Lin","Fang Fang","Yanan Cao"],"pdf_url":"https://arxiv.org/pdf/2510.22967v2.pdf","comment":"The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-025-51369-x}"},{"id":"http://arxiv.org/abs/2505.15356v2","updated":"2025-10-29T07:34:05Z","published":"2025-05-21T10:38:50Z","title":"NL-Debugging: Exploiting Natural Language as an Intermediate\n  Representation for Code Debugging","summary":"  Debugging is a critical aspect of LLM's coding ability. Early debugging\nefforts primarily focused on code-level analysis, which often falls short when\naddressing complex programming errors that require a deeper understanding of\nalgorithmic logic. Recent advancements in large language models (LLMs) have\nshifted attention toward leveraging natural language reasoning to enhance\ncode-related tasks. However, two fundamental questions remain unanswered: What\ntype of natural language format is most effective for debugging tasks? And what\nspecific benefits does natural language reasoning bring to the debugging\nprocess? In this paper, we introduce NL-DEBUGGING, a novel framework that\nemploys natural language as an intermediate representation to improve code\ndebugging. By debugging at a natural language level, we demonstrate that\nNL-DEBUGGING outperforms traditional debugging methods and enables a broader\nmodification space through direct refinement guided by execution feedback. Our\nfindings highlight the potential of natural language reasoning to advance\nautomated code debugging and address complex programming challenges.\n","authors":["Weiming Zhang","Qingyao Li","Xinyi Dai","Jizheng Chen","Kounianhua Du","Weiwen Liu","Yasheng Wang","Ruiming Tang","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.15356v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25232v1","updated":"2025-10-29T07:18:43Z","published":"2025-10-29T07:18:43Z","title":"From Medical Records to Diagnostic Dialogues: A Clinical-Grounded\n  Approach and Dataset for Psychiatric Comorbidity","summary":"  Psychiatric comorbidity is clinically significant yet challenging due to the\ncomplexity of multiple co-occurring disorders. To address this, we develop a\nnovel approach integrating synthetic patient electronic medical record (EMR)\nconstruction and multi-agent diagnostic dialogue generation. We create 502\nsynthetic EMRs for common comorbid conditions using a pipeline that ensures\nclinical relevance and diversity. Our multi-agent framework transfers the\nclinical interview protocol into a hierarchical state machine and context tree,\nsupporting over 130 diagnostic states while maintaining clinical standards.\nThrough this rigorous process, we construct PsyCoTalk, the first large-scale\ndialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic\ndialogues validated by psychiatrists. This dataset enhances diagnostic accuracy\nand treatment planning, offering a valuable resource for psychiatric\ncomorbidity research. Compared to real-world clinical transcripts, PsyCoTalk\nexhibits high structural and linguistic fidelity in terms of dialogue length,\ntoken distribution, and diagnostic reasoning strategies. Licensed psychiatrists\nconfirm the realism and diagnostic validity of the dialogues. This dataset\nenables the development and evaluation of models capable of multi-disorder\npsychiatric screening in a single conversational pass.\n","authors":["Tianxi Wan","Jiaming Luo","Siyuan Chen","Kunyao Lan","Jianhua Chen","Haiyang Geng","Mengyue Wu"],"pdf_url":"https://arxiv.org/pdf/2510.25232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.19902v2","updated":"2025-10-29T07:17:51Z","published":"2025-09-24T08:56:32Z","title":"WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and\n  Interaction","summary":"  In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on\na large language model (LLM) for speech understanding, generation, and\ninteraction. There are three key features of WEST: 1) Fully LLM-based: Standing\non the shoulders of giants by reusing mature architectures, ecosystems (e.g.,\nHugging Face), and methods (e.g., sequence packing) from large models. 2)\nFull-stack: Supports tasks such as recognition, synthesis, understanding,\ndialogue, and multimodal capabilities, with extensibility to incorporate\nopen-source models. 3) Simple and Stupid: A simple and stupid speech toolkit\nthat everyone can Touch. In addition, WEST provides two types of recipes,\nmodels, and experimental results. The first is entirely based on open-source\nmodels and open-source data, allowing users to fully reproduce the experiments\nin this paper and serving as a verification system or minimal system baseline.\nThe second is trained on massive data, offering superior performance so the\nuser can directly apply it out of the box. WEST is publicly avilable at\nhttps://github.com/wenet-e2e/west/\n","authors":["Binbin Zhang","Chengdong Liang","Shuai Wang","Xuelong Geng","Zhao Guo","Haoyu Li","Hao Yin","Xipeng Yang","Pengshen Zhang","Changwei Ma","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2509.19902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25224v1","updated":"2025-10-29T07:00:11Z","published":"2025-10-29T07:00:11Z","title":"ProMediate: A Socio-cognitive framework for evaluating proactive agents\n  in multi-party negotiation","summary":"  While Large Language Models (LLMs) are increasingly used in agentic\nframeworks to assist individual users, there is a growing need for agents that\ncan proactively manage complex, multi-party collaboration. Systematic\nevaluation methods for such proactive agents remain scarce, limiting progress\nin developing AI that can effectively support multiple people together.\nNegotiation offers a demanding testbed for this challenge, requiring\nsocio-cognitive intelligence to navigate conflicting interests between multiple\nparticipants and multiple topics and build consensus. Here, we present\nProMediate, the first framework for evaluating proactive AI mediator agents in\ncomplex, multi-topic, multi-party negotiations. ProMediate consists of two core\ncomponents: (i) a simulation testbed based on realistic negotiation cases and\ntheory-driven difficulty levels (ProMediate-Easy, ProMediate-Medium, and\nProMediate-Hard), with a plug-and-play proactive AI mediator grounded in\nsocio-cognitive mediation theories, capable of flexibly deciding when and how\nto intervene; and (ii) a socio-cognitive evaluation framework with a new suite\nof metrics to measure consensus changes, intervention latency, mediator\neffectiveness, and intelligence. Together, these components establish a\nsystematic framework for assessing the socio-cognitive intelligence of\nproactive AI agents in multi-party settings. Our results show that a socially\nintelligent mediator agent outperforms a generic baseline, via faster,\nbetter-targeted interventions. In the ProMediate-Hard setting, our social\nmediator increases consensus change by 3.6 percentage points compared to the\ngeneric baseline (10.65\\% vs 7.01\\%) while being 77\\% faster in response\n(15.98s vs. 3.71s). In conclusion, ProMediate provides a rigorous,\ntheory-grounded testbed to advance the development of proactive, socially\nintelligent agents.\n","authors":["Ziyi Liu","Bahar Sarrafzadeh","Pei Zhou","Longqi Yang","Jieyu Zhao","Ashish Sharma"],"pdf_url":"https://arxiv.org/pdf/2510.25224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.04458v2","updated":"2025-10-29T06:50:01Z","published":"2025-07-06T16:37:21Z","title":"Think Twice Before You Judge: Mixture of Dual Reasoning Experts for\n  Multimodal Sarcasm Detection","summary":"  Multimodal sarcasm detection has attracted growing interest due to the rise\nof multimedia posts on social media. Understanding sarcastic image-text posts\noften requires external contextual knowledge, such as cultural references or\ncommonsense reasoning. However, existing models struggle to capture the deeper\nrationale behind sarcasm, relying mainly on shallow cues like image captions or\nobject-attribute pairs from images. To address this, we propose \\textbf{MiDRE}\n(\\textbf{Mi}xture of \\textbf{D}ual \\textbf{R}easoning \\textbf{E}xperts), which\nintegrates an internal reasoning expert for detecting incongruities within the\nimage-text pair and an external reasoning expert that utilizes structured\nrationales generated via Chain-of-Thought prompting to a Large Vision-Language\nModel. An adaptive gating mechanism dynamically weighs the two experts,\nselecting the most relevant reasoning path. Unlike prior methods that treat\nexternal knowledge as static input, MiDRE selectively adapts to when such\nknowledge is beneficial, mitigating the risks of hallucinated or irrelevant\nsignals from large models. Experiments on two benchmark datasets show that\nMiDRE achieves superior performance over baselines. Various qualitative\nanalyses highlight the crucial role of external rationales, revealing that even\nwhen they are occasionally noisy, they provide valuable cues that guide the\nmodel toward a better understanding of sarcasm.\n","authors":["Soumyadeep Jana","Abhrajyoti Kundu","Sanasam Ranbir Singh"],"pdf_url":"https://arxiv.org/pdf/2507.04458v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24155v4","updated":"2025-10-29T06:49:14Z","published":"2024-10-31T17:12:14Z","title":"Blind Spot Navigation in Large Language Model Reasoning with Thought\n  Space Explorer","summary":"  Large language models have shown strong reasoning capabilities through\nchain-structured methods such as Chain-of-Thought. Recent studies optimize\nthought structures by generating parallel or tree-like structures, switching\nbetween long and short reasoning modes, or aligning reasoning steps with task\nperformance. However, these approaches mainly rely on previously generated\nlogical directions of the chains, which ignore the unexplored regions of the\nsolution space. Such a phenomenon is defined as blind spots, which limit the\ndiversity and effectiveness of the reasoning process. To this end, we propose\nthe ``Thought Space Explorer'' (TSE), a framework for navigating and expanding\nthought structures to overcome blind spots in LLM reasoning. Our TSE first\nidentifies key nodes with high impact, then generates new nodes by integrating\ninformation from multiple chains. Finally, it extends new branches through\nconnection strategies. We conduct a series of experiments on math and QA\nbenchmarks. Compared with existing baseline methods, TSE improves the accuracy\nof both the final answer and intermediate reasoning steps, while maintaining a\nbetter effectiveness-efficiency trade-off for practical deployment.\n","authors":["Jinghan Zhang","Fengran Mo","Tharindu Cyril Weerasooriya","Xinyue Ye","Dongjie Wang","Yanjie Fu","Kunpeng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.24155v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15201v2","updated":"2025-10-29T06:45:46Z","published":"2025-05-21T07:26:36Z","title":"Pass@K Policy Optimization: Solving Harder Reinforcement Learning\n  Problems","summary":"  Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts\nfor each problem and reward them independently. This optimizes for pass@1\nperformance and prioritizes the strength of isolated samples at the expense of\nthe diversity and collective utility of sets of samples. This under-utilizes\nthe sampling capacity, limiting exploration and eventual improvement on harder\nexamples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a\ntransformation on the final rewards which leads to direct optimization of\npass@k performance, thus optimizing for sets of samples that maximize reward\nwhen considered jointly. Our contribution is to derive novel low variance\nunbiased estimators for pass@k and its gradient, in both the binary and\ncontinuous reward settings. We show optimization with our estimators reduces to\nstandard RL with rewards that have been jointly transformed by a stable and\nefficient transformation function.\n  While previous efforts are restricted to k=n, ours is the first to enable\nrobust optimization of pass@k for any arbitrary k <= n. Moreover, instead of\ntrading off pass@1 performance for pass@k gains, our method allows annealing k\nduring training, optimizing both metrics and often achieving strong pass@1\nnumbers alongside significant pass@k gains.\n  We validate our reward transformations on toy experiments, which reveal the\nvariance reducing properties of our formulations. We also include real-world\nexamples using the open-source LLM, GEMMA-2. We find that our transformation\neffectively optimizes for the target k. Furthermore, higher k values enable\nsolving more and harder problems, while annealing k boosts both the pass@1 and\npass@k . Crucially, for challenging task sets where conventional pass@1\noptimization stalls, our pass@k approach unblocks learning, likely due to\nbetter exploration by prioritizing joint utility over the utility of individual\nsamples.\n","authors":["Christian Walder","Deep Karkhanis"],"pdf_url":"https://arxiv.org/pdf/2505.15201v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.09158v2","updated":"2025-10-29T06:33:58Z","published":"2025-10-10T09:03:31Z","title":"Augmenting Dialog with Think-Aloud Utterances for Modeling Individual\n  Personality Traits by LLM","summary":"  This study proposes augmenting dialog data with think-aloud utterances (TAUs)\nfor modeling individual personalities in text chat by LLM. TAU is a\nverbalization of a speaker's thought before articulating the utterance. We\nexpect \"persona LLMs\" trained with TAU-augmented data can mimic the speaker's\npersonality trait better. We tested whether the trained persona LLMs obtain the\nhuman personality with respect to Big Five, a framework characterizing human\npersonality traits from five aspects. The results showed that LLMs trained with\nTAU-augmented data more closely align to the speakers' Agreeableness and\nNeuroticism of Big Five than those trained with original dialog data. We also\nfound that the quality of TAU-augmentation impacts persona LLM's performance.\n","authors":["Seiya Ishikura","Hiroaki Yamada","Tatsuya Hiraoka","Hiroaki Yamada","Takenobu Tokunaga"],"pdf_url":"https://arxiv.org/pdf/2510.09158v2.pdf","comment":"8 pages, 1 figure. Accepted at the First Workshop on Tailoring AI:\n  Exploring Active and Passive LLM Personalization (PALS2025@EMNLP2025)"},{"id":"http://arxiv.org/abs/2509.02547v2","updated":"2025-10-29T06:27:56Z","published":"2025-09-02T17:46:26Z","title":"The Landscape of Agentic Reinforcement Learning for LLMs: A Survey","summary":"  The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm\nshift from conventional reinforcement learning applied to large language models\n(LLM RL), reframing LLMs from passive sequence generators into autonomous,\ndecision-making agents embedded in complex, dynamic worlds. This survey\nformalizes this conceptual shift by contrasting the degenerate single-step\nMarkov Decision Processes (MDPs) of LLM-RL with the temporally extended,\npartially observable Markov decision processes (POMDPs) that define Agentic RL.\nBuilding on this foundation, we propose a comprehensive twofold taxonomy: one\norganized around core agentic capabilities, including planning, tool use,\nmemory, reasoning, self-improvement, and perception, and the other around their\napplications across diverse task domains. Central to our thesis is that\nreinforcement learning serves as the critical mechanism for transforming these\ncapabilities from static, heuristic modules into adaptive, robust agentic\nbehavior. To support and accelerate future research, we consolidate the\nlandscape of open-source environments, benchmarks, and frameworks into a\npractical compendium. By synthesizing over five hundred recent works, this\nsurvey charts the contours of this rapidly evolving field and highlights the\nopportunities and challenges that will shape the development of scalable,\ngeneral-purpose AI agents.\n","authors":["Guibin Zhang","Hejia Geng","Xiaohang Yu","Zhenfei Yin","Zaibin Zhang","Zelin Tan","Heng Zhou","Zhongzhi Li","Xiangyuan Xue","Yijiang Li","Yifan Zhou","Yang Chen","Chen Zhang","Yutao Fan","Zihu Wang","Songtao Huang","Francisco Piedrahita-Velez","Yue Liao","Hongru Wang","Mengyue Yang","Heng Ji","Michael Littman","Jun Wang","Shuicheng Yan","Philip Torr","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2509.02547v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25206v1","updated":"2025-10-29T06:18:37Z","published":"2025-10-29T06:18:37Z","title":"RAVR: Reference-Answer-guided Variational Reasoning for Large Language\n  Models","summary":"  Reinforcement learning (RL) can refine the reasoning abilities of large\nlanguage models (LLMs), but critically depends on a key prerequisite: the LLM\ncan already generate high-utility reasoning paths with non-negligible\nprobability. For tasks beyond the LLM's current competence, such reasoning path\ncan be hard to sample, and learning risks reinforcing familiar but suboptimal\nreasoning. We are motivated by the insight from cognitive science that Why is\nthis the answer is often an easier question than What is the answer, as it\navoids the heavy cognitive load of open-ended exploration, opting instead for\nexplanatory reconstruction-systematically retracing the reasoning that links a\nquestion to its answer. We show that LLMs can similarly leverage answers to\nderive high-quality reasoning paths. We formalize this phenomenon and prove\nthat conditioning on answer provably increases the expected utility of sampled\nreasoning paths, thereby transforming intractable problems into learnable ones.\nBuilding on this insight, we introduce RAVR (Reference-Answer-guided\nVariational Reasoning), an end-to-end framework that uses answer-conditioned\nreasoning as a variational surrogate for question-only reasoning. Experiments\nin both general and math domains demonstrate consistent improvements over\nstrong baselines. We further analyze the reasoning behavior and find that RAVR\nreduces hesitation, strengthens conclusion consolidation, and promotes\nproblem-specific strategies in reasoning.\n","authors":["Tianqianjin Lin","Xi Zhao","Xingyao Zhang","Rujiao Long","Yi Xu","Zhuoren Jiang","Wenbo Su","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2510.25206v1.pdf","comment":"17 pages, 11 figures"},{"id":"http://arxiv.org/abs/2510.24302v2","updated":"2025-10-29T06:08:17Z","published":"2025-10-28T11:12:02Z","title":"Lookahead Tree-Based Rollouts for Enhanced Trajectory-Level Exploration\n  in Reinforcement Learning with Verifiable Rewards","summary":"  Reinforcement Learning with Verifiable Rewards (RLVR), particularly with\nalgorithms like Group Relative Policy Optimization (GRPO), has proven highly\neffective in enhancing the reasoning capabilities of large language models.\nHowever, a critical bottleneck in current pipelines lies in the limited\ndiversity of sampled trajectories during group rollouts. Homogeneous\ntrajectories and their associated rewards would diminish the return signals for\npolicy updates, thereby hindering effective policy learning. This lack of\ndiversity stems primarily from token-level stochastic sampling, where local\nvariations are likely to collapse into near-identical reasoning paths. To\naddress this limitation, we propose Lookahead Tree-Based Rollouts (LATR), a\nnovel rollout strategy designed to explicitly promotes trajectory-level\ndiversity by enforcing branching into different candidate tokens likely to\nyield distinct continuations. Specifically, LATR iteratively operates in three\nstages: (1) branching at high-uncertainty generation steps, (2) performing\nlookahead simulation for each new branch, and (3) pruning branches that\nexhibits prolonged similarity during simulation. Compared with stochastic\nSampling, LATR accelerates policy learning by 131% on average and improves\nfinal pass@1 performance by 4.2% on both GRPO and Dynamic sAmpling Policy\nOptimization (DAPO) algorithms across different reasoning tasks. Our code and\ndata are publicly available at https://github.com/starreeze/latr.\n","authors":["Shangyu Xing","Siyuan Wang","Chenyuan Yang","Xinyu Dai","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2510.24302v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.04508v2","updated":"2025-10-29T05:51:00Z","published":"2025-07-06T18:51:00Z","title":"Adapter-state Sharing CLIP for Parameter-efficient Multimodal Sarcasm\n  Detection","summary":"  The growing prevalence of multimodal image-text sarcasm on social media poses\nchallenges for opinion mining systems. Existing approaches rely on full\nfine-tuning of large models, making them unsuitable to adapt under\nresource-constrained settings. While recent parameter-efficient fine-tuning\n(PEFT) methods offer promise, their off-the-shelf use underperforms on complex\ntasks like sarcasm detection. We propose AdS-CLIP (Adapter-state Sharing in\nCLIP), a lightweight framework built on CLIP that inserts adapters only in the\nupper layers to preserve low-level unimodal representations in the lower layers\nand introduces a novel adapter-state sharing mechanism, where textual adapters\nguide visual ones to promote efficient cross-modal learning in the upper\nlayers. Experiments on two public benchmarks demonstrate that AdS-CLIP not only\noutperforms standard PEFT methods but also existing multimodal baselines with\nsignificantly fewer trainable parameters.\n","authors":["Soumyadeep Jana","Sahil Danayak","Sanasam Ranbir Singh"],"pdf_url":"https://arxiv.org/pdf/2507.04508v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15189v3","updated":"2025-10-29T05:47:44Z","published":"2024-12-19T18:57:11Z","title":"Face the Facts! Evaluating RAG-based Pipelines for Professional\n  Fact-Checking","summary":"  Natural Language Processing and Generation systems have recently shown the\npotential to complement and streamline the costly and time-consuming job of\nprofessional fact-checkers. In this work, we lift several constraints of\ncurrent state-of-the-art pipelines for automated fact-checking based on the\nRetrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark,\nfollowing professional fact-checking practices, RAG-based methods for the\ngeneration of verdicts - i.e., short texts discussing the veracity of a claim -\nevaluating them on stylistically complex claims and heterogeneous, yet\nreliable, knowledge bases. Our findings show a complex landscape, where, for\nexample, LLM-based retrievers outperform other retrieval techniques, though\nthey still struggle with heterogeneous knowledge bases; larger models excel in\nverdict faithfulness, while smaller models provide better context adherence,\nwith human evaluations favouring zero-shot and one-shot approaches for\ninformativeness, and fine-tuned models for emotional alignment.\n","authors":["Daniel Russo","Stefano Menini","Jacopo Staiano","Marco Guerini"],"pdf_url":"https://arxiv.org/pdf/2412.15189v3.pdf","comment":"Code and data at https://github.com/drusso98/face-the-facts -\n  Accepted for publication at INLG 2025"},{"id":"http://arxiv.org/abs/2504.06426v2","updated":"2025-10-29T05:47:30Z","published":"2025-04-08T20:54:00Z","title":"S'MoRE: Structural Mixture of Residual Experts for Parameter-Efficient\n  LLM Fine-tuning","summary":"  Fine-tuning pre-trained large language models (LLMs) presents a dual\nchallenge of balancing parameter efficiency and model capacity. Existing\nmethods like low-rank adaptations (LoRA) are efficient but lack flexibility,\nwhile Mixture-of-Experts (MoE) enhance model capacity at the cost of more &\nunder-utilized parameters. To address these limitations, we propose Structural\nMixture of Residual Experts (S'MoRE), a novel framework that seamlessly\nintegrates the efficiency of LoRA with the flexibility of MoE. Conceptually,\nS'MoRE employs hierarchical low-rank decomposition of expert weights, yielding\nresiduals of varying orders interconnected in a multi-layer structure. By\nrouting input tokens through sub-trees of residuals, S'MoRE emulates the\ncapacity of numerous experts by instantiating and assembling just a few\nlow-rank matrices. We craft the inter-layer propagation of S'MoRE's residuals\nas a special type of Graph Neural Network (GNN), and prove that under similar\nparameter budget, S'MoRE improves structural flexibility of traditional MoE (or\nMixture-of-LoRA) by exponential order. Comprehensive theoretical analysis and\nempirical results demonstrate that S'MoRE achieves superior fine-tuning\nperformance, offering a transformative approach for efficient LLM adaptation.\nOur implementation is available at: https://github.com/ZimpleX/SMoRE-LLM.\n","authors":["Hanqing Zeng","Yinglong Xia","Zhuokai Zhao","Chuan Jiang","Qiang Zhang","Jiayi Liu","Qunshu Zhang","Lizhu Zhang","Xiangjun Fan","Benyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.06426v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25187v1","updated":"2025-10-29T05:38:06Z","published":"2025-10-29T05:38:06Z","title":"Testing Cross-Lingual Text Comprehension In LLMs Using Next Sentence\n  Prediction","summary":"  While large language models are trained on massive datasets, this data is\nheavily skewed towards English. Does their impressive performance reflect\ngenuine ability or just this data advantage? To find out, we tested them in a\nsetting where they could not rely on data abundance: low-resource languages.\nBuilding on prior work Agarwal et al. (2025) that used Next Sentence Prediction\n(NSP) as a test, we created a large-scale benchmark with 10,000 questions each\nfor English (a high-resource language), Swahili (medium-resource), and Hausa\n(low-resource). We then tested several top models, including GPT-4 Turbo,\nGemini 1.5 Flash, and LLaMA 3 70B, to see how their performance holds up. The\nresults painted a clear picture of how levels of language resources impact\noutcomes. While all models excelled in English, their accuracy dropped in\nSwahili and fell sharply in Hausa, with LLaMA 3 struggling the most. The story\nbecame even more interesting when we introduced Chain-of-Thought (CoT)\nprompting. For the struggling LLaMA 3, CoT acted as a helpful guide,\nsignificantly boosting its accuracy. However, for the more capable GPT-4 and\nGemini, the same technique often backfired, leading to a kind of \"overthinking\"\nthat hurt their results in the cross-lingual context. This reveals that\nChain-of-Thought is not a universal solution; its effectiveness depends heavily\non the model's baseline capability and the specific context of the task. Our\nframework pinpoints LLM weaknesses, highlights when CoT helps or hinders\ncross-lingual NSP performance, and factors influencing their decisions.\n","authors":["Ritesh Sunil Chavan","Jack Mostow"],"pdf_url":"https://arxiv.org/pdf/2510.25187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05158v2","updated":"2025-10-29T04:59:45Z","published":"2025-07-07T16:13:21Z","title":"Steering Information Utility in Key-Value Memory for Language Model\n  Post-Training","summary":"  Recent advancements in language models (LMs) have marked a shift toward the\ngrowing importance of post-training. Yet, post-training approaches such as\nsupervised fine-tuning (SFT) do not guarantee the effective use of knowledge\nacquired during pretraining. We therefore introduce InfoSteer, a lightweight\nmethod that encourages parametric information utilization in LMs during\npost-training. Specifically, InfoSteer treats the feed-forward network (FFN)\nlayer as associate key-value memory and promotes the use of stored memory\nvectors via forward-pass interventions or regularization during\nbackpropagation. This simple guidance during post-training phase yields\nconsistent performance improvements across diverse model families -- including\nQwen, Gemma and Llama -- spanning 15 downstream tasks in both in-distribution\n(ID) and out-of-distribution (OOD) evaluations. Beyond performance gains, we\nalso find that steered LMs can adaptively allocate information by placing more\nemphasis on generating semantically meaningful tokens, while using fewer\nresources on simple transition ones (e.g., `\\texttt{,}' or `\\texttt{and}'). Our\nwork underscores that vanilla post-training does not fully exploit the\npotential gained during pre-training, and that steering LMs in latent\nrepresentation space offers a promising approach to enhance both performance\nand interpretability. The code is available at:\nhttps://github.com/chili-lab/InfoSteer.\n","authors":["Chunyuan Deng","Ruidi Chang","Hanjie Chen"],"pdf_url":"https://arxiv.org/pdf/2507.05158v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25160v1","updated":"2025-10-29T04:29:17Z","published":"2025-10-29T04:29:17Z","title":"Model-Document Protocol for AI Search","summary":"  AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation.\n","authors":["Hongjin Qian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25160v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2510.25150v1","updated":"2025-10-29T04:08:19Z","published":"2025-10-29T04:08:19Z","title":"Explainable Disentanglement on Discrete Speech Representations for\n  Noise-Robust ASR","summary":"  Discrete audio representations are gaining traction in speech modeling due to\ntheir interpretability and compatibility with large language models, but are\nnot always optimized for noisy or real-world environments. Building on existing\nworks that quantize Whisper embeddings for speech-to-unit modeling, we propose\ndisentangling semantic speech content from background noise in the latent\nspace. Our end-to-end model separates clean speech in the form of codebook\ntokens, while extracting interpretable noise vectors as quantization residue\nwhich are supervised via a lightweight classifier. We show that our approach\nimproves alignment between clean/noisy speech and text, producing speech tokens\nthat display a high degree of noiseinvariance, and improves ASR performance.\nKeeping Whisper frozen, we show an 82% reduction in error rate compared to\nWhisper, and 35% improvement over baseline methods on the VBDemand test set.\nFurther analyses show that the learned token space generalizes well to both\nseen and unseen acoustic conditions.\n","authors":["Shreyas Gopal","Ashutosh Anshul","Haoyang Li","Yue Heng Yeo","Hexin Liu","Eng Siong Chng"],"pdf_url":"https://arxiv.org/pdf/2510.25150v1.pdf","comment":"Awarded Best Student Paper at APSIPA ASC 2025"},{"id":"http://arxiv.org/abs/2510.19169v2","updated":"2025-10-29T03:17:43Z","published":"2025-10-22T02:02:27Z","title":"OpenGuardrails: A Configurable, Unified, and Scalable Guardrails\n  Platform for Large Language Models","summary":"  As large language models (LLMs) are increasingly integrated into real-world\napplications, ensuring their safety, robustness, and privacy compliance has\nbecome critical. We present OpenGuardrails, the first fully open-source\nplatform that unifies large-model-based safety detection, manipulation defense,\nand deployable guardrail infrastructure. OpenGuardrails protects against three\nmajor classes of risks: (1) content-safety violations such as harmful or\nexplicit text generation, (2) model-manipulation attacks including prompt\ninjection, jailbreaks, and code-interpreter abuse, and (3) data leakage\ninvolving sensitive or private information. Unlike prior modular or rule-based\nframeworks, OpenGuardrails introduces three core innovations: (1) a\nConfigurable Policy Adaptation mechanism that allows per-request customization\nof unsafe categories and sensitivity thresholds; (2) a Unified LLM-based Guard\nArchitecture that performs both content-safety and manipulation detection\nwithin a single model; and (3) a Quantized, Scalable Model Design that\ncompresses a 14B dense base model to 3.3B via GPTQ while preserving over 98 of\nbenchmark accuracy. The system supports 119 languages, achieves\nstate-of-the-art performance across multilingual safety benchmarks, and can be\ndeployed as a secure gateway or API-based service for enterprise use. All\nmodels, datasets, and deployment scripts are released under the Apache 2.0\nlicense.\n","authors":["Thomas Wang","Haowen Li"],"pdf_url":"https://arxiv.org/pdf/2510.19169v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.01617v3","updated":"2025-10-29T03:16:30Z","published":"2025-10-02T02:50:22Z","title":"AMAS: Adaptively Determining Communication Topology for LLM-based\n  Multi-Agent System","summary":"  Although large language models (LLMs) have revolutionized natural language\nprocessing capabilities, their practical implementation as autonomous\nmulti-agent systems (MAS) for industrial problem-solving encounters persistent\nbarriers. Conventional MAS architectures are fundamentally restricted by\ninflexible, hand-crafted graph topologies that lack contextual responsiveness,\nresulting in diminished efficacy across varied academic and commercial\nworkloads. To surmount these constraints, we introduce AMAS, a\nparadigm-shifting framework that redefines LLM-based MAS through a novel\ndynamic graph designer. This component autonomously identifies task-specific\noptimal graph configurations via lightweight LLM adaptation, eliminating the\nreliance on monolithic, universally applied structural templates. Instead, AMAS\nexploits the intrinsic properties of individual inputs to intelligently direct\nquery trajectories through task-optimized agent pathways. Rigorous validation\nacross question answering, mathematical deduction, and code generation\nbenchmarks confirms that AMAS systematically exceeds state-of-the-art\nsingle-agent and multi-agent approaches across diverse LLM architectures. Our\ninvestigation establishes that context-sensitive structural adaptability\nconstitutes a foundational requirement for high-performance LLM MAS\ndeployments.\n","authors":["Hui Yi Leong","Yuheng Li","Yuqing Wu","Wenwen Ouyang","Wei Zhu","Jiechao Gao","Wei Han"],"pdf_url":"https://arxiv.org/pdf/2510.01617v3.pdf","comment":"Accepted by EMNLP-2025"},{"id":"http://arxiv.org/abs/2510.04655v2","updated":"2025-10-29T03:11:50Z","published":"2025-10-06T09:59:55Z","title":"FT-MDT: Extracting Decision Trees from Medical Texts via a Novel\n  Low-rank Adaptation Method","summary":"  Knowledge of the medical decision process, which can be modeled as medical\ndecision trees (MDTs), is critical to building clinical decision support\nsystems. However, current MDT construction methods rely heavily on\ntime-consuming and laborious manual annotation. To address this challenge, we\npropose PI-LoRA (Path-Integrated LoRA), a novel low-rank adaptation method for\nautomatically extracting MDTs from clinical guidelines and textbooks. We\nintegrate gradient path information to capture synergistic effects between\ndifferent modules, enabling more effective and reliable rank allocation. This\nframework ensures that the most critical modules receive appropriate rank\nallocations while less important ones are pruned, resulting in a more efficient\nand accurate model for extracting medical decision trees from clinical texts.\nExtensive experiments on medical guideline datasets demonstrate that our\nPI-LoRA method significantly outperforms existing parameter-efficient\nfine-tuning approaches for the Text2MDT task, achieving better accuracy with\nsubstantially reduced model complexity. The proposed method achieves\nstate-of-the-art results while maintaining a lightweight architecture, making\nit particularly suitable for clinical decision support systems where\ncomputational resources may be limited.\n","authors":["Yuheng Li","Jiechao Gao","Wei Han","Wenwen Ouyang","Wei Zhu","Hui Yi Leong"],"pdf_url":"https://arxiv.org/pdf/2510.04655v2.pdf","comment":"Accepted by EMNLP-2025"},{"id":"http://arxiv.org/abs/2501.08102v5","updated":"2025-10-29T03:08:26Z","published":"2025-01-14T13:19:47Z","title":"Consistency of Responses and Continuations Generated by Large Language\n  Models on Social Media","summary":"  Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using three open-source models: Gemma, Llama3\nand Llama3.3 and one commercial Model:Claude. By analyzing climate change\ndiscussions from Twitter and Reddit, we examine emotional transitions,\nintensity patterns, and semantic consistency between human-authored and\nLLM-generated content. Our findings reveal that while both models maintain high\nsemantic coherence, they exhibit distinct emotional patterns: these models show\na strong tendency to moderate negative emotions. When the input text carries\nnegative emotions such as anger, disgust, fear, or sadness, LLM tends to\ngenerate content with more neutral emotions, or even convert them into positive\nemotions such as joy or surprise. At the same time, we compared the\nLLM-generated content with human-authored content. The four models\nsystematically generated responses with reduced emotional intensity and showed\na preference for neutral rational emotions in the response task. In addition,\nthese models all maintained a high semantic similarity with the original text,\nalthough their performance in the continuation task and the response task was\ndifferent. These findings provide deep insights into the emotion and semantic\nprocessing capabilities of LLM, which are of great significance for its\ndeployment in social media environments and human-computer interaction design.\n","authors":["Wenlu Fan","Yuqi Zhu","Bin Wang","Wentao Xu"],"pdf_url":"https://arxiv.org/pdf/2501.08102v5.pdf","comment":"This paper has been accepted by the International AAAI Conference on\n  Web and Social Media (ICWSM) 2026 (Los Angeles, California, U.S.)"},{"id":"http://arxiv.org/abs/2510.25117v1","updated":"2025-10-29T02:34:17Z","published":"2025-10-29T02:34:17Z","title":"A Survey on Unlearning in Large Language Models","summary":"  The advancement of Large Language Models (LLMs) has revolutionized natural\nlanguage processing, yet their training on massive corpora poses significant\nrisks, including the memorization of sensitive personal data, copyrighted\nmaterial, and knowledge that could facilitate malicious activities. To mitigate\nthese issues and align with legal and ethical standards such as the \"right to\nbe forgotten\", machine unlearning has emerged as a critical technique to\nselectively erase specific knowledge from LLMs without compromising their\noverall performance. This survey provides a systematic review of over 180\npapers on LLM unlearning published since 2021, focusing exclusively on\nlarge-scale generative models. Distinct from prior surveys, we introduce novel\ntaxonomies for both unlearning methods and evaluations. We clearly categorize\nmethods into training-time, post-training, and inference-time based on the\ntraining stage at which unlearning is applied. For evaluations, we not only\nsystematically compile existing datasets and metrics but also critically\nanalyze their advantages, disadvantages, and applicability, providing practical\nguidance to the research community. In addition, we discuss key challenges and\npromising future research directions. Our comprehensive overview aims to inform\nand guide the ongoing development of secure and reliable LLMs.\n","authors":["Ruichen Qiu","Jiajun Tan","Jiayue Pu","Honglin Wang","Xiao-Shan Gao","Fei Sun"],"pdf_url":"https://arxiv.org/pdf/2510.25117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25116v1","updated":"2025-10-29T02:30:18Z","published":"2025-10-29T02:30:18Z","title":"Pretraining Strategies using Monolingual and Parallel Data for\n  Low-Resource Machine Translation","summary":"  This research article examines the effectiveness of various pretraining\nstrategies for developing machine translation models tailored to low-resource\nlanguages. Although this work considers several low-resource languages,\nincluding Afrikaans, Swahili, and Zulu, the translation model is specifically\ndeveloped for Lingala, an under-resourced African language, building upon the\npretraining approach introduced by Reid and Artetxe (2021), originally designed\nfor high-resource languages. Through a series of comprehensive experiments, we\nexplore different pretraining methodologies, including the integration of\nmultiple languages and the use of both monolingual and parallel data during the\npretraining phase. Our findings indicate that pretraining on multiple languages\nand leveraging both monolingual and parallel data significantly enhance\ntranslation quality. This study offers valuable insights into effective\npretraining strategies for low-resource machine translation, helping to bridge\nthe performance gap between high-resource and low-resource languages. The\nresults contribute to the broader goal of developing more inclusive and\naccurate NLP models for marginalized communities and underrepresented\npopulations. The code and datasets used in this study are publicly available to\nfacilitate further research and ensure reproducibility, with the exception of\ncertain data that may no longer be accessible due to changes in public\navailability.\n","authors":["Idriss Nguepi Nguefack","Mara Finkelstein","Toadoum Sari Sakayo"],"pdf_url":"https://arxiv.org/pdf/2510.25116v1.pdf","comment":"8 pages, 1. figure"},{"id":"http://arxiv.org/abs/2510.25110v1","updated":"2025-10-29T02:21:10Z","published":"2025-10-29T02:21:10Z","title":"DEBATE: A Large-Scale Benchmark for Role-Playing LLM Agents in\n  Multi-Agent, Long-Form Debates","summary":"  Accurately modeling opinion change through social interactions is crucial for\naddressing issues like misinformation and polarization. While role-playing\nlarge language models (LLMs) offer a promising way to simulate human-like\ninteractions, existing research shows that single-agent alignment does not\nguarantee authentic multi-agent group dynamics. Current LLM role-play setups\noften produce unnatural dynamics (e.g., premature convergence), without an\nempirical benchmark to measure authentic human opinion trajectories. To bridge\nthis gap, we introduce DEBATE, the first large-scale empirical benchmark\nexplicitly designed to evaluate the authenticity of the interaction between\nmulti-agent role-playing LLMs. DEBATE contains 29,417 messages from multi-round\ndebate conversations among over 2,792 U.S.-based participants discussing 107\ncontroversial topics, capturing both publicly-expressed messages and\nprivately-reported opinions. Using DEBATE, we systematically evaluate and\nidentify critical discrepancies between simulated and authentic group dynamics.\nWe further demonstrate DEBATE's utility for aligning LLMs with human behavior\nthrough supervised fine-tuning, achieving improvements in surface-level metrics\n(e.g., ROUGE-L and message length) while highlighting limitations in deeper\nsemantic alignment (e.g., semantic similarity). Our findings highlight both the\npotential and current limitations of role-playing LLM agents for realistically\nsimulating human-like social dynamics.\n","authors":["Yun-Shiuan Chuang","Ruixuan Tu","Chengtao Dai","Smit Vasani","Binwei Yao","Michael Henry Tessler","Sijia Yang","Dhavan Shah","Robert Hawkins","Junjie Hu","Timothy T. Rogers"],"pdf_url":"https://arxiv.org/pdf/2510.25110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25101v1","updated":"2025-10-29T02:12:18Z","published":"2025-10-29T02:12:18Z","title":"KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome\n  Supervision for KBQA","summary":"  Knowledge Base Question Answering (KBQA) aims to answer natural-language\nquestions over a structured Knowledge Base (KB). Recent work improves KBQA by\nadopting an agentic reasoning paradigm, in which Large Language Models (LLMs)\niteratively decompose a question, generate its corresponding logical queries,\nand interact with the KB to derive the answer. However, these methods typically\nfine-tune LLMs on reasoning trajectories synthesized via process supervision,\nwhich offers weak incentives for exploration and thus fails to strengthen the\nagentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that\ncan autonomously perform agentic reasoning on KBs to obtain answers. To\nincentivize autonomous exploration, KnowCoder-A1 trains the LLM under\noutcome-only supervision via a multi-stage curriculum reinforcement learning\nwith an easy-to-hard curriculum. To establish foundational agentic\ncapabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of\nhigh-quality trajectories obtained through outcome-based rejection sampling.\nThen, to alleviate the reward sparsity inherent in outcome-only supervision, it\napplies multi-stage curriculum RL with reward schedules that progress from easy\nto hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful\nreasoning behaviors and consistently outperforms prior approaches across three\nmainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1\nachieves up to an 11.1% relative improvement while using only one-twelfth of\nthe training data, demonstrating strong agentic reasoning capabilities.\n","authors":["Zhuo Chen","Fei Wang","Zixuan Li","Zhao Zhang","Weiwei Ding","Chuanguang Yang","Yongjun Xu","Xiaolong Jin","Jiafeng Guo"],"pdf_url":"https://arxiv.org/pdf/2510.25101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17818v2","updated":"2025-10-29T01:54:23Z","published":"2025-05-23T12:34:48Z","title":"PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient\n  Interactions","summary":"  Doctor-patient consultations require multi-turn, context-aware communication\ntailored to diverse patient personas. Training or evaluating doctor LLMs in\nsuch settings requires realistic patient interaction systems. However, existing\nsimulators often fail to reflect the full range of personas seen in clinical\npractice. To address this, we introduce PatientSim, a patient simulator that\ngenerates realistic and diverse patient personas for clinical scenarios,\ngrounded in medical expertise. PatientSim operates using: 1) clinical profiles,\nincluding symptoms and medical history, derived from real-world data in the\nMIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes:\npersonality, language proficiency, medical history recall level, and cognitive\nconfusion level, resulting in 37 unique combinations. We evaluate eight LLMs\nfor factual accuracy and persona consistency. The top-performing open-source\nmodel, Llama 3.3 70B, is validated by four clinicians to confirm the robustness\nof our framework. As an open-source, customizable platform, PatientSim provides\na reproducible and scalable solution that can be customized for specific\ntraining needs. Offering a privacy-compliant environment, it serves as a robust\ntestbed for evaluating medical dialogue systems across diverse patient\npresentations and shows promise as an educational tool for healthcare. The code\nis available at https://github.com/dek924/PatientSim.\n","authors":["Daeun Kyung","Hyunseung Chung","Seongsu Bae","Jiho Kim","Jae Ho Sohn","Taerim Kim","Soo Kyung Kim","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2505.17818v2.pdf","comment":"Accepted as a Spotlight at NeurIPS 2025 Datasets and Benchmarks Track\n  (10 pages for main text, 4 pages for references, 36 pages for supplementary\n  materials)"},{"id":"http://arxiv.org/abs/2510.25087v1","updated":"2025-10-29T01:51:00Z","published":"2025-10-29T01:51:00Z","title":"BioCoref: Benchmarking Biomedical Coreference Resolution with LLMs","summary":"  Coreference resolution in biomedical texts presents unique challenges due to\ncomplex domain-specific terminology, high ambiguity in mention forms, and\nlong-distance dependencies between coreferring expressions. In this work, we\npresent a comprehensive evaluation of generative large language models (LLMs)\nfor coreference resolution in the biomedical domain. Using the CRAFT corpus as\nour benchmark, we assess the LLMs' performance with four prompting experiments\nthat vary in their use of local, contextual enrichment, and domain-specific\ncues such as abbreviations and entity dictionaries. We benchmark these\napproaches against a discriminative span-based encoder, SpanBERT, to compare\nthe efficacy of generative versus discriminative methods. Our results\ndemonstrate that while LLMs exhibit strong surface-level coreference\ncapabilities, especially when supplemented with domain-grounding prompts, their\nperformance remains sensitive to long-range context and mentions ambiguity.\nNotably, the LLaMA 8B and 17B models show superior precision and F1 scores\nunder entity-augmented prompting, highlighting the potential of lightweight\nprompt engineering for enhancing LLM utility in biomedical NLP tasks.\n","authors":["Nourah M Salem","Elizabeth White","Michael Bada","Lawrence Hunter"],"pdf_url":"https://arxiv.org/pdf/2510.25087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.14205v3","updated":"2025-10-29T01:16:53Z","published":"2025-10-16T01:26:38Z","title":"DPRF: A Generalizable Dynamic Persona Refinement Framework for\n  Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents\n  and Humans","summary":"  The emerging large language model role-playing agents (LLM RPAs) aim to\nsimulate individual human behaviors, but the persona fidelity is often\nundermined by manually-created profiles (e.g., cherry-picked information and\npersonality characteristics) without validating the alignment with the target\nindividuals. To address this limitation, our work introduces the Dynamic\nPersona Refinement Framework (DPRF). DPRF aims to optimize the alignment of LLM\nRPAs' behaviors with those of target individuals by iteratively identifying the\ncognitive divergence, either through free-form or theory-grounded, structured\nanalysis, between generated behaviors and human ground truth, and refining the\npersona profile to mitigate these divergences. We evaluate DPRF with five LLMs\non four diverse behavior-prediction scenarios: formal debates, social media\nposts with mental health issues, public interviews, and movie reviews. DPRF can\nconsistently improve behavioral alignment considerably over baseline personas\nand generalizes across models and scenarios. Our work provides a robust\nmethodology for creating high-fidelity persona profiles and enhancing the\nvalidity of downstream applications, such as user simulation, social studies,\nand personalized AI.\n","authors":["Bingsheng Yao","Bo Sun","Yuanzhe Dong","Yuxuan Lu","Dakuo Wang"],"pdf_url":"https://arxiv.org/pdf/2510.14205v3.pdf","comment":"In Submission"},{"id":"http://arxiv.org/abs/2510.25069v1","updated":"2025-10-29T01:14:21Z","published":"2025-10-29T01:14:21Z","title":"TOPol: Capturing and Explaining Multidimensional Semantic Polarity\n  Fields and Vectors","summary":"  Traditional approaches to semantic polarity in computational linguistics\ntreat sentiment as a unidimensional scale, overlooking the multidimensional\nstructure of language. This work introduces TOPol (Topic-Orientation POLarity),\na semi-unsupervised framework for reconstructing and interpreting\nmultidimensional narrative polarity fields under human-on-the-loop (HoTL)\ndefined contextual boundaries (CBs). The framework embeds documents using a\ntransformer-based large language model (tLLM), applies neighbor-tuned UMAP\nprojection, and segments topics via Leiden partitioning. Given a CB between\ndiscourse regimes A and B, TOPol computes directional vectors between\ncorresponding topic-boundary centroids, yielding a polarity field that\nquantifies fine-grained semantic displacement during regime shifts. This\nvectorial representation enables assessing CB quality and detecting polarity\nchanges, guiding HoTL CB refinement. To interpret identified polarity vectors,\nthe tLLM compares their extreme points and produces contrastive labels with\nestimated coverage. Robustness analyses show that only CB definitions (the main\nHoTL-tunable parameter) significantly affect results, confirming methodological\nstability. We evaluate TOPol on two corpora: (i) U.S. Central Bank speeches\naround a macroeconomic breakpoint, capturing non-affective semantic shifts, and\n(ii) Amazon product reviews across rating strata, where affective polarity\naligns with NRC valence. Results demonstrate that TOPol consistently captures\nboth affective and non-affective polarity transitions, providing a scalable,\ngeneralizable, and interpretable framework for context-sensitive\nmultidimensional discourse analysis.\n","authors":["Gabin Taibi","Lucia Gomez"],"pdf_url":"https://arxiv.org/pdf/2510.25069v1.pdf","comment":"7 pages, 3 figures and 2 tables"},{"id":"http://arxiv.org/abs/2510.25064v1","updated":"2025-10-29T01:07:26Z","published":"2025-10-29T01:07:26Z","title":"Can LLMs Estimate Cognitive Complexity of Reading Comprehension Items?","summary":"  Estimating the cognitive complexity of reading comprehension (RC) items is\ncrucial for assessing item difficulty before it is administered to learners.\nUnlike syntactic and semantic features, such as passage length or semantic\nsimilarity between options, cognitive features that arise during answer\nreasoning are not readily extractable using existing NLP tools and have\ntraditionally relied on human annotation. In this study, we examine whether\nlarge language models (LLMs) can estimate the cognitive complexity of RC items\nby focusing on two dimensions-Evidence Scope and Transformation Level-that\nindicate the degree of cognitive burden involved in reasoning about the answer.\nOur experimental results demonstrate that LLMs can approximate the cognitive\ncomplexity of items, indicating their potential as tools for prior difficulty\nanalysis. Further analysis reveals a gap between LLMs' reasoning ability and\ntheir metacognitive awareness: even when they produce correct answers, they\nsometimes fail to correctly identify the features underlying their own\nreasoning process.\n","authors":["Seonjeong Hwang","Hyounghun Kim","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2510.25064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25055v1","updated":"2025-10-29T00:46:45Z","published":"2025-10-29T00:46:45Z","title":"GAPMAP: Mapping Scientific Knowledge Gaps in Biomedical Literature Using\n  Large Language Models","summary":"  Scientific progress is driven by the deliberate articulation of what remains\nunknown. This study investigates the ability of large language models (LLMs) to\nidentify research knowledge gaps in the biomedical literature. We define two\ncategories of knowledge gaps: explicit gaps, clear declarations of missing\nknowledge; and implicit gaps, context-inferred missing knowledge. While prior\nwork has focused mainly on explicit gap detection, we extend this line of\nresearch by addressing the novel task of inferring implicit gaps. We conducted\ntwo experiments on almost 1500 documents across four datasets, including a\nmanually annotated corpus of biomedical articles. We benchmarked both\nclosed-weight models (from OpenAI) and open-weight models (Llama and Gemma 2)\nunder paragraph-level and full-paper settings. To address the reasoning of\nimplicit gaps inference, we introduce \\textbf{\\small TABI}, a Toulmin-Abductive\nBucketed Inference scheme that structures reasoning and buckets inferred\nconclusion candidates for validation. Our results highlight the robust\ncapability of LLMs in identifying both explicit and implicit knowledge gaps.\nThis is true for both open- and closed-weight models, with larger variants\noften performing better. This suggests a strong ability of LLMs for\nsystematically identifying candidate knowledge gaps, which can support\nearly-stage research formulation, policymakers, and funding decisions. We also\nreport observed failure modes and outline directions for robust deployment,\nincluding domain adaptation, human-in-the-loop verification, and benchmarking\nacross open- and closed-weight models.\n","authors":["Nourah M Salem","Elizabeth White","Michael Bada","Lawrence Hunter"],"pdf_url":"https://arxiv.org/pdf/2510.25055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25054v1","updated":"2025-10-29T00:45:36Z","published":"2025-10-29T00:45:36Z","title":"Evaluating Emotion Recognition in Spoken Language Models on Emotionally\n  Incongruent Speech","summary":"  Advancements in spoken language processing have driven the development of\nspoken language models (SLMs), designed to achieve universal audio\nunderstanding by jointly learning text and audio representations for a wide\nrange of tasks. Although promising results have been achieved, there is growing\ndiscussion regarding these models' generalization capabilities and the extent\nto which they truly integrate audio and text modalities in their internal\nrepresentations. In this work, we evaluate four SLMs on the task of speech\nemotion recognition using a dataset of emotionally incongruent speech samples,\na condition under which the semantic content of the spoken utterance conveys\none emotion while speech expressiveness conveys another. Our results indicate\nthat SLMs rely predominantly on textual semantics rather than speech emotion to\nperform the task, indicating that text-related representations largely dominate\nover acoustic representations. We release both the code and the Emotionally\nIncongruent Synthetic Speech dataset (EMIS) to the community.\n","authors":["Pedro CorrÃªa","JoÃ£o Lima","Victor Moreno","Paula Dornhofer Paro Costa"],"pdf_url":"https://arxiv.org/pdf/2510.25054v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2510.13852v2","updated":"2025-10-29T00:31:05Z","published":"2025-10-11T23:32:02Z","title":"ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When\n  Responding to Different Demographic Groups","summary":"  Is an LLM telling you different facts than it's telling me? This paper\nintroduces ConsistencyAI, an independent benchmark for measuring the factual\nconsistency of large language models (LLMs) for different personas.\nConsistencyAI tests whether, when users of different demographics ask identical\nquestions, the model responds with factually inconsistent answers. Designed\nwithout involvement from LLM providers, this benchmark offers impartial\nevaluation and accountability. In our experiment, we queried 19 LLMs with\nprompts that requested 5 facts for each of 15 topics. We repeated this query\n100 times for each LLM, each time adding prompt context from a different\npersona selected from a subset of personas modeling the general population. We\nprocessed the responses into sentence embeddings, computed cross-persona cosine\nsimilarity, and computed the weighted average of cross-persona cosine\nsimilarity to calculate factual consistency scores. In 100-persona experiments,\nscores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as\na benchmark threshold. xAI's Grok-3 is most consistent, while several\nlightweight models rank lowest. Consistency varies by topic: the job market is\nleast consistent, G7 world leaders most consistent, and issues like vaccines or\nthe Israeli-Palestinian conflict diverge by provider. These results show that\nboth the provider and the topic shape the factual consistency. We release our\ncode and interactive demo to support reproducible evaluation and encourage\npersona-invariant prompting strategies.\n","authors":["Peter Banyas","Shristi Sharma","Alistair Simmons","Atharva Vispute"],"pdf_url":"https://arxiv.org/pdf/2510.13852v2.pdf","comment":"For associated code repository, see\n  http://github.com/banyasp/consistencyAI For user-friendly web app, see\n  http://v0-llm-comparison-webapp.vercel.app/"},{"id":"http://arxiv.org/abs/2510.26024v1","updated":"2025-10-29T23:37:54Z","published":"2025-10-29T23:37:54Z","title":"Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural\n  Erasure in Multilingual LLMs","summary":"  Cross-lingual alignment (CLA) aims to align multilingual representations,\nenabling Large Language Models (LLMs) to seamlessly transfer knowledge across\nlanguages. While intuitive, we hypothesize, this pursuit of representational\nconvergence can inadvertently cause \"cultural erasure\", the functional loss of\nproviding culturally-situated responses that should diverge based on the query\nlanguage. In this work, we systematically analyze this trade-off by introducing\na holistic evaluation framework, the transfer-localization plane, which\nquantifies both desirable knowledge transfer and undesirable cultural erasure.\nUsing this framework, we re-evaluate recent CLA approaches and find that they\nconsistently improve factual transfer at the direct cost of cultural\nlocalization across all six languages studied. Our investigation into the\ninternal representations of these models reveals a key insight: universal\nfactual transfer and culturally-specific knowledge are optimally steerable at\ndifferent model layers. Based on this finding, we propose Surgical Steering, a\nnovel inference-time method that disentangles these two objectives. By applying\ntargeted activation steering to distinct layers, our approach achieves a better\nbalance between the two competing dimensions, effectively overcoming the\nlimitations of current alignment techniques.\n","authors":["HyoJung Han","Sweta Agrawal","Eleftheria Briakou"],"pdf_url":"https://arxiv.org/pdf/2510.26024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02820v3","updated":"2025-10-29T23:29:35Z","published":"2025-05-05T17:47:49Z","title":"AutoLibra: Agent Metric Induction from Open-Ended Human Feedback","summary":"  Agents are predominantly evaluated and optimized via task success metrics,\nwhich are coarse, rely on manual design from experts, and fail to reward\nintermediate emergent behaviors. We propose **AutoLibra**, a framework for\nagent evaluation, that transforms open-ended human feedback *e.g.* \"If you find\nthat the button is disabled, don't click it again\", or \"This agent has too much\nautonomy to decide what to do on its own\" into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by\ngrounding feedback to an agent's behavior, clustering similar positive and\nnegative behaviors, and creating concrete metrics with clear definitions and\nconcrete examples, which can be used for prompting LLM-as-a-Judge as\nevaluators. We further propose two meta metrics to evaluate the alignment of a\nset of (induced) metrics with open feedback: \"coverage\" and \"redundancy\".\nThrough optimizing these meta-metrics, we experimentally demonstrate\nAutoLibra's ability to induce more concrete agent evaluation metrics than the\nones proposed in previous agent evaluation benchmarks and discover new metrics\nto analyze agents. We also present two applications of AutoLibra in agent\nimprovement: First, we show that AutoLibra serve human prompt engineers for\ndiagonalize agent failures and improve prompts iterative. Moreover, we find\nthat AutoLibra can induce metrics for automatic optimization for agents, which\nmakes agents improve through self-regulation. Our results suggest that\nAutoLibra is a powerful task-agnostic tool for evaluating and improving\nlanguage agents.\n","authors":["Hao Zhu","Phil Cuvin","Xinkai Yu","Charlotte Ka Yee Yan","Jason Zhang","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2505.02820v3.pdf","comment":"https://github.com/Open-Social-World/autolibra"},{"id":"http://arxiv.org/abs/2510.26020v1","updated":"2025-10-29T23:28:53Z","published":"2025-10-29T23:28:53Z","title":"PORTool: Tool-Use LLM Training with Rewarded Tree","summary":"  Current tool-use large language models (LLMs) are trained on static datasets,\nenabling them to interact with external tools and perform multi-step,\ntool-integrated reasoning, which produces tool-call trajectories. However,\nthese models imitate how a query is resolved in a generic tool-call routine,\nthereby failing to explore possible solutions and demonstrating limited\nperformance in an evolved, dynamic tool-call environment. In this work, we\npropose PORTool, a reinforcement learning (RL) method that encourages a\ntool-use LLM to explore various trajectories yielding the correct answer.\nSpecifically, this method starts with generating multiple rollouts for a given\nquery, and some of them share the first few tool-call steps, thereby forming a\ntree-like structure. Next, we assign rewards to each step, based on its ability\nto produce a correct answer and make successful tool calls. A shared step\nacross different trajectories receives the same reward, while different steps\nunder the same fork receive different rewards. Finally, these step-wise rewards\nare used to calculate fork-relative advantages, blended with\ntrajectory-relative advantages, to train the LLM for tool use. The experiments\nutilize 17 tools to address user queries, covering both time-sensitive and\ntime-invariant topics. We conduct ablation studies to systematically justify\nthe necessity and the design robustness of step-wise rewards. Furthermore, we\ncompare the proposed PORTool with other training approaches and demonstrate\nsignificant improvements in final accuracy and the number of tool-call steps.\n","authors":["Feijie Wu","Weiwu Zhu","Yuxiang Zhang","Soumya Chatterjee","Jiarong Zhu","Fan Mo","Rodin Luo","Jing Gao"],"pdf_url":"https://arxiv.org/pdf/2510.26020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17608v2","updated":"2025-10-29T22:44:11Z","published":"2023-05-28T02:12:00Z","title":"Reward Collapse in Aligning Large Language Models","summary":"  The extraordinary capabilities of large language models (LLMs) such as\nChatGPT and GPT-4 are in part unleashed by aligning them with reward models\nthat are trained on human preferences, which are often represented as rankings\nof responses to prompts. In this paper, we document the phenomenon of\n\\textit{reward collapse}, an empirical observation where the prevailing\nranking-based approach results in an \\textit{identical} reward distribution\n\\textit{regardless} of the prompts during the terminal phase of training. This\noutcome is undesirable as open-ended prompts like ``write a short story about\nyour best friend'' should yield a continuous range of rewards for their\ncompletions, while specific prompts like ``what is the capital of New Zealand''\nshould generate either high or low rewards. Our theoretical investigation\nreveals that reward collapse is primarily due to the insufficiency of the\nranking-based objective function to incorporate prompt-related information\nduring optimization. This insight allows us to derive closed-form expressions\nfor the reward distribution associated with a set of utility functions in an\nasymptotic regime. To overcome reward collapse, we introduce a prompt-aware\noptimization scheme that provably admits a prompt-dependent reward distribution\nwithin the interpolating regime. Our experimental results suggest that our\nproposed prompt-aware utility functions significantly alleviate reward collapse\nduring the training of reward models.\n","authors":["Ziang Song","Tianle Cai","Jason D. Lee","Weijie J. Su"],"pdf_url":"https://arxiv.org/pdf/2305.17608v2.pdf","comment":"Accepted for publication in the Journal of Data Science (JDS),\n  reference JDS1201"},{"id":"http://arxiv.org/abs/2507.13328v2","updated":"2025-10-29T22:38:57Z","published":"2025-07-17T17:47:47Z","title":"Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does\n  Not Fundamentally Alter It","summary":"  Does vision-and-language (VL) training change the linguistic representations\nof language models in meaningful ways? Most results in the literature have\nshown inconsistent or marginal differences, both behaviorally and\nrepresentationally. In this work, we start from the hypothesis that the domain\nin which VL training could have a significant effect is lexical-conceptual\nknowledge, in particular its taxonomic organization. Through comparing minimal\npairs of text-only LMs and their VL-trained counterparts, we first show that\nthe VL models often outperform their text-only counterparts on a text-only\nquestion-answering task that requires taxonomic understanding of concepts\nmentioned in the questions. Using an array of targeted behavioral and\nrepresentational analyses, we show that the LMs and VLMs do not differ\nsignificantly in terms of their taxonomic knowledge itself, but they differ in\nhow they represent questions that contain concepts in a taxonomic relation vs.\na non-taxonomic relation. This implies that the taxonomic knowledge itself does\nnot change substantially through additional VL training, but VL training does\nimprove the deployment of this knowledge in the context of a specific task,\neven when the presentation of the task is purely linguistic.\n","authors":["Yulu Qin","Dheeraj Varghese","Adam Dahlgren LindstrÃ¶m","Lucia Donatelli","Kanishka Misra","Najoung Kim"],"pdf_url":"https://arxiv.org/pdf/2507.13328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26006v1","updated":"2025-10-29T22:34:26Z","published":"2025-10-29T22:34:26Z","title":"CAVE: Detecting and Explaining Commonsense Anomalies in Visual\n  Environments","summary":"  Humans can naturally identify, reason about, and explain anomalies in their\nenvironment. In computer vision, this long-standing challenge remains limited\nto industrial defects or unrealistic, synthetically generated anomalies,\nfailing to capture the richness and unpredictability of real-world anomalies.\nIn this work, we introduce CAVE, the first benchmark of real-world visual\nanomalies. CAVE supports three open-ended tasks: anomaly description,\nexplanation, and justification; with fine-grained annotations for visual\ngrounding and categorizing anomalies based on their visual manifestations,\ntheir complexity, severity, and commonness. These annotations draw inspiration\nfrom cognitive science research on how humans identify and resolve anomalies,\nproviding a comprehensive framework for evaluating Vision-Language Models\n(VLMs) in detecting and understanding anomalies. We show that state-of-the-art\nVLMs struggle with visual anomaly perception and commonsense reasoning, even\nwith advanced prompting strategies. By offering a realistic and cognitively\ngrounded benchmark, CAVE serves as a valuable resource for advancing research\nin anomaly detection and commonsense reasoning in VLMs.\n","authors":["Rishika Bhagwatkar","Syrielle Montariol","Angelika Romanou","Beatriz Borges","Irina Rish","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2510.26006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02878v3","updated":"2025-10-29T22:28:23Z","published":"2025-03-04T18:58:11Z","title":"Language Models can Self-Improve at State-Value Estimation for Better\n  Search","summary":"  Collecting ground-truth rewards or human demonstrations for multi-step\nreasoning tasks is often prohibitively expensive, particularly in interactive\ndomains such as web tasks. We introduce Self-Taught Lookahead (STL), a\nreward-free framework that improves language model-based value functions by\nreasoning explicitly about state transitions. STL can be viewed as a\nchain-of-thought analogue of the value iteration algorithm: instead of\nregressing directly on numeric values, a value LLM is trained to simulate a\nstep of lookahead in natural language - predicting the next action, resulting\nstate, and rationale for its value, thereby refining value estimates without\nany labeled data. This self-supervised procedure yields more accurate\nstate-value predictions, which in turn enable lightweight search algorithms to\nexpand fewer states while maintaining strong performance. Empirically,\nSTL-trained value models built on moderately sized (8B parameter) open-weight\nLLMs boost web agent success rates by 39%, achieving comparable performance\nwith proprietary models. STL also generalizes to multi-hop QA and math puzzles.\nWe find that STL enables small open-source models to guide efficient search,\nreducing inference costs by integrating explicit reasoning with value learning.\n","authors":["Ethan Mendes","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2503.02878v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18125v2","updated":"2025-10-29T22:07:41Z","published":"2025-05-23T17:34:28Z","title":"TabSTAR: A Tabular Foundation Model for Tabular Data with Text Fields","summary":"  While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees. However, recent advancements are\npaving the way for Tabular Foundation Models, which can leverage real-world\nknowledge and generalize across diverse datasets, particularly when the data\ncontains free-text. Although incorporating language model capabilities into\ntabular tasks has been explored, most existing methods utilize static,\ntarget-agnostic textual representations, limiting their effectiveness. We\nintroduce TabSTAR: a Tabular Foundation Model with Semantically Target-Aware\nRepresentations. TabSTAR is designed to enable transfer learning on tabular\ndata with textual features, with an architecture free of dataset-specific\nparameters. It unfreezes a pretrained text encoder and takes as input target\ntokens, which provide the model with the context needed to learn task-specific\nembeddings. TabSTAR achieves state-of-the-art performance for both medium- and\nlarge-sized datasets across known benchmarks of classification tasks with text\nfeatures, and its pretraining phase exhibits scaling laws in the number of\ndatasets, offering a pathway for further performance improvements.\n","authors":["Alan Arazi","Eilam Shapira","Roi Reichart"],"pdf_url":"https://arxiv.org/pdf/2505.18125v2.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25992v1","updated":"2025-10-29T22:05:08Z","published":"2025-10-29T22:05:08Z","title":"Supervised Reinforcement Learning: From Expert Trajectories to Step-wise\n  Reasoning","summary":"  Large Language Models (LLMs) often struggle with problems that require\nmulti-step reasoning. For small-scale open-source models, Reinforcement\nLearning with Verifiable Rewards (RLVR) fails when correct solutions are rarely\nsampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to\noverfit long demonstrations through rigid token-by-token imitation. To address\nthis gap, we propose Supervised Reinforcement Learning (SRL), a framework that\nreformulates problem solving as generating a sequence of logical \"actions\". SRL\ntrains the model to generate an internal reasoning monologue before committing\nto each action. It provides smoother rewards based on the similarity between\nthe model's actions and expert actions extracted from the SFT dataset in a\nstep-wise manner. This supervision offers richer learning signals even when all\nrollouts are incorrect, while encouraging flexible reasoning guided by expert\ndemonstrations. As a result, SRL enables small models to learn challenging\nproblems previously unlearnable by SFT or RLVR. Moreover, initializing training\nwith SRL before refining with RLVR yields the strongest overall performance.\nBeyond reasoning benchmarks, SRL generalizes effectively to agentic software\nengineering tasks, establishing it as a robust and versatile training framework\nfor reasoning-oriented LLMs.\n","authors":["Yihe Deng","I-Hung Hsu","Jun Yan","Zifeng Wang","Rujun Han","Gufeng Zhang","Yanfei Chen","Wei Wang","Tomas Pfister","Chen-Yu Lee"],"pdf_url":"https://arxiv.org/pdf/2510.25992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25979v1","updated":"2025-10-29T21:26:17Z","published":"2025-10-29T21:26:17Z","title":"AttnCache: Accelerating Self-Attention Inference for LLM Prefill via\n  Attention Cache","summary":"  Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation.\n","authors":["Dinghong Song","Yuan Feng","Yiwei Wang","Shangye Chen","Cyril Guyot","Filip Blagojevic","Hyeran Jeon","Pengfei Su","Dong Li"],"pdf_url":"https://arxiv.org/pdf/2510.25979v1.pdf","comment":"10 pages, 6 figures, submitted to Ninth Annual Conference on Machine\n  Learning and Systems (MLSys'26)"},{"id":"http://arxiv.org/abs/2510.25977v1","updated":"2025-10-29T21:22:08Z","published":"2025-10-29T21:22:08Z","title":"NeuronMM: High-Performance Matrix Multiplication for LLM Inference on\n  AWS Trainium","summary":"  AI accelerators, customized to AI workloads, provide cost-effective and\nhigh-performance solutions for training and inference. Trainium, an AI\naccelerator recently developed by Amazon Web Services (AWS), provides an\nattractive option for LLM training and inference through its heterogeneous\narchitecture. However, leveraging Trainium architecture for high performance\ncan be challenging because of its systolic array architecture and special\nrequirement on data layout. In this paper, we design high-performance matrix\nmultiplication (matmul), a critical compute kernel, for LLM inference on\nTrainium. We introduce a series of techniques customized to Trainium based on\nkernel fusion and novel caching strategies to reduce data movement across the\nsoftware-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive\nmatrix transpose. Evaluating with nine datasets and four recent LLMs, we show\nthat our system largely outperforms the state-of-the-art matmul implemented by\nAWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x\nspeedup (up to 2.22x), which translates to an average 1.66x speedup (up to\n2.49x) for end-to-end LLM inference.\n","authors":["Dinghong Song","Jierui Xu","Weichu Yang","Pengfei Su","Dong Li"],"pdf_url":"https://arxiv.org/pdf/2510.25977v1.pdf","comment":"12 pages, 8 figures, submitted to the Proceedings of the Twenty-First\n  European Conference on Computer Systems (EuroSys'26)"},{"id":"http://arxiv.org/abs/2510.25975v1","updated":"2025-10-29T21:17:57Z","published":"2025-10-29T21:17:57Z","title":"SymCode: A Neurosymbolic Approach to Mathematical Reasoning via\n  Verifiable Code Generation","summary":"  Large Language Models (LLMs) often struggle with complex mathematical\nreasoning, where prose-based generation leads to unverified and arithmetically\nunsound solutions. Current prompting strategies like Chain of Thought still\noperate within this unreliable medium, lacking a mechanism for deterministic\nverification. To address these limitations, we introduce SymCode, a\nneurosymbolic framework that reframes mathematical problem-solving as a task of\nverifiable code generation using the SymPy library. We evaluate SymCode on\nchallenging benchmarks, including MATH-500 and OlympiadBench, demonstrating\nsignificant accuracy improvements of up to 13.6 percentage points over\nbaselines. Our analysis shows that SymCode is not only more token-efficient but\nalso fundamentally shifts model failures from opaque logical fallacies towards\ntransparent, programmatic errors. By grounding LLM reasoning in a deterministic\nsymbolic engine, SymCode represents a key step towards more accurate and\ntrustworthy AI in formal domains.\n","authors":["Sina Bagheri Nezhad","Yao Li","Ameeta Agrawal"],"pdf_url":"https://arxiv.org/pdf/2510.25975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25967v1","updated":"2025-10-29T21:11:23Z","published":"2025-10-29T21:11:23Z","title":"Semantic Label Drift in Cross-Cultural Translation","summary":"  Machine Translation (MT) is widely employed to address resource scarcity in\nlow-resource languages by generating synthetic data from high-resource\ncounterparts. While sentiment preservation in translation has long been\nstudied, a critical but underexplored factor is the role of cultural alignment\nbetween source and target languages. In this paper, we hypothesize that\nsemantic labels are drifted or altered during MT due to cultural divergence.\nThrough a series of experiments across culturally sensitive and neutral\ndomains, we establish three key findings: (1) MT systems, including modern\nLarge Language Models (LLMs), induce label drift during translation,\nparticularly in culturally sensitive domains; (2) unlike earlier statistical MT\ntools, LLMs encode cultural knowledge, and leveraging this knowledge can\namplify label drift; and (3) cultural similarity or dissimilarity between\nsource and target languages is a crucial determinant of label preservation. Our\nfindings highlight that neglecting cultural factors in MT not only undermines\nlabel fidelity but also risks misinterpretation and cultural conflict in\ndownstream applications.\n","authors":["Mohsinul Kabir","Tasnim Ahmed","Md Mezbaur Rahman","Polydoros Giannouris","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2510.25967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16325v3","updated":"2025-10-29T21:04:32Z","published":"2024-10-18T14:03:46Z","title":"This Candidate is [MASK]. Prompt-based Sentiment Extraction and\n  Reference Letters","summary":"  I propose a relatively simple way to deploy pre-trained large language models\n(LLMs) in order to extract sentiment and other useful features from text data.\nThe method, which I refer to as prompt-based sentiment extraction, offers\nmultiple advantages over other methods used in economics and finance. In\nparticular, it accepts the text input as is (without pre-processing) and\nproduces a sentiment score that has a probability interpretation. Unlike other\nLLM-based approaches, it does not require any fine-tuning or labeled data. I\napply my prompt-based strategy to a hand-collected corpus of confidential\nreference letters (RLs). I show that the sentiment contents of RLs are clearly\nreflected in job market outcomes. Candidates with higher average sentiment in\ntheir RLs perform markedly better regardless of the measure of success chosen.\nMoreover, I show that sentiment dispersion among letter writers negatively\naffects the job market candidate's performance. I compare my sentiment\nextraction approach to other commonly used methods for sentiment analysis:\n`bag-of-words' approaches, fine-tuned language models, and querying advanced\nchatbots. No other method can fully reproduce the results obtained by\nprompt-based sentiment extraction. Finally, I slightly modify the method to\nobtain `gendered' sentiment scores (as in Eberhardt et al., 2023). I show that\nRLs written for female candidates emphasize `grindstone' personality traits,\nwhereas male candidates' letters emphasize `standout' traits. These gender\ndifferences negatively affect women's job market outcomes.\n","authors":["Fabian Slonimczyk"],"pdf_url":"https://arxiv.org/pdf/2410.16325v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25947v1","updated":"2025-10-29T20:46:03Z","published":"2025-10-29T20:46:03Z","title":"Revisiting Multilingual Data Mixtures in Language Model Pretraining","summary":"  The impact of different multilingual data mixtures in pretraining large\nlanguage models (LLMs) has been a topic of ongoing debate, often raising\nconcerns about potential trade-offs between language coverage and model\nperformance (i.e., the curse of multilinguality). In this work, we investigate\nthese assumptions by training 1.1B and 3B parameter LLMs on diverse\nmultilingual corpora, varying the number of languages from 25 to 400. Our study\nchallenges common beliefs surrounding multilingual training. First, we find\nthat combining English and multilingual data does not necessarily degrade the\nin-language performance of either group, provided that languages have a\nsufficient number of tokens included in the pretraining corpus. Second, we\nobserve that using English as a pivot language (i.e., a high-resource language\nthat serves as a catalyst for multilingual generalization) yields benefits\nacross language families, and contrary to expectations, selecting a pivot\nlanguage from within a specific family does not consistently improve\nperformance for languages within that family. Lastly, we do not observe a\nsignificant \"curse of multilinguality\" as the number of training languages\nincreases in models at this scale. Our findings suggest that multilingual data,\nwhen balanced appropriately, can enhance language model capabilities without\ncompromising performance, even in low-resource settings\n","authors":["Negar Foroutan","Paul Teiletche","Ayush Kumar Tarun","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2510.25947v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2510.25941v1","updated":"2025-10-29T20:36:37Z","published":"2025-10-29T20:36:37Z","title":"RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic\n  Pipeline","summary":"  If we cannot inspect the training data of a large language model (LLM), how\ncan we ever know what it has seen? We believe the most compelling evidence\narises when the model itself freely reproduces the target content. As such, we\npropose RECAP, an agentic pipeline designed to elicit and verify memorized\ntraining data from LLM outputs. At the heart of RECAP is a feedback-driven\nloop, where an initial extraction attempt is evaluated by a secondary language\nmodel, which compares the output against a reference passage and identifies\ndiscrepancies. These are then translated into minimal correction hints, which\nare fed back into the target model to guide subsequent generations. In\naddition, to address alignment-induced refusals, RECAP includes a jailbreaking\nmodule that detects and overcomes such barriers. We evaluate RECAP on\nEchoTrace, a new benchmark spanning over 30 full books, and the results show\nthat RECAP leads to substantial gains over single-iteration approaches. For\ninstance, with GPT-4.1, the average ROUGE-L score for the copyrighted text\nextraction improved from 0.38 to 0.47 - a nearly 24% increase.\n","authors":["AndrÃ© V. Duarte","Xuying li","Bin Zeng","Arlindo L. Oliveira","Lei Li","Zhuo Li"],"pdf_url":"https://arxiv.org/pdf/2510.25941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25932v1","updated":"2025-10-29T20:11:48Z","published":"2025-10-29T20:11:48Z","title":"FakeZero: Real-Time, Privacy-Preserving Misinformation Detection for\n  Facebook and X","summary":"  Social platforms distribute information at unprecedented speed, which in turn\naccelerates the spread of misinformation and threatens public discourse. We\npresent FakeZero, a fully client-side, cross-platform browser extension that\nflags unreliable posts on Facebook and X (formerly Twitter) while the user\nscrolls. All computation, DOM scraping, tokenisation, Transformer inference,\nand UI rendering run locally through the Chromium messaging API, so no personal\ndata leaves the device.FakeZero employs a three-stage training curriculum:\nbaseline fine-tuning and domain-adaptive training enhanced with focal loss,\nadversarial augmentation, and post-training quantisation. Evaluated on a\ndataset of 239,000 posts, the DistilBERT-Quant model (67.6 MB) reaches 97.1%\nmacro-F1, 97.4% accuracy, and an AUROC of 0.996, with a median latency of\napproximately 103 ms on a commodity laptop. A memory-efficient TinyBERT-Quant\nvariant retains 95.7% macro-F1 and 96.1% accuracy while shrinking the model to\n14.7 MB and lowering latency to approximately 40 ms, showing that high-quality\nfake-news detection is feasible under tight resource budgets with only modest\nperformance loss.By providing inline credibility cues, the extension can serve\nas a valuable tool for policymakers seeking to curb the spread of\nmisinformation across social networks. With user consent, FakeZero also opens\nthe door for researchers to collect large-scale datasets of fake news in the\nwild, enabling deeper analysis and the development of more robust detection\ntechniques.\n","authors":["Soufiane Essahli","Oussama Sarsar","Imane Fouad","Anas Motii","Ahmed Bentajer"],"pdf_url":"https://arxiv.org/pdf/2510.25932v1.pdf","comment":"Accepted for publication in the Proceedings of the 24th IEEE\n  International Conference on Trust, Security and Privacy in Computing and\n  Communications (TrustCom 2025) Privacy track, 11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2504.04953v2","updated":"2025-10-29T20:00:58Z","published":"2025-04-07T11:37:26Z","title":"M-Prometheus: A Suite of Open Multilingual LLM Judges","summary":"  The use of language models for automatically evaluating long-form text\n(LLM-as-a-judge) is becoming increasingly common, yet most LLM judges are\noptimized exclusively for English, with strategies for enhancing their\nmultilingual evaluation capabilities remaining largely unexplored in the\ncurrent literature. This has created a disparity in the quality of automatic\nevaluation methods for non-English languages, ultimately hindering the\ndevelopment of models with better multilingual capabilities. To bridge this\ngap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from\n3B to 14B parameters that can provide both direct assessment and pairwise\ncomparison feedback on multilingual outputs. M-Prometheus models outperform\nstate-of-the-art open LLM judges on multilingual reward benchmarks spanning\nmore than 20 languages, as well as on literary machine translation (MT)\nevaluation covering 4 language pairs. Furthermore, M-Prometheus models can be\nleveraged at decoding time to significantly improve generated outputs across\nall 3 tested languages, showcasing their utility for the development of better\nmultilingual models. Lastly, through extensive ablations, we identify the key\nfactors for obtaining an effective multilingual judge, including backbone model\nselection and training on synthetic multilingual feedback data instead of\ntranslated data. We release our models, training dataset, and code.\n","authors":["JosÃ© Pombal","Dongkeun Yoon","Patrick Fernandes","Ian Wu","Seungone Kim","Ricardo Rei","Graham Neubig","AndrÃ© F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2504.04953v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.01001v2","updated":"2025-10-29T19:55:23Z","published":"2025-04-01T17:40:08Z","title":"Zero-shot Benchmarking: A Framework for Flexible and Scalable Automatic\n  Evaluation of Language Models","summary":"  As language models improve and become capable of performing more complex\ntasks across modalities, evaluating them automatically becomes increasingly\nchallenging. Developing strong and robust task-specific automatic metrics gets\nharder, and human-annotated test sets -- which are expensive to create --\nsaturate more quickly. A compelling alternative is to design reliable\nstrategies to automate the creation of test data and evaluation, but previous\nattempts either rely on pre-existing data, or focus solely on individual tasks.\nWe present Zero-shot Benchmarking (ZSB), a framework for creating high-quality\nbenchmarks for any task by leveraging language models for both synthetic test\ndata creation and evaluation. ZSB is simple and flexible: it requires only the\ncreation of a prompt for data generation and one for evaluation; it is scalable\nto tasks and languages where collecting real-world data is costly or\nimpractical; it is model-agnostic, allowing the creation of increasingly\nchallenging benchmarks as models improve. To assess the effectiveness of our\nframework, we create benchmarks for five text-only tasks and a multi-modal one:\ngeneral capabilities in four languages (English, Chinese, French, and Korean),\ntranslation, and general vision-language capabilities in English. We then rank\na broad range of open and closed systems on our benchmarks. ZSB rankings\nconsistently correlate strongly with human rankings, outperforming\nwidely-adopted standard benchmarks. Through ablations, we find that strong\nbenchmarks can be created with open models, and that judge model size and\ndataset variety are crucial drivers of performance. We release all our\nbenchmarks, and code to reproduce our experiments and to produce new\nbenchmarks.\n","authors":["JosÃ© Pombal","Nuno M. Guerreiro","Ricardo Rei","AndrÃ© F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2504.01001v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13464v2","updated":"2025-10-29T19:33:20Z","published":"2025-06-16T13:24:50Z","title":"Unveiling the Learning Mind of Language Models: A Cognitive Framework\n  and Empirical Study","summary":"  Large language models (LLMs) have shown impressive capabilities across tasks\nsuch as mathematics, coding, and reasoning, yet their learning ability, which\nis crucial for adapting to dynamic environments and acquiring new knowledge,\nremains underexplored. In this work, we address this gap by introducing a\nframework inspired by cognitive psychology and education. Specifically, we\ndecompose general learning ability into three distinct, complementary\ndimensions: Learning from Instructor (acquiring knowledge via explicit\nguidance), Learning from Concept (internalizing abstract structures and\ngeneralizing to new contexts), and Learning from Experience (adapting through\naccumulated exploration and feedback). We conduct a comprehensive empirical\nstudy across the three learning dimensions and identify several insightful\nfindings, such as (i) interaction improves learning; (ii) conceptual\nunderstanding is scale-emergent and benefits larger models; and (iii) LLMs are\neffective few-shot learners but not many-shot learners. Based on our framework\nand empirical findings, we introduce a benchmark that provides a unified and\nrealistic evaluation of LLMs' general learning abilities across three learning\ncognition dimensions. It enables diagnostic insights and supports evaluation\nand development of more adaptive and human-like models.\n","authors":["Zhengyu Hu","Jianxun Lian","Zheyuan Xiao","Seraphina Zhang","Tianfu Wang","Nicholas Jing Yuan","Xing Xie","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2506.13464v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00176v2","updated":"2025-10-29T19:28:15Z","published":"2024-03-29T22:11:54Z","title":"The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks","summary":"  Lexical Semantic Change Detection (LSCD) is a complex, lemma-level task,\nwhich is usually operationalized based on two subsequently applied usage-level\ntasks: First, Word-in-Context (WiC) labels are derived for pairs of usages.\nThen, these labels are represented in a graph on which Word Sense Induction\n(WSI) is applied to derive sense clusters. Finally, LSCD labels are derived by\ncomparing sense clusters over time. This modularity is reflected in most LSCD\ndatasets and models. It also leads to a large heterogeneity in modeling options\nand task definitions, which is exacerbated by a variety of dataset versions,\npreprocessing options and evaluation metrics. This heterogeneity makes it\ndifficult to evaluate models under comparable conditions, to choose optimal\nmodel combinations or to reproduce results. Hence, we provide a benchmark\nrepository standardizing LSCD evaluation. Through transparent implementation\nresults become easily reproducible and by standardization different components\ncan be freely combined. The repository reflects the task's modularity by\nallowing model evaluation for WiC, WSI and LSCD. This allows for careful\nevaluation of increasingly complex model components providing new ways of model\noptimization. We use the implemented benchmark to conduct a number of\nexperiments with recent models and systematically improve the state-of-the-art.\n","authors":["Dominik Schlechtweg","Sachin Yadav","Nikolay Arefyev"],"pdf_url":"https://arxiv.org/pdf/2404.00176v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08188v2","updated":"2025-10-29T19:24:51Z","published":"2025-06-09T19:56:42Z","title":"GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors","summary":"  In this paper, we introduce GradEscape, the first gradient-based evader\ndesigned to attack AI-generated text (AIGT) detectors. GradEscape overcomes the\nundifferentiable computation problem, caused by the discrete nature of text, by\nintroducing a novel approach to construct weighted embeddings for the detector\ninput. It then updates the evader model parameters using feedback from victim\ndetectors, achieving high attack success with minimal text modification. To\naddress the issue of tokenizer mismatch between the evader and the detector, we\nintroduce a warm-started evader method, enabling GradEscape to adapt to\ndetectors across any language model architecture. Moreover, we employ novel\ntokenizer inference and model extraction techniques, facilitating effective\nevasion even in query-only access.\n  We evaluate GradEscape on four datasets and three widely-used language\nmodels, benchmarking it against four state-of-the-art AIGT evaders.\nExperimental results demonstrate that GradEscape outperforms existing evaders\nin various scenarios, including with an 11B paraphrase model, while utilizing\nonly 139M parameters. We have successfully applied GradEscape to two real-world\ncommercial AIGT detectors. Our analysis reveals that the primary vulnerability\nstems from disparity in text expression styles within the training data. We\nalso propose a potential defense strategy to mitigate the threat of AIGT\nevaders. We open-source our GradEscape for developing more robust AIGT\ndetectors.\n","authors":["Wenlong Meng","Shuguo Fan","Chengkun Wei","Min Chen","Yuwei Li","Yuanchao Zhang","Zhikun Zhang","Wenzhi Chen"],"pdf_url":"https://arxiv.org/pdf/2506.08188v2.pdf","comment":"Accepted by USENIX Security'25; Update badges and Artifact Appendix"},{"id":"http://arxiv.org/abs/2510.24134v2","updated":"2025-10-29T19:17:39Z","published":"2025-10-28T07:19:01Z","title":"VC4VG: Optimizing Video Captions for Text-to-Video Generation","summary":"  Recent advances in text-to-video (T2V) generation highlight the critical role\nof high-quality video-text pairs in training models capable of producing\ncoherent and instruction-aligned videos. However, strategies for optimizing\nvideo captions specifically for T2V training remain underexplored. In this\npaper, we introduce VC4VG (Video Captioning for Video Generation), a\ncomprehensive caption optimization framework tailored to the needs of T2V\nmodels. We begin by analyzing caption content from a T2V perspective,\ndecomposing the essential elements required for video reconstruction into\nmultiple dimensions, and proposing a principled caption design methodology. To\nsupport evaluation, we construct VC4VG-Bench, a new benchmark featuring\nfine-grained, multi-dimensional, and necessity-graded metrics aligned with\nT2V-specific requirements. Extensive T2V fine-tuning experiments demonstrate a\nstrong correlation between improved caption quality and video generation\nperformance, validating the effectiveness of our approach. We release all\nbenchmark tools and code at https://github.com/alimama-creative/VC4VG to\nsupport further research.\n","authors":["Yang Du","Zhuoran Lin","Kaiqiang Song","Biao Wang","Zhicheng Zheng","Tiezheng Ge","Bo Zheng","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2510.24134v2.pdf","comment":"Accepted by EMNLP 2025"},{"id":"http://arxiv.org/abs/2506.07001v2","updated":"2025-10-29T19:16:47Z","published":"2025-06-08T05:15:01Z","title":"Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated\n  Text","summary":"  The increasing capabilities of Large Language Models (LLMs) have raised\nconcerns about their misuse in AI-generated plagiarism and social engineering.\nWhile various AI-generated text detectors have been proposed to mitigate these\nrisks, many remain vulnerable to simple evasion techniques such as\nparaphrasing. However, recent detectors have shown greater robustness against\nsuch basic attacks. In this work, we introduce Adversarial Paraphrasing, a\ntraining-free attack framework that universally humanizes any AI-generated text\nto evade detection more effectively. Our approach leverages an off-the-shelf\ninstruction-following LLM to paraphrase AI-generated content under the guidance\nof an AI text detector, producing adversarial examples that are specifically\noptimized to bypass detection. Extensive experiments show that our attack is\nboth broadly effective and highly transferable across several detection\nsystems. For instance, compared to simple paraphrasing attack--which,\nironically, increases the true positive at 1% false positive (T@1%F) by 8.57%\non RADAR and 15.03% on Fast-DetectGPT--adversarial paraphrasing, guided by\nOpenAI-RoBERTa-Large, reduces T@1%F by 64.49% on RADAR and a striking 98.96% on\nFast-DetectGPT. Across a diverse set of detectors--including neural\nnetwork-based, watermark-based, and zero-shot approaches--our attack achieves\nan average T@1%F reduction of 87.88% under the guidance of\nOpenAI-RoBERTa-Large. We also analyze the tradeoff between text quality and\nattack success to find that our method can significantly reduce detection\nrates, with mostly a slight degradation in text quality. Our adversarial setup\nhighlights the need for more robust and resilient detection strategies in the\nlight of increasingly sophisticated evasion techniques.\n","authors":["Yize Cheng","Vinu Sankar Sadasivan","Mehrdad Saberi","Shoumik Saha","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2506.07001v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25904v1","updated":"2025-10-29T19:13:48Z","published":"2025-10-29T19:13:48Z","title":"Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized\n  Setting: the Case of FrameNet Annotation","summary":"  The use of LLM-based applications as a means to accelerate and/or substitute\nhuman labor in the creation of language resources and dataset is a reality.\nNonetheless, despite the potential of such tools for linguistic research,\ncomprehensive evaluation of their performance and impact on the creation of\nannotated datasets, especially under a perspectivized approach to NLP, is still\nmissing. This paper contributes to reduction of this gap by reporting on an\nextensive evaluation of the (semi-)automatization of FrameNet-like semantic\nannotation by the use of an LLM-based semantic role labeler. The methodology\nemployed compares annotation time, coverage and diversity in three experimental\nsettings: manual, automatic and semi-automatic annotation. Results show that\nthe hybrid, semi-automatic annotation setting leads to increased frame\ndiversity and similar annotation coverage, when compared to the human-only\nsetting, while the automatic setting performs considerably worse in all\nmetrics, except for annotation time.\n","authors":["Frederico Belcavello","Ely Matos","Arthur Lorenzi","Lisandra Bonoto","LÃ­via Ruiz","Luiz Fernando Pereira","Victor Herbst","Yulla Navarro","Helen de Andrade Abreu","LÃ­via Dutra","Tiago Timponi Torrent"],"pdf_url":"https://arxiv.org/pdf/2510.25904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23925v2","updated":"2025-10-29T18:48:20Z","published":"2025-10-27T23:10:06Z","title":"Latent Chain-of-Thought for Visual Reasoning","summary":"  Chain-of-thought (CoT) reasoning is critical for improving the\ninterpretability and reliability of Large Vision-Language Models (LVLMs).\nHowever, existing training algorithms such as SFT, PPO, and GRPO may not\ngeneralize well across unseen reasoning tasks and heavily rely on a biased\nreward model. To address this challenge, we reformulate reasoning in LVLMs as\nposterior inference and propose a scalable training algorithm based on\namortized variational inference. By leveraging diversity-seeking reinforcement\nlearning algorithms, we introduce a novel sparse reward function for\ntoken-level learning signals that encourage diverse, high-likelihood latent\nCoT, overcoming deterministic sampling limitations and avoiding reward hacking.\nAdditionally, we implement a Bayesian inference-scaling strategy that replaces\ncostly Best-of-N and Beam Search with a marginal likelihood to efficiently rank\noptimal rationales and answers. We empirically demonstrate that the proposed\nmethod enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in\nterms of effectiveness, generalization, and interpretability.\n","authors":["Guohao Sun","Hang Hua","Jian Wang","Jiebo Luo","Sohail Dianat","Majid Rabbani","Raghuveer Rao","Zhiqiang Tao"],"pdf_url":"https://arxiv.org/pdf/2510.23925v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.02175v2","updated":"2025-10-29T18:37:02Z","published":"2025-06-02T19:01:53Z","title":"AI Debate Aids Assessment of Controversial Claims","summary":"  As AI grows more powerful, it will increasingly shape how we understand the\nworld. But with this influence comes the risk of amplifying misinformation and\ndeepening social divides-especially on consequential topics where factual\naccuracy directly impacts well-being. Scalable Oversight aims to ensure AI\nsystems remain truthful even when their capabilities exceed those of their\nevaluators. Yet when humans serve as evaluators, their own beliefs and biases\ncan impair judgment. We study whether AI debate can guide biased judges toward\nthe truth by having two AI systems debate opposing sides of controversial\nfactuality claims on COVID-19 and climate change where people hold strong prior\nbeliefs. We conduct two studies. Study I recruits human judges with either\nmainstream or skeptical beliefs who evaluate claims through two protocols:\ndebate (interaction with two AI advisors arguing opposing sides) or consultancy\n(interaction with a single AI advisor). Study II uses AI judges with and\nwithout human-like personas to evaluate the same protocols. In Study I, debate\nconsistently improves human judgment accuracy and confidence calibration,\noutperforming consultancy by 4-10% across COVID-19 and climate change claims.\nThe improvement is most significant for judges with mainstream beliefs (up to\n+15.2% accuracy on COVID-19 claims), though debate also helps skeptical judges\nwho initially misjudge claims move toward accurate views (+4.7% accuracy). In\nStudy II, AI judges with human-like personas achieve even higher accuracy\n(78.5%) than human judges (70.1%) and default AI judges without personas\n(69.8%), suggesting their potential for supervising frontier AI models. These\nfindings highlight AI debate as a promising path toward scalable,\nbias-resilient oversight in contested domains.\n","authors":["Salman Rahman","Sheriff Issaka","Ashima Suvarna","Genglin Liu","James Shiffer","Jaeyoung Lee","Md Rizwan Parvez","Hamid Palangi","Shi Feng","Nanyun Peng","Yejin Choi","Julian Michael","Liwei Jiang","Saadia Gabriel"],"pdf_url":"https://arxiv.org/pdf/2506.02175v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25884v1","updated":"2025-10-29T18:32:53Z","published":"2025-10-29T18:32:53Z","title":"Approximating Human Preferences Using a Multi-Judge Learned System","summary":"  Aligning LLM-based judges with human preferences is a significant challenge,\nas they are difficult to calibrate and often suffer from rubric sensitivity,\nbias, and instability. Overcoming this challenge advances key applications,\nsuch as creating reliable reward models for Reinforcement Learning from Human\nFeedback (RLHF) and building effective routing systems that select the\nbest-suited model for a given user query. In this work, we propose a framework\nfor modeling diverse, persona-based preferences by learning to aggregate\noutputs from multiple rubric-conditioned judges. We investigate the performance\nof this approach against naive baselines and assess its robustness through case\nstudies on both human and LLM-judges biases. Our primary contributions include\na persona-based method for synthesizing preference labels at scale and two\ndistinct implementations of our aggregator: Generalized Additive Model (GAM)\nand a Multi-Layer Perceptron (MLP).\n","authors":["EitÃ¡n Sprejer","Fernando Avalos","Augusto Bernardi","Jose Pedro Brito de Azevedo Faustino","Jacob Haimes","Narmeen Fatimah Oozeer"],"pdf_url":"https://arxiv.org/pdf/2510.25884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.21204v2","updated":"2025-10-29T18:23:49Z","published":"2025-08-28T20:46:13Z","title":"Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive\n  Scaffolding","summary":"  We study how prompt-level inductive biases influence the cognitive behavior\nof large language models (LLMs) in instructional dialogue. We introduce a\nsymbolic scaffolding method paired with a short-term memory schema designed to\npromote adaptive, structured reasoning in Socratic tutoring. Using controlled\nablation across five system variants, we evaluate model outputs via\nexpert-designed rubrics covering scaffolding, responsiveness, symbolic\nreasoning, and conversational memory. We present preliminary results using an\nLLM-based evaluation framework aligned to a cognitively grounded rubric. This\nenables scalable, systematic comparisons across architectural variants in\nearly-stage experimentation. The preliminary results show that our full system\nconsistently outperforms baseline variants. Analysis reveals that removing\nmemory or symbolic structure degrades key cognitive behaviors, including\nabstraction, adaptive probing, and conceptual continuity. These findings\nsupport a processing-level account in which prompt-level cognitive scaffolds\ncan reliably shape emergent instructional strategies in LLMs.\n","authors":["Vanessa Figueiredo"],"pdf_url":"https://arxiv.org/pdf/2508.21204v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25860v1","updated":"2025-10-29T18:03:44Z","published":"2025-10-29T18:03:44Z","title":"Through the Judge's Eyes: Inferred Thinking Traces Improve Reliability\n  of LLM Raters","summary":"  Large language models (LLMs) are increasingly used as raters for evaluation\ntasks. However, their reliability is often limited for subjective tasks, when\nhuman judgments involve subtle reasoning beyond annotation labels. Thinking\ntraces, the reasoning behind a judgment, are highly informative but challenging\nto collect and curate. We present a human-LLM collaborative framework to infer\nthinking traces from label-only annotations. The proposed framework uses a\nsimple and effective rejection sampling method to reconstruct these traces at\nscale. These inferred thinking traces are applied to two complementary tasks:\n(1) fine-tuning open LLM raters; and (2) synthesizing clearer annotation\nguidelines for proprietary LLM raters. Across multiple datasets, our methods\nlead to significantly improved LLM-human agreement. Additionally, the refined\nannotation guidelines increase agreement among different LLM models. These\nresults suggest that LLMs can serve as practical proxies for otherwise\nunrevealed human thinking traces, enabling label-only corpora to be extended\ninto thinking-trace-augmented resources that enhance the reliability of LLM\nraters.\n","authors":["Xingjian Zhang","Tianhong Gao","Suliang Jin","Tianhao Wang","Teng Ye","Eytan Adar","Qiaozhu Mei"],"pdf_url":"https://arxiv.org/pdf/2510.25860v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2510.25772v1","updated":"2025-10-29T17:59:53Z","published":"2025-10-29T17:59:53Z","title":"VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context\n  Learning","summary":"  Visual effects (VFX) are crucial to the expressive power of digital media,\nyet their creation remains a major challenge for generative AI. Prevailing\nmethods often rely on the one-LoRA-per-effect paradigm, which is\nresource-intensive and fundamentally incapable of generalizing to unseen\neffects, thus limiting scalability and creation. To address this challenge, we\nintroduce VFXMaster, the first unified, reference-based framework for VFX video\ngeneration. It recasts effect generation as an in-context learning task,\nenabling it to reproduce diverse dynamic effects from a reference video onto\ntarget content. In addition, it demonstrates remarkable generalization to\nunseen effect categories. Specifically, we design an in-context conditioning\nstrategy that prompts the model with a reference example. An in-context\nattention mask is designed to precisely decouple and inject the essential\neffect attributes, allowing a single unified model to master the effect\nimitation without information leakage. In addition, we propose an efficient\none-shot effect adaptation mechanism to boost generalization capability on\ntough unseen effects from a single user-provided video rapidly. Extensive\nexperiments demonstrate that our method effectively imitates various categories\nof effect information and exhibits outstanding generalization to out-of-domain\neffects. To foster future research, we will release our code, models, and a\ncomprehensive dataset to the community.\n","authors":["Baolu Li","Yiming Zhang","Qinghe Wang","Liqian Ma","Xiaoyu Shi","Xintao Wang","Pengfei Wan","Zhenfei Yin","Yunzhi Zhuge","Huchuan Lu","Xu Jia"],"pdf_url":"https://arxiv.org/pdf/2510.25772v1.pdf","comment":"Project Page URL:https://libaolu312.github.io/VFXMaster/"},{"id":"http://arxiv.org/abs/2510.25765v1","updated":"2025-10-29T17:58:14Z","published":"2025-10-29T17:58:14Z","title":"FreeArt3D: Training-Free Articulated Object Generation using 3D\n  Diffusion","summary":"  Articulated 3D objects are central to many applications in robotics, AR/VR,\nand animation. Recent approaches to modeling such objects either rely on\noptimization-based reconstruction pipelines that require dense-view supervision\nor on feed-forward generative models that produce coarse geometric\napproximations and often overlook surface texture. In contrast, open-world 3D\ngeneration of static objects has achieved remarkable success, especially with\nthe advent of native 3D diffusion models such as Trellis. However, extending\nthese methods to articulated objects by training native 3D diffusion models\nposes significant challenges. In this work, we present FreeArt3D, a\ntraining-free framework for articulated 3D object generation. Instead of\ntraining a new model on limited articulated data, FreeArt3D repurposes a\npre-trained static 3D diffusion model (e.g., Trellis) as a powerful shape\nprior. It extends Score Distillation Sampling (SDS) into the 3D-to-4D domain by\ntreating articulation as an additional generative dimension. Given a few images\ncaptured in different articulation states, FreeArt3D jointly optimizes the\nobject's geometry, texture, and articulation parameters without requiring\ntask-specific training or access to large-scale articulated datasets. Our\nmethod generates high-fidelity geometry and textures, accurately predicts\nunderlying kinematic structures, and generalizes well across diverse object\ncategories. Despite following a per-instance optimization paradigm, FreeArt3D\ncompletes in minutes and significantly outperforms prior state-of-the-art\napproaches in both quality and versatility.\n","authors":["Chuhao Chen","Isabella Liu","Xinyue Wei","Hao Su","Minghua Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25760v1","updated":"2025-10-29T17:55:43Z","published":"2025-10-29T17:55:43Z","title":"Multimodal Spatial Reasoning in the Large Model Era: A Survey and\n  Benchmarks","summary":"  Humans possess spatial reasoning abilities that enable them to understand\nspaces through multimodal observations, such as vision and sound. Large\nmultimodal reasoning models extend these abilities by learning to perceive and\nreason, showing promising performance across diverse spatial tasks. However,\nsystematic reviews and publicly available benchmarks for these models remain\nlimited. In this survey, we provide a comprehensive review of multimodal\nspatial reasoning tasks with large models, categorizing recent progress in\nmultimodal large language models (MLLMs) and introducing open benchmarks for\nevaluation. We begin by outlining general spatial reasoning, focusing on\npost-training techniques, explainability, and architecture. Beyond classical 2D\ntasks, we examine spatial relationship reasoning, scene and layout\nunderstanding, as well as visual question answering and grounding in 3D space.\nWe also review advances in embodied AI, including vision-language navigation\nand action models. Additionally, we consider emerging modalities such as audio\nand egocentric video, which contribute to novel spatial understanding through\nnew sensors. We believe this survey establishes a solid foundation and offers\ninsights into the growing field of multimodal spatial reasoning. Updated\ninformation about this survey, codes and implementation of the open benchmarks\ncan be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.\n","authors":["Xu Zheng","Zihao Dongfang","Lutao Jiang","Boyuan Zheng","Yulong Guo","Zhenquan Zhang","Giuliano Albanese","Runyi Yang","Mengjiao Ma","Zixin Zhang","Chenfei Liao","Dingcheng Zhen","Yuanhuiyi Lyu","Yuqian Fu","Bin Ren","Linfeng Zhang","Danda Pani Paudel","Nicu Sebe","Luc Van Gool","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2510.25760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25739v1","updated":"2025-10-29T17:43:31Z","published":"2025-10-29T17:43:31Z","title":"Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image\n  Generation","summary":"  Autoregressive (AR) image generation models are capable of producing\nhigh-fidelity images but often suffer from slow inference due to their\ninherently sequential, token-by-token decoding process. Speculative decoding,\nwhich employs a lightweight draft model to approximate the output of a larger\nAR model, has shown promise in accelerating text generation without\ncompromising quality. However, its application to image generation remains\nlargely underexplored. The challenges stem from a significantly larger sampling\nspace, which complicates the alignment between the draft and target model\noutputs, coupled with the inadequate use of the two-dimensional spatial\nstructure inherent in images, thereby limiting the modeling of local\ndependencies. To overcome these challenges, we introduce Hawk, a new approach\nthat harnesses the spatial structure of images to guide the speculative model\ntoward more accurate and efficient predictions. Experimental results on\nmultiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR\nmodels, while preserving both image fidelity and diversity.\n","authors":["Zhi-Kai Chen","Jun-Peng Jiang","Han-Jia Ye","De-Chuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2510.25739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.20600v2","updated":"2025-10-29T17:35:15Z","published":"2025-08-28T09:43:59Z","title":"GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac\n  MRI Reconstruction","summary":"  Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction\nremains a critical challenge due to the trade-off between scan time and image\nquality, particularly when generalizing across diverse acquisition settings. We\npropose GENRE-CMR, a generative adversarial network (GAN)-based architecture\nemploying a residual deep unrolled reconstruction framework to enhance\nreconstruction fidelity and generalization. The architecture unrolls iterative\noptimization into a cascade of convolutional subnetworks, enriched with\nresidual connections to enable progressive feature propagation from shallow to\ndeeper stages. To further improve performance, we integrate two loss functions:\n(1) an Edge-Aware Region (EAR) loss, which guides the network to focus on\nstructurally informative regions and helps prevent common reconstruction\nblurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which\nregularizes the feature space across diverse data distributions via a symmetric\nKL divergence formulation. Extensive experiments confirm that GENRE-CMR\nsurpasses state-of-the-art methods on training and unseen data, achieving\n0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various\nacceleration factors and sampling trajectories. Ablation studies confirm the\ncontribution of each proposed component to reconstruction quality and\ngeneralization. Our framework presents a unified and robust solution for\nhigh-quality CMR reconstruction, paving the way for clinically adaptable\ndeployment across heterogeneous acquisition protocols.\n","authors":["Kian Anvari Hamedani","Narges Razizadeh","Shahabedin Nabavi","Mohsen Ebrahimi Moghaddam"],"pdf_url":"https://arxiv.org/pdf/2508.20600v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.24096v2","updated":"2025-10-29T16:13:52Z","published":"2025-06-30T17:48:54Z","title":"MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient\n  Surface Reconstruction","summary":"  While recent advances in Gaussian Splatting have enabled fast reconstruction\nof high-quality 3D scenes from images, extracting accurate surface meshes\nremains a challenge. Current approaches extract the surface through costly\npost-processing steps, resulting in the loss of fine geometric details or\nrequiring significant time and leading to very dense meshes with millions of\nvertices. More fundamentally, the a posteriori conversion from a volumetric to\na surface representation limits the ability of the final mesh to preserve all\ngeometric structures captured during training. We present MILo, a novel\nGaussian Splatting framework that bridges the gap between volumetric and\nsurface representations by differentiably extracting a mesh from the 3D\nGaussians. We design a fully differentiable procedure that constructs the\nmesh-including both vertex locations and connectivity-at every iteration\ndirectly from the parameters of the Gaussians, which are the only quantities\noptimized during training. Our method introduces three key technical\ncontributions: a bidirectional consistency framework ensuring both\nrepresentations-Gaussians and the extracted mesh-capture the same underlying\ngeometry during training; an adaptive mesh extraction process performed at each\ntraining iteration, which uses Gaussians as differentiable pivots for Delaunay\ntriangulation; a novel method for computing signed distance values from the 3D\nGaussians that enables precise surface extraction while avoiding geometric\nerosion. Our approach can reconstruct complete scenes, including backgrounds,\nwith state-of-the-art quality while requiring an order of magnitude fewer mesh\nvertices than previous methods. Due to their light weight and empty interior,\nour meshes are well suited for downstream applications such as physics\nsimulations or animation.\n","authors":["Antoine GuÃ©don","Diego Gomez","Nissim Maruani","Bingchen Gong","George Drettakis","Maks Ovsjanikov"],"pdf_url":"https://arxiv.org/pdf/2506.24096v2.pdf","comment":"10 pages. A presentation video of our approach is available at\n  https://youtu.be/_SGNhhNz0fE"},{"id":"http://arxiv.org/abs/2506.07464v3","updated":"2025-10-29T15:59:41Z","published":"2025-06-09T06:15:54Z","title":"DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO","summary":"  Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training for enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success using a PPO-style reinforcement algorithm\nwith group-normalized rewards. However, the effectiveness of GRPO in Video\nLarge Language Models (VideoLLMs) has still been less studyed. In this paper,\nwe explore GRPO and identify two problems that deteriorate the effective\nlearning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate\nthese challenges, we propose DeepVideo-R1, a video large language model trained\nwith Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation.\nReg-GRPO reformulates the GRPO loss function into a regression task that\ndirectly predicts the advantage in GRPO, eliminating the need for safeguards\nsuch as the clipping and min functions. It directly aligns the model with\nadvantages, providing guidance to prefer better ones. The difficulty-aware data\naugmentation strategy augments input prompts/videos to locate the difficulty of\nsamples at solvable difficulty levels, enabling diverse reward signals. Our\nexperimental results show that our approach significantly improves video\nreasoning performance across multiple benchmarks.\n","authors":["Jinyoung Park","Jeehye Na","Jinyoung Kim","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2506.07464v3.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2309.13672v8","updated":"2025-10-29T15:35:18Z","published":"2023-09-24T15:40:40Z","title":"RL-I2IT: Image-to-Image Translation with Deep Reinforcement Learning","summary":"  Most existing Image-to-Image Translation (I2IT) methods generate images in a\nsingle run of a deep learning (DL) model. However, designing such a single-step\nmodel is always challenging, requiring a huge number of parameters and easily\nfalling into bad global minimums and overfitting. In this work, we reformulate\nI2IT as a step-wise decision-making problem via deep reinforcement learning\n(DRL) and propose a novel framework that performs RL-based I2IT (RL-I2IT). The\nkey feature in the RL-I2IT framework is to decompose a monolithic learning\nprocess into small steps with a lightweight model to progressively transform a\nsource image successively to a target image. Considering that it is challenging\nto handle high dimensional continuous state and action spaces in the\nconventional RL framework, we introduce meta policy with a new concept Plan to\nthe standard Actor-Critic model, which is of a lower dimension than the\noriginal image and can facilitate the actor to generate a tractable high\ndimensional action. In the RL-I2IT framework, we also employ a task-specific\nauxiliary learning strategy to stabilize the training process and improve the\nperformance of the corresponding task. Experiments on several I2IT tasks\ndemonstrate the effectiveness and robustness of the proposed method when facing\nhigh-dimensional continuous action space problems. Our implementation of the\nRL-I2IT framework is available at\nhttps://github.com/Algolzw/SPAC-Deformable-Registration.\n","authors":["Jing Hu","Chengming Feng","Shu Hu","Ming-Ching Chang","Xin Li","Xi Wu","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2309.13672v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23118v3","updated":"2025-10-29T15:24:05Z","published":"2025-10-27T08:38:52Z","title":"Quantizing Space and Time: Fusing Time Series and Images for Earth\n  Observation","summary":"  We propose a task-agnostic framework for multimodal fusion of time series and\nsingle timestamp images, enabling cross-modal generation and robust downstream\nperformance. Our approach explores deterministic and learned strategies for\ntime series quantization and then leverages a masked correlation learning\nobjective, aligning discrete image and time series tokens in a unified\nrepresentation space. Instantiated in the Earth observation domain, the\npretrained model generates consistent global temperature profiles from\nsatellite imagery and is validated through counterfactual experiments. Across\ndownstream tasks, our task-agnostic pretraining outperforms task-specific\nfusion by 6% in R^2 and 2% in RMSE on average, and exceeds baseline methods by\n50% in R^2 and 12% in RMSE. Finally, we analyze gradient sensitivity across\nmodalities, providing insights into model robustness. Code, data, and weights\nwill be released under a permissive license.\n","authors":["Gianfranco Basile","Johannes Jakubik","Benedikt Blumenstiel","Thomas Brunschwiler","Juan Bernabe Moreno"],"pdf_url":"https://arxiv.org/pdf/2510.23118v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25594v1","updated":"2025-10-29T15:03:46Z","published":"2025-10-29T15:03:46Z","title":"Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for\n  Local Learning","summary":"  Training deep neural networks (DNNs) with backpropagation (BP) achieves\nstate-of-the-art accuracy but requires global error propagation and full\nparameterization, leading to substantial memory and computational overhead.\nDirect Feedback Alignment (DFA) enables local, parallelizable updates with\nlower memory requirements but is limited by unstructured feedback and poor\nscalability in deeper architectures, specially convolutional neural networks.\nTo address these limitations, we propose a structured local learning framework\nthat operates directly on low-rank manifolds defined by the Singular Value\nDecomposition (SVD) of weight matrices. Each layer is trained in its decomposed\nform, with updates applied to the SVD components using a composite loss that\nintegrates cross-entropy, subspace alignment, and orthogonality regularization.\nFeedback matrices are constructed to match the SVD structure, ensuring\nconsistent alignment between forward and feedback pathways. Our method reduces\nthe number of trainable parameters relative to the original DFA model, without\nrelying on pruning or post hoc compression. Experiments on CIFAR-10, CIFAR-100,\nand ImageNet show that our method achieves accuracy comparable to that of BP.\nAblation studies confirm the importance of each loss term in the low-rank\nsetting. These results establish local learning on low-rank manifolds as a\nprincipled and scalable alternative to full-rank gradient-based training.\n","authors":["Arani Roy","Marco P. Apolinario","Shristi Das Biswas","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2510.25594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25590v1","updated":"2025-10-29T14:58:37Z","published":"2025-10-29T14:58:37Z","title":"RegionE: Adaptive Region-Aware Generation for Efficient Image Editing","summary":"  Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved.\n","authors":["Pengtao Chen","Xianfang Zeng","Maosen Zhao","Mingzhu Shen","Peng Ye","Bangyin Xiang","Zhibo Wang","Wei Cheng","Gang Yu","Tao Chen"],"pdf_url":"https://arxiv.org/pdf/2510.25590v1.pdf","comment":"26 pages, 10 figures, 18 tables"},{"id":"http://arxiv.org/abs/2506.21710v2","updated":"2025-10-29T14:46:17Z","published":"2025-06-26T18:51:04Z","title":"FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering","summary":"  While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and three types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.\n","authors":["Liangyu Zhong","Fabio Rosenthal","Joachim Sicking","Fabian HÃ¼ger","Thorsten Bagdonat","Hanno Gottschalk","Leo Schwinn"],"pdf_url":"https://arxiv.org/pdf/2506.21710v2.pdf","comment":"Accepted by NeurIPS 2025 - main track. Project page:\n  https://focus-mllm-vqa.github.io/"},{"id":"http://arxiv.org/abs/2510.25522v1","updated":"2025-10-29T13:46:19Z","published":"2025-10-29T13:46:19Z","title":"Comparative Study of UNet-based Architectures for Liver Tumor\n  Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography","summary":"  Segmentation of liver structures in multi-phase contrast-enhanced computed\ntomography (CECT) plays a crucial role in computer-aided diagnosis and\ntreatment planning for liver diseases, including tumor detection. In this\nstudy, we investigate the performance of UNet-based architectures for liver\ntumor segmentation, starting from the original UNet and extending to UNet3+\nwith various backbone networks. We evaluate ResNet, Transformer-based, and\nState-space (Mamba) backbones, all initialized with pretrained weights.\nSurprisingly, despite the advances in modern architecture, ResNet-based models\nconsistently outperform Transformer- and Mamba-based alternatives across\nmultiple evaluation metrics. To further improve segmentation quality, we\nintroduce attention mechanisms into the backbone and observe that incorporating\nthe Convolutional Block Attention Module (CBAM) yields the best performance.\nResNetUNet3+ with CBAM module not only produced the best overlap metrics with a\nDice score of 0.755 and IoU of 0.662, but also achieved the most precise\nboundary delineation, evidenced by the lowest HD95 distance of 77.911. The\nmodel's superiority was further cemented by its leading overall accuracy of\n0.925 and specificity of 0.926, showcasing its robust capability in accurately\nidentifying both lesion and healthy tissue. To further enhance\ninterpretability, Grad-CAM visualizations were employed to highlight the\nregion's most influential predictions, providing insights into its\ndecision-making process. These findings demonstrate that classical ResNet\narchitecture, when combined with modern attention modules, remain highly\ncompetitive for medical image segmentation tasks, offering a promising\ndirection for liver tumor detection in clinical practice.\n","authors":["Doan-Van-Anh Ly","Thi-Thu-Hien Pham","Thanh-Hai Le"],"pdf_url":"https://arxiv.org/pdf/2510.25522v1.pdf","comment":"27 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.23763v2","updated":"2025-10-29T13:37:19Z","published":"2025-10-27T18:49:03Z","title":"RoboOmni: Proactive Robot Manipulation in Omni-modal Context","summary":"  Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.\n","authors":["Siyin Wang","Jinlan Fu","Feihong Liu","Xinzhe He","Huangxuan Wu","Junhao Shi","Kexin Huang","Zhaoye Fei","Jingjing Gong","Zuxuan Wu","Yugang Jiang","See-Kiong Ng","Tat-Seng Chua","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2510.23763v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25512v1","updated":"2025-10-29T13:35:46Z","published":"2025-10-29T13:35:46Z","title":"FaCT: Faithful Concept Traces for Explaining Neural Network Decisions","summary":"  Deep networks have shown remarkable performance across a wide range of tasks,\nyet getting a global concept-level understanding of how they function remains a\nkey challenge. Many post-hoc concept-based approaches have been introduced to\nunderstand their workings, yet they are not always faithful to the model.\nFurther, they make restrictive assumptions on the concepts a model learns, such\nas class-specificity, small spatial extent, or alignment to human expectations.\nIn this work, we put emphasis on the faithfulness of such concept-based\nexplanations and propose a new model with model-inherent mechanistic\nconcept-explanations. Our concepts are shared across classes and, from any\nlayer, their contribution to the logit and their input-visualization can be\nfaithfully traced. We also leverage foundation models to propose a new\nconcept-consistency metric, C$^2$-Score, that can be used to evaluate\nconcept-based methods. We show that, compared to prior work, our concepts are\nquantitatively more consistent and users find our concepts to be more\ninterpretable, all while retaining competitive ImageNet performance.\n","authors":["Amin Parchami-Araghi","Sukrut Rao","Jonas Fischer","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2510.25512v1.pdf","comment":"Accepted to NeurIPS 2025; Code is available at\n  https://github.com/m-parchami/FaCT"},{"id":"http://arxiv.org/abs/2411.10237v2","updated":"2025-10-29T13:22:04Z","published":"2024-11-15T14:51:30Z","title":"ScribbleVS: Scribble-Supervised Medical Image Segmentation via Dynamic\n  Competitive Pseudo Label Selection","summary":"  In clinical medicine, precise image segmentation can provide substantial\nsupport to clinicians. However, obtaining high-quality segmentation typically\ndemands extensive pixel-level annotations, which are labor-intensive and\nexpensive. Scribble annotations offer a more cost-effective alternative by\nimproving labeling efficiency. Nonetheless, using such sparse supervision for\ntraining reliable medical image segmentation models remains a significant\nchallenge. Some studies employ pseudo-labeling to enhance supervision, but\nthese methods are susceptible to noise interference. To address these\nchallenges, we introduce ScribbleVS, a framework designed to learn from\nscribble annotations. We introduce a Regional Pseudo Labels Diffusion Module to\nexpand the scope of supervision and reduce the impact of noise present in\npseudo labels. Additionally, we introduce a Dynamic Competitive Selection\nmodule for enhanced refinement in selecting pseudo labels. Experiments\nconducted on the ACDC, MSCMRseg, WORD, and BraTS2020 datasets demonstrate\npromising results, achieving segmentation precision comparable to fully\nsupervised models. The codes of this study are available at\nhttps://github.com/ortonwang/ScribbleVS.\n","authors":["Tao Wang","Xinlin Zhang","Zhenxuan Zhang","Yuanbo Zhou","Yuanbin Chen","Longxuan Zhao","Chaohui Xu","Shun Chen","Guang Yang","Tong Tong"],"pdf_url":"https://arxiv.org/pdf/2411.10237v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09518v2","updated":"2025-10-29T13:12:44Z","published":"2025-06-11T08:45:08Z","title":"HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for\n  Dynamic Scene","summary":"  Reconstructing dynamic 3D scenes from monocular videos remains a fundamental\nchallenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time\nrendering in static settings, extending it to dynamic scenes is challenging due\nto the difficulty of learning structured and temporally consistent motion\nrepresentations. This challenge often manifests as three limitations in\nexisting methods: redundant Gaussian updates, insufficient motion supervision,\nand weak modeling of complex non-rigid deformations. These issues collectively\nhinder coherent and efficient dynamic reconstruction. To address these\nlimitations, we propose HAIF-GS, a unified framework that enables structured\nand consistent dynamic modeling through sparse anchor-driven deformation. It\nfirst identifies motion-relevant regions via an Anchor Filter to suppress\nredundant updates in static areas. A self-supervised Induced Flow-Guided\nDeformation module induces anchor motion using multi-frame feature aggregation,\neliminating the need for explicit flow labels. To further handle fine-grained\ndeformations, a Hierarchical Anchor Propagation mechanism increases anchor\nresolution based on motion complexity and propagates multi-level\ntransformations. Extensive experiments on synthetic and real-world benchmarks\nvalidate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in\nrendering quality, temporal coherence, and reconstruction efficiency.\n","authors":["Jianing Chen","Zehao Li","Yujun Cai","Hao Jiang","Chengxuan Qian","Juyuan Kang","Shuqin Gao","Honglong Zhao","Tianlu Mao","Yucheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.09518v2.pdf","comment":"Accepted to NeurIPS 2025. Project page:\n  https://echopickle.github.io/HAIF-GS.github.io/"},{"id":"http://arxiv.org/abs/2503.08068v2","updated":"2025-10-29T12:57:01Z","published":"2025-03-11T05:59:43Z","title":"Simulating Automotive Radar with Lidar and Camera Inputs","summary":"  Low-cost millimeter automotive radar has received more and more attention due\nto its ability to handle adverse weather and lighting conditions in autonomous\ndriving. However, the lack of quality datasets hinders research and\ndevelopment. We report a new method that is able to simulate 4D millimeter wave\nradar signals including pitch, yaw, range, and Doppler velocity along with\nradar signal strength (RSS) using camera image, light detection and ranging\n(lidar) point cloud, and ego-velocity. The method is based on two new neural\nnetworks: 1) DIS-Net, which estimates the spatial distribution and number of\nradar signals, and 2) RSS-Net, which predicts the RSS of the signal based on\nappearance and geometric information. We have implemented and tested our method\nusing open datasets from 3 different models of commercial automotive radar. The\nexperimental results show that our method can successfully generate\nhigh-fidelity radar signals. Moreover, we have trained a popular object\ndetection neural network with data augmented by our synthesized radar. The\nnetwork outperforms the counterpart trained only on raw radar data, a promising\nresult to facilitate future radar-based research and development.\n","authors":["Peili Song","Dezhen Song","Yifan Yang","Enfan Lan","Jingtai Liu"],"pdf_url":"https://arxiv.org/pdf/2503.08068v2.pdf","comment":"Accepted by IROS 2025"},{"id":"http://arxiv.org/abs/2505.17685v2","updated":"2025-10-29T12:46:23Z","published":"2025-05-23T09:55:32Z","title":"FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for\n  Autonomous Driving","summary":"  Vision-Language-Action (VLA) models are increasingly used for end-to-end\ndriving due to their world knowledge and reasoning ability. Most prior work,\nhowever, inserts textual chains-of-thought (CoT) as intermediate steps tailored\nto the current scene. Such symbolic compressions can blur spatio-temporal\nrelations and discard fine visual cues, creating a cross-modal gap between\nperception and planning. We propose FSDrive, a visual spatio-temporal CoT\nframework that enables VLAs to think in images. The model first acts as a world\nmodel to generate a unified future frame that overlays coarse but\nphysically-plausible priors-future lane dividers and 3D boxes-on the predicted\nfuture image. This unified frame serves as the visual CoT, capturing both\nspatial structure and temporal evolution. The same VLA then functions as an\ninverse-dynamics model, planning trajectories from current observations and the\nvisual CoT. To equip VLAs with image generation while preserving understanding,\nwe introduce a unified pre-training paradigm that expands the vocabulary to\ninclude visual tokens and jointly optimizes VQA (for semantics) and\nfuture-frame prediction (for dynamics). A progressive easy-to-hard scheme first\npredicts lane/box priors to enforce physical constraints, then completes full\nfuture frames for fine details. On nuScenes and NAVSIM, FSDrive improves\ntrajectory accuracy and reduces collisions under both ST-P3 and UniAD metrics,\nand attains competitive FID for future-frame generation despite using\nlightweight autoregression. It also advances scene understanding on DriveLM.\nTogether, these results indicate that visual CoT narrows the cross-modal gap\nand yields safer, more anticipatory planning. Code is available at\nhttps://github.com/MIV-XJTU/FSDrive.\n","authors":["Shuang Zeng","Xinyuan Chang","Mengwei Xie","Xinran Liu","Yifan Bai","Zheng Pan","Mu Xu","Xing Wei"],"pdf_url":"https://arxiv.org/pdf/2505.17685v2.pdf","comment":"Accepted to NeurIPS 2025 as Spotlight Presentation. Code:\n  https://github.com/MIV-XJTU/FSDrive"},{"id":"http://arxiv.org/abs/2510.25463v1","updated":"2025-10-29T12:37:34Z","published":"2025-10-29T12:37:34Z","title":"SPADE: Sparsity Adaptive Depth Estimator for Zero-Shot, Real-Time,\n  Monocular Depth Estimation in Underwater Environments","summary":"  Underwater infrastructure requires frequent inspection and maintenance due to\nharsh marine conditions. Current reliance on human divers or remotely operated\nvehicles is limited by perceptual and operational challenges, especially around\ncomplex structures or in turbid water. Enhancing the spatial awareness of\nunderwater vehicles is key to reducing piloting risks and enabling greater\nautonomy. To address these challenges, we present SPADE: SParsity Adaptive\nDepth Estimator, a monocular depth estimation pipeline that combines\npre-trained relative depth estimator with sparse depth priors to produce dense,\nmetric scale depth maps. Our two-stage approach first scales the relative depth\nmap with the sparse depth points, then refines the final metric prediction with\nour proposed Cascade Conv-Deformable Transformer blocks. Our approach achieves\nimproved accuracy and generalisation over state-of-the-art baselines and runs\nefficiently at over 15 FPS on embedded hardware, promising to support practical\nunderwater inspection and intervention. This work has been submitted to IEEE\nJournal of Oceanic Engineering Special Issue of AUV 2026.\n","authors":["Hongjie Zhang","Gideon Billings","Stefan B. Williams"],"pdf_url":"https://arxiv.org/pdf/2510.25463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17897v4","updated":"2025-10-29T12:15:39Z","published":"2025-07-23T19:48:27Z","title":"Multimodal Recurrent Ensembles for Predicting Brain Responses to\n  Naturalistic Movies (Algonauts 2025)","summary":"  Accurately predicting distributed cortical responses to naturalistic stimuli\nrequires models that integrate visual, auditory and semantic information over\ntime. We present a hierarchical multimodal recurrent ensemble that maps\npretrained video, audio, and language embeddings to fMRI time series recorded\nwhile four subjects watched almost 80 hours of movies provided by the Algonauts\n2025 challenge. Modality-specific bidirectional RNNs encode temporal dynamics;\ntheir hidden states are fused and passed to a second recurrent layer, and\nlightweight subject-specific heads output responses for 1000 cortical parcels.\nTraining relies on a composite MSE-correlation loss and a curriculum that\ngradually shifts emphasis from early sensory to late association regions.\nAveraging 100 model variants further boosts robustness. The resulting system\nranked third on the competition leaderboard, achieving an overall Pearson r =\n0.2094 and the highest single-parcel peak score (mean r = 0.63) among all\nparticipants, with particularly strong gains for the most challenging subject\n(Subject 5). The approach establishes a simple, extensible baseline for future\nmultimodal brain-encoding benchmarks.\n","authors":["Semih Eren","Deniz Kucukahmetler","Nico Scherf"],"pdf_url":"https://arxiv.org/pdf/2507.17897v4.pdf","comment":"8 pages, 2 figures, 1 table. Invited report, CCN 2025 Algonauts\n  Project session (3rd-place team). Code:\n  https://github.com/erensemih/Algonauts2025_ModalityRNN v3: Added equal\n  contribution footnote to author list. Corrected reference list"},{"id":"http://arxiv.org/abs/2509.09349v2","updated":"2025-10-29T12:14:41Z","published":"2025-09-11T11:05:14Z","title":"Classification of Driver Behaviour Using External Observation Techniques\n  for Autonomous Vehicles","summary":"  Road traffic accidents remain a significant global concern, with human error,\nparticularly distracted and impaired driving, among the leading causes. This\nstudy introduces a novel driver behaviour classification system that uses\nexternal observation techniques to detect indicators of distraction and\nimpairment. The proposed framework employs advanced computer vision\nmethodologies, including real-time object tracking, lateral displacement\nanalysis, and lane position monitoring. The system identifies unsafe driving\nbehaviours such as excessive lateral movement and erratic trajectory patterns\nby implementing the YOLO object detection model and custom lane estimation\nalgorithms. Unlike systems reliant on inter-vehicular communication, this\nvision-based approach enables behavioural analysis of non-connected vehicles.\nExperimental evaluations on diverse video datasets demonstrate the framework's\nreliability and adaptability across varying road and environmental conditions.\n","authors":["Ian Nell","Shane Gilroy"],"pdf_url":"https://arxiv.org/pdf/2509.09349v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25440v1","updated":"2025-10-29T12:06:42Z","published":"2025-10-29T12:06:42Z","title":"More than a Moment: Towards Coherent Sequences of Audio Descriptions","summary":"  Audio Descriptions (ADs) convey essential on-screen information, allowing\nvisually impaired audiences to follow videos. To be effective, ADs must form a\ncoherent sequence that helps listeners to visualise the unfolding scene, rather\nthan describing isolated moments. However, most automatic methods generate each\nAD independently, often resulting in repetitive, incoherent descriptions. To\naddress this, we propose a training-free method, CoherentAD, that first\ngenerates multiple candidate descriptions for each AD time interval, and then\nperforms auto-regressive selection across the sequence to form a coherent and\ninformative narrative. To evaluate AD sequences holistically, we introduce a\nsequence-level metric, StoryRecall, which measures how well the predicted ADs\nconvey the ground truth narrative, alongside repetition metrics that capture\nthe redundancy across consecutive AD outputs. Our method produces coherent AD\nsequences with enhanced narrative understanding, outperforming prior approaches\nthat rely on independent generations.\n","authors":["Eshika Khandelwal","Junyu Xie","Tengda Han","Max Bain","Arsha Nagrani","Andrew Zisserman","GÃ¼l Varol","Makarand Tapaswi"],"pdf_url":"https://arxiv.org/pdf/2510.25440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05780v2","updated":"2025-10-29T10:59:02Z","published":"2024-08-11T14:11:45Z","title":"U-DECN: End-to-End Underwater Object Detection ConvNet with Improved\n  DeNoising Training","summary":"  Underwater object detection has higher requirements of running speed and\ndeployment efficiency for the detector due to its specific environmental\nchallenges. NMS of two- or one-stage object detectors and transformer\narchitecture of query-based end-to-end object detectors are not conducive to\ndeployment on underwater embedded devices with limited processing power. As for\nthe detrimental effect of underwater color cast noise, recent underwater object\ndetectors make network architecture or training complex, which also hinders\ntheir application and deployment on unmanned underwater vehicles. In this\npaper, we propose the Underwater DECO with improved deNoising training\n(U-DECN), the query-based end-to-end object detector (with ConvNet\nencoder-decoder architecture) for underwater color cast noise that addresses\nthe above problems. We integrate advanced technologies from DETR variants into\nDECO and design optimization methods specifically for the ConvNet architecture,\nincluding Deformable Convolution in SIM and Separate Contrastive DeNoising\nForward methods. To address the underwater color cast noise issue, we propose\nan Underwater Color DeNoising Query method to improve the generalization of the\nmodel for the biased object feature information by different color cast noise.\nOur U-DECN, with ResNet-50 backbone, achieves the best 64.0 AP on DUO and the\nbest 58.1 AP on RUOD, and 21 FPS (5 times faster than Deformable DETR and DINO\n4 FPS) on NVIDIA AGX Orin by TensorRT FP16, outperforming the other\nstate-of-the-art query-based end-to-end object detectors. The code is available\nat https://github.com/LEFTeyex/U-DECN.\n","authors":["Zhuoyan Liu","Bo Wang","Bing Wang","Ye Li"],"pdf_url":"https://arxiv.org/pdf/2408.05780v2.pdf","comment":"10 pages, 6 figures, 7 tables, accepted by IEEE TGRS"},{"id":"http://arxiv.org/abs/2510.25387v1","updated":"2025-10-29T10:57:59Z","published":"2025-10-29T10:57:59Z","title":"Instance-Level Composed Image Retrieval","summary":"  The progress of composed image retrieval (CIR), a popular research direction\nin image retrieval, where a combined visual and textual query is used, is held\nback by the absence of high-quality training and evaluation data. We introduce\na new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an\ninstance-level class definition. The goal is to retrieve images that contain\nthe same particular object as the visual query, presented under a variety of\nmodifications defined by textual queries. Its design and curation process keep\nthe dataset compact to facilitate future research, while maintaining its\nchallenge-comparable to retrieval among more than 40M random\ndistractors-through a semi-automated selection of hard negatives.\n  To overcome the challenge of obtaining clean, diverse, and suitable training\ndata, we leverage pre-trained vision-and-language models (VLMs) in a\ntraining-free approach called BASIC. The method separately estimates\nquery-image-to-image and query-text-to-image similarities, performing late\nfusion to upweight images that satisfy both queries, while down-weighting those\nthat exhibit high similarity with only one of the two. Each individual\nsimilarity is further improved by a set of components that are simple and\nintuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR\ndatasets that follow a semantic-level class definition. Project page:\nhttps://vrg.fel.cvut.cz/icir/.\n","authors":["Bill Psomas","George Retsinas","Nikos Efthymiadis","Panagiotis Filntisis","Yannis Avrithis","Petros Maragos","Ondrej Chum","Giorgos Tolias"],"pdf_url":"https://arxiv.org/pdf/2510.25387v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.20274v2","updated":"2025-10-29T10:47:16Z","published":"2025-05-26T17:53:28Z","title":"Probabilistic Kernel Function for Fast Angle Testing","summary":"  In this paper, we study the angle testing problem in the context of\nsimilarity search in high-dimensional Euclidean spaces and propose two\nprojection-based probabilistic kernel functions, one designed for angle\ncomparison and the other for angle thresholding. Unlike existing approaches\nthat rely on random projection vectors drawn from Gaussian distributions, our\napproach leverages reference angles and employs a deterministic structure for\nthe projection vectors. Notably, our kernel functions do not require asymptotic\nassumptions, such as the number of projection vectors tending to infinity, and\ncan be both theoretically and experimentally shown to outperform\nGaussian-distribution-based kernel functions. We apply the proposed kernel\nfunction to Approximate Nearest Neighbor Search (ANNS) and demonstrate that our\napproach achieves a 2.5X ~ 3X higher query-per-second (QPS) throughput compared\nto the widely-used graph-based search algorithm HNSW.\n","authors":["Kejing Lu","Chuan Xiao","Yoshiharu Ishikawa"],"pdf_url":"https://arxiv.org/pdf/2505.20274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25372v1","updated":"2025-10-29T10:42:56Z","published":"2025-10-29T10:42:56Z","title":"Prompt Estimation from Prototypes for Federated Prompt Tuning of Vision\n  Transformers","summary":"  Visual Prompt Tuning (VPT) of pre-trained Vision Transformers (ViTs) has\nproven highly effective as a parameter-efficient fine-tuning technique for\nadapting large models to downstream tasks with limited data. Its parameter\nefficiency makes it particularly suitable for Federated Learning (FL), where\nboth communication and computation budgets are often constrained. However,\nglobal prompt tuning struggles to generalize across heterogeneous clients,\nwhile personalized tuning overfits to local data and lacks generalization. We\npropose PEP-FedPT (Prompt Estimation from Prototypes for Federated Prompt\nTuning), a unified framework designed to achieve both generalization and\npersonalization in federated prompt tuning of ViTs. Within this framework, we\nintroduce the novel Class-Contextualized Mixed Prompt (CCMP) - based on\nclass-specific prompts maintained alongside a globally shared prompt. For each\ninput, CCMP adaptively combines class-specific prompts using weights derived\nfrom global class prototypes and client class priors. This approach enables\nper-sample prompt personalization without storing client-dependent trainable\nparameters. The prompts are collaboratively optimized via traditional federated\naveraging technique on the same. Comprehensive evaluations on CIFAR-100,\nTinyImageNet, DomainNet, and iNaturalist datasets demonstrate that PEP-FedPT\nconsistently surpasses the state-of-the-art baselines under diverse data\nheterogeneity scenarios, establishing a strong foundation for efficient and\ngeneralizable federated prompt tuning of Vision Transformers.\n","authors":["M Yashwanth","Sharannya Ghosh","Aditay Tripathi","Anirban Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2510.25372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22842v2","updated":"2025-10-29T10:18:17Z","published":"2025-10-26T21:21:27Z","title":"FastJAM: a Fast Joint Alignment Model for Images","summary":"  Joint Alignment (JA) of images aims to align a collection of images into a\nunified coordinate frame, such that semantically-similar features appear at\ncorresponding spatial locations. Most existing approaches often require long\ntraining times, large-capacity models, and extensive hyperparameter tuning. We\nintroduce FastJAM, a rapid, graph-based method that drastically reduces the\ncomputational complexity of joint alignment tasks. FastJAM leverages pairwise\nmatches computed by an off-the-shelf image matcher, together with a rapid\nnonparametric clustering, to construct a graph representing intra- and\ninter-image keypoint relations. A graph neural network propagates and\naggregates these correspondences, efficiently predicting per-image homography\nparameters via image-level pooling. Utilizing an inverse-compositional loss,\nthat eliminates the need for a regularization term over the predicted\ntransformations (and thus also obviates the hyperparameter tuning associated\nwith such terms), FastJAM performs image JA quickly and effectively.\nExperimental results on several benchmarks demonstrate that FastJAM achieves\nresults better than existing modern JA methods in terms of alignment quality,\nwhile reducing computation time from hours or minutes to mere seconds. Our code\nis available at our project webpage, https://bgu-cs-vil.github.io/FastJAM/\n","authors":["Omri Hirsch","Ron Shapira Weber","Shira Ifergane","Oren Freifeld"],"pdf_url":"https://arxiv.org/pdf/2510.22842v2.pdf","comment":"Accepted to NeurIPS 2025. Pages 1-10 are the Main Paper. Pages 23-31\n  are Supplemental Material. FastJAM website -\n  https://bgu-cs-vil.github.io/FastJAM/"},{"id":"http://arxiv.org/abs/2510.25347v1","updated":"2025-10-29T10:04:47Z","published":"2025-10-29T10:04:47Z","title":"3D CT-Based Coronary Calcium Assessment: A Feature-Driven Machine\n  Learning Framework","summary":"  Coronary artery calcium (CAC) scoring plays a crucial role in the early\ndetection and risk stratification of coronary artery disease (CAD). In this\nstudy, we focus on non-contrast coronary computed tomography angiography (CCTA)\nscans, which are commonly used for early calcification detection in clinical\nsettings. To address the challenge of limited annotated data, we propose a\nradiomics-based pipeline that leverages pseudo-labeling to generate training\nlabels, thereby eliminating the need for expert-defined segmentations.\nAdditionally, we explore the use of pretrained foundation models, specifically\nCT-FM and RadImageNet, to extract image features, which are then used with\ntraditional classifiers. We compare the performance of these deep learning\nfeatures with that of radiomics features. Evaluation is conducted on a clinical\nCCTA dataset comprising 182 patients, where individuals are classified into two\ngroups: zero versus non-zero calcium scores. We further investigate the impact\nof training on non-contrast datasets versus combined contrast and non-contrast\ndatasets, with testing performed only on non contrast scans. Results show that\nradiomics-based models significantly outperform CNN-derived embeddings from\nfoundation models (achieving 84% accuracy and p<0.05), despite the\nunavailability of expert annotations.\n","authors":["Ayman Abaid","Gianpiero Guidone","Sara Alsubai","Foziyah Alquahtani","Talha Iqbal","Ruth Sharif","Hesham Elzomor","Emiliano Bianchini","Naeif Almagal","Michael G. Madden","Faisal Sharif","Ihsan Ullah"],"pdf_url":"https://arxiv.org/pdf/2510.25347v1.pdf","comment":"11 pages, 2 Figures, MICCAI AMAI 2025 workshop, to be published in\n  Volume 16206 of the Lecture Notes in Computer Science series"},{"id":"http://arxiv.org/abs/2510.25345v1","updated":"2025-10-29T10:03:33Z","published":"2025-10-29T10:03:33Z","title":"Informative Sample Selection Model for Skeleton-based Action Recognition\n  with Limited Training Samples","summary":"  Skeleton-based human action recognition aims to classify human skeletal\nsequences, which are spatiotemporal representations of actions, into predefined\ncategories. To reduce the reliance on costly annotations of skeletal sequences\nwhile maintaining competitive recognition accuracy, the task of 3D Action\nRecognition with Limited Training Samples, also known as semi-supervised 3D\nAction Recognition, has been proposed. In addition, active learning, which aims\nto proactively select the most informative unlabeled samples for annotation,\nhas been explored in semi-supervised 3D Action Recognition for training sample\nselection. Specifically, researchers adopt an encoder-decoder framework to\nembed skeleton sequences into a latent space, where clustering information,\ncombined with a margin-based selection strategy using a multi-head mechanism,\nis utilized to identify the most informative sequences in the unlabeled set for\nannotation. However, the most representative skeleton sequences may not\nnecessarily be the most informative for the action recognizer, as the model may\nhave already acquired similar knowledge from previously seen skeleton samples.\nTo solve it, we reformulate Semi-supervised 3D action recognition via active\nlearning from a novel perspective by casting it as a Markov Decision Process\n(MDP). Built upon the MDP framework and its training paradigm, we train an\ninformative sample selection model to intelligently guide the selection of\nskeleton sequences for annotation. To enhance the representational capacity of\nthe factors in the state-action pairs within our method, we project them from\nEuclidean space to hyperbolic space. Furthermore, we introduce a meta tuning\nstrategy to accelerate the deployment of our method in real-world scenarios.\nExtensive experiments on three 3D action recognition benchmarks demonstrate the\neffectiveness of our method.\n","authors":["Zhigang Tu","Zhengbo Zhang","Jia Gong","Junsong Yuan","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2510.25345v1.pdf","comment":"Accepted by IEEE Transactions on Image Processing (TIP), 2025"},{"id":"http://arxiv.org/abs/2503.11094v3","updated":"2025-10-29T09:54:24Z","published":"2025-03-14T05:35:38Z","title":"Open3D-VQA: A Benchmark for Comprehensive Spatial Reasoning with\n  Multimodal Large Language Model in Open Space","summary":"  Spatial reasoning is a fundamental capability of multimodal large language\nmodels (MLLMs), yet their performance in open aerial environments remains\nunderexplored. In this work, we present Open3D-VQA, a novel benchmark for\nevaluating MLLMs' ability to reason about complex spatial relationships from an\naerial perspective. The benchmark comprises 73k QA pairs spanning 7 general\nspatial reasoning tasks, including multiple-choice, true/false, and\nshort-answer formats, and supports both visual and point cloud modalities. The\nquestions are automatically generated from spatial relations extracted from\nboth real-world and simulated aerial scenes. Evaluation on 13 popular MLLMs\nreveals that: 1) Models are generally better at answering questions about\nrelative spatial relations than absolute distances, 2) 3D LLMs fail to\ndemonstrate significant advantages over 2D LLMs, and 3) Fine-tuning solely on\nthe simulated dataset can significantly improve the model's spatial reasoning\nperformance in real-world scenarios. We release our benchmark, data generation\npipeline, and evaluation toolkit to support further research:\nhttps://github.com/EmbodiedCity/Open3D-VQA.code.\n","authors":["Weichen Zhang","Zile Zhou","Xin Zeng","Xuchen Liu","Jianjie Fang","Chen Gao","Yong Li","Jinqiang Cui","Xinlei Chen","Xiao-Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.11094v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25332v1","updated":"2025-10-29T09:47:38Z","published":"2025-10-29T09:47:38Z","title":"StreamingCoT: A Dataset for Temporal Dynamics and Multimodal\n  Chain-of-Thought Reasoning in Streaming VideoQA","summary":"  The rapid growth of streaming video applications demands multimodal models\nwith enhanced capabilities for temporal dynamics understanding and complex\nreasoning. However, current Video Question Answering (VideoQA) datasets suffer\nfrom two critical limitations: 1) Static annotation mechanisms fail to capture\nthe evolving nature of answers in temporal video streams, and 2) The absence of\nexplicit reasoning process annotations restricts model interpretability and\nlogical deduction capabilities. To address these challenges, We introduce\nStreamingCoT, the first dataset explicitly designed for temporally evolving\nreasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks. Our\nframework first establishes a dynamic hierarchical annotation architecture that\ngenerates per-second dense descriptions and constructs temporally-dependent\nsemantic segments through similarity fusion, paired with question-answer sets\nconstrained by temporal evolution patterns. We further propose an explicit\nreasoning chain generation paradigm that extracts spatiotemporal objects via\nkeyframe semantic alignment, derives object state transition-based reasoning\npaths using large language models, and ensures logical coherence through\nhuman-verified validation. This dataset establishes a foundation for advancing\nresearch in streaming video understanding, complex temporal reasoning, and\nmultimodal inference. Our StreamingCoT and its construction toolkit can be\naccessed at https://github.com/Fleeting-hyh/StreamingCoT.\n","authors":["Yuhang Hu","Zhenyu Yang","Shihan Wang","Shengsheng Qian","Bin Wen","Fan Yang","Tingting Gao","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2510.25332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25327v1","updated":"2025-10-29T09:41:03Z","published":"2025-10-29T09:41:03Z","title":"MMEdge: Accelerating On-device Multimodal Inference via Pipelined\n  Sensing and Encoding","summary":"  Real-time multimodal inference on resource-constrained edge devices is\nessential for applications such as autonomous driving, human-computer\ninteraction, and mobile health. However, prior work often overlooks the tight\ncoupling between sensing dynamics and model execution, as well as the complex\ninter-modality dependencies. In this paper, we propose MMEdge, an new on-device\nmulti-modal inference framework based on pipelined sensing and encoding.\nInstead of waiting for complete sensor inputs, MMEdge decomposes the entire\ninference process into a sequence of fine-grained sensing and encoding units,\nallowing computation to proceed incrementally as data arrive. MMEdge also\nintroduces a lightweight but effective temporal aggregation module that\ncaptures rich temporal dynamics across different pipelined units to maintain\naccuracy performance. Such pipelined design also opens up opportunities for\nfine-grained cross-modal optimization and early decision-making during\ninference. To further enhance system performance under resource variability and\ninput data complexity, MMEdge incorporates an adaptive multimodal configuration\noptimizer that dynamically selects optimal sensing and model configurations for\neach modality under latency constraints, and a cross-modal speculative skipping\nmechanism that bypasses future units of slower modalities when early\npredictions reach sufficient confidence. We evaluate MMEdge using two public\nmultimodal datasets and deploy it on a real-world unmanned aerial vehicle\n(UAV)-based multimodal testbed. The results show that MMEdge significantly\nreduces end-to-end latency while maintaining high task accuracy across various\nsystem and data dynamics.\n","authors":["Runxi Huang","Mingxuan Yu","Mingyu Tsoi","Xiaomin Ouyang"],"pdf_url":"https://arxiv.org/pdf/2510.25327v1.pdf","comment":"Accepted by SenSys 2026"},{"id":"http://arxiv.org/abs/2510.25318v1","updated":"2025-10-29T09:32:42Z","published":"2025-10-29T09:32:42Z","title":"Prototype-Driven Adaptation for Few-Shot Object Detection","summary":"  Few-shot object detection (FSOD) often suffers from base-class bias and\nunstable calibration when only a few novel samples are available. We propose\nPrototype-Driven Alignment (PDA), a lightweight, plug-in metric head for DeFRCN\nthat provides a prototype-based \"second opinion\" complementary to the linear\nclassifier. PDA maintains support-only prototypes in a learnable\nidentity-initialized projection space and optionally applies\nprototype-conditioned RoI alignment to reduce geometric mismatch. During\nfine-tuning, prototypes can be adapted via exponential moving average(EMA)\nupdates on labeled foreground RoIs-without introducing class-specific\nparameters-and are frozen at inference to ensure strict protocol compliance.\nPDA employs a best-of-K matching scheme to capture intra-class multi-modality\nand temperature-scaled fusion to combine metric similarities with detector\nlogits. Experiments on VOC FSOD and GFSOD benchmarks show that PDA consistently\nimproves novel-class performance with minimal impact on base classes and\nnegligible computational overhead.\n","authors":["Yushen Huang","Zhiming Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25318v1.pdf","comment":"7 pages,1 figure,2 tables,Preprint"},{"id":"http://arxiv.org/abs/2510.25314v1","updated":"2025-10-29T09:27:38Z","published":"2025-10-29T09:27:38Z","title":"Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired\n  Monocentric Design","summary":"  Achieving high-fidelity, compact RGBD imaging presents a dual challenge:\nconventional compact optics struggle with RGB sharpness across the entire\ndepth-of-field, while software-only Monocular Depth Estimation (MDE) is an\nill-posed problem reliant on unreliable semantic priors. While deep optics with\nelements like DOEs can encode depth, they introduce trade-offs in fabrication\ncomplexity and chromatic aberrations, compromising simplicity. To address this,\nwe first introduce a novel bio-inspired all-spherical monocentric lens, around\nwhich we build the Bionic Monocentric Imaging (BMI) framework, a holistic\nco-design. This optical design naturally encodes depth into its depth-varying\nPoint Spread Functions (PSFs) without requiring complex diffractive or freeform\nelements. We establish a rigorous physically-based forward model to generate a\nsynthetic dataset by precisely simulating the optical degradation process. This\nsimulation pipeline is co-designed with a dual-head, multi-scale reconstruction\nnetwork that employs a shared encoder to jointly recover a high-fidelity\nAll-in-Focus (AiF) image and a precise depth map from a single coded capture.\nExtensive experiments validate the state-of-the-art performance of the proposed\nframework. In depth estimation, the method attains an Abs Rel of 0.026 and an\nRMSE of 0.130, markedly outperforming leading software-only approaches and\nother deep optics systems. For image restoration, the system achieves an SSIM\nof 0.960 and a perceptual LPIPS score of 0.082, thereby confirming a superior\nbalance between image fidelity and depth accuracy. This study illustrates that\nthe integration of bio-inspired, fully spherical optics with a joint\nreconstruction algorithm constitutes an effective strategy for addressing the\nintrinsic challenges in high-performance compact RGBD imaging. Source code will\nbe publicly available at https://github.com/ZongxiYu-ZJU/BMI.\n","authors":["Zongxi Yu","Xiaolong Qian","Shaohua Gao","Qi Jiang","Yao Gao","Kailun Yang","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25314v1.pdf","comment":"The source code will be publicly available at\n  https://github.com/ZongxiYu-ZJU/BMI"},{"id":"http://arxiv.org/abs/2510.25301v1","updated":"2025-10-29T09:14:07Z","published":"2025-10-29T09:14:07Z","title":"GaTector+: A Unified Head-free Framework for Gaze Object and Gaze\n  Following Prediction","summary":"  Gaze object detection and gaze following are fundamental tasks for\ninterpreting human gaze behavior or intent. However, most previous methods\nusually solve these two tasks separately, and their prediction of gaze objects\nand gaze following typically depend on head-related prior knowledge during both\nthe training phase and real-world deployment. This dependency necessitates an\nauxiliary network to extract head location, thus precluding joint optimization\nacross the entire system and constraining the practical applicability. To this\nend, we propose GaTector+, a unified framework for gaze object detection and\ngaze following, which eliminates the dependence on the head-related priors\nduring inference. Specifically, GaTector+ uses an expanded\nspecific-general-specific feature extractor that leverages a shared backbone,\nwhich extracts general features for gaze following and object detection using\nthe shared backbone while using specific blocks before and after the shared\nbackbone to better consider the specificity of each sub-task. To obtain\nhead-related knowledge without prior information, we first embed a head\ndetection branch to predict the head of each person. Then, before regressing\nthe gaze point, a head-based attention mechanism is proposed to fuse the sense\nfeature and gaze feature with the help of head location. Since the\nsuboptimization of the gaze point heatmap leads to the performance bottleneck,\nwe propose an attention supervision mechanism to accelerate the learning of the\ngaze heatmap. Finally, we propose a novel evaluation metric, mean Similarity\nover Candidates (mSoC), for gaze object detection, which is more sensitive to\nvariations between bounding boxes. The experimental results on multiple\nbenchmark datasets demonstrate the effectiveness of our model in both gaze\nobject detection and gaze following tasks.\n","authors":["Yang Jin","Guangyu Guo","Binglu Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.23051v2","updated":"2025-10-29T09:09:07Z","published":"2025-09-27T02:12:09Z","title":"Activation Matching for Explanation Generation","summary":"  In this paper we introduce an activation-matching--based approach to generate\nminimal, faithful explanations for the decision-making of a pretrained\nclassifier on any given image. Given an input image $x$ and a frozen model $f$,\nwe train a lightweight autoencoder to output a binary mask $m$ such that the\nexplanation $e = m \\odot x$ preserves both the model's prediction and the\nintermediate activations of \\(x\\). Our objective combines: (i) multi-layer\nactivation matching with KL divergence to align distributions and cross-entropy\nto retain the top-1 label for both the image and the explanation; (ii) mask\npriors -- L1 area for minimality, a binarization penalty for crisp 0/1 masks,\nand total variation for compactness; and (iii) abductive constraints for\nfaithfulness and necessity. Together, these objectives yield small,\nhuman-interpretable masks that retain classifier behavior while discarding\nirrelevant input regions, providing practical and faithful minimalist\nexplanations for the decision making of the underlying model.\n","authors":["Pirzada Suhail","Aditya Anand","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2509.23051v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19162v2","updated":"2025-10-29T08:59:36Z","published":"2024-06-27T13:29:25Z","title":"Single Image Estimation of Cell Migration Direction by Deep Circular\n  Regression","summary":"  In this paper, we address the problem of estimating the migration direction\nof cells based on a single image. A solution to this problem lays the\nfoundation for a variety of applications that were previously not possible. To\nour knowledge, there is only one related work that employs a classification CNN\nwith four classes (quadrants). However, this approach does not allow for\ndetailed directional resolution. We tackle the single image estimation problem\nusing deep circular regression, with a particular focus on cycle-sensitive\nmethods. On two common datasets, we achieve a mean estimation error of\n$\\sim\\!17^\\circ$, representing a significant improvement over previous work,\nwhich reported estimation error of $30^\\circ$ and $34^\\circ$, respectively.\n","authors":["Lennart Bruns","Lucas Lamparter","Milos Galic","Xiaoyi Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.19162v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24385v2","updated":"2025-10-29T08:55:13Z","published":"2025-10-28T13:01:42Z","title":"When are radiology reports useful for training medical image\n  classifiers?","summary":"  Medical images used to train machine learning models are often accompanied by\nradiology reports containing rich expert annotations. However, relying on these\nreports as inputs for clinical prediction requires the timely manual work of a\ntrained radiologist. This raises a natural question: when can radiology reports\nbe leveraged during training to improve image-only classification? Prior works\nare limited to evaluating pre-trained image representations by fine-tuning them\nto predict diagnostic labels, often extracted from reports, ignoring tasks with\nlabels that are weakly associated with the text. To address this gap, we\nconduct a systematic study of how radiology reports can be used during both\npre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,\n12-month readmission), and under varying training set sizes. Our findings\nreveal that: (1) Leveraging reports during pre-training is beneficial for\ndownstream classification tasks where the label is well-represented in the\ntext; however, pre-training through explicit image-text alignment can be\ndetrimental in settings where it's not; (2) Fine-tuning with reports can lead\nto significant improvements and even have a larger impact than the pre-training\nmethod in certain settings. These results provide actionable insights into when\nand how to leverage privileged text data to train medical image classifiers\nwhile highlighting gaps in current research.\n","authors":["Herman BergstrÃ¶m","Zhongqi Yue","Fredrik D. Johansson"],"pdf_url":"https://arxiv.org/pdf/2510.24385v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25279v1","updated":"2025-10-29T08:38:03Z","published":"2025-10-29T08:38:03Z","title":"Diffusion-Driven Progressive Target Manipulation for Source-Free Domain\n  Adaptation","summary":"  Source-free domain adaptation (SFDA) is a challenging task that tackles\ndomain shifts using only a pre-trained source model and unlabeled target data.\nExisting SFDA methods are restricted by the fundamental limitation of\nsource-target domain discrepancy. Non-generation SFDA methods suffer from\nunreliable pseudo-labels in challenging scenarios with large domain\ndiscrepancies, while generation-based SFDA methods are evidently degraded due\nto enlarged domain discrepancies in creating pseudo-source data. To address\nthis limitation, we propose a novel generation-based framework named\nDiffusion-Driven Progressive Target Manipulation (DPTM) that leverages\nunlabeled target data as references to reliably generate and progressively\nrefine a pseudo-target domain for SFDA. Specifically, we divide the target\nsamples into a trust set and a non-trust set based on the reliability of\npseudo-labels to sufficiently and reliably exploit their information. For\nsamples from the non-trust set, we develop a manipulation strategy to\nsemantically transform them into the newly assigned categories, while\nsimultaneously maintaining them in the target distribution via a latent\ndiffusion model. Furthermore, we design a progressive refinement mechanism that\nprogressively reduces the domain discrepancy between the pseudo-target domain\nand the real target domain via iterative refinement. Experimental results\ndemonstrate that DPTM outperforms existing methods by a large margin and\nachieves state-of-the-art performance on four prevailing SFDA benchmark\ndatasets with different scales. Remarkably, DPTM can significantly enhance the\nperformance by up to 18.6% in scenarios with large source-target gaps.\n","authors":["Yuyang Huang","Yabo Chen","Junyu Zhou","Wenrui Dai","Xiaopeng Zhang","Junni Zou","Hongkai Xiong","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2510.25279v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2504.21497v3","updated":"2025-10-29T08:32:29Z","published":"2025-04-30T10:30:46Z","title":"MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric\n  Guidance","summary":"  In this study, we propose a method for video face reenactment that integrates\na 3D face parametric model into a latent diffusion framework, aiming to improve\nshape consistency and motion control in existing video-based face generation\napproaches. Our approach employs the FLAME (Faces Learned with an Articulated\nModel and Expressions) model as the 3D face parametric representation,\nproviding a unified framework for modeling face expressions and head pose. This\nnot only enables precise extraction of motion features from driving videos, but\nalso contributes to the faithful preservation of face shape and geometry.\nSpecifically, we enhance the latent diffusion model with rich 3D expression and\ndetailed pose information by incorporating depth maps, normal maps, and\nrendering maps derived from FLAME sequences. These maps serve as motion\nguidance and are encoded into the denoising UNet through a specifically\ndesigned Geometric Guidance Encoder (GGE). A multi-layer feature fusion module\nwith integrated self-attention mechanisms is used to combine facial appearance\nand motion latent features within the spatial domain. By utilizing the 3D face\nparametric model as motion guidance, our method enables parametric alignment of\nface identity between the reference image and the motion captured from the\ndriving video. Experimental results on benchmark datasets show that our method\nexcels at generating high-quality face animations with precise expression and\nhead pose variation modeling. In addition, it demonstrates strong\ngeneralization performance on out-of-domain images. Code is publicly available\nat https://github.com/weimengting/MagicPortrait.\n","authors":["Mengting Wei","Yante Li","Tuomas Varanka","Yan Jiang","Guoying Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.21497v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25268v1","updated":"2025-10-29T08:27:00Z","published":"2025-10-29T08:27:00Z","title":"SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object\n  with Discrete Human Object Interaction Representation","summary":"  Generating hand grasps with language instructions is a widely studied topic\nthat benefits from embodied AI and VR/AR applications. While transferring into\nhand articulatied object interaction (HAOI), the hand grasps synthesis requires\nnot only object functionality but also long-term manipulation sequence along\nthe object deformation. This paper proposes a novel HAOI sequence generation\nframework SynHLMA, to synthesize hand language manipulation for articulated\nobjects. Given a complete point cloud of an articulated object, we utilize a\ndiscrete HAOI representation to model each hand object interaction frame. Along\nwith the natural language embeddings, the representations are trained by an\nHAOI manipulation language model to align the grasping process with its\nlanguage description in a shared representation space. A joint-aware loss is\nemployed to ensure hand grasps follow the dynamic variations of articulated\nobject joints. In this way, our SynHLMA achieves three typical hand\nmanipulation tasks for articulated objects of HAOI generation, HAOI prediction\nand HAOI interpolation. We evaluate SynHLMA on our built HAOI-lang dataset and\nexperimental results demonstrate the superior hand grasp sequence generation\nperformance comparing with state-of-the-art. We also show a robotics grasp\napplication that enables dexterous grasps execution from imitation learning\nusing the manipulation sequence provided by our SynHLMA. Our codes and datasets\nwill be made publicly available.\n","authors":["Wang zhi","Yuyan Liu","Liu Liu","Li Zhang","Ruixuan Lu","Dan Guo"],"pdf_url":"https://arxiv.org/pdf/2510.25268v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.20322v2","updated":"2025-10-29T08:24:40Z","published":"2025-10-23T08:16:44Z","title":"HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large\n  Language Models","summary":"  Multi-modal large language models (MLLMs) have emerged as a transformative\napproach for aligning visual and textual understanding. They typically require\nextremely high computational resources (e.g., thousands of GPUs) for training\nto achieve cross-modal alignment at multi-granularity levels. We argue that a\nkey source of this inefficiency lies in the vision encoders they widely equip\nwith, e.g., CLIP and SAM, which lack the alignment with language at\nmulti-granularity levels. To address this issue, in this paper, we leverage\nhyperbolic space, which inherently models hierarchical levels and thus provides\na principled framework for bridging the granularity gap between visual and\ntextual modalities at an arbitrary granularity level. Concretely, we propose an\nefficient training paradigm for MLLMs, dubbed as HyperET, which can optimize\nvisual representations to align with their textual counterparts at an arbitrary\ngranularity level through dynamic hyperbolic radius adjustment in hyperbolic\nspace. HyperET employs learnable matrices with M\\\"{o}bius multiplication\noperations, implemented via three effective configurations: diagonal scaling\nmatrices, block-diagonal matrices, and banded matrices, providing a flexible\nyet efficient parametrization strategy. Comprehensive experiments across\nmultiple MLLM benchmarks demonstrate that HyperET consistently improves both\nexisting pre-training and fine-tuning MLLMs clearly with less than 1\\%\nadditional parameters.\n","authors":["Zelin Peng","Zhengqin Xu","Qingyang Liu","Xiaokang Yang","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2510.20322v2.pdf","comment":"Accepted by NeurIPS2025 (Oral)"},{"id":"http://arxiv.org/abs/2510.25263v1","updated":"2025-10-29T08:21:59Z","published":"2025-10-29T08:21:59Z","title":"LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part\n  Segmentation","summary":"  We propose LangHOPS, the first Multimodal Large Language Model (MLLM) based\nframework for open-vocabulary object-part instance segmentation. Given an\nimage, LangHOPS can jointly detect and segment hierarchical object and part\ninstances from open-vocabulary candidate categories. Unlike prior approaches\nthat rely on heuristic or learnable visual grouping, our approach grounds\nobject-part hierarchies in language space. It integrates the MLLM into the\nobject-part parsing pipeline to leverage its rich knowledge and reasoning\ncapabilities, and link multi-granularity concepts within the hierarchies. We\nevaluate LangHOPS across multiple challenging scenarios, including in-domain\nand cross-dataset object-part instance segmentation, and zero-shot semantic\nsegmentation. LangHOPS achieves state-of-the-art results, surpassing previous\nmethods by 5.5% Average Precision (AP) (in-domain) and 4.8% (cross-dataset) on\nthe PartImageNet dataset and by 2.5% mIOU on unseen object parts in ADE20K\n(zero-shot). Ablation studies further validate the effectiveness of the\nlanguage-grounded hierarchy and MLLM driven part query refinement strategy. The\ncode will be released here.\n","authors":["Yang Miao","Jan-Nico Zaech","Xi Wang","Fabien Despinoy","Danda Pani Paudel","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2510.25263v1.pdf","comment":"10 pages, 5 figures, 14 tables, Neurips 2025"},{"id":"http://arxiv.org/abs/2503.19311v2","updated":"2025-10-29T08:14:38Z","published":"2025-03-25T03:17:42Z","title":"DGTRSD & DGTRS-CLIP: A Dual-Granularity Remote Sensing Image-Text\n  Dataset and Vision Language Foundation Model for Alignment","summary":"  Vision Language Foundation Models based on CLIP architecture for remote\nsensing primarily rely on short text captions, which often result in incomplete\nsemantic representations. Although longer captions convey richer information,\nexisting models struggle to process them effectively because of limited\ntext-encoding capacity, and there remains a shortage of resources that align\nremote sensing images with both short text and long text captions. To address\nthis gap, we introduce DGTRSD, a dual-granularity remote sensing image-text\ndataset, where each image is paired with both a short text caption and a long\ntext description, providing a solid foundation for dual-granularity semantic\nmodeling. Based on this, we further propose DGTRS-CLIP, a dual-granularity\ncurriculum learning framework that combines short text and long text\nsupervision to achieve dual-granularity semantic alignment. Extensive\nexperiments on four typical zero-shot tasks: long text cross-modal retrieval,\nshort text cross-modal retrieval, image classification, and semantic\nlocalization demonstrate that DGTRS-CLIP consistently outperforms existing\nmethods across all tasks. The code has been open-sourced and is available at\nhttps://github.com/MitsuiChen14/DGTRS.\n","authors":["Weizhi Chen","Yupeng Deng","Jin Wei","Jingbo Chen","Jiansheng Chen","Yuman Feng","Zhihao Xi","Diyou Liu","Kai Li","Yu Meng"],"pdf_url":"https://arxiv.org/pdf/2503.19311v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25257v1","updated":"2025-10-29T08:13:17Z","published":"2025-10-29T08:13:17Z","title":"RT-DETRv4: Painlessly Furthering Real-Time Object Detection with Vision\n  Foundation Models","summary":"  Real-time object detection has achieved substantial progress through\nmeticulously designed architectures and optimization strategies. However, the\npursuit of high-speed inference via lightweight network designs often leads to\ndegraded feature representation, which hinders further performance improvements\nand practical on-device deployment. In this paper, we propose a cost-effective\nand highly adaptable distillation framework that harnesses the rapidly evolving\ncapabilities of Vision Foundation Models (VFMs) to enhance lightweight object\ndetectors. Given the significant architectural and learning objective\ndisparities between VFMs and resource-constrained detectors, achieving stable\nand task-aligned semantic transfer is challenging. To address this, on one\nhand, we introduce a Deep Semantic Injector (DSI) module that facilitates the\nintegration of high-level representations from VFMs into the deep layers of the\ndetector. On the other hand, we devise a Gradient-guided Adaptive Modulation\n(GAM) strategy, which dynamically adjusts the intensity of semantic transfer\nbased on gradient norm ratios. Without increasing deployment and inference\noverhead, our approach painlessly delivers striking and consistent performance\ngains across diverse DETR-based models, underscoring its practical utility for\nreal-time detection. Our new model family, RT-DETRv4, achieves state-of-the-art\nresults on COCO, attaining AP scores of 49.7/53.5/55.4/57.0 at corresponding\nspeeds of 273/169/124/78 FPS.\n","authors":["Zijun Liao","Yian Zhao","Xin Shan","Yu Yan","Chang Liu","Lei Lu","Xiangyang Ji","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2510.25257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10258v2","updated":"2025-10-29T07:51:00Z","published":"2025-04-14T14:19:57Z","title":"XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a\n  Novel Benchmark","summary":"  Document Reading Order Recovery is a fundamental task in document image\nunderstanding, playing a pivotal role in enhancing Retrieval-Augmented\nGeneration (RAG) and serving as a critical preprocessing step for large\nlanguage models (LLMs). Existing methods often struggle with complex\nlayouts(e.g., multi-column newspapers), high-overhead interactions between\ncross-modal elements (visual regions and textual semantics), and a lack of\nrobust evaluation benchmarks. We introduce XY-Cut++, an advanced layout\nordering method that integrates pre-mask processing, multi-granularity\nsegmentation, and cross-modal matching to address these challenges. Our method\nsignificantly enhances layout ordering accuracy compared to traditional XY-Cut\ntechniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8\nBLEU overall) while maintaining simplicity and efficiency. It outperforms\nexisting baselines by up to 24\\% and demonstrates consistent accuracy across\nsimple and complex layouts on the newly introduced DocBench-100 dataset. This\nadvancement establishes a reliable foundation for document structure recovery,\nsetting a new standard for layout ordering tasks and facilitating more\neffective RAG and LLM preprocessing.\n","authors":["Shuai Liu","Youmeng Li","Jizeng Wei"],"pdf_url":"https://arxiv.org/pdf/2504.10258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25239v1","updated":"2025-10-29T07:37:19Z","published":"2025-10-29T07:37:19Z","title":"Mapping and Classification of Trees Outside Forests using Deep Learning","summary":"  Trees Outside Forests (TOF) play an important role in agricultural landscapes\nby supporting biodiversity, sequestering carbon, and regulating microclimates.\nYet, most studies have treated TOF as a single class or relied on rigid\nrule-based thresholds, limiting ecological interpretation and adaptability\nacross regions. To address this, we evaluate deep learning for TOF\nclassification using a newly generated dataset and high-resolution aerial\nimagery from four agricultural landscapes in Germany. Specifically, we compare\nconvolutional neural networks (CNNs), vision transformers, and hybrid\nCNN-transformer models across six semantic segmentation architectures (ABCNet,\nLSKNet, FT-UNetFormer, DC-Swin, BANet, and U-Net) to map four categories of\nwoody vegetation: Forest, Patch, Linear, and Tree, derived from previous\nstudies and governmental products. Overall, the models achieved good\nclassification accuracy across the four landscapes, with the FT-UNetFormer\nperforming best (mean Intersection-over-Union 0.74; mean F1 score 0.84),\nunderscoring the importance of spatial context understanding in TOF mapping and\nclassification. Our results show good results for Forest and Linear class and\nreveal challenges particularly in classifying complex structures with high edge\ndensity, notably the Patch and Tree class. Our generalization experiments\nhighlight the need for regionally diverse training data to ensure reliable\nlarge-scale mapping. The dataset and code are openly available at\nhttps://github.com/Moerizzy/TOFMapper\n","authors":["Moritz Lucas","Hamid Ebrahimy","Viacheslav Barkov","Ralf Pecenka","Kai-Uwe KÃ¼hnberger","BjÃ¶rn Waske"],"pdf_url":"https://arxiv.org/pdf/2510.25239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25238v1","updated":"2025-10-29T07:37:08Z","published":"2025-10-29T07:37:08Z","title":"VADB: A Large-Scale Video Aesthetic Database with Professional and\n  Multi-Dimensional Annotations","summary":"  Video aesthetic assessment, a vital area in multimedia computing, integrates\ncomputer vision with human cognition. Its progress is limited by the lack of\nstandardized datasets and robust models, as the temporal dynamics of video and\nmultimodal fusion challenges hinder direct application of image-based methods.\nThis study introduces VADB, the largest video aesthetic database with 10,490\ndiverse videos annotated by 37 professionals across multiple aesthetic\ndimensions, including overall and attribute-specific aesthetic scores, rich\nlanguage comments and objective tags. We propose VADB-Net, a dual-modal\npre-training framework with a two-stage training strategy, which outperforms\nexisting video quality assessment models in scoring tasks and supports\ndownstream video aesthetic assessment tasks. The dataset and source code are\navailable at https://github.com/BestiVictory/VADB.\n","authors":["Qianqian Qiao","DanDan Zheng","Yihang Bo","Bao Peng","Heng Huang","Longteng Jiang","Huaye Wang","Jingdong Chen","Jun Zhou","Xin Jin"],"pdf_url":"https://arxiv.org/pdf/2510.25238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25237v1","updated":"2025-10-29T07:35:29Z","published":"2025-10-29T07:35:29Z","title":"DeepShield: Fortifying Deepfake Video Detection with Local and Global\n  Forgery Analysis","summary":"  Recent advances in deep generative models have made it easier to manipulate\nface videos, raising significant concerns about their potential misuse for\nfraud and misinformation. Existing detectors often perform well in in-domain\nscenarios but fail to generalize across diverse manipulation techniques due to\ntheir reliance on forgery-specific artifacts. In this work, we introduce\nDeepShield, a novel deepfake detection framework that balances local\nsensitivity and global generalization to improve robustness across unseen\nforgeries. DeepShield enhances the CLIP-ViT encoder through two key components:\nLocal Patch Guidance (LPG) and Global Forgery Diversification (GFD). LPG\napplies spatiotemporal artifact modeling and patch-wise supervision to capture\nfine-grained inconsistencies often overlooked by global models. GFD introduces\ndomain feature augmentation, leveraging domain-bridging and boundary-expanding\nfeature generation to synthesize diverse forgeries, mitigating overfitting and\nenhancing cross-domain adaptability. Through the integration of novel local and\nglobal analysis for deepfake detection, DeepShield outperforms state-of-the-art\nmethods in cross-dataset and cross-manipulation evaluations, achieving superior\nrobustness against unseen deepfake attacks.\n","authors":["Yinqi Cai","Jichang Li","Zhaolun Li","Weikai Chen","Rushi Lan","Xi Xie","Xiaonan Luo","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2510.25237v1.pdf","comment":"ICCV 2025"},{"id":"http://arxiv.org/abs/2510.25234v1","updated":"2025-10-29T07:29:21Z","published":"2025-10-29T07:29:21Z","title":"Learning Disentangled Speech- and Expression-Driven Blendshapes for 3D\n  Talking Face Animation","summary":"  Expressions are fundamental to conveying human emotions. With the rapid\nadvancement of AI-generated content (AIGC), realistic and expressive 3D facial\nanimation has become increasingly crucial. Despite recent progress in\nspeech-driven lip-sync for talking-face animation, generating emotionally\nexpressive talking faces remains underexplored. A major obstacle is the\nscarcity of real emotional 3D talking-face datasets due to the high cost of\ndata capture. To address this, we model facial animation driven by both speech\nand emotion as a linear additive problem. Leveraging a 3D talking-face dataset\nwith neutral expressions (VOCAset) and a dataset of 3D expression sequences\n(Florence4D), we jointly learn a set of blendshapes driven by speech and\nemotion. We introduce a sparsity constraint loss to encourage disentanglement\nbetween the two types of blendshapes while allowing the model to capture\ninherent secondary cross-domain deformations present in the training data. The\nlearned blendshapes can be further mapped to the expression and jaw pose\nparameters of the FLAME model, enabling the animation of 3D Gaussian avatars.\nQualitative and quantitative experiments demonstrate that our method naturally\ngenerates talking faces with specified expressions while maintaining accurate\nlip synchronization. Perceptual studies further show that our approach achieves\nsuperior emotional expressivity compared to existing methods, without\ncompromising lip-sync quality.\n","authors":["Yuxiang Mao","Zhijie Zhang","Zhiheng Zhang","Jiawei Liu","Chen Zeng","Shihong Xia"],"pdf_url":"https://arxiv.org/pdf/2510.25234v1.pdf","comment":"18 pages, 6 figures, accepted to ICXR 2025 conference"},{"id":"http://arxiv.org/abs/2510.21122v2","updated":"2025-10-29T07:06:34Z","published":"2025-10-24T03:23:34Z","title":"NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection\n  and Bayesian Estimation","summary":"  Reinforcement learning (RL) has shown promise in enhancing the general\nChain-of-Thought (CoT) reasoning capabilities of multimodal large language\nmodels (MLLMs). However, when applied to improve general CoT reasoning,\nexisting RL frameworks often struggle to generalize beyond the training\ndistribution. To address this, we propose NoisyGRPO, a systematic multimodal RL\nframework that introduces controllable noise into visual inputs for enhanced\nexploration and explicitly models the advantage estimation process via a\nBayesian framework. Specifically, NoisyGRPO improves RL training by: (1)\nNoise-Injected Exploration Policy: Perturbing visual inputs with Gaussian noise\nto encourage exploration across a wider range of visual scenarios; and (2)\nBayesian Advantage Estimation: Formulating advantage estimation as a principled\nBayesian inference problem, where the injected noise level serves as a prior\nand the observed trajectory reward as the likelihood. This Bayesian modeling\nfuses both sources of information to compute a robust posterior estimate of\ntrajectory advantage, effectively guiding MLLMs to prefer visually grounded\ntrajectories over noisy ones. Experiments on standard CoT quality, general\ncapability, and hallucination benchmarks demonstrate that NoisyGRPO\nsubstantially improves generalization and robustness, especially in RL settings\nwith small-scale MLLMs such as Qwen2.5-VL 3B. The project page is available at\nhttps://artanic30.github.io/project_pages/NoisyGRPO/.\n","authors":["Longtian Qiu","Shan Ning","Jiaxuan Sun","Xuming He"],"pdf_url":"https://arxiv.org/pdf/2510.21122v2.pdf","comment":"Accepted by Neurips2025, Project page at at\n  https://artanic30.github.io/project_pages/NoisyGRPO/"},{"id":"http://arxiv.org/abs/2510.25229v1","updated":"2025-10-29T07:06:01Z","published":"2025-10-29T07:06:01Z","title":"Balanced conic rectified flow","summary":"  Rectified flow is a generative model that learns smooth transport mappings\nbetween two distributions through an ordinary differential equation (ODE).\nUnlike diffusion-based generative models, which require costly numerical\nintegration of a generative ODE to sample images with state-of-the-art quality,\nrectified flow uses an iterative process called reflow to learn smooth and\nstraight ODE paths. This allows for relatively simple and efficient generation\nof high-quality images. However, rectified flow still faces several challenges.\n1) The reflow process requires a large number of generative pairs to preserve\nthe target distribution, leading to significant computational costs. 2) Since\nthe model is typically trained using only generated image pairs, its\nperformance heavily depends on the 1-rectified flow model, causing it to become\nbiased towards the generated data.\n  In this work, we experimentally expose the limitations of the original\nrectified flow and propose a novel approach that incorporates real images into\nthe training process. By preserving the ODE paths for real images, our method\neffectively reduces reliance on large amounts of generated data. Instead, we\ndemonstrate that the reflow process can be conducted efficiently using a much\nsmaller set of generated and real images. In CIFAR-10, we achieved\nsignificantly better FID scores, not only in one-step generation but also in\nfull-step simulations, while using only of the generative pairs compared to the\noriginal method. Furthermore, our approach induces straighter paths and avoids\nsaturation on generated images during reflow, leading to more robust ODE\nlearning while preserving the distribution of real images.\n","authors":["Kim Shin Seong","Mingi Kwon","Jaeseok Jeong","Youngjung Uh"],"pdf_url":"https://arxiv.org/pdf/2510.25229v1.pdf","comment":"Main paper: 10 pages (total 40 pages including appendix), 5 figures.\n  Accepted at NeurIPS 2025 (Poster). Acknowledgment: Supported by the NRF of\n  Korea (RS-2023-00223062) and IITP grants (RS-2020-II201361, RS-2024-00439762)\n  funded by the Korean government (MSIT)"},{"id":"http://arxiv.org/abs/2510.25227v1","updated":"2025-10-29T07:05:26Z","published":"2025-10-29T07:05:26Z","title":"Aligning What You Separate: Denoised Patch Mixing for Source-Free Domain\n  Adaptation in Medical Image Segmentation","summary":"  Source-Free Domain Adaptation (SFDA) is emerging as a compelling solution for\nmedical image segmentation under privacy constraints, yet current approaches\noften ignore sample difficulty and struggle with noisy supervision under domain\nshift. We present a new SFDA framework that leverages Hard Sample Selection and\nDenoised Patch Mixing to progressively align target distributions. First,\nunlabeled images are partitioned into reliable and unreliable subsets through\nentropy-similarity analysis, allowing adaptation to start from easy samples and\ngradually incorporate harder ones. Next, pseudo-labels are refined via Monte\nCarlo-based denoising masks, which suppress unreliable pixels and stabilize\ntraining. Finally, intra- and inter-domain objectives mix patches between\nsubsets, transferring reliable semantics while mitigating noise. Experiments on\nbenchmark datasets show consistent gains over prior SFDA and UDA methods,\ndelivering more accurate boundary delineation and achieving state-of-the-art\nDice and ASSD scores. Our study highlights the importance of progressive\nadaptation and denoised supervision for robust segmentation under domain shift.\n","authors":["Quang-Khai Bui-Tran","Thanh-Huy Nguyen","Hoang-Thien Nguyen","Ba-Thinh Lam","Nguyen Lan Vi Vu","Phat K. Huynh","Ulas Bagci","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2510.25227v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2508.12015v2","updated":"2025-10-29T07:05:00Z","published":"2025-08-16T11:17:31Z","title":"InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes","summary":"  Reconstructing dynamic driving scenes from dashcam videos has attracted\nincreasing attention due to its significance in autonomous driving and scene\nunderstanding. While recent advances have made impressive progress, most\nmethods still unify all background elements into a single representation,\nhindering both instance-level understanding and flexible scene editing. Some\napproaches attempt to lift 2D segmentation into 3D space, but often rely on\npre-processed instance IDs or complex pipelines to map continuous features to\ndiscrete identities. Moreover, these methods are typically designed for indoor\nscenes with rich viewpoints, making them less applicable to outdoor driving\nscenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian\nSplatting framework tailored for the interactive reconstruction of dynamic\ndriving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D\nfeature learning via contrastive loss and pseudo-supervised objectives. At the\n3D level, we introduce regularization to implicitly encode instance identities\nand enforce consistency through a voxel-based loss. A lightweight static\ncodebook further bridges continuous features and discrete identities without\nrequiring data pre-processing or complex optimization. Quantitative and\nqualitative experiments demonstrate the effectiveness of InstDrive, and to the\nbest of our knowledge, it is the first framework to achieve 3D instance\nsegmentation in dynamic, open-world driving scenes.More visualizations are\navailable at our project page.\n","authors":["Hongyuan Liu","Haochen Yu","Bochao Zou","Jianfei Jiang","Qiankun Liu","Jiansheng Chen","Huimin Ma"],"pdf_url":"https://arxiv.org/pdf/2508.12015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.17732v2","updated":"2025-10-29T07:04:04Z","published":"2025-04-24T16:46:32Z","title":"DPMambaIR: All-in-One Image Restoration via Degradation-Aware Prompt\n  State Space Model","summary":"  All-in-One image restoration aims to address multiple image degradation\nproblems using a single model, offering a more practical and versatile solution\ncompared to designing dedicated models for each degradation type. Existing\napproaches typically rely on Degradation-specific models or coarse-grained\ndegradation prompts to guide image restoration. However, they lack fine-grained\nmodeling of degradation information and face limitations in balancing\nmulti-task conflicts. To overcome these limitations, we propose DPMambaIR, a\nnovel All-in-One image restoration framework that introduces a fine-grained\ndegradation extractor and a Degradation-Aware Prompt State Space Model\n(DP-SSM). The DP-SSM leverages the fine-grained degradation features captured\nby the extractor as dynamic prompts, which are then incorporated into the state\nspace modeling process. This enhances the model's adaptability to diverse\ndegradation types, while a complementary High-Frequency Enhancement Block (HEB)\nrecovers local high-frequency details. Extensive experiments on a mixed dataset\ncontaining seven degradation types show that DPMambaIR achieves the best\nperformance, with 27.69dB and 0.893 in PSNR and SSIM, respectively. These\nresults highlight the potential and superiority of DPMambaIR as a unified\nsolution for All-in-One image restoration.\n","authors":["Zhanwen Liu","Sai Zhou","Yuchao Dai","Yang Wang","Yisheng An","Xiangmo Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.17732v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25221v1","updated":"2025-10-29T06:56:30Z","published":"2025-10-29T06:56:30Z","title":"MSF-Net: Multi-Stage Feature Extraction and Fusion for Robust\n  Photometric Stereo","summary":"  Photometric stereo is a technique aimed at determining surface normals\nthrough the utilization of shading cues derived from images taken under\ndifferent lighting conditions. However, existing learning-based approaches\noften fail to accurately capture features at multiple stages and do not\nadequately promote interaction between these features. Consequently, these\nmodels tend to extract redundant features, especially in areas with intricate\ndetails such as wrinkles and edges. To tackle these issues, we propose MSF-Net,\na novel framework for extracting information at multiple stages, paired with\nselective update strategy, aiming to extract high-quality feature information,\nwhich is critical for accurate normal construction. Additionally, we have\ndeveloped a feature fusion module to improve the interplay among different\nfeatures. Experimental results on the DiLiGenT benchmark show that our proposed\nMSF-Net significantly surpasses previous state-of-the-art methods in the\naccuracy of surface normal estimation.\n","authors":["Shiyu Qin","Zhihao Cai","Kaixuan Wang","Lin Qi","Junyu Dong"],"pdf_url":"https://arxiv.org/pdf/2510.25221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.16729v2","updated":"2025-10-29T06:53:04Z","published":"2025-10-19T06:45:37Z","title":"Vision-Centric 4D Occupancy Forecasting and Planning via Implicit\n  Residual World Models","summary":"  End-to-end autonomous driving systems increasingly rely on vision-centric\nworld models to understand and predict their environment. However, a common\nineffectiveness in these models is the full reconstruction of future scenes,\nwhich expends significant capacity on redundantly modeling static backgrounds.\nTo address this, we propose IR-WM, an Implicit Residual World Model that\nfocuses on modeling the current state and evolution of the world. IR-WM first\nestablishes a robust bird's-eye-view representation of the current state from\nthe visual observation. It then leverages the BEV features from the previous\ntimestep as a strong temporal prior and predicts only the \"residual\", i.e., the\nchanges conditioned on the ego-vehicle's actions and scene context. To\nalleviate error accumulation over time, we further apply an alignment module to\ncalibrate semantic and dynamic misalignments. Moreover, we investigate\ndifferent forecasting-planning coupling schemes and demonstrate that the\nimplicit future state generated by world models substantially improves planning\naccuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D\noccupancy forecasting and trajectory planning.\n","authors":["Jianbiao Mei","Yu Yang","Xuemeng Yang","Licheng Wen","Jiajun Lv","Botian Shi","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2510.16729v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25210v1","updated":"2025-10-29T06:20:21Z","published":"2025-10-29T06:20:21Z","title":"U-CAN: Unsupervised Point Cloud Denoising with Consistency-Aware\n  Noise2Noise Matching","summary":"  Point clouds captured by scanning sensors are often perturbed by noise, which\nhave a highly negative impact on downstream tasks (e.g. surface reconstruction\nand shape understanding). Previous works mostly focus on training neural\nnetworks with noisy-clean point cloud pairs for learning denoising priors,\nwhich requires extensively manual efforts. In this work, we introduce U-CAN, an\nUnsupervised framework for point cloud denoising with Consistency-Aware\nNoise2Noise matching. Specifically, we leverage a neural network to infer a\nmulti-step denoising path for each point of a shape or scene with a noise to\nnoise matching scheme. We achieve this by a novel loss which enables\nstatistical reasoning on multiple noisy point cloud observations. We further\nintroduce a novel constraint on the denoised geometry consistency for learning\nconsistency-aware denoising patterns. We justify that the proposed constraint\nis a general term which is not limited to 3D domain and can also contribute to\nthe area of 2D image denoising. Our evaluations under the widely used\nbenchmarks in point cloud denoising, upsampling and image denoising show\nsignificant improvement over the state-of-the-art unsupervised methods, where\nU-CAN also produces comparable results with the supervised methods.\n","authors":["Junsheng Zhou","Xingyu Shi","Haichuan Song","Yi Fang","Yu-Shen Liu","Zhizhong Han"],"pdf_url":"https://arxiv.org/pdf/2510.25210v1.pdf","comment":"Accepted by NeurIPS 2025. Project page:\n  https://gloriasze.github.io/U-CAN/"},{"id":"http://arxiv.org/abs/2505.19028v4","updated":"2025-10-29T06:12:40Z","published":"2025-05-25T08:28:03Z","title":"InfoChartQA: A Benchmark for Multimodal Question Answering on\n  Infographic Charts","summary":"  Understanding infographic charts with design-driven visual elements (e.g.,\npictograms, icons) requires both visual recognition and reasoning, posing\nchallenges for multimodal large language models (MLLMs). However, existing\nvisual-question answering benchmarks fall short in evaluating these\ncapabilities of MLLMs due to the lack of paired plain charts and\nvisual-element-based questions. To bridge this gap, we introduce InfoChartQA, a\nbenchmark for evaluating MLLMs on infographic chart understanding. It includes\n5,642 pairs of infographic and plain charts, each sharing the same underlying\ndata but differing in visual presentations. We further design\nvisual-element-based questions to capture their unique visual designs and\ncommunicative intent. Evaluation of 20 MLLMs reveals a substantial performance\ndecline on infographic charts, particularly for visual-element-based questions\nrelated to metaphors. The paired infographic and plain charts enable\nfine-grained error analysis and ablation studies, which highlight new\nopportunities for advancing MLLMs in infographic chart understanding. We\nrelease InfoChartQA at https://github.com/CoolDawnAnt/InfoChartQA.\n","authors":["Tianchi Xie","Minzhi Lin","Mengchen Liu","Yilin Ye","Changjian Chen","Shixia Liu"],"pdf_url":"https://arxiv.org/pdf/2505.19028v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25199v1","updated":"2025-10-29T06:09:17Z","published":"2025-10-29T06:09:17Z","title":"AI-Powered Early Detection of Critical Diseases using Image Processing\n  and Audio Analysis","summary":"  Early diagnosis of critical diseases can significantly improve patient\nsurvival and reduce treatment costs. However, existing diagnostic techniques\nare often costly, invasive, and inaccessible in low-resource regions. This\npaper presents a multimodal artificial intelligence (AI) diagnostic framework\nintegrating image analysis, thermal imaging, and audio signal processing for\nearly detection of three major health conditions: skin cancer, vascular blood\nclots, and cardiopulmonary abnormalities. A fine-tuned MobileNetV2\nconvolutional neural network was trained on the ISIC 2019 dataset for skin\nlesion classification, achieving 89.3% accuracy, 91.6% sensitivity, and 88.2%\nspecificity. A support vector machine (SVM) with handcrafted features was\nemployed for thermal clot detection, achieving 86.4% accuracy (AUC = 0.89) on\nsynthetic and clinical data. For cardiopulmonary analysis, lung and heart sound\ndatasets from PhysioNet and Pascal were processed using Mel-Frequency Cepstral\nCoefficients (MFCC) and classified via Random Forest, reaching 87.2% accuracy\nand 85.7% sensitivity. Comparative evaluation against state-of-the-art models\ndemonstrates that the proposed system achieves competitive results while\nremaining lightweight and deployable on low-cost devices. The framework\nprovides a promising step toward scalable, real-time, and accessible AI-based\npre-diagnostic healthcare solutions.\n","authors":["Manisha More","Kavya Bhand","Kaustubh Mukdam","Kavya Sharma","Manas Kawtikwar","Hridayansh Kaware","Prajwal Kavhar"],"pdf_url":"https://arxiv.org/pdf/2510.25199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.19638v3","updated":"2025-10-29T06:04:58Z","published":"2025-05-26T07:55:49Z","title":"HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and\n  Semantic Alignment","summary":"  Virtual try-on technology has become increasingly important in the fashion\nand retail industries, enabling the generation of high-fidelity garment images\nthat adapt seamlessly to target human models. While existing methods have\nachieved notable progress, they still face significant challenges in\nmaintaining consistency across different poses. Specifically, geometric\ndistortions lead to a lack of spatial consistency, mismatches in garment\nstructure and texture across poses result in semantic inconsistency, and the\nloss or distortion of fine-grained details diminishes visual fidelity. To\naddress these challenges, we propose HF-VTON, a novel framework that ensures\nhigh-fidelity virtual try-on performance across diverse poses. HF-VTON consists\nof three key modules: (1) the Appearance-Preserving Warp Alignment Module\n(APWAM), which aligns garments to human poses, addressing geometric\ndeformations and ensuring spatial consistency; (2) the Semantic Representation\nand Comprehension Module (SRCM), which captures fine-grained garment attributes\nand multi-pose data to enhance semantic representation, maintaining structural,\ntextural, and pattern consistency; and (3) the Multimodal Prior-Guided\nAppearance Generation Module (MPAGM), which integrates multimodal features and\nprior knowledge from pre-trained models to optimize appearance generation,\nensuring both semantic and geometric consistency. Additionally, to overcome\ndata limitations in existing benchmarks, we introduce the SAMP-VTONS dataset,\nfeaturing multi-pose pairs and rich textual annotations for a more\ncomprehensive evaluation. Experimental results demonstrate that HF-VTON\noutperforms state-of-the-art methods on both VITON-HD and SAMP-VTONS, excelling\nin visual fidelity, semantic consistency, and detail preservation.\n","authors":["Ming Meng","Qi Dong","Jiajie Li","Zhe Zhu","Xingyu Wang","Zhaoxin Fan","Wei Zhao","Wenjun Wu"],"pdf_url":"https://arxiv.org/pdf/2505.19638v3.pdf","comment":"After the publication of the paper, we discovered some significant\n  errors/omissions that need to be corrected and improved"},{"id":"http://arxiv.org/abs/2508.08549v3","updated":"2025-10-29T05:58:05Z","published":"2025-08-12T01:33:30Z","title":"Diverse Teaching and Label Propagation for Generic Semi-Supervised\n  Medical Image Segmentation","summary":"  Both limited annotation and domain shift are significant challenges\nfrequently encountered in medical image segmentation, leading to derivative\nscenarios like semi-supervised medical (SSMIS), semi-supervised medical domain\ngeneralization (Semi-MDG) and unsupervised medical domain adaptation (UMDA).\nConventional methods are generally tailored to specific tasks in isolation, the\nerror accumulation hinders the effective utilization of unlabeled data and\nlimits further improvements, resulting in suboptimal performance when these\nissues occur. In this paper, we aim to develop a generic framework that masters\nall three tasks. We found that the key to solving the problem lies in how to\ngenerate reliable pseudo labels for the unlabeled data in the presence of\ndomain shift with labeled data and increasing the diversity of the model. To\ntackle this issue, we employ a Diverse Teaching and Label Propagation Network\n(DTLP-Net) to boosting the Generic Semi-Supervised Medical Image Segmentation.\nOur DTLP-Net involves a single student model and two diverse teacher models,\nwhich can generate reliable pseudo-labels for the student model. The first\nteacher model decouple the training process with labeled and unlabeled data,\nThe second teacher is momentum-updated periodically, thus generating reliable\nyet divers pseudo-labels. To fully utilize the information within the data, we\nadopt inter-sample and intra-sample data augmentation to learn the global and\nlocal knowledge. In addition, to further capture the voxel-level correlations,\nwe propose label propagation to enhance the model robust. We evaluate our\nproposed framework on five benchmark datasets for SSMIS, UMDA, and Semi-MDG\ntasks. The results showcase notable improvements compared to state-of-the-art\nmethods across all five settings, indicating the potential of our framework to\ntackle more challenging SSL scenarios.\n","authors":["Wei Li","Pengcheng Zhou","Linye Ma","Wenyi Zhao","Huihua Yang","Yuchen Guo"],"pdf_url":"https://arxiv.org/pdf/2508.08549v3.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2510.25184v1","updated":"2025-10-29T05:30:16Z","published":"2025-10-29T05:30:16Z","title":"Mask-Robust Face Verification for Online Learning via YOLOv5 and\n  Residual Networks","summary":"  In the contemporary landscape, the fusion of information technology and the\nrapid advancement of artificial intelligence have ushered school education into\na transformative phase characterized by digitization and heightened\nintelligence. Concurrently, the global paradigm shift caused by the Covid-19\npandemic has catalyzed the evolution of e-learning, accentuating its\nsignificance. Amidst these developments, one pivotal facet of the online\neducation paradigm that warrants attention is the authentication of identities\nwithin the digital learning sphere. Within this context, our study delves into\na solution for online learning authentication, utilizing an enhanced\nconvolutional neural network architecture, specifically the residual network\nmodel. By harnessing the power of deep learning, this technological approach\naims to galvanize the ongoing progress of online education, while concurrently\nbolstering its security and stability. Such fortification is imperative in\nenabling online education to seamlessly align with the swift evolution of the\neducational landscape. This paper's focal proposition involves the deployment\nof the YOLOv5 network, meticulously trained on our proprietary dataset. This\nnetwork is tasked with identifying individuals' faces culled from images\ncaptured by students' open online cameras. The resultant facial information is\nthen channeled into the residual network to extract intricate features at a\ndeeper level. Subsequently, a comparative analysis of Euclidean distances\nagainst students' face databases is performed, effectively ascertaining the\nidentity of each student.\n","authors":["Zhifeng Wang","Minghui Wang","Chunyan Zeng","Jialong Yao","Yang Yang","Hongmin Xu"],"pdf_url":"https://arxiv.org/pdf/2510.25184v1.pdf","comment":"9 pages, 10 figures"},{"id":"http://arxiv.org/abs/2510.25175v1","updated":"2025-10-29T05:19:38Z","published":"2025-10-29T05:19:38Z","title":"Test-Time Adaptive Object Detection with Foundation Model","summary":"  In recent years, test-time adaptive object detection has attracted increasing\nattention due to its unique advantages in online domain adaptation, which\naligns more closely with real-world application scenarios. However, existing\napproaches heavily rely on source-derived statistical characteristics while\nmaking the strong assumption that the source and target domains share an\nidentical category space. In this paper, we propose the first foundation\nmodel-powered test-time adaptive object detection method that eliminates the\nneed for source data entirely and overcomes traditional closed-set limitations.\nSpecifically, we design a Multi-modal Prompt-based Mean-Teacher framework for\nvision-language detector-driven test-time adaptation, which incorporates text\nand visual prompt tuning to adapt both language and vision representation\nspaces on the test data in a parameter-efficient manner. Correspondingly, we\npropose a Test-time Warm-start strategy tailored for the visual prompts to\neffectively preserve the representation capability of the vision branch.\nFurthermore, to guarantee high-quality pseudo-labels in every test batch, we\nmaintain an Instance Dynamic Memory (IDM) module that stores high-quality\npseudo-labels from previous test samples, and propose two novel\nstrategies-Memory Enhancement and Memory Hallucination-to leverage IDM's\nhigh-quality instances for enhancing original predictions and hallucinating\nimages without available pseudo-labels, respectively. Extensive experiments on\ncross-corruption and cross-dataset benchmarks demonstrate that our method\nconsistently outperforms previous state-of-the-art methods, and can adapt to\narbitrary cross-domain and cross-category target data. Code is available at\nhttps://github.com/gaoyingjay/ttaod_foundation.\n","authors":["Yingjie Gao","Yanan Zhang","Zhi Cai","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2510.25175v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25174v1","updated":"2025-10-29T05:17:13Z","published":"2025-10-29T05:17:13Z","title":"Classifier Enhancement Using Extended Context and Domain Experts for\n  Semantic Segmentation","summary":"  Prevalent semantic segmentation methods generally adopt a vanilla classifier\nto categorize each pixel into specific classes.\n  Although such a classifier learns global information from the training data,\nthis information is represented by a set of fixed parameters (weights and\nbiases).\n  However, each image has a different class distribution, which prevents the\nclassifier from addressing the unique characteristics of individual images.\n  At the dataset level, class imbalance leads to segmentation results being\nbiased towards majority classes, limiting the model's effectiveness in\nidentifying and segmenting minority class regions.\n  In this paper, we propose an Extended Context-Aware Classifier (ECAC) that\ndynamically adjusts the classifier using global (dataset-level) and local\n(image-level) contextual information.\n  Specifically, we leverage a memory bank to learn dataset-level contextual\ninformation of each class, incorporating the class-specific contextual\ninformation from the current image to improve the classifier for precise pixel\nlabeling.\n  Additionally, a teacher-student network paradigm is adopted, where the domain\nexpert (teacher network) dynamically adjusts contextual information with ground\ntruth and transfers knowledge to the student network.\n  Comprehensive experiments illustrate that the proposed ECAC can achieve\nstate-of-the-art performance across several datasets, including ADE20K,\nCOCO-Stuff10K, and Pascal-Context.\n","authors":["Huadong Tang","Youpeng Zhao","Min Xu","Jun Wang","Qiang Wu"],"pdf_url":"https://arxiv.org/pdf/2510.25174v1.pdf","comment":"Accepted at IEEE TRANSACTIONS ON MULTIMEDIA (TMM)"},{"id":"http://arxiv.org/abs/2510.25173v1","updated":"2025-10-29T05:13:09Z","published":"2025-10-29T05:13:09Z","title":"$D^2GS$: Dense Depth Regularization for LiDAR-free Urban Scene\n  Reconstruction","summary":"  Recently, Gaussian Splatting (GS) has shown great potential for urban scene\nreconstruction in the field of autonomous driving. However, current urban scene\nreconstruction methods often depend on multimodal sensors as inputs,\n\\textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDAR\npoint clouds can largely mitigate ill-posedness in reconstruction, acquiring\nsuch accurate LiDAR data is still challenging in practice: i) precise\nspatiotemporal calibration between LiDAR and other sensors is required, as they\nmay not capture data simultaneously; ii) reprojection errors arise from spatial\nmisalignment when LiDAR and cameras are mounted at different locations. To\navoid the difficulty of acquiring accurate LiDAR depth, we propose $D^2GS$, a\nLiDAR-free urban scene reconstruction framework. In this work, we obtain\ngeometry priors that are as effective as LiDAR while being denser and more\naccurate. $\\textbf{First}$, we initialize a dense point cloud by\nback-projecting multi-view metric depth predictions. This point cloud is then\noptimized by a Progressive Pruning strategy to improve the global consistency.\n$\\textbf{Second}$, we jointly refine Gaussian geometry and predicted dense\nmetric depth via a Depth Enhancer. Specifically, we leverage diffusion priors\nfrom a depth foundation model to enhance the depth maps rendered by Gaussians.\nIn turn, the enhanced depths provide stronger geometric constraints during\nGaussian training. $\\textbf{Finally}$, we improve the accuracy of ground\ngeometry by constraining the shape and normal attributes of Gaussians within\nroad regions. Extensive experiments on the Waymo dataset demonstrate that our\nmethod consistently outperforms state-of-the-art methods, producing more\naccurate geometry even when compared with those using ground-truth LiDAR data.\n","authors":["Kejing Xia","Jidong Jia","Ke Jin","Yucai Bai","Li Sun","Dacheng Tao","Youjian Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.25173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25166v1","updated":"2025-10-29T04:57:49Z","published":"2025-10-29T04:57:49Z","title":"A Study on Inference Latency for Vision Transformers on Mobile Devices","summary":"  Given the significant advances in machine learning techniques on mobile\ndevices, particularly in the domain of computer vision, in this work we\nquantitatively study the performance characteristics of 190 real-world vision\ntransformers (ViTs) on mobile devices. Through a comparison with 102 real-world\nconvolutional neural networks (CNNs), we provide insights into the factors that\ninfluence the latency of ViT architectures on mobile devices. Based on these\ninsights, we develop a dataset including measured latencies of 1000 synthetic\nViTs with representative building blocks and state-of-the-art architectures\nfrom two machine learning frameworks and six mobile platforms. Using this\ndataset, we show that inference latency of new ViTs can be predicted with\nsufficient accuracy for real-world applications.\n","authors":["Zhuojin Li","Marco Paolieri","Leana Golubchik"],"pdf_url":"https://arxiv.org/pdf/2510.25166v1.pdf","comment":"To appear in Springer LNICST, volume 663, Proceedings of VALUETOOLS\n  2024"},{"id":"http://arxiv.org/abs/2510.25164v1","updated":"2025-10-29T04:49:20Z","published":"2025-10-29T04:49:20Z","title":"Transformers in Medicine: Improving Vision-Language Alignment for\n  Medical Image Captioning","summary":"  We present a transformer-based multimodal framework for generating clinically\nrelevant captions for MRI scans. Our system combines a DEiT-Small vision\ntransformer as an image encoder, MediCareBERT for caption embedding, and a\ncustom LSTM-based decoder. The architecture is designed to semantically align\nimage and textual embeddings, using hybrid cosine-MSE loss and contrastive\ninference via vector similarity. We benchmark our method on the MultiCaRe\ndataset, comparing performance on filtered brain-only MRIs versus general MRI\nimages against state-of-the-art medical image captioning methods including\nBLIP, R2GenGPT, and recent transformer-based approaches. Results show that\nfocusing on domain-specific data improves caption accuracy and semantic\nalignment. Our work proposes a scalable, interpretable solution for automated\nmedical image reporting.\n","authors":["Yogesh Thakku Suresh","Vishwajeet Shivaji Hogale","Luca-Alexandru Zamfira","Anandavardhana Hegde"],"pdf_url":"https://arxiv.org/pdf/2510.25164v1.pdf","comment":"This work is to appear in the Proceedings of MICAD 2025, the 6th\n  International Conference on Medical Imaging and Computer-Aided Diagnosis"},{"id":"http://arxiv.org/abs/2510.25163v1","updated":"2025-10-29T04:49:15Z","published":"2025-10-29T04:49:15Z","title":"Target-Guided Bayesian Flow Networks for Quantitatively Constrained CAD\n  Generation","summary":"  Deep generative models, such as diffusion models, have shown promising\nprogress in image generation and audio generation via simplified continuity\nassumptions. However, the development of generative modeling techniques for\ngenerating multi-modal data, such as parametric CAD sequences, still lags\nbehind due to the challenges in addressing long-range constraints and parameter\nsensitivity. In this work, we propose a novel framework for quantitatively\nconstrained CAD generation, termed Target-Guided Bayesian Flow Network (TGBFN).\nFor the first time, TGBFN handles the multi-modality of CAD sequences (i.e.,\ndiscrete commands and continuous parameters) in a unified continuous and\ndifferentiable parameter space rather than in the discrete data space. In\naddition, TGBFN penetrates the parameter update kernel and introduces a guided\nBayesian flow to control the CAD properties. To evaluate TGBFN, we construct a\nnew dataset for quantitatively constrained CAD generation. Extensive\ncomparisons across single-condition and multi-condition constrained generation\ntasks demonstrate that TGBFN achieves state-of-the-art performance in\ngenerating high-fidelity, condition-aware CAD sequences. The code is available\nat https://github.com/scu-zwh/TGBFN.\n","authors":["Wenhao Zheng","Chenwei Sun","Wenbo Zhang","Jiancheng Lv","Xianggen Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06497v3","updated":"2025-10-29T04:35:35Z","published":"2025-03-09T07:53:19Z","title":"Evaluation of Safety Cognition Capability in Vision-Language Models for\n  Autonomous Driving","summary":"  Ensuring the safety of vision-language models (VLMs) in autonomous driving\nsystems is of paramount importance, yet existing research has largely focused\non conventional benchmarks rather than safety-critical evaluation. In this\nwork, we present SCD-Bench (Safety Cognition Driving Benchmark) a novel\nframework specifically designed to assess the safety cognition capabilities of\nVLMs within interactive driving scenarios. To address the scalability challenge\nof data annotation, we introduce ADA (Autonomous Driving Annotation), a\nsemi-automated labeling system, further refined through expert review by\nprofessionals with domain-specific knowledge in autonomous driving. To\nfacilitate scalable and consistent evaluation, we also propose an automated\nassessment pipeline leveraging large language models, which demonstrates over\n98% agreement with human expert judgments. In addressing the broader challenge\nof aligning VLMs with safety cognition in driving environments, we construct\nSCD-Training, the first large-scale dataset tailored for this task, comprising\n324.35K high-quality samples. Through extensive experiments, we show that\nmodels trained on SCD-Training exhibit marked improvements not only on\nSCD-Bench, but also on general and domain-specific benchmarks, offering a new\nperspective on enhancing safety-aware interactions in vision-language systems\nfor autonomous driving.\n","authors":["Enming Zhang","Peizhe Gong","Xingyuan Dai","Min Huang","Yisheng Lv","Qinghai Miao"],"pdf_url":"https://arxiv.org/pdf/2503.06497v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.05746v2","updated":"2025-10-29T04:32:16Z","published":"2025-09-06T15:35:37Z","title":"Depth-Aware Super-Resolution via Distance-Adaptive Variational\n  Formulation","summary":"  Single image super-resolution traditionally assumes spatially-invariant\ndegradation models, yet real-world imaging systems exhibit complex\ndistance-dependent effects including atmospheric scattering, depth-of-field\nvariations, and perspective distortions. This fundamental limitation\nnecessitates spatially-adaptive reconstruction strategies that explicitly\nincorporate geometric scene understanding for optimal performance. We propose a\nrigorous variational framework that characterizes super-resolution as a\nspatially-varying inverse problem, formulating the degradation operator as a\npseudodifferential operator with distance-dependent spectral characteristics\nthat enable theoretical analysis of reconstruction limits across depth ranges.\nOur neural architecture implements discrete gradient flow dynamics through\ncascaded residual blocks with depth-conditional convolution kernels, ensuring\nconvergence to stationary points of the theoretical energy functional while\nincorporating learned distance-adaptive regularization terms that dynamically\nadjust smoothness constraints based on local geometric structure. Spectral\nconstraints derived from atmospheric scattering theory prevent bandwidth\nviolations and noise amplification in far-field regions, while adaptive kernel\ngeneration networks learn continuous mappings from depth to reconstruction\nfilters. Comprehensive evaluation across five benchmark datasets demonstrates\nstate-of-the-art performance, achieving 36.89/0.9516 and 30.54/0.8721 PSNR/SSIM\nat 2 and 4 scales on KITTI outdoor scenes, outperforming existing methods by\n0.44dB and 0.36dB respectively. This work establishes the first\ntheoretically-grounded distance-adaptive super-resolution framework and\ndemonstrates significant improvements on depth-variant scenarios while\nmaintaining competitive performance across traditional benchmarks.\n","authors":["Tianhao Guo","Bingjie Lu","Feng Wang","Zhengyang Lu"],"pdf_url":"https://arxiv.org/pdf/2509.05746v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25157v1","updated":"2025-10-29T04:19:52Z","published":"2025-10-29T04:19:52Z","title":"Towards Real-Time Inference of Thin Liquid Film Thickness Profiles from\n  Interference Patterns Using Vision Transformers","summary":"  Thin film interferometry is a powerful technique for non-invasively measuring\nliquid film thickness with applications in ophthalmology, but its clinical\ntranslation is hindered by the challenges in reconstructing thickness profiles\nfrom interference patterns - an ill-posed inverse problem complicated by phase\nperiodicity, imaging noise and ambient artifacts. Traditional reconstruction\nmethods are either computationally intensive, sensitive to noise, or require\nmanual expert analysis, which is impractical for real-time diagnostics. To\naddress this challenge, here we present a vision transformer-based approach for\nreal-time inference of thin liquid film thickness profiles directly from\nisolated interferograms. Trained on a hybrid dataset combining\nphysiologically-relevant synthetic and experimental tear film data, our model\nleverages long-range spatial correlations to resolve phase ambiguities and\nreconstruct temporally coherent thickness profiles in a single forward pass\nfrom dynamic interferograms acquired in vivo and ex vivo. The network\ndemonstrates state-of-the-art performance on noisy, rapidly-evolving films with\nmotion artifacts, overcoming limitations of conventional phase-unwrapping and\niterative fitting methods. Our data-driven approach enables automated,\nconsistent thickness reconstruction at real-time speeds on consumer hardware,\nopening new possibilities for continuous monitoring of pre-lens ocular tear\nfilms and non-invasive diagnosis of conditions such as the dry eye disease.\n","authors":["Gautam A. Viruthagiri","Arnuv Tandon","Gerald G. Fuller","Vinny Chandran Suja"],"pdf_url":"https://arxiv.org/pdf/2510.25157v1.pdf","comment":"6 pages, 2 figures, will be updated"},{"id":"http://arxiv.org/abs/2510.23807v2","updated":"2025-10-29T04:15:53Z","published":"2025-10-27T19:44:52Z","title":"Why Foundation Models in Pathology Are Failing","summary":"  In non-medical domains, foundation models (FMs) have revolutionized computer\nvision and language processing through large-scale self-supervised and\nmultimodal learning. Consequently, their rapid adoption in computational\npathology was expected to deliver comparable breakthroughs in cancer diagnosis,\nprognostication, and multimodal retrieval. However, recent systematic\nevaluations reveal fundamental weaknesses: low diagnostic accuracy, poor\nrobustness, geometric instability, heavy computational demands, and concerning\nsafety vulnerabilities. This short paper examines these shortcomings and argues\nthat they stem from deeper conceptual mismatches between the assumptions\nunderlying generic foundation modeling in mainstream AI and the intrinsic\ncomplexity of human tissue. Seven interrelated causes are identified:\nbiological complexity, ineffective self-supervision, overgeneralization,\nexcessive architectural complexity, lack of domain-specific innovation,\ninsufficient data, and a fundamental design flaw related to tissue patch size.\nThese findings suggest that current pathology foundation models remain\nconceptually misaligned with the nature of tissue morphology and call for a\nfundamental rethinking of the paradigm itself.\n","authors":["Hamid R. Tizhoosh"],"pdf_url":"https://arxiv.org/pdf/2510.23807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22946v2","updated":"2025-10-29T04:07:45Z","published":"2025-10-27T02:59:57Z","title":"LightBagel: A Light-weighted, Double Fusion Framework for Unified\n  Multimodal Understanding and Generation","summary":"  Unified multimodal models have recently shown remarkable gains in both\ncapability and versatility, yet most leading systems are still trained from\nscratch and require substantial computational resources. In this paper, we show\nthat competitive performance can be obtained far more efficiently by\nstrategically fusing publicly available models specialized for either\ngeneration or understanding. Our key design is to retain the original blocks\nwhile additionally interleaving multimodal self-attention blocks throughout the\nnetworks. This double fusion mechanism (1) effectively enables rich multi-modal\nfusion while largely preserving the original strengths of the base models, and\n(2) catalyzes synergistic fusion of high-level semantic representations from\nthe understanding encoder with low-level spatial signals from the generation\nencoder. By training with only ~ 35B tokens, this approach achieves strong\nresults across multiple benchmarks: 0.91 on GenEval for compositional\ntext-to-image generation, 82.16 on DPG-Bench for complex text-to-image\ngeneration, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By\nfully releasing the entire suite of code, model weights, and datasets, we hope\nto support future research on unified multimodal modeling.\n","authors":["Zeyu Wang","Zilong Chen","Chenhui Gou","Feng Li","Chaorui Deng","Deyao Zhu","Kunchang Li","Weihao Yu","Haoqin Tu","Haoqi Fan","Cihang Xie"],"pdf_url":"https://arxiv.org/pdf/2510.22946v2.pdf","comment":"Withdrawn because the submission was premature and not agreed by all\n  parties in collaboration"},{"id":"http://arxiv.org/abs/2505.03318v3","updated":"2025-10-29T04:02:02Z","published":"2025-05-06T08:46:41Z","title":"Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning","summary":"  Recent advances in multimodal Reward Models (RMs) have shown significant\npromise in delivering reward signals to align vision models with human\npreferences. However, current RMs are generally restricted to providing direct\nresponses or engaging in shallow reasoning processes with limited depth, often\nleading to inaccurate reward signals. We posit that incorporating explicit long\nchains of thought (CoT) into the reward reasoning process can significantly\nstrengthen their reliability and robustness. Furthermore, we believe that once\nRMs internalize CoT reasoning, their direct response accuracy can also be\nimproved through implicit reasoning capabilities. To this end, this paper\nproposes UnifiedReward-Think, the first unified multimodal CoT-based reward\nmodel, capable of multi-dimensional, step-by-step long-chain reasoning for both\nvisual understanding and generation reward tasks. Specifically, we adopt an\nexploration-driven reinforcement fine-tuning approach to elicit and incentivize\nthe model's latent complex reasoning ability: (1) We first use a small amount\nof image generation preference data to distill the reasoning process of GPT-4o,\nwhich is then used for the model's cold start to learn the format and structure\nof CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge\nand generalization capabilities, we prepare large-scale unified multimodal\npreference data to elicit the model's reasoning process across various vision\ntasks. During this phase, correct reasoning outputs are retained for rejection\nsampling to refine the model (3) while incorrect predicted samples are finally\nused for Group Relative Policy Optimization (GRPO) based reinforcement\nfine-tuning, enabling the model to explore diverse reasoning paths and optimize\nfor correct and robust solutions. Extensive experiments across various vision\nreward tasks demonstrate the superiority of our model.\n","authors":["Yibin Wang","Zhimin Li","Yuhang Zang","Chunyu Wang","Qinglin Lu","Cheng Jin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2505.03318v3.pdf","comment":"[NeurIPS2025] Project Page:\n  https://codegoat24.github.io/UnifiedReward/think"},{"id":"http://arxiv.org/abs/2510.25146v1","updated":"2025-10-29T03:56:41Z","published":"2025-10-29T03:56:41Z","title":"EA3D: Online Open-World 3D Object Extraction from Streaming Videos","summary":"  Current 3D scene understanding methods are limited by offline-collected\nmulti-view data or pre-constructed 3D geometry. In this paper, we present\nExtractAnything3D (EA3D), a unified online framework for open-world 3D object\nextraction that enables simultaneous geometric reconstruction and holistic\nscene understanding. Given a streaming video, EA3D dynamically interprets each\nframe using vision-language and 2D vision foundation encoders to extract\nobject-level knowledge. This knowledge is integrated and embedded into a\nGaussian feature map via a feed-forward online update strategy. We then\niteratively estimate visual odometry from historical frames and incrementally\nupdate online Gaussian features with new observations. A recurrent joint\noptimization module directs the model's attention to regions of interest,\nsimultaneously enhancing both geometric reconstruction and semantic\nunderstanding. Extensive experiments across diverse benchmarks and tasks,\nincluding photo-realistic rendering, semantic and instance segmentation, 3D\nbounding box and semantic occupancy estimation, and 3D mesh generation,\ndemonstrate the effectiveness of EA3D. Our method establishes a unified and\nefficient framework for joint online 3D reconstruction and holistic scene\nunderstanding, enabling a broad range of downstream tasks.\n","authors":["Xiaoyu Zhou","Jingqi Wang","Yuang Jia","Yongtao Wang","Deqing Sun","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2510.25146v1.pdf","comment":"The Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems(NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.25141v1","updated":"2025-10-29T03:45:03Z","published":"2025-10-29T03:45:03Z","title":"Revisiting Reconstruction-based AI-generated Image Detection: A\n  Geometric Perspective","summary":"  The rise of generative Artificial Intelligence (AI) has made detecting\nAI-generated images a critical challenge for ensuring authenticity. Existing\nreconstruction-based methods lack theoretical foundations and on empirical\nheuristics, limiting interpretability and reliability. In this paper, we\nintroduce the Jacobian-Spectral Lower Bound for reconstruction error from a\ngeometric perspective, showing that real images off the reconstruction manifold\nexhibit a non-trivial error lower bound, while generated images on the manifold\nhave near-zero error. Furthermore, we reveal the limitations of existing\nmethods that rely on static reconstruction error from a single pass. These\nmethods often fail when some real images exhibit lower error than generated\nones. This counterintuitive behavior reduces detection accuracy and requires\ndata-specific threshold tuning, limiting their applicability in real-world\nscenarios. To address these challenges, we propose ReGap, a training-free\nmethod that computes dynamic reconstruction error by leveraging structured\nediting operations to introduce controlled perturbations. This enables\nmeasuring error changes before and after editing, improving detection accuracy\nby enhancing error separation. Experimental results show that our method\noutperforms existing baselines, exhibits robustness to common post-processing\noperations and generalizes effectively across diverse conditions.\n","authors":["Wan Jiang","Jing Yan","Ruixuan Zhang","Xiaojing Chen","Changtao Miao","Zhe Li","Chenhao Lin","Yunfeng Diao","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2510.25141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25140v1","updated":"2025-10-29T03:40:40Z","published":"2025-10-29T03:40:40Z","title":"DINO-YOLO: Self-Supervised Pre-training for Data-Efficient Object\n  Detection in Civil Engineering Applications","summary":"  Object detection in civil engineering applications is constrained by limited\nannotated data in specialized domains. We introduce DINO-YOLO, a hybrid\narchitecture combining YOLOv12 with DINOv3 self-supervised vision transformers\nfor data-efficient detection. DINOv3 features are strategically integrated at\ntwo locations: input preprocessing (P0) and mid-backbone enhancement (P3).\nExperimental validation demonstrates substantial improvements: Tunnel Segment\nCrack detection (648 images) achieves 12.4% improvement, Construction PPE (1K\nimages) gains 13.7%, and KITTI (7K images) shows 88.6% improvement, while\nmaintaining real-time inference (30-47 FPS). Systematic ablation across five\nYOLO scales and nine DINOv3 variants reveals that Medium-scale architectures\nachieve optimal performance with DualP0P3 integration (55.77% mAP@0.5), while\nSmall-scale requires Triple Integration (53.63%). The 2-4x inference overhead\n(21-33ms versus 8-16ms baseline) remains acceptable for field deployment on\nNVIDIA RTX 5090. DINO-YOLO establishes state-of-the-art performance for civil\nengineering datasets (<10K images) while preserving computational efficiency,\nproviding practical solutions for construction safety monitoring and\ninfrastructure inspection in data-constrained environments.\n","authors":["Malaisree P","Youwai S","Kitkobsin T","Janrungautai S","Amorndechaphon D","Rojanavasu P"],"pdf_url":"https://arxiv.org/pdf/2510.25140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06677v2","updated":"2025-10-29T03:38:36Z","published":"2025-06-07T06:15:49Z","title":"RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic\n  Manipulation Evaluation","summary":"  Recent advances in vision-language models (VLMs) have enabled\ninstruction-conditioned robotic systems with improved generalization. However,\nmost existing work focuses on reactive System 1 policies, underutilizing VLMs'\nstrengths in semantic reasoning and long-horizon planning. These System 2\ncapabilities-characterized by deliberative, goal-directed thinking-remain under\nexplored due to the limited temporal scale and structural complexity of current\nbenchmarks. To address this gap, we introduce RoboCerebra, a benchmark for\nevaluating high-level reasoning in long-horizon robotic manipulation.\nRoboCerebra includes: (1) a large-scale simulation dataset with extended task\nhorizons and diverse subtask sequences in household environments; (2) a\nhierarchical framework combining a high-level VLM planner with a low-level\nvision-language-action (VLA) controller; and (3) an evaluation protocol\ntargeting planning, reflection, and memory through structured System 1-System 2\ninteraction. The dataset is constructed via a top-down pipeline, where GPT\ngenerates task instructions and decomposes them into subtask sequences. Human\noperators execute the subtasks in simulation, yielding high-quality\ntrajectories with dynamic object variations. Compared to prior benchmarks,\nRoboCerebra features significantly longer action sequences and denser\nannotations. We further benchmark state-of-the-art VLMs as System 2 modules and\nanalyze their performance across key cognitive dimensions, advancing the\ndevelopment of more capable and generalizable robotic planners.\n","authors":["Songhao Han","Boxiang Qiu","Yue Liao","Siyuan Huang","Chen Gao","Shuicheng Yan","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2506.06677v2.pdf","comment":"25 pages, 18 figures, Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25134v1","updated":"2025-10-29T03:28:18Z","published":"2025-10-29T03:28:18Z","title":"Region-CAM: Towards Accurate Object Regions in Class Activation Maps for\n  Weakly Supervised Learning Tasks","summary":"  Class Activation Mapping (CAM) methods are widely applied in weakly\nsupervised learning tasks due to their ability to highlight object regions.\nHowever, conventional CAM methods highlight only the most discriminative\nregions of the target. These highlighted regions often fail to cover the entire\nobject and are frequently misaligned with object boundaries, thereby limiting\nthe performance of downstream weakly supervised learning tasks, particularly\nWeakly Supervised Semantic Segmentation (WSSS), which demands pixel-wise\naccurate activation maps to get the best results. To alleviate the above\nproblems, we propose a novel activation method, Region-CAM. Distinct from\nnetwork feature weighting approaches, Region-CAM generates activation maps by\nextracting semantic information maps (SIMs) and performing semantic information\npropagation (SIP) by considering both gradients and features in each of the\nstages of the baseline classification model. Our approach highlights a greater\nproportion of object regions while ensuring activation maps to have precise\nboundaries that align closely with object edges. Region-CAM achieves 60.12% and\n58.43% mean intersection over union (mIoU) using the baseline model on the\nPASCAL VOC training and validation datasets, respectively, which are\nimprovements of 13.61% and 13.13% over the original CAM (46.51% and 45.30%). On\nthe MS COCO validation set, Region-CAM achieves 36.38%, a 16.23% improvement\nover the original CAM (20.15%). We also demonstrate the superiority of\nRegion-CAM in object localization tasks, using the ILSVRC2012 validation set.\nRegion-CAM achieves 51.7% in Top-1 Localization accuracy Loc1. Compared with\nLayerCAM, an activation method designed for weakly supervised object\nlocalization, Region-CAM achieves 4.5% better performance in Loc1.\n","authors":["Qingdong Cai","Charith Abhayaratne"],"pdf_url":"https://arxiv.org/pdf/2510.25134v1.pdf","comment":"Preprint for journal paper"},{"id":"http://arxiv.org/abs/2510.25129v1","updated":"2025-10-29T03:17:58Z","published":"2025-10-29T03:17:58Z","title":"AtlasGS: Atlanta-world Guided Surface Reconstruction with Implicit\n  Structured Gaussians","summary":"  3D reconstruction of indoor and urban environments is a prominent research\ntopic with various downstream applications. However, existing geometric priors\nfor addressing low-texture regions in indoor and urban settings often lack\nglobal consistency. Moreover, Gaussian Splatting and implicit SDF fields often\nsuffer from discontinuities or exhibit computational inefficiencies, resulting\nin a loss of detail. To address these issues, we propose an Atlanta-world\nguided implicit-structured Gaussian Splatting that achieves smooth indoor and\nurban scene reconstruction while preserving high-frequency details and\nrendering efficiency. By leveraging the Atlanta-world model, we ensure the\naccurate surface reconstruction for low-texture regions, while the proposed\nnovel implicit-structured GS representations provide smoothness without\nsacrificing efficiency and high-frequency details. Specifically, we propose a\nsemantic GS representation to predict the probability of all semantic regions\nand deploy a structure plane regularization with learnable plane indicators for\nglobal accurate surface reconstruction. Extensive experiments demonstrate that\nour method outperforms state-of-the-art approaches in both indoor and urban\nscenes, delivering superior surface reconstruction quality.\n","authors":["Xiyu Zhang","Chong Bao","Yipeng Chen","Hongjia Zhai","Yitong Dong","Hujun Bao","Zhaopeng Cui","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.25129v1.pdf","comment":"18 pages, 11 figures. NeurIPS 2025; Project page:\n  https://zju3dv.github.io/AtlasGS/"},{"id":"http://arxiv.org/abs/2503.11245v4","updated":"2025-10-29T03:13:22Z","published":"2025-03-14T09:52:54Z","title":"L2RSI: Cross-view LiDAR-based Place Recognition for Large-scale Urban\n  Scenes via Remote Sensing Imagery","summary":"  We tackle the challenge of LiDAR-based place recognition, which traditionally\ndepends on costly and time-consuming prior 3D maps. To overcome this, we first\nconstruct LiRSI-XA dataset, which encompasses approximately $110,000$ remote\nsensing submaps and $13,000$ LiDAR point cloud submaps captured in urban\nscenes, and propose a novel method, L2RSI, for cross-view LiDAR place\nrecognition using high-resolution Remote Sensing Imagery. This approach enables\nlarge-scale localization capabilities at a reduced cost by leveraging readily\navailable overhead images as map proxies. L2RSI addresses the dual challenges\nof cross-view and cross-modal place recognition by learning feature alignment\nbetween point cloud submaps and remote sensing submaps in the semantic domain.\nAdditionally, we introduce a novel probability propagation method based on\nparticle estimation to refine position predictions, effectively leveraging\ntemporal and spatial information. This approach enables large-scale retrieval\nand cross-scene generalization without fine-tuning. Extensive experiments on\nLiRSI-XA demonstrate that, within a $100km^2$ retrieval range, L2RSI accurately\nlocalizes $83.27\\%$ of point cloud submaps within a $30m$ radius for top-$1$\nretrieved location. Our project page is publicly available at\nhttps://shizw695.github.io/L2RSI/.\n","authors":["Ziwei Shi","Xiaoran Zhang","Wenjing Xu","Yan Xia","Yu Zang","Siqi Shen","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2503.11245v4.pdf","comment":"17 pages, 7 figures, NeurIPS 2025"},{"id":"http://arxiv.org/abs/2509.22689v2","updated":"2025-10-29T02:47:25Z","published":"2025-09-19T04:41:03Z","title":"Graph-Theoretic Consistency for Robust and Topology-Aware\n  Semi-Supervised Histopathology Segmentation","summary":"  Semi-supervised semantic segmentation (SSSS) is vital in computational\npathology, where dense annotations are costly and limited. Existing methods\noften rely on pixel-level consistency, which propagates noisy pseudo-labels and\nproduces fragmented or topologically invalid masks. We propose Topology Graph\nConsistency (TGC), a framework that integrates graph-theoretic constraints by\naligning Laplacian spectra, component counts, and adjacency statistics between\nprediction graphs and references. This enforces global topology and improves\nsegmentation accuracy. Experiments on GlaS and CRAG demonstrate that TGC\nachieves state-of-the-art performance under 5-10% supervision and significantly\nnarrows the gap to full supervision.\n","authors":["Ha-Hieu Pham","Minh Le","Han Huynh","Nguyen Quoc Khanh Le","Huy-Hieu Pham"],"pdf_url":"https://arxiv.org/pdf/2509.22689v2.pdf","comment":"Accepted to the AAAI 2026 Student Abstract and Poster Program"},{"id":"http://arxiv.org/abs/2405.04605v4","updated":"2025-10-29T02:33:21Z","published":"2024-05-07T18:36:40Z","title":"AI in Lung Health: Benchmarking Detection and Diagnostic Models Across\n  Multiple CT Scan Datasets","summary":"  Background: Development of artificial intelligence (AI) models for lung\ncancer screening requires large, well-annotated low-dose computed tomography\n(CT) datasets and rigorous performance benchmarks. Purpose: To create a\nreproducible benchmarking resource leveraging the Duke Lung Cancer Screening\n(DLCS) and multiple public datasets to develop and evaluate models for nodule\ndetection and classification. Materials & Methods: This retrospective study\nuses the DLCS dataset (1,613 patients; 2,487 nodules) and external datasets\nincluding LUNA16, LUNA25, and NLST-3D. For detection, MONAI RetinaNet models\nwere trained on DLCS (DLCS-De) and LUNA16 (LUNA16-De) and evaluated using the\nCompetition Performance Metric (CPM). For nodule-level classification, we\ncompare five strategies: pretrained models (Models Genesis, Med3D), a\nself-supervised foundation model (FMCB), and ResNet50 with random\ninitialization versus Strategic Warm-Start (ResNet50-SWS) pretrained with\ndetection-derived candidate patches stratified by confidence. Results: For\ndetection on the DLCS test set, DLCS-De achieved sensitivity 0.82 at 2 false\npositives/scan (CPM 0.63) versus LUNA16-De (0.62, CPM 0.45). For external\nvalidation on NLST-3D, DLCS-De (sensitivity 0.72, CPM 0.58) also outperformed\nLUNA16-De (sensitivity 0.64, CPM 0.49). For classification across multiple\ndatasets, ResNet50-SWS attained AUCs of 0.71 (DLCS; 95% CI, 0.61-0.81), 0.90\n(LUNA16; 0.87-0.93), 0.81 (NLST-3D; 0.79-0.82), and 0.80 (LUNA25; 0.78-0.82),\nmatching or exceeding pretrained/self-supervised baselines. Performance\ndifferences reflected dataset label standards. Conclusion: This work\nestablishes a standardized benchmarking resource for lung cancer AI research,\nsupporting model development, validation, and translation. All code, models,\nand data are publicly released to promote reproducibility.\n","authors":["Fakrul Islam Tushar","Avivah Wang","Lavsen Dahal","Ehsan Samei","Michael R. Harowicz","Jayashree Kalpathy-Cramer","Kyle J. Lafata","Tina D. Tailor","Cynthia Rudin","Joseph Y. Lo"],"pdf_url":"https://arxiv.org/pdf/2405.04605v4.pdf","comment":"2 tables, 5 figures"},{"id":"http://arxiv.org/abs/2510.22035v3","updated":"2025-10-29T02:18:32Z","published":"2025-10-24T21:41:32Z","title":"Caption-Driven Explainability: Probing CNNs for Bias via CLIP","summary":"  Robustness has become one of the most critical problems in machine learning\n(ML). The science of interpreting ML models to understand their behavior and\nimprove their robustness is referred to as explainable artificial intelligence\n(XAI). One of the state-of-the-art XAI methods for computer vision problems is\nto generate saliency maps. A saliency map highlights the pixel space of an\nimage that excites the ML model the most. However, this property could be\nmisleading if spurious and salient features are present in overlapping pixel\nspaces. In this paper, we propose a caption-based XAI method, which integrates\na standalone model to be explained into the contrastive language-image\npre-training (CLIP) model using a novel network surgery approach. The resulting\ncaption-based XAI model identifies the dominant concept that contributes the\nmost to the models prediction. This explanation minimizes the risk of the\nstandalone model falling for a covariate shift and contributes significantly\ntowards developing robust ML models. Our code is available at\nhttps://github.com/patch0816/caption-driven-xai\n","authors":["Patrick Koller","Amil V. Dravid","Guido M. Schuster","Aggelos K. Katsaggelos"],"pdf_url":"https://arxiv.org/pdf/2510.22035v3.pdf","comment":"Accepted and presented at the IEEE ICIP 2025 Satellite Workshop\n  \"Generative AI for World Simulations and Communications & Celebrating 40\n  Years of Excellence in Education: Honoring Professor Aggelos Katsaggelos\",\n  Anchorage, Alaska, USA, September 14, 2025. Camera-ready preprint; the\n  official IEEE Xplore publication will follow. Code:\n  https://github.com/patch0816/caption-driven-xai"},{"id":"http://arxiv.org/abs/2510.07316v2","updated":"2025-10-29T02:15:20Z","published":"2025-10-08T17:59:33Z","title":"Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers","summary":"  This paper presents Pixel-Perfect Depth, a monocular depth estimation model\nbased on pixel-space diffusion generation that produces high-quality,\nflying-pixel-free point clouds from estimated depth maps. Current generative\ndepth estimation models fine-tune Stable Diffusion and achieve impressive\nperformance. However, they require a VAE to compress depth maps into latent\nspace, which inevitably introduces \\textit{flying pixels} at edges and details.\nOur model addresses this challenge by directly performing diffusion generation\nin the pixel space, avoiding VAE-induced artifacts. To overcome the high\ncomplexity associated with pixel-space generation, we introduce two novel\ndesigns: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which\nincorporate semantic representations from vision foundation models into DiT to\nprompt the diffusion process, thereby preserving global semantic consistency\nwhile enhancing fine-grained visual details; and 2) Cascade DiT Design that\nprogressively increases the number of tokens to further enhance efficiency and\naccuracy. Our model achieves the best performance among all published\ngenerative models across five benchmarks, and significantly outperforms all\nother models in edge-aware point cloud evaluation.\n","authors":["Gangwei Xu","Haotong Lin","Hongcheng Luo","Xianqi Wang","Jingfeng Yao","Lianghui Zhu","Yuechuan Pu","Cheng Chi","Haiyang Sun","Bing Wang","Guang Chen","Hangjun Ye","Sida Peng","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2510.07316v2.pdf","comment":"NeurIPS 2025. Project page: https://pixel-perfect-depth.github.io/"},{"id":"http://arxiv.org/abs/2510.16765v2","updated":"2025-10-29T02:07:16Z","published":"2025-10-19T09:11:58Z","title":"WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and\n  Mamba-based Channel Modeling with Texture Enhancement","summary":"  Image restoration is a fundamental and challenging task in computer vision,\nwhere CNN-based frameworks demonstrate significant computational efficiency.\nHowever, previous CNN-based methods often face challenges in adequately\nrestoring fine texture details, which are limited by the small receptive field\nof CNN structures and the lack of channel feature modeling. In this paper, we\npropose WaMaIR, which is a novel framework with a large receptive field for\nimage perception and improves the reconstruction of texture details in restored\nimages. Specifically, we introduce the Global Multiscale Wavelet Transform\nConvolutions (GMWTConvs) for expandding the receptive field to extract image\nfeatures, preserving and enriching texture features in model inputs. Meanwhile,\nwe propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to\ncapture long-range dependencies within feature channels, which enhancing the\nmodel sensitivity to color, edges, and texture information. Additionally, we\npropose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to\nguide the model in preserving detailed texture structures effectively.\nExtensive experiments confirm that WaMaIR outperforms state-of-the-art methods,\nachieving better image restoration and efficient computational performance of\nthe model.\n","authors":["Shengyu Zhu","Congyi Fan","Fuxuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.16765v2.pdf","comment":"Chinese Conference on Pattern Recognition and Computer Vision (PRCV),\n  Oral"},{"id":"http://arxiv.org/abs/2510.25094v1","updated":"2025-10-29T01:58:35Z","published":"2025-10-29T01:58:35Z","title":"Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI\n  Detection","summary":"  Zero-shot Human-Object Interaction detection aims to localize humans and\nobjects in an image and recognize their interaction, even when specific\nverb-object pairs are unseen during training. Recent works have shown promising\nresults using prompt learning with pretrained vision-language models such as\nCLIP, which align natural language prompts with visual features in a shared\nembedding space. However, existing approaches still fail to handle the visual\ncomplexity of interaction, including (1) intra-class visual diversity, where\ninstances of the same verb appear in diverse poses and contexts, and (2)\ninter-class visual entanglement, where distinct verbs yield visually similar\npatterns. To address these challenges, we propose VDRP, a framework for Visual\nDiversity and Region-aware Prompt learning. First, we introduce a visual\ndiversity-aware prompt learning strategy that injects group-wise visual\nvariance into the context embedding. We further apply Gaussian perturbation to\nencourage the prompts to capture diverse visual variations of a verb. Second,\nwe retrieve region-specific concepts from the human, object, and union regions.\nThese are used to augment the diversity-aware prompt embeddings, yielding\nregion-aware prompts that enhance verb-level discrimination. Experiments on the\nHICO-DET benchmark demonstrate that our method achieves state-of-the-art\nperformance under four zero-shot evaluation settings, effectively addressing\nboth intra-class diversity and inter-class visual entanglement. Code is\navailable at https://github.com/mlvlab/VDRP.\n","authors":["Chanhyeong Yang","Taehoon Song","Jihwan Park","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2510.25094v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25084v1","updated":"2025-10-29T01:42:23Z","published":"2025-10-29T01:42:23Z","title":"PSTF-AttControl: Per-Subject-Tuning-Free Personalized Image Generation\n  with Controllable Face Attributes","summary":"  Recent advancements in personalized image generation have significantly\nimproved facial identity preservation, particularly in fields such as\nentertainment and social media. However, existing methods still struggle to\nachieve precise control over facial attributes in a per-subject-tuning-free\n(PSTF) way. Tuning-based techniques like PreciseControl have shown promise by\nproviding fine-grained control over facial features, but they often require\nextensive technical expertise and additional training data, limiting their\naccessibility. In contrast, PSTF approaches simplify the process by enabling\nimage generation from a single facial input, but they lack precise control over\nfacial attributes. In this paper, we introduce a novel, PSTF method that\nenables both precise control over facial attributes and high-fidelity\npreservation of facial identity. Our approach utilizes a face recognition model\nto extract facial identity features, which are then mapped into the $W^+$\nlatent space of StyleGAN2 using the e4e encoder. We further enhance the model\nwith a Triplet-Decoupled Cross-Attention module, which integrates facial\nidentity, attribute features, and text embeddings into the UNet architecture,\nensuring clean separation of identity and attribute information. Trained on the\nFFHQ dataset, our method allows for the generation of personalized images with\nfine-grained control over facial attributes, while without requiring additional\nfine-tuning or training data for individual identities. We demonstrate that our\napproach successfully balances personalization with precise facial attribute\ncontrol, offering a more efficient and user-friendly solution for high-quality,\nadaptable facial image synthesis. The code is publicly available at\nhttps://github.com/UnicomAI/PSTF-AttControl.\n","authors":["Xiang liu","Zhaoxiang Liu","Huan Hu","Zipeng Wang","Ping Chen","Zezhou Chen","Kai Wang","Shiguo Lian"],"pdf_url":"https://arxiv.org/pdf/2510.25084v1.pdf","comment":"Accepted by Image and Vision Computing (18 pages, 8 figures)"},{"id":"http://arxiv.org/abs/2510.25077v1","updated":"2025-10-29T01:24:49Z","published":"2025-10-29T01:24:49Z","title":"Neighborhood Feature Pooling for Remote Sensing Image Classification","summary":"  In this work, we propose neighborhood feature pooling (NFP) as a novel\ntexture feature extraction method for remote sensing image classification. The\nNFP layer captures relationships between neighboring inputs and efficiently\naggregates local similarities across feature dimensions. Implemented using\nconvolutional layers, NFP can be seamlessly integrated into any network.\nResults comparing the baseline models and the NFP method indicate that NFP\nconsistently improves performance across diverse datasets and architectures\nwhile maintaining minimal parameter overhead.\n","authors":["Fahimeh Orvati Nia","Amirmohammad Mohammadi","Salim Al Kharsa","Pragati Naikare","Zigfried Hampel-Arias","Joshua Peeples"],"pdf_url":"https://arxiv.org/pdf/2510.25077v1.pdf","comment":"9 pages, 5 figures. Accepted to WACV 2026 (Winter Conference on\n  Applications of Computer Vision)"},{"id":"http://arxiv.org/abs/2505.16854v3","updated":"2025-10-29T01:19:12Z","published":"2025-05-22T16:13:29Z","title":"Think or Not? Selective Reasoning via Reinforcement Learning for\n  Vision-Language Models","summary":"  Reinforcement Learning (RL) has proven to be an effective post-training\nstrategy for enhancing reasoning in vision-language models (VLMs). Group\nRelative Policy Optimization (GRPO) is a recent prominent method that\nencourages models to generate complete reasoning traces before answering,\nleading to increased token usage and computational cost. Inspired by the\nhuman-like thinking process-where people skip reasoning for easy questions but\nthink carefully when needed-we explore how to enable VLMs to first decide when\nreasoning is necessary. To realize this, we propose TON, a two-stage training\nstrategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective\n'thought dropout' operation, where reasoning traces are randomly replaced with\nempty thoughts. This introduces a think-or-not format that serves as a cold\nstart for selective reasoning; (ii) a GRPO stage that enables the model to\nfreely explore when to think or not, while maximizing task-aware outcome\nrewards. Experimental results show that TON can reduce the completion length by\nup to 90% compared to vanilla GRPO, without sacrificing performance or even\nimproving it. Further evaluations across LLM (GSM8K), VLM (CLEVR, Super-CLEVR,\nGeoQA), and Agentic (AITZ) tasks-covering a range of reasoning difficulties\nunder both 3B and 7B models-consistently reveal that the model progressively\nlearns to bypass unnecessary reasoning steps as training advances. These\nfindings shed light on the path toward human-like reasoning patterns in RL\napproaches. Our code is available at https://github.com/kokolerk/TON.\n","authors":["Jiaqi Wang","Kevin Qinghong Lin","James Cheng","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2505.16854v3.pdf","comment":"camera ready revision"},{"id":"http://arxiv.org/abs/2510.25070v1","updated":"2025-10-29T01:16:21Z","published":"2025-10-29T01:16:21Z","title":"Vision-Language Integration for Zero-Shot Scene Understanding in\n  Real-World Environments","summary":"  Zero-shot scene understanding in real-world settings presents major\nchallenges due to the complexity and variability of natural scenes, where\nmodels must recognize new objects, actions, and contexts without prior labeled\nexamples. This work proposes a vision-language integration framework that\nunifies pre-trained visual encoders (e.g., CLIP, ViT) and large language models\n(e.g., GPT-based architectures) to achieve semantic alignment between visual\nand textual modalities. The goal is to enable robust zero-shot comprehension of\nscenes by leveraging natural language as a bridge to generalize over unseen\ncategories and contexts. Our approach develops a unified model that embeds\nvisual inputs and textual prompts into a shared space, followed by multimodal\nfusion and reasoning layers for contextual interpretation. Experiments on\nVisual Genome, COCO, ADE20K, and custom real-world datasets demonstrate\nsignificant gains over state-of-the-art zero-shot models in object recognition,\nactivity detection, and scene captioning. The proposed system achieves up to\n18% improvement in top-1 accuracy and notable gains in semantic coherence\nmetrics, highlighting the effectiveness of cross-modal alignment and language\ngrounding in enhancing generalization for real-world scene understanding.\n","authors":["Manjunath Prasad Holenarasipura Rajiv","B. M. Vidyavathi"],"pdf_url":"https://arxiv.org/pdf/2510.25070v1.pdf","comment":"Preprint under review at IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI), 2025"},{"id":"http://arxiv.org/abs/2510.25067v1","updated":"2025-10-29T01:10:28Z","published":"2025-10-29T01:10:28Z","title":"DRIP: Dynamic patch Reduction via Interpretable Pooling","summary":"  Recently, the advances in vision-language models, including contrastive\npretraining and instruction tuning, have greatly pushed the frontier of\nmultimodal AI. However, owing to the large-scale and hence expensive\npretraining, the efficiency concern has discouraged researchers from attempting\nto pretrain a vision language model from scratch. In this work, we propose\nDynamic patch Reduction via Interpretable Pooling (DRIP), which adapts to the\ninput images and dynamically merges tokens in the deeper layers of a visual\nencoder. Our results on both ImageNet training from scratch and CLIP\ncontrastive pretraining demonstrate a significant GFLOP reduction while\nmaintaining comparable classification/zero-shot performance. To further\nvalidate our proposed method, we conduct continual pretraining on a large\nbiology dataset, extending its impact into scientific domains.\n","authors":["Yusen Peng","Sachin Kumar"],"pdf_url":"https://arxiv.org/pdf/2510.25067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25058v1","updated":"2025-10-29T00:49:33Z","published":"2025-10-29T00:49:33Z","title":"Auto3DSeg for Brain Tumor Segmentation from 3D MRI in BraTS 2023\n  Challenge","summary":"  In this work, we describe our solution to the BraTS 2023 cluster of\nchallenges using Auto3DSeg from MONAI. We participated in all 5 segmentation\nchallenges, and achieved the 1st place results in three of them: Brain\nMetastasis, Brain Meningioma, BraTS-Africa challenges, and the 2nd place\nresults in the remaining two: Adult and Pediatic Glioma challenges.\n","authors":["Andriy Myronenko","Dong Yang","Yufan He","Daguang Xu"],"pdf_url":"https://arxiv.org/pdf/2510.25058v1.pdf","comment":"BraTS23 winner"},{"id":"http://arxiv.org/abs/2510.25051v1","updated":"2025-10-29T00:37:18Z","published":"2025-10-29T00:37:18Z","title":"Breast Cancer VLMs: Clinically Practical Vision-Language Train-Inference\n  Models","summary":"  Breast cancer remains the most commonly diagnosed malignancy among women in\nthe developed world. Early detection through mammography screening plays a\npivotal role in reducing mortality rates. While computer-aided diagnosis (CAD)\nsystems have shown promise in assisting radiologists, existing approaches face\ncritical limitations in clinical deployment - particularly in handling the\nnuanced interpretation of multi-modal data and feasibility due to the\nrequirement of prior clinical history. This study introduces a novel framework\nthat synergistically combines visual features from 2D mammograms with\nstructured textual descriptors derived from easily accessible clinical metadata\nand synthesized radiological reports through innovative tokenization modules.\nOur proposed methods in this study demonstrate that strategic integration of\nconvolutional neural networks (ConvNets) with language representations achieves\nsuperior performance to vision transformer-based models while handling\nhigh-resolution images and enabling practical deployment across diverse\npopulations. By evaluating it on multi-national cohort screening mammograms,\nour multi-modal approach achieves superior performance in cancer detection and\ncalcification identification compared to unimodal baselines, with particular\nimprovements. The proposed method establishes a new paradigm for developing\nclinically viable VLM-based CAD systems that effectively leverage imaging data\nand contextual patient information through effective fusion mechanisms.\n","authors":["Shunjie-Fabian Zheng","Hyeonjun Lee","Thijs Kooi","Ali Diba"],"pdf_url":"https://arxiv.org/pdf/2510.25051v1.pdf","comment":"Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)\n  Workshop at ICCV 2025"},{"id":"http://arxiv.org/abs/2510.26027v1","updated":"2025-10-29T23:50:57Z","published":"2025-10-29T23:50:57Z","title":"Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal\n  Attention in Vision Encoders","summary":"  Despite significant advances in Multimodal Large Language Models (MLLMs),\nunderstanding complex temporal dynamics in videos remains a major challenge.\nOur experiments show that current Video Large Language Model (Video-LLM)\narchitectures have critical limitations in temporal understanding, struggling\nwith tasks that require detailed comprehension of action sequences and temporal\nprogression. In this work, we propose a Video-LLM architecture that introduces\nstacked temporal attention modules directly within the vision encoder. This\ndesign incorporates a temporal attention in vision encoder, enabling the model\nto better capture the progression of actions and the relationships between\nframes before passing visual tokens to the LLM. Our results show that this\napproach significantly improves temporal reasoning and outperforms existing\nmodels in video question answering tasks, specifically in action recognition.\nWe improve on benchmarks including VITATECS, MVBench, and Video-MME by up to\n+5.5%. By enhancing the vision encoder with temporal structure, we address a\ncritical gap in video understanding for Video-LLMs. Project page and code are\navailable at: https://alirasekh.github.io/STAVEQ2/.\n","authors":["Ali Rasekh","Erfan Bagheri Soula","Omid Daliran","Simon Gottschalk","Mohsen Fayyaz"],"pdf_url":"https://arxiv.org/pdf/2510.26027v1.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26022v1","updated":"2025-10-29T23:30:09Z","published":"2025-10-29T23:30:09Z","title":"Groupwise Registration with Physics-Informed Test-Time Adaptation on\n  Multi-parametric Cardiac MRI","summary":"  Multiparametric mapping MRI has become a viable tool for myocardial tissue\ncharacterization. However, misalignment between multiparametric maps makes\npixel-wise analysis challenging. To address this challenge, we developed a\ngeneralizable physics-informed deep-learning model using test-time adaptation\nto enable group image registration across contrast weighted images acquired\nfrom multiple physical models (e.g., a T1 mapping model and T2 mapping model).\nThe physics-informed adaptation utilized the synthetic images from specific\nphysics model as registration reference, allows for transductive learning for\nvarious tissue contrast. We validated the model in healthy volunteers with\nvarious MRI sequences, demonstrating its improvement for multi-modal\nregistration with a wide range of image contrast variability.\n","authors":["Xinqi Li","Yi Zhang","Li-Ting Huang","Hsiao-Huang Chang","Thoralf Niendorf","Min-Chi Ku","Qian Tao","Hsin-Jung Yang"],"pdf_url":"https://arxiv.org/pdf/2510.26022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26017v1","updated":"2025-10-29T23:23:11Z","published":"2025-10-29T23:23:11Z","title":"Climate Adaptation-Aware Flood Prediction for Coastal Cities Using Deep\n  Learning","summary":"  Climate change and sea-level rise (SLR) pose escalating threats to coastal\ncities, intensifying the need for efficient and accurate methods to predict\npotential flood hazards. Traditional physics-based hydrodynamic simulators,\nalthough precise, are computationally expensive and impractical for city-scale\ncoastal planning applications. Deep Learning (DL) techniques offer promising\nalternatives, however, they are often constrained by challenges such as data\nscarcity and high-dimensional output requirements. Leveraging a recently\nproposed vision-based, low-resource DL framework, we develop a novel,\nlightweight Convolutional Neural Network (CNN)-based model designed to predict\ncoastal flooding under variable SLR projections and shoreline adaptation\nscenarios. Furthermore, we demonstrate the ability of the model to generalize\nacross diverse geographical contexts by utilizing datasets from two distinct\nregions: Abu Dhabi and San Francisco. Our findings demonstrate that the\nproposed model significantly outperforms state-of-the-art methods, reducing the\nmean absolute error (MAE) in predicted flood depth maps on average by nearly\n20%. These results highlight the potential of our approach to serve as a\nscalable and practical tool for coastal flood management, empowering\ndecision-makers to develop effective mitigation strategies in response to the\ngrowing impacts of climate change. Project Page: https://caspiannet.github.io/\n","authors":["Bilal Hassan","Areg Karapetyan","Aaron Chung Hin Chow","Samer Madanat"],"pdf_url":"https://arxiv.org/pdf/2510.26017v1.pdf","comment":"Submitted to Hydrology and Earth System Sciences"},{"id":"http://arxiv.org/abs/2505.12191v2","updated":"2025-10-29T23:02:31Z","published":"2025-05-18T01:37:58Z","title":"Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised\n  Learning from Data Curriculum","summary":"  Self-Supervised Learning (SSL) has become a powerful solution to extract rich\nrepresentations from unlabeled data. Yet, SSL research is mostly focused on\nclean, curated and high-quality datasets. As a result, applying SSL on noisy\ndata remains a challenge, despite being crucial to applications such as\nastrophysics, medical imaging, geophysics or finance. In this work, we present\na fully self-supervised framework that enables noise-robust representation\nlearning without requiring a denoiser at inference or downstream fine-tuning.\nOur method first trains an SSL denoiser on noisy data, then uses it to\nconstruct a denoised-to-noisy data curriculum (i.e., training first on\ndenoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),\ncombined with a teacher-guided regularization that anchors noisy embeddings to\ntheir denoised counterparts. This process encourages the model to internalize\nnoise robustness. Notably, the denoiser can be discarded after pretraining,\nsimplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise\n($\\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by\n4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from\nnoise-aware pretraining. The code is available at\nhttps://github.com/wenquanlu/noisy_dinov2.\n","authors":["Wenquan Lu","Jiaqi Zhang","Hugues Van Assel","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2505.12191v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2401.13267v4","updated":"2025-10-29T23:00:50Z","published":"2024-01-24T07:13:06Z","title":"Dynamic Traceback Learning for Medical Report Generation","summary":"  Automated medical report generation has demonstrated the potential to\nsignificantly reduce the workload associated with time-consuming medical\nreporting. Recent generative representation learning methods have shown promise\nin integrating vision and language modalities for medical report generation.\nHowever, when trained end-to-end and applied directly to medical image-to-text\ngeneration, they face two significant challenges: i) difficulty in accurately\ncapturing subtle yet crucial pathological details, and ii) reliance on both\nvisual and textual inputs during inference, leading to performance degradation\nin zero-shot inference when only images are available. To address these\nchallenges, this study proposes a novel multimodal dynamic traceback learning\nframework (DTrace). Specifically, we introduce a traceback mechanism to\nsupervise the semantic validity of generated content and a dynamic learning\nstrategy to adapt to various proportions of image and text input, enabling text\ngeneration without strong reliance on the input from both modalities during\ninference. The learning of cross-modal knowledge is enhanced by supervising the\nmodel to recover masked semantic information from a complementary counterpart.\nExtensive experiments conducted on two benchmark datasets, IU-Xray and\nMIMIC-CXR, demonstrate that the proposed DTrace framework outperforms\nstate-of-the-art methods for medical report generation.\n","authors":["Shuchang Ye","Mingyuan Meng","Mingjian Li","Dagan Feng","Usman Naseem","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2401.13267v4.pdf","comment":"Accepted to IEEE Transactions on Multimedia (TMM)"},{"id":"http://arxiv.org/abs/2505.21996v2","updated":"2025-10-29T22:39:29Z","published":"2025-05-28T05:55:44Z","title":"Learning World Models for Interactive Video Generation","summary":"  Foundational world models must be both interactive and preserve\nspatiotemporal coherence for effective future planning with action choices.\nHowever, present models for long video generation have limited inherent world\nmodeling capabilities due to two main challenges: compounding errors and\ninsufficient memory mechanisms. We enhance image-to-video models with\ninteractive capabilities through additional action conditioning and\nautoregressive framework, and reveal that compounding error is inherently\nirreducible in autoregressive video generation, while insufficient memory\nmechanism leads to incoherence of world models. We propose video retrieval\naugmented generation (VRAG) with explicit global state conditioning, which\nsignificantly reduces long-term compounding errors and increases spatiotemporal\nconsistency of world models. In contrast, naive autoregressive generation with\nextended context windows and retrieval-augmented generation prove less\neffective for video generation, primarily due to the limited in-context\nlearning capabilities of current video models. Our work illuminates the\nfundamental challenges in video world models and establishes a comprehensive\nbenchmark for improving video generation models with internal world modeling\ncapabilities.\n","authors":["Taiye Chen","Xun Hu","Zihan Ding","Chi Jin"],"pdf_url":"https://arxiv.org/pdf/2505.21996v2.pdf","comment":"Project page: https://sites.google.com/view/vrag"},{"id":"http://arxiv.org/abs/2510.26006v1","updated":"2025-10-29T22:34:26Z","published":"2025-10-29T22:34:26Z","title":"CAVE: Detecting and Explaining Commonsense Anomalies in Visual\n  Environments","summary":"  Humans can naturally identify, reason about, and explain anomalies in their\nenvironment. In computer vision, this long-standing challenge remains limited\nto industrial defects or unrealistic, synthetically generated anomalies,\nfailing to capture the richness and unpredictability of real-world anomalies.\nIn this work, we introduce CAVE, the first benchmark of real-world visual\nanomalies. CAVE supports three open-ended tasks: anomaly description,\nexplanation, and justification; with fine-grained annotations for visual\ngrounding and categorizing anomalies based on their visual manifestations,\ntheir complexity, severity, and commonness. These annotations draw inspiration\nfrom cognitive science research on how humans identify and resolve anomalies,\nproviding a comprehensive framework for evaluating Vision-Language Models\n(VLMs) in detecting and understanding anomalies. We show that state-of-the-art\nVLMs struggle with visual anomaly perception and commonsense reasoning, even\nwith advanced prompting strategies. By offering a realistic and cognitively\ngrounded benchmark, CAVE serves as a valuable resource for advancing research\nin anomaly detection and commonsense reasoning in VLMs.\n","authors":["Rishika Bhagwatkar","Syrielle Montariol","Angelika Romanou","Beatriz Borges","Irina Rish","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2510.26006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10501v2","updated":"2025-10-29T22:32:43Z","published":"2024-11-15T11:19:25Z","title":"OnlyFlow: Optical Flow based Motion Conditioning for Video Diffusion\n  Models","summary":"  We consider the problem of text-to-video generation tasks with precise\ncontrol for various applications such as camera movement control and\nvideo-to-video editing. Most methods tacking this problem rely on providing\nuser-defined controls, such as binary masks or camera movement embeddings. In\nour approach we propose OnlyFlow, an approach leveraging the optical flow\nfirstly extracted from an input video to condition the motion of generated\nvideos. Using a text prompt and an input video, OnlyFlow allows the user to\ngenerate videos that respect the motion of the input video as well as the text\nprompt. This is implemented through an optical flow estimation model applied on\nthe input video, which is then fed to a trainable optical flow encoder. The\noutput feature maps are then injected into the text-to-video backbone model. We\nperform quantitative, qualitative and user preference studies to show that\nOnlyFlow positively compares to state-of-the-art methods on a wide range of\ntasks, even though OnlyFlow was not specifically trained for such tasks.\nOnlyFlow thus constitutes a versatile, lightweight yet efficient method for\ncontrolling motion in text-to-video generation. Models and code will be made\navailable on GitHub and HuggingFace.\n","authors":["Mathis Koroglu","Hugo Caselles-DuprÃ©","Guillaume Jeanneret Sanmiguel","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2411.10501v2.pdf","comment":"8 pages, 1 supplementary page, 9 figures"},{"id":"http://arxiv.org/abs/2311.07734v2","updated":"2025-10-29T22:32:42Z","published":"2023-11-13T20:36:54Z","title":"Quality-Aware Prototype Memory for Face Representation Learning","summary":"  Prototype Memory is a powerful model for face representation learning. It\nenables training face recognition models on datasets of any size by generating\nprototypes (classifier weights) on the fly and efficiently utilizing them.\nPrototype Memory demonstrated strong results in many face recognition\nbenchmarks. However, the algorithm of prototype generation, used in it, is\nprone to the problems of imperfectly calculated prototypes in case of\nlow-quality or poorly recognizable faces in the images, selected for the\nprototype creation. All images of the same person presented in the mini-batch\nare used with equal weights, and the resulting averaged prototype can be\ncontaminated by imperfect embeddings of low-quality face images. This may lead\nto misleading training signals and degrade the performance of the trained\nmodels. In this paper, we propose a simple and effective way to improve\nPrototype Memory with quality-aware prototype generation. Quality-Aware\nPrototype Memory uses different weights for images of different quality in the\nprocess of prototype generation. With this improvement, prototypes receive more\ninformative signals from high-quality images and are less affected by\nlow-quality ones. We propose and compare several methods of quality estimation\nand usage, perform extensive experiments on the different face recognition\nbenchmarks and demonstrate the advantages of the proposed model compared to the\nbasic version of Prototype Memory.\n","authors":["Evgeny Smirnov","Vasiliy Galyuk","Evgeny Lukyanets"],"pdf_url":"https://arxiv.org/pdf/2311.07734v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2510.26004v1","updated":"2025-10-29T22:32:16Z","published":"2025-10-29T22:32:16Z","title":"DARTS: A Drone-Based AI-Powered Real-Time Traffic Incident Detection\n  System","summary":"  Rapid and reliable incident detection is critical for reducing crash-related\nfatalities, injuries, and congestion. However, conventional methods, such as\nclosed-circuit television, dashcam footage, and sensor-based detection,\nseparate detection from verification, suffer from limited flexibility, and\nrequire dense infrastructure or high penetration rates, restricting\nadaptability and scalability to shifting incident hotspots. To overcome these\nchallenges, we developed DARTS, a drone-based, AI-powered real-time traffic\nincident detection system. DARTS integrates drones' high mobility and aerial\nperspective for adaptive surveillance, thermal imaging for better\nlow-visibility performance and privacy protection, and a lightweight deep\nlearning framework for real-time vehicle trajectory extraction and incident\ndetection. The system achieved 99% detection accuracy on a self-collected\ndataset and supports simultaneous online visual verification, severity\nassessment, and incident-induced congestion propagation monitoring via a\nweb-based interface. In a field test on Interstate 75 in Florida, DARTS\ndetected and verified a rear-end collision 12 minutes earlier than the local\ntransportation management center and monitored incident-induced congestion\npropagation, suggesting potential to support faster emergency response and\nenable proactive traffic control to reduce congestion and secondary crash risk.\nCrucially, DARTS's flexible deployment architecture reduces dependence on\nfrequent physical patrols, indicating potential scalability and\ncost-effectiveness for use in remote areas and resource-constrained settings.\nThis study presents a promising step toward a more flexible and integrated\nreal-time traffic incident detection system, with significant implications for\nthe operational efficiency and responsiveness of modern transportation\nmanagement.\n","authors":["Bai Li","Achilleas Kourtellis","Rong Cao","Joseph Post","Brian Porter","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26004v1.pdf","comment":"Preprint version. This manuscript is currently under review at\n  Transportation Research Part C: Emerging Technologies. The PDF corresponds to\n  the version submitted in June 2025. The main findings of this work were\n  recognized with the Best Intelligent Transportation Systems Paper Award at\n  the 2025 TRB Annual Meeting"},{"id":"http://arxiv.org/abs/2510.26001v1","updated":"2025-10-29T22:25:48Z","published":"2025-10-29T22:25:48Z","title":"Larger Hausdorff Dimension in Scanning Pattern Facilitates Mamba-Based\n  Methods in Low-Light Image Enhancement","summary":"  We propose an innovative enhancement to the Mamba framework by increasing the\nHausdorff dimension of its scanning pattern through a novel Hilbert Selective\nScan mechanism. This mechanism explores the feature space more effectively,\ncapturing intricate fine-scale details and improving overall coverage. As a\nresult, it mitigates information inconsistencies while refining spatial\nlocality to better capture subtle local interactions without sacrificing the\nmodel's ability to handle long-range dependencies. Extensive experiments on\npublicly available benchmarks demonstrate that our approach significantly\nimproves both the quantitative metrics and qualitative visual fidelity of\nexisting Mamba-based low-light image enhancement methods, all while reducing\ncomputational resource consumption and shortening inference time. We believe\nthat this refined strategy not only advances the state-of-the-art in low-light\nimage enhancement but also holds promise for broader applications in fields\nthat leverage Mamba-based techniques.\n","authors":["Xinhua Wang","Caibo Feng","Xiangjun Fu","Chunxiao Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06220v2","updated":"2025-10-29T22:25:02Z","published":"2025-06-06T16:28:03Z","title":"GenIR: Generative Visual Feedback for Mental Image Retrieval","summary":"  Vision-language models (VLMs) have shown strong performance on text-to-image\nretrieval benchmarks. However, bridging this success to real-world applications\nremains a challenge. In practice, human search behavior is rarely a one-shot\naction. Instead, it is often a multi-round process guided by clues in mind.\nThat is, a mental image ranging from vague recollections to vivid mental\nrepresentations of the target image. Motivated by this gap, we study the task\nof Mental Image Retrieval (MIR), which targets the realistic yet underexplored\nsetting where users refine their search for a mentally envisioned image through\nmulti-round interactions with an image search engine. Central to successful\ninteractive retrieval is the capability of machines to provide users with\nclear, actionable feedback; however, existing methods rely on indirect or\nabstract verbal feedback, which can be ambiguous, misleading, or ineffective\nfor users to refine the query. To overcome this, we propose GenIR, a generative\nmulti-round retrieval paradigm leveraging diffusion-based image generation to\nexplicitly reify the AI system's understanding at each round. These synthetic\nvisual representations provide clear, interpretable feedback, enabling users to\nrefine their queries intuitively and effectively. We further introduce a fully\nautomated pipeline to generate a high-quality multi-round MIR dataset.\nExperimental results demonstrate that GenIR significantly outperforms existing\ninteractive methods in the MIR scenario. This work establishes a new task with\na dataset and an effective generative retrieval method, providing a foundation\nfor future research in this direction\n","authors":["Diji Yang","Minghao Liu","Chung-Hsiang Lo","Yi Zhang","James Davis"],"pdf_url":"https://arxiv.org/pdf/2506.06220v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25990v1","updated":"2025-10-29T21:57:12Z","published":"2025-10-29T21:57:12Z","title":"Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI","summary":"  In this work, we address the TrackRAD2025 challenge of real-time tumor\ntracking in cine-MRI sequences of the thoracic and abdominal regions under\nstrong data scarcity constraints. Two complementary strategies were explored:\n(i) unsupervised registration with the IMPACT similarity metric and (ii)\nfoundation model-based segmentation leveraging SAM 2.1 and its recent variants\nthrough prompt-based interaction. Due to the one-second runtime constraint, the\nSAM-based method was ultimately selected. The final configuration used SAM2.1\nb+ with mask-based prompts from the first annotated slice, fine-tuned solely on\nthe small labeled subset from TrackRAD2025. Training was configured to minimize\noverfitting, using 1024x1024 patches (batch size 1), standard augmentations,\nand a balanced Dice + IoU loss. A low uniform learning rate (0.0001) was\napplied to all modules (prompt encoder, decoder, Hiera backbone) to preserve\ngeneralization while adapting to annotator-specific styles. Training lasted 300\nepochs (~12h on RTX A6000, 48GB). The same inference strategy was consistently\napplied across all anatomical sites and MRI field strengths. Test-time\naugmentation was considered but ultimately discarded due to negligible\nperformance gains. The final model was selected based on the highest Dice\nSimilarity Coefficient achieved on the validation set after fine-tuning. On the\nhidden test set, the model reached a Dice score of 0.8794, ranking 6th overall\nin the TrackRAD2025 challenge. These results highlight the strong potential of\nfoundation models for accurate and real-time tumor tracking in MRI-guided\nradiotherapy.\n","authors":["Valentin Boussot","CÃ©dric HÃ©mon","Jean-Claude Nunes","Jean-Louis Dillenseger"],"pdf_url":"https://arxiv.org/pdf/2510.25990v1.pdf","comment":"Paper for the Trackrad2025 challenge, Team BreizhTrack"},{"id":"http://arxiv.org/abs/2506.05696v2","updated":"2025-10-29T21:34:31Z","published":"2025-06-06T02:52:13Z","title":"MoralCLIP: Contrastive Alignment of Vision-and-Language Representations\n  with Moral Foundations Theory","summary":"  Recent advances in vision-language models have enabled rich semantic\nunderstanding across modalities. However, these encoding methods lack the\nability to interpret or reason about the moral dimensions of content-a crucial\naspect of human cognition. In this paper, we address this gap by introducing\nMoralCLIP, a novel embedding representation method that extends multimodal\nlearning with explicit moral grounding based on Moral Foundations Theory (MFT).\nOur approach integrates visual and textual moral cues into a unified embedding\nspace, enabling cross-modal moral alignment. MoralCLIP is grounded on the\nmulti-label dataset Social-Moral Image Database to identify co-occurring moral\nfoundations in visual content. For MoralCLIP training, we design a moral data\naugmentation strategy to scale our annotated dataset to 15,000 image-text pairs\nlabeled with MFT-aligned dimensions. Our results demonstrate that explicit\nmoral supervision improves both unimodal and multimodal understanding of moral\ncontent, establishing a foundation for morally-aware AI systems capable of\nrecognizing and aligning with human moral values.\n","authors":["Ana Carolina Condez","Diogo Tavares","JoÃ£o MagalhÃ£es"],"pdf_url":"https://arxiv.org/pdf/2506.05696v2.pdf","comment":"Updated version: corresponds to the ACM MM '25 published paper and\n  includes full appendix material"},{"id":"http://arxiv.org/abs/2510.25976v1","updated":"2025-10-29T21:21:54Z","published":"2025-10-29T21:21:54Z","title":"Brain-IT: Image Reconstruction from fMRI via Brain-Interaction\n  Transformer","summary":"  Reconstructing images seen by people from their fMRI brain recordings\nprovides a non-invasive window into the human brain. Despite recent progress\nenabled by diffusion models, current methods often lack faithfulness to the\nactual seen images. We present \"Brain-IT\", a brain-inspired approach that\naddresses this challenge through a Brain Interaction Transformer (BIT),\nallowing effective interactions between clusters of functionally-similar\nbrain-voxels. These functional-clusters are shared by all subjects, serving as\nbuilding blocks for integrating information both within and across brains. All\nmodel components are shared by all clusters & subjects, allowing efficient\ntraining with a limited amount of data. To guide the image reconstruction, BIT\npredicts two complementary localized patch-level image features: (i)high-level\nsemantic features which steer the diffusion model toward the correct semantic\ncontent of the image; and (ii)low-level structural features which help to\ninitialize the diffusion process with the correct coarse layout of the image.\nBIT's design enables direct flow of information from brain-voxel clusters to\nlocalized image features. Through these principles, our method achieves image\nreconstructions from fMRI that faithfully reconstruct the seen images, and\nsurpass current SotA approaches both visually and by standard objective\nmetrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve\nresults comparable to current methods trained on full 40-hour recordings.\n","authors":["Roman Beliy","Amit Zalcher","Jonathan Kogman","Navve Wasserman","Michal Irani"],"pdf_url":"https://arxiv.org/pdf/2510.25976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25970v1","updated":"2025-10-29T21:12:58Z","published":"2025-10-29T21:12:58Z","title":"SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing","summary":"  Rectified flow models have become a de facto standard in image generation due\nto their stable sampling trajectories and high-fidelity outputs. Despite their\nstrong generative capabilities, they face critical limitations in image editing\ntasks: inaccurate inversion processes for mapping real images back into the\nlatent space, and gradient entanglement issues during editing often result in\noutputs that do not faithfully reflect the target prompt. Recent efforts have\nattempted to directly map source and target distributions via ODE-based\napproaches without inversion; however,these methods still yield suboptimal\nediting quality. In this work, we propose a flow decomposition-and-aggregation\nframework built upon an inversion-free formulation to address these\nlimitations. Specifically, we semantically decompose the target prompt into\nmultiple sub-prompts, compute an independent flow for each, and aggregate them\nto form a unified editing trajectory. While we empirically observe that\ndecomposing the original flow enhances diversity in the target space,\ngenerating semantically aligned outputs still requires consistent guidance\ntoward the full target prompt. To this end, we design a projection and\nsoft-aggregation mechanism for flow, inspired by gradient conflict resolution\nin multi-task learning. This approach adaptively weights the sub-target\nvelocity fields, suppressing semantic redundancy while emphasizing distinct\ndirections, thereby preserving both diversity and consistency in the final\nedited output. Experimental results demonstrate that our method outperforms\nexisting zero-shot editing approaches in terms of semantic fidelity and\nattribute disentanglement. The code is available at\nhttps://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.\n","authors":["Sung-Hoon Yoon","Minghan Li","Gaspard Beaudouin","Congcong Wen","Muhammad Rafay Azhar","Mengyu Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25970v1.pdf","comment":"Camera-ready version for NeurIPS 2025, 10 pages (main paper)"},{"id":"http://arxiv.org/abs/2503.04852v3","updated":"2025-10-29T20:44:13Z","published":"2025-03-06T03:40:01Z","title":"CAUSAL3D: A Comprehensive Benchmark for Causal Learning from Visual Data","summary":"  True intelligence hinges on the ability to uncover and leverage hidden causal\nrelations. Despite significant progress in AI and computer vision (CV), there\nremains a lack of benchmarks for assessing models' abilities to infer latent\ncausality from complex visual data. In this paper, we introduce\n\\textsc{\\textbf{Causal3D}}, a novel and comprehensive benchmark that integrates\nstructured data (tables) with corresponding visual representations (images) to\nevaluate causal reasoning. Designed within a systematic framework, Causal3D\ncomprises 19 3D-scene datasets capturing diverse causal relations, views, and\nbackgrounds, enabling evaluations across scenes of varying complexity. We\nassess multiple state-of-the-art methods, including classical causal discovery,\ncausal representation learning, and large/vision-language models (LLMs/VLMs).\nOur experiments show that as causal structures grow more complex without prior\nknowledge, performance declines significantly, highlighting the challenges even\nadvanced methods face in complex causal scenarios. Causal3D serves as a vital\nresource for advancing causal reasoning in CV and fostering trustworthy AI in\ncritical domains.\n","authors":["Disheng Liu","Yiran Qiao","Wuche Liu","Yiren Lu","Yunlai Zhou","Tuo Liang","Yu Yin","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2503.04852v3.pdf","comment":"Datasets link:\n  https://huggingface.co/datasets/LLDDSS/Causal3D_Dataset"},{"id":"http://arxiv.org/abs/2505.23158v2","updated":"2025-10-29T19:53:57Z","published":"2025-05-29T06:50:57Z","title":"LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient\n  Rendering","summary":"  In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian\nSplatting that enables real-time rendering of large-scale scenes on\nmemory-constrained devices. Our approach introduces a hierarchical LOD\nrepresentation that iteratively selects optimal subsets of Gaussians based on\ncamera distance, thus largely reducing both rendering time and GPU memory\nusage. We construct each LOD level by applying a depth-aware 3D smoothing\nfilter, followed by importance-based pruning and fine-tuning to maintain visual\nfidelity. To further reduce memory overhead, we partition the scene into\nspatial chunks and dynamically load only relevant Gaussians during rendering,\nemploying an opacity-blending mechanism to avoid visual artifacts at chunk\nboundaries. Our method achieves state-of-the-art performance on both outdoor\n(Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality\nrenderings with reduced latency and memory requirements.\n","authors":["Jonas Kulhanek","Marie-Julie Rakotosaona","Fabian Manhardt","Christina Tsalicoglou","Michael Niemeyer","Torsten Sattler","Songyou Peng","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2505.23158v2.pdf","comment":"NeurIPS 2025; Web: https://lodge-gs.github.io/"},{"id":"http://arxiv.org/abs/2510.25921v1","updated":"2025-10-29T19:50:34Z","published":"2025-10-29T19:50:34Z","title":"Generative Image Restoration and Super-Resolution using Physics-Informed\n  Synthetic Data for Scanning Tunneling Microscopy","summary":"  Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and\natom manipulation, but its utility is often limited by tip degradation and slow\nserial data acquisition. Fabrication adds another layer of complexity since the\ntip is often subjected to large voltages, which may alter the shape of its\napex, requiring it to be conditioned. Here, we propose a machine learning (ML)\napproach for image repair and super-resolution to alleviate both challenges.\nUsing a dataset of only 36 pristine experimental images of Si(001):H, we\ndemonstrate that a physics-informed synthetic data generation pipeline can be\nused to train several state-of-the-art flow-matching and diffusion models.\nQuantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy\n(CMMD) score and structural similarity demonstrates that our models are able to\neffectively restore images and offer a two- to fourfold reduction in image\nacquisition time by accurately reconstructing images from sparsely sampled\ndata. Our framework has the potential to significantly increase STM\nexperimental throughput by offering a route to reducing the frequency of\ntip-conditioning procedures and to enhancing frame rates in existing high-speed\nSTM systems.\n","authors":["Nikola L. Kolev","Tommaso Rodani","Neil J. Curson","Taylor J. Z. Stock","Alberto Cazzaniga"],"pdf_url":"https://arxiv.org/pdf/2510.25921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17345v2","updated":"2025-10-29T19:47:47Z","published":"2024-06-25T07:58:47Z","title":"NerfBaselines: Consistent and Reproducible Evaluation of Novel View\n  Synthesis Methods","summary":"  Novel view synthesis is an important problem with many applications,\nincluding AR/VR, gaming, and robotic simulations. With the recent rapid\ndevelopment of Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS)\nmethods, it is becoming difficult to keep track of the current state of the art\n(SoTA) due to methods using different evaluation protocols, codebases being\ndifficult to install and use, and methods not generalizing well to novel 3D\nscenes. In our experiments, we show that even tiny differences in the\nevaluation protocols of various methods can artificially boost the performance\nof these methods. This raises questions about the validity of quantitative\ncomparisons performed in the literature. To address these questions, we propose\nNerfBaselines, an evaluation framework which provides consistent benchmarking\ntools, ensures reproducibility, and simplifies the installation and use of\nvarious methods. We validate our implementation experimentally by reproducing\nthe numbers reported in the original papers. For improved accessibility, we\nrelease a web platform that compares commonly used methods on standard\nbenchmarks. We strongly believe NerfBaselines is a valuable contribution to the\ncommunity as it ensures that quantitative results are comparable and thus truly\nmeasure progress in the field of novel view synthesis.\n","authors":["Jonas Kulhanek","Torsten Sattler"],"pdf_url":"https://arxiv.org/pdf/2406.17345v2.pdf","comment":"NeurIPS 2025 D&B; Web: https://jkulhanek.com/nerfbaselines"},{"id":"http://arxiv.org/abs/2510.24134v2","updated":"2025-10-29T19:17:39Z","published":"2025-10-28T07:19:01Z","title":"VC4VG: Optimizing Video Captions for Text-to-Video Generation","summary":"  Recent advances in text-to-video (T2V) generation highlight the critical role\nof high-quality video-text pairs in training models capable of producing\ncoherent and instruction-aligned videos. However, strategies for optimizing\nvideo captions specifically for T2V training remain underexplored. In this\npaper, we introduce VC4VG (Video Captioning for Video Generation), a\ncomprehensive caption optimization framework tailored to the needs of T2V\nmodels. We begin by analyzing caption content from a T2V perspective,\ndecomposing the essential elements required for video reconstruction into\nmultiple dimensions, and proposing a principled caption design methodology. To\nsupport evaluation, we construct VC4VG-Bench, a new benchmark featuring\nfine-grained, multi-dimensional, and necessity-graded metrics aligned with\nT2V-specific requirements. Extensive T2V fine-tuning experiments demonstrate a\nstrong correlation between improved caption quality and video generation\nperformance, validating the effectiveness of our approach. We release all\nbenchmark tools and code at https://github.com/alimama-creative/VC4VG to\nsupport further research.\n","authors":["Yang Du","Zhuoran Lin","Kaiqiang Song","Biao Wang","Zhicheng Zheng","Tiezheng Ge","Bo Zheng","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2510.24134v2.pdf","comment":"Accepted by EMNLP 2025"},{"id":"http://arxiv.org/abs/2508.08186v2","updated":"2025-10-29T19:12:08Z","published":"2025-08-11T17:06:55Z","title":"KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold\n  Representation Learning","summary":"  Semantic segmentation of structural defects in civil infrastructure remains\nchallenging due to variable defect appearances, harsh imaging conditions, and\nsignificant class imbalance. Current deep learning methods, despite their\neffectiveness, typically require millions of parameters, rendering them\nimpractical for real-time inspection systems. We introduce KARMA\n(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient\nsemantic segmentation framework that models complex defect patterns through\ncompositions of one-dimensional functions rather than conventional\nconvolutions. KARMA features three technical innovations: (1) a\nparameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging\nlow-rank factorization for KAN-based feature transformation; (2) an optimized\nfeature pyramid structure with separable convolutions for multi-scale defect\nanalysis; and (3) a static-dynamic prototype mechanism that enhances feature\nrepresentation for imbalanced classes. Extensive experiments on benchmark\ninfrastructure inspection datasets demonstrate that KARMA achieves competitive\nor superior mean IoU performance compared to state-of-the-art approaches, while\nusing significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).\nOperating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for\nreal-time deployment, enabling practical automated infrastructure inspection\nsystems without compromising accuracy. The source code can be accessed at the\nfollowing URL: https://github.com/faeyelab/karma.\n","authors":["Md Meftahul Ferdaus","Mahdi Abdelguerfi","Elias Ioup","Steven Sloan","Kendall N. Niles","Ken Pathak"],"pdf_url":"https://arxiv.org/pdf/2508.08186v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2510.25718v1","updated":"2025-10-29T17:27:21Z","published":"2025-10-29T17:27:21Z","title":"Retrieval-Augmented Search for Large-Scale Map Collections with ColPali","summary":"  Multimodal approaches have shown great promise for searching and navigating\ndigital collections held by libraries, archives, and museums. In this paper, we\nintroduce map-RAS: a retrieval-augmented search system for historic maps. In\naddition to introducing our framework, we detail our publicly-hosted demo for\nsearching 101,233 map images held by the Library of Congress. With our system,\nusers can multimodally query the map collection via ColPali, summarize search\nresults using Llama 3.2, and upload their own collections to perform\ninter-collection search. We articulate potential use cases for archivists,\ncurators, and end-users, as well as future work with our system in both machine\nlearning and the digital humanities. Our demo can be viewed at:\nhttp://www.mapras.com.\n","authors":["Jamie Mahowald","Benjamin Charles Germain Lee"],"pdf_url":"https://arxiv.org/pdf/2510.25718v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.25622v1","updated":"2025-10-29T15:27:23Z","published":"2025-10-29T15:27:23Z","title":"MMQ-v2: Align, Denoise, and Amplify: Adaptive Behavior Mining for\n  Semantic IDs Learning in Recommendation","summary":"  Industrial recommender systems rely on unique Item Identifiers (ItemIDs).\nHowever, this method struggles with scalability and generalization in large,\ndynamic datasets that have sparse long-tail data.Content-based Semantic IDs\n(SIDs) address this by sharing knowledge through content quantization. However,\nby ignoring dynamic behavioral properties, purely content-based SIDs have\nlimited expressive power. Existing methods attempt to incorporate behavioral\ninformation but overlook a critical distinction: unlike relatively uniform\ncontent features, user-item interactions are highly skewed and diverse,\ncreating a vast information gap in quality and quantity between popular and\nlong-tail items. This oversight leads to two critical limitations: (1) Noise\nCorruption: Indiscriminate behavior-content alignment allows collaborative\nnoise from long-tail items to corrupt their content representations, leading to\nthe loss of critical multimodal information. (2)Signal Obscurity: The\nequal-weighting scheme for SIDs fails to reflect the varying importance of\ndifferent behavioral signals, making it difficult for downstream tasks to\ndistinguish important SIDs from uninformative ones. To tackle these issues, we\npropose a mixture-of-quantization framework, MMQ-v2, to adaptively Align,\nDenoise, and Amplify multimodal information from content and behavior\nmodalities for semantic IDs learning. The semantic IDs generated by this\nframework named ADA-SID. It introduces two innovations: an adaptive\nbehavior-content alignment that is aware of information richness to shield\nrepresentations from noise, and a dynamic behavioral router to amplify critical\nsignals by applying different weights to SIDs. Extensive experiments on public\nand large-scale industrial datasets demonstrate ADA-SID's significant\nsuperiority in both generative and discriminative recommendation tasks.\n","authors":["Yi Xu","Moyu Zhang","Chaofan Fan","Jinxin Hu","Xiaochen Li","Yu Zhang","Xiaoyi Zeng","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.25622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25621v1","updated":"2025-10-29T15:25:34Z","published":"2025-10-29T15:25:34Z","title":"FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering","summary":"  The advent of Large Language Models (LLMs) has revolutionized Natural\nLanguage Processing, yet their application in high-stakes, specialized domains\nlike religious question answering is hindered by challenges like hallucination\nand unfaithfulness to authoritative sources. This issue is particularly\ncritical for the Persian-speaking Muslim community, where accuracy and\ntrustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG)\nsystems, relying on simplistic single-pass pipelines, fall short on complex,\nmulti-hop queries requiring multi-step reasoning and evidence aggregation. To\naddress this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful\nAdvanced Question Answering in the Persian Islamic domain. FARSIQA is built\nupon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative\nRefinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting\nprocess: it adaptively decomposes complex queries, assesses evidence\nsufficiency, and enters an iterative loop to generate sub-queries,\nprogressively filling information gaps. Operating on a curated knowledge base\nof over one million authoritative Islamic documents, FARSIQA demonstrates\nsuperior performance. Rigorous evaluation on the challenging IslamicPCQA\nbenchmark shows state-of-the-art performance: the system achieves a remarkable\n97.0% in Negative Rejection - a 40-point improvement over baselines - and a\nhigh Answer Correctness score of 74.3%. Our work establishes a new standard for\nPersian Islamic QA and validates that our iterative, adaptive architecture is\ncrucial for building faithful, reliable AI systems in sensitive domains.\n","authors":["Mohammad Aghajani Asl","Behrooz Minaei Bidgoli"],"pdf_url":"https://arxiv.org/pdf/2510.25621v1.pdf","comment":"37 pages, 5 figures, 10 tables. Keywords: Retrieval-Augmented\n  Generation (RAG), Question Answering (QA), Islamic Knowledge Base, Faithful\n  AI, Persian NLP, Multi-hop Reasoning, Large Language Models (LLMs)"},{"id":"http://arxiv.org/abs/2510.13738v2","updated":"2025-10-29T15:00:42Z","published":"2025-10-15T16:45:59Z","title":"HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based\n  Sequential Recommendation","summary":"  Large language models (LLMs) have recently demonstrated strong potential for\nsequential recommendation. However, current LLM-based approaches face critical\nlimitations in modeling users' long-term and diverse interests. First, due to\ninference latency and feature fetching bandwidth constraints, existing methods\ntypically truncate user behavior sequences to include only the most recent\ninteractions, resulting in the loss of valuable long-range preference signals.\nSecond, most current methods rely on next-item prediction with a single\npredicted embedding, overlooking the multifaceted nature of user interests and\nlimiting recommendation diversity. To address these challenges, we propose\nHyMiRec, a hybrid multi-interest sequential recommendation framework, which\nleverages a lightweight recommender to extracts coarse interest embeddings from\nlong user sequences and an LLM-based recommender to captures refined interest\nembeddings. To alleviate the overhead of fetching features, we introduce a\nresidual codebook based on cosine similarity, enabling efficient compression\nand reuse of user history embeddings. To model the diverse preferences of\nusers, we design a disentangled multi-interest learning module, which leverages\nmultiple interest queries to learn disentangles multiple interest signals\nadaptively, allowing the model to capture different facets of user intent.\nExtensive experiments are conducted on both benchmark datasets and a collected\nindustrial dataset, demonstrating our effectiveness over existing\nstate-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec\nbrings consistent improvements in real-world recommendation systems. Code is\navailable at https://github.com/FireRedTeam/FireRedSeqRec.\n","authors":["Jingyi Zhou","Cheng Chen","Kai Zuo","Manjie Xu","Zhendong Fu","Yibo Chen","Xu Tang","Yao Hu"],"pdf_url":"https://arxiv.org/pdf/2510.13738v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10940v3","updated":"2025-10-29T14:42:46Z","published":"2025-05-16T07:26:41Z","title":"Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced\n  Logical Recommendation","summary":"  Recommender systems filter contents/items valuable to users by inferring\npreferences from user features and historical behaviors. Mainstream approaches\nfollow the learning-to-rank paradigm, which focus on discovering and modeling\nitem topics (e.g., categories), and capturing user preferences on these topics\nbased on historical interactions. However, this paradigm often neglects the\nmodeling of user characteristics and their social roles, which are logical\nconfounders influencing the correlated interest and user preference transition.\nTo bridge this gap, we introduce the user role identification task and the\nbehavioral logic modeling task that aim to explicitly model user roles and\nlearn the logical relations between item topics and user social roles. We show\nthat it is possible to explicitly solve these tasks through an efficient\nintegration framework of Large Language Model (LLM) and recommendation systems,\nfor which we propose TagCF. On the one hand, TagCF exploits the (Multi-modal)\nLLM's world knowledge and logic inference ability to extract realistic\ntag-based virtual logic graphs that reveal dynamic and expressive knowledge of\nusers, refining our understanding of user behaviors. On the other hand, TagCF\npresents empirically effective integration modules that take advantage of the\nextracted tag-logic information, augmenting the recommendation performance. We\nconduct both online experiments and offline experiments with industrial and\npublic datasets as verification of TagCF's effectiveness, and we empirically\nshow that the user role modeling strategy is potentially a better choice than\nthe modeling of item topics. Additionally, we provide evidence that the\nextracted logic graphs are empirically a general and transferable knowledge\nthat can benefit a wide range of recommendation tasks. Our code is available in\nhttps://github.com/Code2Q/TagCF.\n","authors":["Qing Yu","Xiaobei Wang","Shuchang Liu","Yandong Bai","Xiaoyu Yang","Xueliang Wang","Chang Meng","Shanshan Wu","Hailan Yang","Huihui Xiao","Xiang Li","Fan Yang","Xiaoqiang Feng","Lantao Hu","Han Li","Kun Gai","Lixin Zou"],"pdf_url":"https://arxiv.org/pdf/2505.10940v3.pdf","comment":"to be published in NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25488v1","updated":"2025-10-29T13:08:35Z","published":"2025-10-29T13:08:35Z","title":"Generalized Pseudo-Relevance Feedback","summary":"  Query rewriting is a fundamental technique in information retrieval (IR). It\ntypically employs the retrieval result as relevance feedback to refine the\nquery and thereby addresses the vocabulary mismatch between user queries and\nrelevant documents. Traditional pseudo-relevance feedback (PRF) and its\nvector-based extension (VPRF) improve retrieval performance by leveraging\ntop-retrieved documents as relevance feedback. However, they are constructed\nbased on two major hypotheses: the relevance assumption (top documents are\nrelevant) and the model assumption (rewriting methods need to be designed\nspecifically for particular model architectures). While recent large language\nmodels (LLMs)-based generative relevance feedback (GRF) enables model-free\nquery reformulation, it either suffers from severe LLM hallucination or, again,\nrelies on the relevance assumption to guarantee the effectiveness of rewriting\nquality. To overcome these limitations, we introduce an assumption-relaxed\nframework: \\textit{Generalized Pseudo Relevance Feedback} (GPRF), which\nperforms model-free, natural language rewriting based on retrieved documents,\nnot only eliminating the model assumption but also reducing dependence on the\nrelevance assumption. Specifically, we design a utility-oriented training\npipeline with reinforcement learning to ensure robustness against noisy\nfeedback. Extensive experiments across multiple benchmarks and retrievers\ndemonstrate that GPRF consistently outperforms strong baselines, establishing\nit as an effective and generalizable framework for query rewriting.\n","authors":["Yiteng Tu","Weihang Su","Yujia Zhou","Yiqun Liu","Fen Lin","Qin Liu","Qingyao Ai"],"pdf_url":"https://arxiv.org/pdf/2510.25488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25428v1","updated":"2025-10-29T11:50:52Z","published":"2025-10-29T11:50:52Z","title":"Alibaba International E-commerce Product Search Competition DcuRAGONs\n  Team Technical Report","summary":"  This report details our methodology and results developed for the\nMultilingual E-commerce Search Competition. The problem aims to recognize\nrelevance between user queries versus product items in a multilingual context\nand improve recommendation performance on e-commerce platforms. Utilizing Large\nLanguage Models (LLMs) and their capabilities in other tasks, our data-centric\nmethod achieved the highest score compared to other solutions during the\ncompetition. Final leaderboard is publised at\nhttps://alibaba-international-cikm2025.github.io. The source code for our\nproject is published at https://github.com/nhtlongcs/e-commerce-product-search.\n","authors":["Thang-Long Nguyen-Ho","Minh-Khoi Pham","Hoang-Bao Le"],"pdf_url":"https://arxiv.org/pdf/2510.25428v1.pdf","comment":"Alibaba International E-commerce Product Search Competition @ CIKM\n  2025"},{"id":"http://arxiv.org/abs/2510.25402v1","updated":"2025-10-29T11:20:18Z","published":"2025-10-29T11:20:18Z","title":"Towards Automated Quality Assurance of Patent Specifications: A\n  Multi-Dimensional LLM Framework","summary":"  Despite the surge in patent applications and emergence of AI drafting tools,\nsystematic evaluation of patent content quality has received limited research\nattention. To address this gap, We propose to evaluate patents using regulatory\ncompliance, technical coherence, and figure-reference consistency detection\nmodules, and then generate improvement suggestions via an integration module.\nThe framework is validated on a comprehensive dataset comprising 80\nhuman-authored and 80 AI-generated patents from two patent drafting tools.\nExperimental results show balanced accuracies of 99.74\\%, 82.12\\%, and 91.2\\%\nrespectively across the three detection modules when validated against expert\nannotations. Additional analysis was conducted to examine defect distributions\nacross patent sections, technical domains, and authoring sources. Section-based\nanalysis indicates that figure-text consistency and technical detail precision\nrequire particular attention. Mechanical Engineering and Construction show more\nclaim-specification inconsistencies due to complex technical documentation\nrequirements. AI-generated patents show a significant gap compared to\nhuman-authored ones. While human-authored patents primarily contain\nsurface-level errors like typos, AI-generated patents exhibit more structural\ndefects in figure-text alignment and cross-references.\n","authors":["Yuqian Chai","Chaochao Wang","Weilei Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25285v1","updated":"2025-10-29T08:42:15Z","published":"2025-10-29T08:42:15Z","title":"Revisiting scalable sequential recommendation with Multi-Embedding\n  Approach and Mixture-of-Experts","summary":"  In recommendation systems, how to effectively scale up recommendation models\nhas been an essential research topic. While significant progress has been made\nin developing advanced and scalable architectures for sequential\nrecommendation(SR) models, there are still challenges due to items'\nmulti-faceted characteristics and dynamic item relevance in the user context.\nTo address these issues, we propose Fuxi-MME, a framework that integrates a\nmulti-embedding strategy with a Mixture-of-Experts (MoE) architecture.\nSpecifically, to efficiently capture diverse item characteristics in a\ndecoupled manner, we decompose the conventional single embedding matrix into\nseveral lower-dimensional embedding matrices. Additionally, by substituting\nrelevant parameters in the Fuxi Block with an MoE layer, our model achieves\nadaptive and specialized transformation of the enriched representations.\nEmpirical results on public datasets show that our proposed framework\noutperforms several competitive baselines.\n","authors":["Qiushi Pan","Hao Wang","Guoyuan An","Luankang Zhang","Wei Guo","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25283v1","updated":"2025-10-29T08:39:36Z","published":"2025-10-29T08:39:36Z","title":"Measuring the Research Output and Performance of the University of\n  Ibadan from 2014 to 2023: A Scientometric Analysis","summary":"  This study employs scientometric methods to assess the research output and\nperformance of the University of Ibadan from 2014 to 2023. By analyzing\npublication trends, citation patterns, and collaboration networks, the research\naims to comprehensively evaluate the university's research productivity,\nimpact, and disciplinary focus. This article's endeavors are characterized by\ninnovation, interdisciplinary collaboration, and commitment to excellence,\nmaking the University of Ibadan a significant hub for cutting-edge research in\nNigeria and beyond. The goal of the current study is to ascertain the influence\nof the university's research output and publication patterns between 2014 and\n2023. The study focuses on the departments at the University of Ibadan that\ncontribute the most, the best journals for publishing, the nations that\ncollaborate, the impact of citations both locally and globally, well-known\nauthors and their total production, and the research output broken down by\nyear. According to the university's ten-year publication data, 7159 papers with\nan h-index of 75 were published between 2014 and 2023, garnering 218572\ncitations. Furthermore, the VOSviewer software mapping approach is used to\nillustrate the stenographical mapping of data through graphs. The findings of\nthis study will contribute to understanding the university's research\nstrengths, weaknesses, and potential areas for improvement. Additionally, the\nresults will inform evidence-based decision-making for enhancing research\nstrategies and policies at the University of Ibadan.\n","authors":["Muneer Ahmad","Undie Felicia Nkatv"],"pdf_url":"https://arxiv.org/pdf/2510.25283v1.pdf","comment":"16 pages, 5 figures, Research Paper"},{"id":"http://arxiv.org/abs/2503.05493v2","updated":"2025-10-29T08:19:03Z","published":"2025-03-07T15:05:23Z","title":"Can LLMs Outshine Conventional Recommenders? A Comparative Evaluation","summary":"  In recent years, integrating large language models (LLMs) into recommender\nsystems has created new opportunities for improving recommendation quality.\nHowever, a comprehensive benchmark is needed to thoroughly evaluate and compare\nthe recommendation capabilities of LLMs with traditional recommender systems.\nIn this paper, we introduce RecBench, which systematically investigates various\nitem representation forms (including unique identifier, text, semantic\nembedding, and semantic identifier) and evaluates two primary recommendation\ntasks, i.e., click-through rate prediction (CTR) and sequential recommendation\n(SeqRec). Our extensive experiments cover up to 17 large models and are\nconducted across five diverse datasets from fashion, news, video, books, and\nmusic domains. Our findings indicate that LLM-based recommenders outperform\nconventional recommenders, achieving up to a 5% AUC improvement in the CTR\nscenario and up to a 170% NDCG@10 improvement in the SeqRec scenario. However,\nthese substantial performance gains come at the expense of significantly\nreduced inference efficiency, rendering the LLM-as-RS paradigm impractical for\nreal-time recommendation environments. We aim for our findings to inspire\nfuture research, including recommendation-specific model acceleration methods.\nWe will release our code, data, configurations, and platform to enable other\nresearchers to reproduce and build upon our experimental results.\n","authors":["Qijiong Liu","Jieming Zhu","Lu Fan","Kun Wang","Hengchang Hu","Wei Guo","Yong Liu","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2503.05493v2.pdf","comment":"NeurIPS 2025 DB Track Accepted Paper"},{"id":"http://arxiv.org/abs/2510.25259v1","updated":"2025-10-29T08:14:03Z","published":"2025-10-29T08:14:03Z","title":"TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation","summary":"  Recently, convolutional filters have been increasingly adopted in sequential\nrecommendation for their ability to capture local sequential patterns. However,\nmost of these models complement convolutional filters with self-attention. This\nis because convolutional filters alone, generally fixed filters, struggle to\ncapture global interactions necessary for accurate recommendation. We propose\nTime-Variant Convolutional Filters for Sequential Recommendation (TV-Rec), a\nmodel inspired by graph signal processing, where time-variant graph filters\ncapture position-dependent temporal variations in user sequences. By replacing\nboth fixed kernels and self-attention with time-variant filters, TV-Rec\nachieves higher expressive power and better captures complex interaction\npatterns in user behavior. This design not only eliminates the need for\nself-attention but also reduces computation while accelerating inference.\nExtensive experiments on six public benchmarks show that TV-Rec outperforms\nstate-of-the-art baselines by an average of 7.49%.\n","authors":["Yehjin Shin","Jeongwhan Choi","Seojin Kim","Noseong Park"],"pdf_url":"https://arxiv.org/pdf/2510.25259v1.pdf","comment":"The 39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)"},{"id":"http://arxiv.org/abs/2510.25220v1","updated":"2025-10-29T06:54:42Z","published":"2025-10-29T06:54:42Z","title":"GReF: A Unified Generative Framework for Efficient Reranking via Ordered\n  Multi-token Prediction","summary":"  In a multi-stage recommendation system, reranking plays a crucial role in\nmodeling intra-list correlations among items. A key challenge lies in exploring\noptimal sequences within the combinatorial space of permutations. Recent\nresearch follows a two-stage (generator-evaluator) paradigm, where a generator\nproduces multiple feasible sequences, and an evaluator selects the best one. In\npractice, the generator is typically implemented as an autoregressive model.\nHowever, these two-stage methods face two main challenges. First, the\nseparation of the generator and evaluator hinders end-to-end training. Second,\nautoregressive generators suffer from inference efficiency. In this work, we\npropose a Unified Generative Efficient Reranking Framework (GReF) to address\nthe two primary challenges. Specifically, we introduce Gen-Reranker, an\nautoregressive generator featuring a bidirectional encoder and a dynamic\nautoregressive decoder to generate causal reranking sequences. Subsequently, we\npre-train Gen-Reranker on the item exposure order for high-quality parameter\ninitialization. To eliminate the need for the evaluator while integrating\nsequence-level evaluation during training for end-to-end optimization, we\npropose post-training the model through Rerank-DPO. Moreover, for efficient\nautoregressive inference, we introduce ordered multi-token prediction (OMTP),\nwhich trains Gen-Reranker to simultaneously generate multiple future items\nwhile preserving their order, ensuring practical deployment in real-time\nrecommender systems. Extensive offline experiments demonstrate that GReF\noutperforms state-of-the-art reranking methods while achieving latency that is\nnearly comparable to non-autoregressive models. Additionally, GReF has also\nbeen deployed in a real-world video app Kuaishou with over 300 million daily\nactive users, significantly improving online recommendation quality.\n","authors":["Zhijie Lin","Zhuofeng Li","Chenglei Dai","Wentian Bao","Shuai Lin","Enyun Yu","Haoxiang Zhang","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2510.25220v1.pdf","comment":"Accepted by CIKM 2025"},{"id":"http://arxiv.org/abs/2506.07466v2","updated":"2025-10-29T04:50:52Z","published":"2025-06-09T06:20:23Z","title":"Capturing User Interests from Data Streams for Continual Sequential\n  Recommendation","summary":"  Transformer-based sequential recommendation (SR) models excel at modeling\nlong-range dependencies in user behavior via self-attention. However, updating\nthem with continuously arriving behavior sequences incurs high computational\ncosts or leads to catastrophic forgetting. Although continual learning, a\nstandard approach for non-stationary data streams, has recently been applied to\nrecommendation, existing methods gradually forget long-term user preferences\nand remain underexplored in SR. In this paper, we introduce Continual\nSequential Transformer for Recommendation (CSTRec). CSTRec is designed to\neffectively adapt to current interests by leveraging well-preserved historical\nones, thus capturing the trajectory of user interests over time. The core of\nCSTRec is Continual Sequential Attention (CSA), a linear attention tailored for\ncontinual SR, which enables CSTRec to partially retain historical knowledge\nwithout direct access to prior data. CSA has two key components: (1)\nCauchy-Schwarz Normalization that stabilizes learning over time under uneven\nuser interaction frequencies; (2) Collaborative Interest Enrichment that\nalleviates forgetting through shared, learnable interest pools. In addition, we\nintroduce a new technique to facilitate the adaptation of new users by\ntransferring historical knowledge from existing users with similar interests.\nExtensive experiments on three real-world datasets show that CSTRec outperforms\nstate-of-the-art models in both knowledge retention and acquisition.\n","authors":["Gyuseok Lee","Hyunsik Yoo","Junyoung Hwang","SeongKu Kang","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2506.07466v2.pdf","comment":"WSDM'26"},{"id":"http://arxiv.org/abs/2510.25160v1","updated":"2025-10-29T04:29:17Z","published":"2025-10-29T04:29:17Z","title":"Model-Document Protocol for AI Search","summary":"  AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation.\n","authors":["Hongjin Qian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25160v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2510.06658v2","updated":"2025-10-29T03:24:46Z","published":"2025-10-08T05:17:33Z","title":"Can We Hide Machines in the Crowd? Quantifying Equivalence in\n  LLM-in-the-loop Annotation Tasks","summary":"  Many evaluations of large language models (LLMs) in text annotation focus\nprimarily on the correctness of the output, typically comparing model-generated\nlabels to human-annotated ``ground truth'' using standard performance metrics.\nIn contrast, our study moves beyond effectiveness alone. We aim to explore how\nlabeling decisions -- by both humans and LLMs -- can be statistically evaluated\nacross individuals. Rather than treating LLMs purely as annotation systems, we\napproach LLMs as an alternative annotation mechanism that may be capable of\nmimicking the subjective judgments made by humans. To assess this, we develop a\nstatistical evaluation method based on Krippendorff's $\\alpha$, paired\nbootstrapping, and the Two One-Sided t-Tests (TOST) equivalence test procedure.\nThis evaluation method tests whether an LLM can blend into a group of human\nannotators without being distinguishable.\n  We apply this approach to two datasets -- MovieLens 100K and PolitiFact --\nand find that the LLM is statistically indistinguishable from a human annotator\nin the former ($p = 0.004$), but not in the latter ($p = 0.155$), highlighting\ntask-dependent differences. It also enables early evaluation on a small sample\nof human data to inform whether LLMs are suitable for large-scale annotation in\na given application.\n","authors":["Jiaman He","Zikang Leng","Dana McKay","Damiano Spina","Johanne R. Trippas"],"pdf_url":"https://arxiv.org/pdf/2510.06658v2.pdf","comment":"Accepted at SIGIR-AP 2025"},{"id":"http://arxiv.org/abs/2510.25093v1","updated":"2025-10-29T01:57:38Z","published":"2025-10-29T01:57:38Z","title":"Continual Low-Rank Adapters for LLM-based Generative Recommender Systems","summary":"  While large language models (LLMs) achieve strong performance in\nrecommendation, they face challenges in continual learning as users, items, and\nuser preferences evolve over time. Existing LoRA-based continual methods\nprimarily focus on preserving performance on previous tasks, but this overlooks\nthe unique nature of recommendation: the goal is not to predict past\npreferences, and outdated preferences can even harm performance when current\ninterests shift significantly. To address this, we propose PESO (Proximally\nrEgularized Single evolving lOra, a continual adaptation method for LoRA in\nrecommendation. PESO introduces a proximal regularizer that anchors the current\nadapter to its most recent frozen state, enabling the model to flexibly balance\nadaptation and preservation, and to better capture recent user behaviors.\nTheoretically, we show that this proximal design provides data-aware,\ndirection-wise guidance in the LoRA subspace. Empirically, PESO consistently\noutperforms existing LoRA-based continual learning methods.\n","authors":["Hyunsik Yoo","Ting-Wei Li","SeongKu Kang","Zhining Liu","Charlie Xu","Qilin Qi","Hanghang Tong"],"pdf_url":"https://arxiv.org/pdf/2510.25093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26007v1","updated":"2025-10-29T22:35:34Z","published":"2025-10-29T22:35:34Z","title":"The Quest for Reliable Metrics of Responsible AI","summary":"  The development of Artificial Intelligence (AI), including AI in Science\n(AIS), should be done following the principles of responsible AI. Progress in\nresponsible AI is often quantified through evaluation metrics, yet there has\nbeen less work on assessing the robustness and reliability of the metrics\nthemselves. We reflect on prior work that examines the robustness of fairness\nmetrics for recommender systems as a type of AI application and summarise their\nkey takeaways into a set of non-exhaustive guidelines for developing reliable\nmetrics of responsible AI. Our guidelines apply to a broad spectrum of AI\napplications, including AIS.\n","authors":["Theresia Veronika Rampisela","Maria Maistro","Tuukka Ruotsalo","Christina Lioma"],"pdf_url":"https://arxiv.org/pdf/2510.26007v1.pdf","comment":"Accepted for presentation at the AI in Science Summit 2025"},{"id":"http://arxiv.org/abs/1810.06818v3","updated":"2025-10-29T14:50:15Z","published":"2018-10-16T05:40:03Z","title":"Large Language Models for Few-Shot Named Entity Recognition","summary":"  Named entity recognition (NER) is a fundamental task in numerous downstream\napplications. Recently, researchers have employed pre-trained language models\n(PLMs) and large language models (LLMs) to address this task. However, fully\nleveraging the capabilities of PLMs and LLMs with minimal human effort remains\nchallenging. In this paper, we propose GPT4NER, a method that prompts LLMs to\nresolve the few-shot NER task. GPT4NER constructs effective prompts using three\nkey components: entity definition, few-shot examples, and chain-of-thought. By\nprompting LLMs with these effective prompts, GPT4NER transforms few-shot NER,\nwhich is traditionally considered as a sequence-labeling problem, into a\nsequence-generation problem. We conduct experiments on two benchmark datasets,\nCoNLL2003 and OntoNotes5.0, and compare the performance of GPT4NER to\nrepresentative state-of-the-art models in both few-shot and fully supervised\nsettings. Experimental results demonstrate that GPT4NER achieves the $F_1$ of\n83.15\\% on CoNLL2003 and 70.37\\% on OntoNotes5.0, significantly outperforming\nfew-shot baselines by an average margin of 7 points. Compared to\nfully-supervised baselines, GPT4NER achieves 87.9\\% of their best performance\non CoNLL2003 and 76.4\\% of their best performance on OntoNotes5.0. We also\nutilize a relaxed-match metric for evaluation and report performance in the\nsub-task of named entity extraction (NEE), and experiments demonstrate their\nusefulness to help better understand model behaviors in the NER task.\n","authors":["Yufei Zhao","Xiaoshi Zhong","Erik Cambria","Jagath C. Rajapakse"],"pdf_url":"https://arxiv.org/pdf/1810.06818v3.pdf","comment":"17 pages, 2 figures. Accepted by AI, Computer Science and Robotics\n  Technology (ACRT)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2510.25770v1","updated":"2025-10-29T17:59:16Z","published":"2025-10-29T17:59:16Z","title":"E-Scores for (In)Correctness Assessment of Generative Model Outputs","summary":"  While generative models, especially large language models (LLMs), are\nubiquitous in today's world, principled mechanisms to assess their\n(in)correctness are limited. Using the conformal prediction framework, previous\nworks construct sets of LLM responses where the probability of including an\nincorrect response, or error, is capped at a desired user-defined tolerance\nlevel. However, since these methods are based on p-values, they are susceptible\nto p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the\nguarantees. We therefore leverage e-values to complement generative model\noutputs with e-scores as a measure of incorrectness. In addition to achieving\nthe same statistical guarantees as before, e-scores provide users flexibility\nin adaptively choosing tolerance levels after observing the e-scores\nthemselves, by upper bounding a post-hoc notion of error called size\ndistortion. We experimentally demonstrate their efficacy in assessing LLM\noutputs for different correctness types: mathematical factuality and property\nconstraints satisfaction.\n","authors":["Guneet S. Dhillon","Javier GonzÃ¡lez","Teodora Pandeva","Alicia Curth"],"pdf_url":"https://arxiv.org/pdf/2510.25770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25769v1","updated":"2025-10-29T17:59:06Z","published":"2025-10-29T17:59:06Z","title":"Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE\n  Solutions","summary":"  Stochastic differential equations (SDEs) are well suited to modelling noisy\nand irregularly sampled time series found in finance, physics, and machine\nlearning. Traditional approaches require costly numerical solvers to sample\nbetween arbitrary time points. We introduce Neural Stochastic Flows (NSFs) and\ntheir latent variants, which directly learn (latent) SDE transition laws using\nconditional normalising flows with architectural constraints that preserve\nproperties inherited from stochastic flows. This enables one-shot sampling\nbetween arbitrary states and yields up to two orders of magnitude speed-ups at\nlarge time gaps. Experiments on synthetic SDE simulations and on real-world\ntracking and video data show that NSFs maintain distributional accuracy\ncomparable to numerical approaches while dramatically reducing computation for\narbitrary time-point sampling.\n","authors":["Naoki Kiyohara","Edward Johns","Yingzhen Li"],"pdf_url":"https://arxiv.org/pdf/2510.25769v1.pdf","comment":"NeurIPS 2025 (poster). Project page:\n  https://nkiyohara.github.io/nsf-neurips2025/"},{"id":"http://arxiv.org/abs/2510.18905v2","updated":"2025-10-29T17:57:23Z","published":"2025-10-21T01:03:46Z","title":"3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and\n  Latency","summary":"  AI inference scaling is often tuned through 1D heuristics (a fixed reasoning\npasses) or 2D bivariate trade-offs (e.g., performance vs. compute), which fail\nto consider cost and latency constraints. We introduce a 3D optimization\nframework that jointly calibrates accuracy, cost, and latency within a unified\ndecision space, enabling constraints-aware inference scaling. Using Monte Carlo\nsimulations across three representative scenarios and nine simulated large\nlanguage models, we evaluate four optimization methods to address the 3D\nmulti-objective optimization (MOO) problem. Framing inference scaling in MOO\nshapes a feasible space that 1D and 2D optimizations fail to capture, enabling\nenvironmentadaptive selection of the inference scaling k. Results show that\nknee-point optimization achieves the best balance, while accuracy-maximization\nremains favorable when precision is prioritized. The framework establishes a\ntheoretical foundation for deployment-aware inference scaling across diverse\noperational contexts.\n","authors":["Minseok Jung","Abhas Ricky","Muhammad Rameez Chatni"],"pdf_url":"https://arxiv.org/pdf/2510.18905v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01939v3","updated":"2025-10-29T17:57:03Z","published":"2025-07-02T17:49:52Z","title":"SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars","summary":"  In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy.\n","authors":["Xiaosheng Zhao","Yang Huang","Guirong Xue","Xiao Kong","Jifeng Liu","Xiaoyu Tang","Timothy C. Beers","Yuan-Sen Ting","A-Li Luo"],"pdf_url":"https://arxiv.org/pdf/2507.01939v3.pdf","comment":"27 pages, 8 figures, 5 tables. Minor update: added corrected\n  acknowledgments and corrected a misstated hyperparameter value (noted in\n  footnote) for reproducibility. Submitted to AAS Journals. Comments welcome"},{"id":"http://arxiv.org/abs/2510.25759v1","updated":"2025-10-29T17:55:17Z","published":"2025-10-29T17:55:17Z","title":"Synthetic Data Reveals Generalization Gaps in Correlated Multiple\n  Instance Learning","summary":"  Multiple instance learning (MIL) is often used in medical imaging to classify\nhigh-resolution 2D images by processing patches or classify 3D volumes by\nprocessing slices. However, conventional MIL approaches treat instances\nseparately, ignoring contextual relationships such as the appearance of nearby\npatches or slices that can be essential in real applications. We design a\nsynthetic classification task where accounting for adjacent instance features\nis crucial for accurate prediction. We demonstrate the limitations of\noff-the-shelf MIL approaches by quantifying their performance compared to the\noptimal Bayes estimator for this task, which is available in closed-form. We\nempirically show that newer correlated MIL methods still struggle to generalize\nas well as possible when trained from scratch on tens of thousands of\ninstances.\n","authors":["Ethan Harvey","Dennis Johan Loevlie","Michael C. Hughes"],"pdf_url":"https://arxiv.org/pdf/2510.25759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25755v1","updated":"2025-10-29T17:52:39Z","published":"2025-10-29T17:52:39Z","title":"MLPrE -- A tool for preprocessing and exploratory data analysis prior to\n  machine learning model construction","summary":"  With the recent growth of Deep Learning for AI, there is a need for tools to\nmeet the demand of data flowing into those models. In some cases, source data\nmay exist in multiple formats, and therefore the source data must be\ninvestigated and properly engineered for a Machine Learning model or graph\ndatabase. Overhead and lack of scalability with existing workflows limit\nintegration within a larger processing pipeline such as Apache Airflow, driving\nthe need for a robust, extensible, and lightweight tool to preprocess arbitrary\ndatasets that scales with data type and size. To address this, we present\nMachine Learning Preprocessing and Exploratory Data Analysis, MLPrE, in which\nSparkDataFrames were utilized to hold data during processing and ensure\nscalability. A generalizable JSON input file format was utilized to describe\nstepwise changes to that DataFrame. Stages were implemented for input and\noutput, filtering, basic statistics, feature engineering, and exploratory data\nanalysis. A total of 69 stages were implemented into MLPrE, of which we\nhighlight and demonstrate key stages using six diverse datasets. We further\nhighlight MLPrE's ability to independently process multiple fields in flat\nfiles and recombine them, otherwise requiring an additional pipeline, using a\nUniProt glossary term dataset. Building on this advantage, we demonstrated the\nclustering stage with available wine quality data. Lastly, we demonstrate the\npreparation of data for a graph database in the final stages of MLPrE using\nphosphosite kinase data. Overall, our MLPrE tool offers a generalizable and\nscalable tool for preprocessing and early data analysis, filling a critical\nneed for such a tool given the ever expanding use of machine learning. This\ntool serves to accelerate and simplify early stage development in larger\nworkflows.\n","authors":["David S Maxwell","Michael Darkoh","Sidharth R Samudrala","Caroline Chung","Stephanie T Schmidt","Bissan Al-Lazikani"],"pdf_url":"https://arxiv.org/pdf/2510.25755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.17022v2","updated":"2025-10-29T17:52:01Z","published":"2025-10-19T22:04:57Z","title":"Curiosity-driven RL for symbolic equation solving","summary":"  We explore if RL can be useful for symbolic mathematics. Previous work showed\ncontrastive learning can solve linear equations in one variable. We show\nmodel-free PPO \\cite{schulman2017proximal} augmented with curiosity-based\nexploration and graph-based actions can solve nonlinear equations such as those\ninvolving radicals, exponentials, and trig functions. Our work suggests\ncuriosity-based exploration may be useful for general symbolic reasoning tasks.\n","authors":["Kevin P. O'Keeffe"],"pdf_url":"https://arxiv.org/pdf/2510.17022v2.pdf","comment":"Accepted at the NeurIPS 2025 MATH-AI Workshop"},{"id":"http://arxiv.org/abs/2510.25753v1","updated":"2025-10-29T17:51:57Z","published":"2025-10-29T17:51:57Z","title":"How Data Mixing Shapes In-Context Learning: Asymptotic Equivalence for\n  Transformers with MLPs","summary":"  Pretrained Transformers demonstrate remarkable in-context learning (ICL)\ncapabilities, enabling them to adapt to new tasks from demonstrations without\nparameter updates. However, theoretical studies often rely on simplified\narchitectures (e.g., omitting MLPs), data models (e.g., linear regression with\nisotropic inputs), and single-source training, limiting their relevance to\nrealistic settings. In this work, we study ICL in pretrained Transformers with\nnonlinear MLP heads on nonlinear tasks drawn from multiple data sources with\nheterogeneous input, task, and noise distributions. We analyze a model where\nthe MLP comprises two layers, with the first layer trained via a single\ngradient step and the second layer fully optimized. Under high-dimensional\nasymptotics, we prove that such models are equivalent in ICL error to\nstructured polynomial predictors, leveraging results from the theory of\nGaussian universality and orthogonal polynomials. This equivalence reveals that\nnonlinear MLPs meaningfully enhance ICL performance, particularly on nonlinear\ntasks, compared to linear baselines. It also enables a precise analysis of data\nmixing effects: we identify key properties of high-quality data sources (low\nnoise, structured covariances) and show that feature learning emerges only when\nthe task covariance exhibits sufficient structure. These results are validated\nempirically across various activation functions, model sizes, and data\ndistributions. Finally, we experiment with a real-world scenario involving\nmultilingual sentiment analysis where each language is treated as a different\nsource. Our experimental results for this case exemplify how our findings\nextend to real-world cases. Overall, our work advances the theoretical\nfoundations of ICL in Transformers and provides actionable insight into the\nrole of architecture and data in ICL.\n","authors":["Samet Demir","Zafer Dogan"],"pdf_url":"https://arxiv.org/pdf/2510.25753v1.pdf","comment":"NeurIPS 2025, 24 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.18549v3","updated":"2025-10-29T17:50:44Z","published":"2025-07-24T16:13:56Z","title":"The Price equation reveals a universal force-metric-bias law of\n  algorithmic learning and natural selection","summary":"  Diverse learning algorithms, optimization methods, and natural selection\nshare a common mathematical structure, despite their apparent differences. Here\nI show that a simple notational partitioning of change by the Price equation\nreveals a universal force-metric-bias (FMB) law: $\\Delta\\mathbf{\\theta} =\n\\mathbf{M}\\,\\mathbf{f} + \\mathbf{b} + \\mathbf{\\xi}$. The force $\\mathbf{f}$\ndrives improvement in parameters, $\\Delta\\mathbf{\\theta}$, in proportion to the\nslope of performance with respect to the parameters. The metric $\\mathbf{M}$\nrescales movement by inverse curvature. The bias $\\mathbf{b}$ adds momentum or\nchanges in the frame of reference. The noise $\\mathbf{\\xi}$ enables\nexploration. This framework unifies natural selection, Bayesian updating,\nNewton's method, stochastic gradient descent, stochastic Langevin dynamics,\nAdam optimization, and most other algorithms as special cases of the same\nunderlying process. The Price equation also reveals why Fisher information,\nKullback-Leibler divergence, and d'Alembert's principle arise naturally in\nlearning dynamics. By exposing this common structure, the FMB law provides a\nprincipled foundation for understanding, comparing, and designing learning\nalgorithms across disciplines.\n","authors":["Steven A. Frank"],"pdf_url":"https://arxiv.org/pdf/2507.18549v3.pdf","comment":"Version 2: fixed definition of force in abstract; Version 3: added\n  citations and some minor editing"},{"id":"http://arxiv.org/abs/2510.25752v1","updated":"2025-10-29T17:49:40Z","published":"2025-10-29T17:49:40Z","title":"Meshless solutions of PDE inverse problems on irregular geometries","summary":"  Solving inverse and optimization problems over solutions of nonlinear partial\ndifferential equations (PDEs) on complex spatial domains is a long-standing\nchallenge. Here we introduce a method that parameterizes the solution using\nspectral bases on arbitrary spatiotemporal domains, whereby the basis is\ndefined on a hyperrectangle containing the true domain. We find the\ncoefficients of the basis expansion by solving an optimization problem whereby\nboth the equations, the boundary conditions and any optimization targets are\nenforced by a loss function, building on a key idea from Physics-Informed\nNeural Networks (PINNs). Since the representation of the function natively has\nexponential convergence, so does the solution of the optimization problem, as\nlong as it can be solved efficiently. We find empirically that the optimization\nprotocols developed for machine learning find solutions with exponential\nconvergence on a wide range of equations. The method naturally allows for the\nincorporation of data assimilation by including additional terms in the loss\nfunction, and for the efficient solution of optimization problems over the PDE\nsolutions.\n","authors":["James V. Roggeveen","Michael P. Brenner"],"pdf_url":"https://arxiv.org/pdf/2510.25752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20762v3","updated":"2025-10-29T17:47:39Z","published":"2025-03-26T17:50:13Z","title":"ASGO: Adaptive Structured Gradient Optimization","summary":"  Training deep neural networks is a structured optimization problem, because\nthe parameters are naturally represented by matrices and tensors rather than by\nvectors. Under this structural representation, it has been widely observed that\ngradients are low-rank and Hessians are approximately block diagonal. These\nstructured properties are crucial for designing efficient optimization\nalgorithms, but are not utilized by many current popular optimizers like Adam.\nIn this paper, we present a novel optimization algorithm ASGO that capitalizes\non these properties by employing a preconditioner that is adaptively updated\nusing structured gradients. By a fine-grained theoretical analysis, ASGO is\nproven to achieve superior convergence rates compared to existing structured\ngradient methods. Based on this convergence theory, we further demonstrate that\nASGO can benefit from low-rank gradients and block diagonal Hessians. We also\ndiscuss practical modifications of ASGO and empirically verify ASGO's\neffectiveness on language model tasks. Code is available at\nhttps://github.com/infinity-stars/ASGO.\n","authors":["Kang An","Yuxing Liu","Rui Pan","Yi Ren","Shiqian Ma","Donald Goldfarb","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.20762v3.pdf","comment":"39 pages"},{"id":"http://arxiv.org/abs/2502.21269v3","updated":"2025-10-29T17:46:23Z","published":"2025-02-28T17:45:26Z","title":"Dynamical Decoupling of Generalization and Overfitting in Large\n  Two-Layer Networks","summary":"  Understanding the inductive bias and generalization properties of large\noverparametrized machine learning models requires to characterize the dynamics\nof the training algorithm. We study the learning dynamics of large two-layer\nneural networks via dynamical mean field theory, a well established technique\nof non-equilibrium statistical physics. We show that, for large network width\n$m$, and large number of samples per input dimension $n/d$, the training\ndynamics exhibits a separation of timescales which implies: $(i)$~The emergence\nof a slow time scale associated with the growth in Gaussian/Rademacher\ncomplexity of the network; $(ii)$~Inductive bias towards small complexity if\nthe initialization has small enough complexity; $(iii)$~A dynamical decoupling\nbetween feature learning and overfitting regimes; $(iv)$~A non-monotone\nbehavior of the test error, associated `feature unlearning' regime at large\ntimes.\n","authors":["Andrea Montanari","Pierfrancesco Urbani"],"pdf_url":"https://arxiv.org/pdf/2502.21269v3.pdf","comment":"88 pages; 63 pdf figures"},{"id":"http://arxiv.org/abs/2510.25739v1","updated":"2025-10-29T17:43:31Z","published":"2025-10-29T17:43:31Z","title":"Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image\n  Generation","summary":"  Autoregressive (AR) image generation models are capable of producing\nhigh-fidelity images but often suffer from slow inference due to their\ninherently sequential, token-by-token decoding process. Speculative decoding,\nwhich employs a lightweight draft model to approximate the output of a larger\nAR model, has shown promise in accelerating text generation without\ncompromising quality. However, its application to image generation remains\nlargely underexplored. The challenges stem from a significantly larger sampling\nspace, which complicates the alignment between the draft and target model\noutputs, coupled with the inadequate use of the two-dimensional spatial\nstructure inherent in images, thereby limiting the modeling of local\ndependencies. To overcome these challenges, we introduce Hawk, a new approach\nthat harnesses the spatial structure of images to guide the speculative model\ntoward more accurate and efficient predictions. Experimental results on\nmultiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR\nmodels, while preserving both image fidelity and diversity.\n","authors":["Zhi-Kai Chen","Jun-Peng Jiang","Han-Jia Ye","De-Chuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2510.25739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25731v1","updated":"2025-10-29T17:37:27Z","published":"2025-10-29T17:37:27Z","title":"LieSolver: A PDE-constrained solver for IBVPs using Lie symmetries","summary":"  We introduce a method for efficiently solving initial-boundary value problems\n(IBVPs) that uses Lie symmetries to enforce the associated partial differential\nequation (PDE) exactly by construction. By leveraging symmetry transformations,\nthe model inherently incorporates the physical laws and learns solutions from\ninitial and boundary data. As a result, the loss directly measures the model's\naccuracy, leading to improved convergence. Moreover, for well-posed IBVPs, our\nmethod enables rigorous error estimation. The approach yields compact models,\nfacilitating an efficient optimization. We implement LieSolver and demonstrate\nits application to linear homogeneous PDEs with a range of initial conditions,\nshowing that it is faster and more accurate than physics-informed neural\nnetworks (PINNs). Overall, our method improves both computational efficiency\nand the reliability of predictions for PDE-constrained problems.\n","authors":["RenÃ© P. Klausen","Ivan Timofeev","Johannes Frank","Jonas Naujoks","Thomas Wiegand","Sebastian Lapuschkin","Wojciech Samek"],"pdf_url":"https://arxiv.org/pdf/2510.25731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25729v1","updated":"2025-10-29T17:34:10Z","published":"2025-10-29T17:34:10Z","title":"Physics-Guided Conditional Diffusion Networks for Microwave Image\n  Reconstruction","summary":"  A conditional latent-diffusion based framework for solving the\nelectromagnetic inverse scattering problem associated with microwave imaging is\nintroduced. This generative machine-learning model explicitly mirrors the\nnon-uniqueness of the ill-posed inverse problem. Unlike existing inverse\nsolvers utilizing deterministic machine learning techniques that produce a\nsingle reconstruction, the proposed latent-diffusion model generates multiple\nplausible permittivity maps conditioned on measured scattered-field data,\nthereby generating several potential instances in the range-space of the\nnon-unique inverse mapping. A forward electromagnetic solver is integrated into\nthe reconstruction pipeline as a physics-based evaluation mechanism. The space\nof candidate reconstructions form a distribution of possibilities consistent\nwith the conditioning data and the member of this space yielding the lowest\nscattered-field data discrepancy between the predicted and measured scattered\nfields is reported as the final solution. Synthetic and experimental labeled\ndatasets are used for training and evaluation of the model. An innovative\nlabeled synthetic dataset is created that exemplifies a varied set of\nscattering features. Training of the model using this new dataset produces high\nquality permittivity reconstructions achieving improved generalization with\nexcellent fidelity to shape recognition. The results highlight the potential of\nhybrid generative physics frameworks as a promising direction for robust,\ndata-driven microwave imaging.\n","authors":["Shirin Chehelgami","Joe LoVetri","Vahab Khoshdel"],"pdf_url":"https://arxiv.org/pdf/2510.25729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.21355v2","updated":"2025-10-29T17:23:18Z","published":"2025-06-26T15:08:18Z","title":"SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context\n  Learning","summary":"  Multimodal in-context learning (ICL) remains underexplored despite\nsignificant potential for domains such as medicine. Clinicians routinely\nencounter diverse, specialized tasks requiring adaptation from limited\nexamples, such as drawing insights from a few relevant prior cases or\nconsidering a constrained set of differential diagnoses. While multimodal large\nlanguage models (MLLMs) have shown advances in medical visual question\nanswering (VQA), their ability to learn multimodal tasks from context is\nlargely unknown. We introduce SMMILE, the first expert-driven multimodal ICL\nbenchmark for medical tasks. Eleven medical experts curated problems, each\nincluding a multimodal query and multimodal in-context examples as task\ndemonstrations. SMMILE encompasses 111 problems (517 question-image-answer\ntriplets) covering 6 medical specialties and 13 imaging modalities. We further\nintroduce SMMILE++, an augmented variant with 1038 permuted problems. A\ncomprehensive evaluation of 15 MLLMs demonstrates that most models exhibit\nmoderate to poor multimodal ICL ability in medical tasks. In open-ended\nevaluations, ICL contributes only an 8% average improvement over zero-shot on\nSMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant\nin-context examples: even a single noisy or irrelevant example can degrade\nperformance by up to 9.5%. Moreover, we observe that MLLMs are affected by a\nrecency bias, where placing the most relevant example last can lead to\nsubstantial performance improvements of up to 71%. Our findings highlight\ncritical limitations and biases in current MLLMs when learning multimodal\nmedical tasks from context. SMMILE is available at\nhttps://smmile-benchmark.github.io.\n","authors":["Melanie Rieff","Maya Varma","Ossian Rabow","Subathra Adithan","Julie Kim","Ken Chang","Hannah Lee","Nidhi Rohatgi","Christian Bluethgen","Mohamed S. Muneer","Jean-Benoit Delbrouck","Michael Moor"],"pdf_url":"https://arxiv.org/pdf/2506.21355v2.pdf","comment":"NeurIPS 2025 (Datasets & Benchmarks Track)"},{"id":"http://arxiv.org/abs/2502.02270v2","updated":"2025-10-29T17:15:10Z","published":"2025-02-04T12:31:00Z","title":"Exact Sequence Interpolation with Transformers","summary":"  We prove that transformers can exactly interpolate datasets of finite input\nsequences in $\\mathbb{R}^d$, $d\\geq 2$, with corresponding output sequences of\nsmaller or equal length. Specifically, given $N$ sequences of arbitrary but\nfinite lengths in $\\mathbb{R}^d$ and output sequences of lengths $m^1, \\dots,\nm^N \\in \\mathbb{N}$, we construct a transformer with $\\mathcal{O}(\\sum_{j=1}^N\nm^j)$ blocks and $\\mathcal{O}(d \\sum_{j=1}^N m^j)$ parameters that exactly\ninterpolates the dataset. Our construction provides complexity estimates that\nare independent of the input sequence length, by alternating feed-forward and\nself-attention layers and by capitalizing on the clustering effect inherent to\nthe latter. Our novel constructive method also uses low-rank parameter matrices\nin the self-attention mechanism, a common feature of practical transformer\nimplementations. These results are first established in the hardmax\nself-attention setting, where the geometric structure permits an explicit and\nquantitative analysis, and are then extended to the softmax setting. Finally,\nwe demonstrate the applicability of our exact interpolation construction to\nlearning problems, in particular by providing convergence guarantees to a\nglobal minimizer under regularized training strategies. Our analysis\ncontributes to the theoretical understanding of transformer models, offering an\nexplanation for their excellent performance in exact sequence-to-sequence\ninterpolation tasks.\n","authors":["Albert Alcalde","Giovanni Fantuzzi","Enrique Zuazua"],"pdf_url":"https://arxiv.org/pdf/2502.02270v2.pdf","comment":"27 pages, 9 figures. Funded by the European Union (Horizon Europe\n  MSCA project ModConFlex, grant number 101073558)"},{"id":"http://arxiv.org/abs/2510.25704v1","updated":"2025-10-29T17:12:21Z","published":"2025-10-29T17:12:21Z","title":"Scaling flow-based approaches for topology sampling in $\\mathrm{SU}(3)$\n  gauge theory","summary":"  We develop a methodology based on out-of-equilibrium simulations to mitigate\ntopological freezing when approaching the continuum limit of lattice gauge\ntheories. We reduce the autocorrelation of the topological charge employing\nopen boundary conditions, while removing exactly their unphysical effects using\na non-equilibrium Monte Carlo approach in which periodic boundary conditions\nare gradually switched on. We perform a detailed analysis of the computational\ncosts of this strategy in the case of the four-dimensional $\\mathrm{SU}(3)$\nYang-Mills theory. After achieving full control of the scaling, we outline a\nclear strategy to sample topology efficiently in the continuum limit, which we\ncheck at lattice spacings as small as $0.045$ fm. We also generalize this\napproach by designing a customized Stochastic Normalizing Flow for evolutions\nin the boundary conditions, obtaining superior performances with respect to the\npurely stochastic non-equilibrium approach, and paving the way for more\nefficient future flow-based solutions.\n","authors":["Claudio Bonanno","Andrea Bulgarelli","Elia Cellini","Alessandro Nada","Dario Panfalone","Davide Vadacchino","Lorenzo Verzichelli"],"pdf_url":"https://arxiv.org/pdf/2510.25704v1.pdf","comment":"1+39 pages, 14 figures"},{"id":"http://arxiv.org/abs/2310.02806v3","updated":"2025-10-29T17:05:43Z","published":"2023-10-04T13:33:37Z","title":"MP-FVM: Enhancing Finite Volume Method for Water Infiltration Modeling\n  in Unsaturated Soils via Message-passing Encoder-decoder Network","summary":"  The spatiotemporal water flow dynamics in unsaturated soils can generally be\nmodeled by the Richards equation. To overcome the computational challenges\nassociated with solving this highly nonlinear partial differential equation\n(PDE), we present a novel solution algorithm, which we name as the MP-FVM\n(Message Passing-Finite Volume Method), to holistically integrate adaptive\nfixed-point iteration scheme, encoder-decoder neural network architecture,\nSobolev training, and message passing mechanism in a finite volume\ndiscretization framework. We thoroughly discuss the need and benefits of\nintroducing these components to achieve synergistic improvements in accuracy\nand stability of the solution. We also show that our MP-FVM algorithm can\naccurately solve the mixed-form $n$-dimensional Richards equation with\nguaranteed convergence under reasonable assumptions. Through several\nillustrative examples, we demonstrate that our MP-FVM algorithm not only\nachieves superior accuracy, but also better preserves the underlying physical\nlaws and mass conservation of the Richards equation compared to\nstate-of-the-art solution algorithms and the commercial HYDRUS solver.\n","authors":["Zeyuan Song","Zheyu Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.02806v3.pdf","comment":"36 pages, 14 figures, Accepted by Computers and Geotechnics"},{"id":"http://arxiv.org/abs/2510.25696v1","updated":"2025-10-29T17:00:36Z","published":"2025-10-29T17:00:36Z","title":"Convolutional Spiking-based GRU Cell for Spatio-temporal Data","summary":"  Spike-based temporal messaging enables SNNs to efficiently process both\npurely temporal and spatio-temporal time-series or event-driven data. Combining\nSNNs with Gated Recurrent Units (GRUs), a variant of recurrent neural networks,\ngives rise to a robust framework for sequential data processing; however,\ntraditional RNNs often lose local details when handling long sequences.\nPrevious approaches, such as SpikGRU, fail to capture fine-grained local\ndependencies in event-based spatio-temporal data. In this paper, we introduce\nthe Convolutional Spiking GRU (CS-GRU) cell, which leverages convolutional\noperations to preserve local structure and dependencies while integrating the\ntemporal precision of spiking neurons with the efficient gating mechanisms of\nGRUs. This versatile architecture excels on both temporal datasets (NTIDIGITS,\nSHD) and spatio-temporal benchmarks (MNIST, DVSGesture, CIFAR10DVS). Our\nexperiments show that CS-GRU outperforms state-of-the-art GRU variants by an\naverage of 4.35%, achieving over 90% accuracy on sequential tasks and up to\n99.31% on MNIST. It is worth noting that our solution achieves 69% higher\nefficiency compared to SpikGRU. The code is available at:\nhttps://github.com/YesmineAbdennadher/CS-GRU.\n","authors":["Yesmine Abdennadher","Eleonora Cicciarella","Michele Rossi"],"pdf_url":"https://arxiv.org/pdf/2510.25696v1.pdf","comment":"6 pages, 1 figure. Published in 2025 IEEE International Workshop On\n  Machine Learning for Signal Processing, Aug. 31-Sep. 3, 2025, Istanbul,\n  Turkey"},{"id":"http://arxiv.org/abs/2510.23323v2","updated":"2025-10-29T16:59:39Z","published":"2025-10-24T14:47:49Z","title":"Towards Scaling Deep Neural Networks with Predictive Coding: Theory and\n  Practice","summary":"  Backpropagation (BP) is the standard algorithm for training the deep neural\nnetworks that power modern artificial intelligence including large language\nmodels. However, BP is energy inefficient and unlikely to be implemented by the\nbrain. This thesis studies an alternative, potentially more efficient\nbrain-inspired algorithm called predictive coding (PC). Unlike BP, PC networks\n(PCNs) perform inference by iterative equilibration of neuron activities before\nlearning or weight updates. Recent work has suggested that this iterative\ninference procedure provides a range of benefits over BP, such as faster\ntraining. However, these advantages have not been consistently observed, the\ninference and learning dynamics of PCNs are still poorly understood, and deep\nPCNs remain practically untrainable. Here, we make significant progress towards\nscaling PCNs by taking a theoretical approach grounded in optimisation theory.\nFirst, we show that the learning dynamics of PC can be understood as an\napproximate trust-region method using second-order information, despite\nexplicitly using only first-order local updates. Second, going beyond this\napproximation, we show that PC can in principle make use of arbitrarily\nhigher-order information, such that for feedforward networks the effective\nlandscape on which PC learns is far more benign and robust to vanishing\ngradients than the (mean squared error) loss landscape. Third, motivated by a\nstudy of the inference dynamics of PCNs, we propose a new parameterisation\ncalled \"$\\mu$PC\", which for the first time allows stable training of 100+ layer\nnetworks with little tuning and competitive performance on simple tasks.\nOverall, this thesis significantly advances our fundamental understanding of\nthe inference and learning dynamics of PCNs, while highlighting the need for\nfuture research to focus on hardware co-design if PC is to compete with BP at\nscale.\n","authors":["Francesco Innocenti"],"pdf_url":"https://arxiv.org/pdf/2510.23323v2.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2510.25693v1","updated":"2025-10-29T16:57:54Z","published":"2025-10-29T16:57:54Z","title":"PyDPF: A Python Package for Differentiable Particle Filtering","summary":"  State-space models (SSMs) are a widely used tool in time series analysis. In\nthe complex systems that arise from real-world data, it is common to employ\nparticle filtering (PF), an efficient Monte Carlo method for estimating the\nhidden state corresponding to a sequence of observations. Applying particle\nfiltering requires specifying both the parametric form and the parameters of\nthe system, which are often unknown and must be estimated. Gradient-based\noptimisation techniques cannot be applied directly to standard particle\nfilters, as the filters themselves are not differentiable. However, several\nrecently proposed methods modify the resampling step to make particle filtering\ndifferentiable. In this paper, we present an implementation of several such\ndifferentiable particle filters (DPFs) with a unified API built on the popular\nPyTorch framework. Our implementation makes these algorithms easily accessible\nto a broader research community and facilitates straightforward comparison\nbetween them. We validate our framework by reproducing experiments from several\nexisting studies and demonstrate how DPFs can be applied to address several\ncommon challenges with state space modelling.\n","authors":["John-Joseph Brady","Benjamin Cox","VÃ­ctor Elvira","Yunpeng Li"],"pdf_url":"https://arxiv.org/pdf/2510.25693v1.pdf","comment":"42 pages, 0 figures, under review at the Journal of Statistical\n  Software, the python package can be found at https://pypi.org/project/pydpf/\n  , the full documentation at\n  https://python-dpf.readthedocs.io/en/latest/#documentation-index , and the\n  source code including experiments and replication material at\n  https://github.com/John-JoB/pydpf"},{"id":"http://arxiv.org/abs/2510.25692v1","updated":"2025-10-29T16:57:33Z","published":"2025-10-29T16:57:33Z","title":"A Configuration-First Framework for Reproducible, Low-Code Localization","summary":"  Machine learning is increasingly permeating radio-based localization\nservices. To keep results credible and comparable, everyday workflows should\nmake rigorous experiment specification and exact repeatability the default,\nwithout blocking advanced experimentation. However, in practice, researchers\nface a three-way gap that could be filled by a framework that offers (i) low\ncoding effort for end-to-end studies, (ii) reproducibility by default including\nversioned code, data, and configurations, controlled randomness, isolated runs,\nand recorded artifacts, and (iii) built-in extensibility so new models,\nmetrics, and stages can be added with minimal integration effort. Existing\ntools rarely deliver all three for machine learning in general and localization\nworkflows in particular. In this paper we introduce LOCALIZE, a low-code,\nconfiguration-first framework for radio localization in which experiments are\ndeclared in human-readable configuration, a workflow orchestrator runs\nstandardized pipelines from data preparation to reporting, and all artifacts,\nsuch as datasets, models, metrics, and reports, are versioned. The\npreconfigured, versioned datasets reduce initial setup and boilerplate,\nspeeding up model development and evaluation. The design, with clear extension\npoints, allows experts to add components without reworking the infrastructure.\nIn a qualitative comparison and a head-to-head study against a plain Jupyter\nnotebook baseline, we show that the framework reduces authoring effort while\nmaintaining comparable runtime and memory behavior. Furthermore, using a\nBluetooth Low Energy dataset, we show that scaling across training data (1x to\n10x) keeps orchestration overheads bounded as data grows. Overall, the\nframework makes reproducible machine-learning-based localization\nexperimentation practical, accessible, and extensible.\n","authors":["Tim Strnad","BlaÅ¾ BertalaniÄ","Carolina Fortuna"],"pdf_url":"https://arxiv.org/pdf/2510.25692v1.pdf","comment":"20 pages, 7 figures. Preprint submitted to ACM Transactions on\n  Software Engineering and Methodology (TOSEM), 2025"},{"id":"http://arxiv.org/abs/2510.25687v1","updated":"2025-10-29T16:50:54Z","published":"2025-10-29T16:50:54Z","title":"Model Inversion Attacks Meet Cryptographic Fuzzy Extractors","summary":"  Model inversion attacks pose an open challenge to privacy-sensitive\napplications that use machine learning (ML) models. For example, face\nauthentication systems use modern ML models to compute embedding vectors from\nface images of the enrolled users and store them. If leaked, inversion attacks\ncan accurately reconstruct user faces from the leaked vectors. There is no\nsystematic characterization of properties needed in an ideal defense against\nmodel inversion, even for the canonical example application of a face\nauthentication system susceptible to data breaches, despite a decade of\nbest-effort solutions.\n  In this paper, we formalize the desired properties of a provably strong\ndefense against model inversion and connect it, for the first time, to the\ncryptographic concept of fuzzy extractors. We further show that existing fuzzy\nextractors are insecure for use in ML-based face authentication. We do so\nthrough a new model inversion attack called PIPE, which achieves a success rate\nof over 89% in most cases against prior schemes. We then propose L2FE-Hash, the\nfirst candidate fuzzy extractor which supports standard Euclidean distance\ncomparators as needed in many ML-based applications, including face\nauthentication. We formally characterize its computational security guarantees,\neven in the extreme threat model of full breach of stored secrets, and\nempirically show its usable accuracy in face authentication for practical face\ndistributions. It offers attack-agnostic security without requiring any\nre-training of the ML model it protects. Empirically, it nullifies both prior\nstate-of-the-art inversion attacks as well as our new PIPE attack.\n","authors":["Mallika Prabhakar","Louise Xu","Prateek Saxena"],"pdf_url":"https://arxiv.org/pdf/2510.25687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02804v3","updated":"2025-10-29T16:49:31Z","published":"2023-12-05T14:44:58Z","title":"Score-Aware Policy-Gradient and Performance Guarantees using Local\n  Lyapunov Stability","summary":"  In this paper, we introduce a policy-gradient method for model-based\nreinforcement learning (RL) that exploits a type of stationary distributions\ncommonly obtained from Markov decision processes (MDPs) in stochastic networks,\nqueueing systems, and statistical mechanics. Specifically, when the stationary\ndistribution of the MDP belongs to an exponential family that is parametrized\nby policy parameters, we can improve existing policy gradient methods for\naverage-reward RL. Our key identification is a family of gradient estimators,\ncalled score-aware gradient estimators (SAGEs), that enable policy gradient\nestimation without relying on value-function estimation in the aforementioned\nsetting. We show that SAGE-based policy-gradient locally converges, and we\nobtain its regret. This includes cases when the state space of the MDP is\ncountable and unstable policies can exist. Under appropriate assumptions such\nas starting sufficiently close to a maximizer and the existence of a local\nLyapunov function, the policy under SAGE-based stochastic gradient ascent has\nan overwhelming probability of converging to the associated optimal policy.\nFurthermore, we conduct a numerical comparison between a SAGE-based\npolicy-gradient method and an actor-critic method on several examples inspired\nfrom stochastic networks, queueing systems, and models derived from statistical\nphysics. Our results demonstrate that a SAGE-based method finds\nclose-to-optimal policies faster than an actor-critic method.\n","authors":["CÃ©line Comte","Matthieu Jonckheere","Jaron Sanders","Albert Senen-Cerda"],"pdf_url":"https://arxiv.org/pdf/2312.02804v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25683v1","updated":"2025-10-29T16:47:24Z","published":"2025-10-29T16:47:24Z","title":"Graph Network-based Structural Simulator: Graph Neural Networks for\n  Structural Dynamics","summary":"  Graph Neural Networks (GNNs) have recently been explored as surrogate models\nfor numerical simulations. While their applications in computational fluid\ndynamics have been investigated, little attention has been given to structural\nproblems, especially for dynamic cases. To address this gap, we introduce the\nGraph Network-based Structural Simulator (GNSS), a GNN framework for surrogate\nmodeling of dynamic structural problems.\n  GNSS follows the encode-process-decode paradigm typical of GNN-based machine\nlearning models, and its design makes it particularly suited for dynamic\nsimulations thanks to three key features: (i) expressing node kinematics in\nnode-fixed local frames, which avoids catastrophic cancellation in\nfinite-difference velocities; (ii) employing a sign-aware regression loss,\nwhich reduces phase errors in long rollouts; and (iii) using a\nwavelength-informed connectivity radius, which optimizes graph construction.\n  We evaluate GNSS on a case study involving a beam excited by a 50kHz\nHanning-modulated pulse. The results show that GNSS accurately reproduces the\nphysics of the problem over hundreds of timesteps and generalizes to unseen\nloading conditions, where existing GNNs fail to converge or deliver meaningful\npredictions.\n  Compared with explicit finite element baselines, GNSS achieves substantial\ninference speedups while preserving spatial and temporal fidelity. These\nfindings demonstrate that locality-preserving GNNs with physics-consistent\nupdate rules are a competitive alternative for dynamic, wave-dominated\nstructural simulations.\n","authors":["Alessandro Lucchetti","Francesco Cadini","Marco Giglio","Luca Lomazzi"],"pdf_url":"https://arxiv.org/pdf/2510.25683v1.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2510.25674v1","updated":"2025-10-29T16:42:07Z","published":"2025-10-29T16:42:07Z","title":"Mechanistic Interpretability of RNNs emulating Hidden Markov Models","summary":"  Recurrent neural networks (RNNs) provide a powerful approach in neuroscience\nto infer latent dynamics in neural populations and to generate hypotheses about\nthe neural computations underlying behavior. However, past work has focused on\nrelatively simple, input-driven, and largely deterministic behaviors - little\nis known about the mechanisms that would allow RNNs to generate the richer,\nspontaneous, and potentially stochastic behaviors observed in natural settings.\nModeling with Hidden Markov Models (HMMs) has revealed a segmentation of\nnatural behaviors into discrete latent states with stochastic transitions\nbetween them, a type of dynamics that may appear at odds with the continuous\nstate spaces implemented by RNNs. Here we first show that RNNs can replicate\nHMM emission statistics and then reverse-engineer the trained networks to\nuncover the mechanisms they implement. In the absence of inputs, the activity\nof trained RNNs collapses towards a single fixed point. When driven by\nstochastic input, trajectories instead exhibit noise-sustained dynamics along\nclosed orbits. Rotation along these orbits modulates the emission probabilities\nand is governed by transitions between regions of slow, noise-driven dynamics\nconnected by fast, deterministic transitions. The trained RNNs develop highly\nstructured connectivity, with a small set of \"kick neurons\" initiating\ntransitions between these regions. This mechanism emerges during training as\nthe network shifts into a regime of stochastic resonance, enabling it to\nperform probabilistic computations. Analyses across multiple HMM architectures\n- fully connected, cyclic, and linear-chain - reveal that this solution\ngeneralizes through the modular reuse of the same dynamical motif, suggesting a\ncompositional principle by which RNNs can emulate complex discrete latent\ndynamics.\n","authors":["Elia Torre","Michele Viscione","Lucas Pompe","Benjamin F Grewe","Valerio Mante"],"pdf_url":"https://arxiv.org/pdf/2510.25674v1.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25670v1","updated":"2025-10-29T16:36:00Z","published":"2025-10-29T16:36:00Z","title":"Spectral Perturbation Bounds for Low-Rank Approximation with\n  Applications to Privacy","summary":"  A central challenge in machine learning is to understand how noise or\nmeasurement errors affect low-rank approximations, particularly in the spectral\nnorm. This question is especially important in differentially private low-rank\napproximation, where one aims to preserve the top-$p$ structure of a\ndata-derived matrix while ensuring privacy. Prior work often analyzes Frobenius\nnorm error or changes in reconstruction quality, but these metrics can over- or\nunder-estimate true subspace distortion. The spectral norm, by contrast,\ncaptures worst-case directional error and provides the strongest utility\nguarantees. We establish new high-probability spectral-norm perturbation bounds\nfor symmetric matrices that refine the classical Eckart--Young--Mirsky theorem\nand explicitly capture interactions between a matrix $A \\in \\mathbb{R}^{n\n\\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and\nnorm conditions, our bounds yield sharp estimates for $\\|(A + E)_p - A_p\\|$,\nwhere $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up\nto a factor of $\\sqrt{n}$. As an application, we derive improved utility\nguarantees for differentially private PCA, resolving an open problem in the\nliterature. Our analysis relies on a novel contour bootstrapping method from\ncomplex analysis and extends it to a broad class of spectral functionals,\nincluding polynomials and matrix exponentials. Empirical results on real-world\ndatasets confirm that our bounds closely track the actual spectral error under\ndiverse perturbation regimes.\n","authors":["Phuc Tran","Nisheeth K. Vishnoi","Van H. Vu"],"pdf_url":"https://arxiv.org/pdf/2510.25670v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.21717v5","updated":"2025-10-29T16:25:55Z","published":"2025-05-27T20:02:59Z","title":"Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient\n  Sequence Modeling","summary":"  We present LrcSSM, a $\\textit{non-linear}$ recurrent model that processes\nlong sequences as fast as today's linear state-space layers. By forcing its\nJacobian matrix to be diagonal, the full sequence can be solved in parallel,\ngiving $\\mathcal{O}(TD)$ time and memory and only $\\mathcal{O}(\\log T)$\nsequential depth, for input-sequence length $T$ and a state dimension $D$.\nMoreover, LrcSSM offers a formal gradient-stability guarantee that other\ninput-varying systems such as Liquid-S4 and Mamba do not provide. Importantly,\nthe diagonal Jacobian structure of our model results in no performance loss\ncompared to the original model with dense Jacobian, and the approach can be\ngeneralized to other non-linear recurrent models, demonstrating broader\napplicability. On a suite of long-range forecasting tasks, we demonstrate that\nLrcSSM outperforms Transformers, LRU, S5, and Mamba.\n","authors":["MÃ³nika Farsang","Ramin Hasani","Daniela Rus","Radu Grosu"],"pdf_url":"https://arxiv.org/pdf/2505.21717v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25657v1","updated":"2025-10-29T16:22:32Z","published":"2025-10-29T16:22:32Z","title":"Subgraph Federated Learning via Spectral Methods","summary":"  We consider the problem of federated learning (FL) with graph-structured data\ndistributed across multiple clients. In particular, we address the prevalent\nscenario of interconnected subgraphs, where interconnections between clients\nsignificantly influence the learning process. Existing approaches suffer from\ncritical limitations, either requiring the exchange of sensitive node\nembeddings, thereby posing privacy risks, or relying on\ncomputationally-intensive steps, which hinders scalability. To tackle these\nchallenges, we propose FedLap, a novel framework that leverages global\nstructure information via Laplacian smoothing in the spectral domain to\neffectively capture inter-node dependencies while ensuring privacy and\nscalability. We provide a formal analysis of the privacy of FedLap,\ndemonstrating that it preserves privacy. Notably, FedLap is the first subgraph\nFL scheme with strong privacy guarantees. Extensive experiments on benchmark\ndatasets demonstrate that FedLap achieves competitive or superior utility\ncompared to existing techniques.\n","authors":["Javad Aliakbari","Johan Ãstman","Ashkan Panahi","Alexandre Graell i Amat"],"pdf_url":"https://arxiv.org/pdf/2510.25657v1.pdf","comment":"To be presented at The Annual Conference on Neural Information\n  Processing Systems (NeurIPS) 2025"},{"id":"http://arxiv.org/abs/2510.25648v1","updated":"2025-10-29T16:09:24Z","published":"2025-10-29T16:09:24Z","title":"Continuous subsurface property retrieval from sparse radar observations\n  using physics informed neural networks","summary":"  Estimating subsurface dielectric properties is essential for applications\nranging from environmental surveys of soils to nondestructive evaluation of\nconcrete in infrastructure. Conventional wave inversion methods typically\nassume few discrete homogeneous layers and require dense measurements or strong\nprior knowledge of material boundaries, limiting scalability and accuracy in\nrealistic settings where properties vary continuously. We present a physics\ninformed machine learning framework that reconstructs subsurface permittivity\nas a fully neural, continuous function of depth, trained to satisfy both\nmeasurement data and Maxwells equations. We validate the framework with both\nsimulations and custom built radar experiments on multilayered natural\nmaterials. Results show close agreement with in-situ permittivity measurements\n(R^2=0.93), with sensitivity to even subtle variations (Delta eps_r=2).\nParametric analysis reveals that accurate profiles can be recovered with as few\nas three strategically placed sensors in two layer systems. This approach\nreframes subsurface inversion from boundary-driven to continuous property\nestimation, enabling accurate characterization of smooth permittivity\nvariations and advancing electromagnetic imaging using low cost radar systems.\n","authors":["Ishfaq Aziz","Mohamad Alipour"],"pdf_url":"https://arxiv.org/pdf/2510.25648v1.pdf","comment":"22 pages, 9 main text figures + 2 supplementary figures"},{"id":"http://arxiv.org/abs/2507.14785v2","updated":"2025-10-29T15:56:28Z","published":"2025-07-20T02:00:21Z","title":"Exploring the In-Context Learning Capabilities of LLMs for Money\n  Laundering Detection in Financial Graphs","summary":"  The complexity and interconnectivity of entities involved in money laundering\ndemand investigative reasoning over graph-structured data. This paper explores\nthe use of large language models (LLMs) as reasoning engines over localized\nsubgraphs extracted from a financial knowledge graph. We propose a lightweight\npipeline that retrieves k-hop neighborhoods around entities of interest,\nserializes them into structured text, and prompts an LLM via few-shot\nin-context learning to assess suspiciousness and generate justifications. Using\nsynthetic anti-money laundering (AML) scenarios that reflect common laundering\nbehaviors, we show that LLMs can emulate analyst-style logic, highlight red\nflags, and provide coherent explanations. While this study is exploratory, it\nillustrates the potential of LLM-based graph reasoning in AML and lays\ngroundwork for explainable, language-driven financial crime analytics.\n","authors":["Erfan Pirmorad"],"pdf_url":"https://arxiv.org/pdf/2507.14785v2.pdf","comment":"Accepted at AI4FCF-ICDM 2025"},{"id":"http://arxiv.org/abs/2510.23965v2","updated":"2025-10-29T15:51:35Z","published":"2025-10-28T00:42:38Z","title":"The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity","summary":"  Traditional LLM alignment methods are vulnerable to heterogeneity in human\npreferences. Fitting a na\\\"ive probabilistic model to pairwise comparison data\n(say over prompt-completion pairs) yields an inconsistent estimate of the\npopulation-average utility -a canonical measure of social welfare. We propose a\nnew method, dubbed the sign estimator, that provides a simple, provably\nconsistent, and efficient estimator by replacing cross-entropy with binary\nclassification loss in the aggregation step. This simple modification recovers\nconsistent ordinal alignment under mild assumptions and achieves the first\npolynomial finite-sample error bounds in this setting. In realistic simulations\nof LLM alignment using digital twins, the sign estimator substantially reduces\npreference distortion over a panel of simulated personas, cutting (angular)\nestimation error by nearly 35% and decreasing disagreement with true population\npreferences from 12% to 8% compared to standard RLHF. Our method also compares\nfavorably to panel data heuristics that explicitly model user heterogeneity and\nrequire tracking individual-level preference data-all while maintaining the\nimplementation simplicity of existing LLM alignment pipelines.\n","authors":["Ali Aouad","Aymane El Gadarri","Vivek F. Farias"],"pdf_url":"https://arxiv.org/pdf/2510.23965v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00812v4","updated":"2025-10-29T15:40:34Z","published":"2025-05-01T19:12:58Z","title":"Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic\n  Optimization","summary":"  Recent studies indicate that deep neural networks degrade in generalization\nperformance under noisy supervision. Existing methods focus on isolating clean\nsubsets or correcting noisy labels, facing limitations such as high\ncomputational costs, heavy hyperparameter tuning process, and coarse-grained\noptimization. To address these challenges, we propose a novel two-stage noisy\nlearning framework that enables instance-level optimization through a\ndynamically weighted loss function, avoiding hyperparameter tuning. To obtain\nstable and accurate information about noise modeling, we introduce a simple yet\neffective metric, termed wrong event, which dynamically models the cleanliness\nand difficulty of individual samples while maintaining computational costs. Our\nframework first collects wrong event information and builds a strong base\nmodel. Then we perform noise-robust training on the base model, using a\nprobabilistic model to handle the wrong event information of samples.\nExperiments on five synthetic and real-world LNL benchmarks demonstrate our\nmethod surpasses state-of-the-art methods in performance, achieves a nearly 75%\nreduction in computational time and improves model scalability.\n","authors":["Kuan Zhang","Chengliang Chai","Jingzhe Xu","Chi Zhang","Han Han","Ye Yuan","Guoren Wang","Lei Cao"],"pdf_url":"https://arxiv.org/pdf/2505.00812v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23117v2","updated":"2025-10-29T15:34:30Z","published":"2025-05-29T05:37:53Z","title":"Decom-Renorm-Merge: Model Merging on the Right Space Improves\n  Multitasking","summary":"  In the era of large-scale training, model merging has evolved into a tool for\ncreating multitasking models efficiently. It enables the knowledge of models to\nbe fused, without the need for heavy computation as required in traditional\nmultitask learning. Existing merging methods often assume that entries at\nidentical positions in weight matrices serve the same function, enabling\nstraightforward entry-wise comparison and merging. However, this assumption\noverlooks the complexity of finetuned neural networks, where neurons may\ndevelop distinct feature compositions, making direct entry-wise merging\nproblematic. We present Decom-Renorm-Merge (DRM), a simple yet effective\napproach that leverages Singular Value Decomposition to decompose and\ncoordinate weight matrices into an aligned joint space, where entry-wise\nmerging becomes possible. We showcase the effectiveness of DRM across various\nsettings ranging from smaller encoder-based such as ViT and DeBERTa,\nencoder-decoder-based such as T5, and larger decoder-based such as Llama3.1-8B.\nOur experimental results show that DRM outperforms several state-of-the-art\nmerging techniques across full finetuning and low-rank adaptation settings.\nMoreover, our analysis reveals renormalization as the crucial component for\ncreating a robust and even joint space for merging, significantly contributing\nto the method's performance.\n","authors":["Yuatyong Chaichana","Thanapat Trachu","Peerat Limkonchotiwat","Konpat Preechakul","Tirasan Khandhawit","Ekapol Chuangsuwanich"],"pdf_url":"https://arxiv.org/pdf/2505.23117v2.pdf","comment":"Code and models are available at\n  https://github.com/yophis/decom-renorm-merge"},{"id":"http://arxiv.org/abs/2510.01850v3","updated":"2025-10-29T15:33:51Z","published":"2025-10-02T09:47:56Z","title":"NGGAN: Noise Generation GAN Based on the Practical Measurement Dataset\n  for Narrowband Powerline Communications","summary":"  To effectively process impulse noise for narrowband powerline communications\n(NB-PLCs) transceivers, capturing comprehensive statistics of nonperiodic\nasynchronous impulsive noise (APIN) is a critical task. However, existing\nmathematical noise generative models only capture part of the characteristics\nof noise. In this study, we propose a novel generative adversarial network\n(GAN) called noise generation GAN (NGGAN) that learns the complicated\ncharacteristics of practically measured noise samples for data synthesis. To\nclosely match the statistics of complicated noise over the NB-PLC systems, we\nmeasured the NB-PLC noise via the analog coupling and bandpass filtering\ncircuits of a commercial NB-PLC modem to build a realistic dataset. To train\nNGGAN, we adhere to the following principles: 1) we design the length of input\nsignals that the NGGAN model can fit to facilitate cyclostationary noise\ngeneration; 2) the Wasserstein distance is used as a loss function to enhance\nthe similarity between the generated noise and training data; and 3) to measure\nthe similarity performances of GAN-based models based on the mathematical and\npractically measured datasets, we conduct both quantitative and qualitative\nanalyses. The training datasets include: 1) a piecewise spectral\ncyclostationary Gaussian model (PSCGM); 2) a frequency-shift (FRESH) filter;\nand 3) practical measurements from NB-PLC systems. Simulation results\ndemonstrate that the generated noise samples from the proposed NGGAN are highly\nclose to the real noise samples. The principal component analysis (PCA) scatter\nplots and Fr\\'echet inception distance (FID) analysis have shown that NGGAN\noutperforms other GAN-based models by generating noise samples with superior\nfidelity and higher diversity.\n","authors":["Ying-Ren Chien","Po-Heng Chou","You-Jie Peng","Chun-Yuan Huang","Hen-Wai Tsao","Yu Tsao"],"pdf_url":"https://arxiv.org/pdf/2510.01850v3.pdf","comment":"16 pages, 15 figures, 11 tables, and published in IEEE Transactions\n  on Instrumentation and Measurement, 2025"},{"id":"http://arxiv.org/abs/2510.25626v1","updated":"2025-10-29T15:30:31Z","published":"2025-10-29T15:30:31Z","title":"Are Language Models Efficient Reasoners? A Perspective from Logic\n  Programming","summary":"  Modern language models (LMs) exhibit strong deductive reasoning capabilities,\nyet standard evaluations emphasize correctness while overlooking a key aspect\nof human-like reasoning: efficiency. In real-world reasoning scenarios, much of\nthe available information is irrelevant, and effective deductive inference\nrequires identifying and ignoring such distractions. We propose a framework for\nassessing LM reasoning efficiency through the lens of logic programming,\nintroducing a simple method to align proofs written in natural language -- as\ngenerated by an LM -- with shortest proofs found by executing the logic\nprogram. Efficiency is quantified by measuring how well a model avoids\nunnecessary inference. Empirically, we construct a dataset of math word\nproblems injected with various number of irrelevant axioms that vary in\nsemantic overlap with the goal theorem. We find that current LMs show marked\naccuracy declines under such conditions -- even with minimal, domain-consistent\ndistractions -- and the proofs they generate frequently exhibit detours through\nirrelevant inferences.\n","authors":["Andreas Opedal","Yanick Zengaffinen","Haruki Shirakami","Clemente Pasti","Mrinmaya Sachan","Abulhair Saparov","Ryan Cotterell","Bernhard SchÃ¶lkopf"],"pdf_url":"https://arxiv.org/pdf/2510.25626v1.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25616v1","updated":"2025-10-29T15:20:10Z","published":"2025-10-29T15:20:10Z","title":"Don't Blind Your VLA: Aligning Visual Representations for OOD\n  Generalization","summary":"  The growing success of Vision-Language-Action (VLA) models stems from the\npromise that pretrained Vision-Language Models (VLMs) can endow agents with\ntransferable world knowledge and vision-language (VL) grounding, laying a\nfoundation for action models with broader generalization. Yet when these VLMs\nare adapted to the action modality, it remains unclear to what extent their\noriginal VL representations and knowledge are preserved. In this work, we\nconduct a systematic study of representation retention during VLA fine-tuning,\nshowing that naive action fine-tuning leads to degradation of visual\nrepresentations. To characterize and measure these effects, we probe VLA's\nhidden representations and analyze attention maps, further, we design a set of\ntargeted tasks and methods that contrast VLA models with their counterpart\nVLMs, isolating changes in VL capabilities induced by action fine-tuning. We\nfurther evaluate a range of strategies for aligning visual representations and\nintroduce a simple yet effective method that mitigates degradation and yields\nimproved generalization to out-of-distribution (OOD) scenarios. Taken together,\nour analysis clarifies the trade-off between action fine-tuning and the\ndegradation of VL representations and highlights practical approaches to\nrecover inherited VL capabilities. Code is publicly available:\nhttps://blind-vla-paper.github.io\n","authors":["Nikita Kachaev","Mikhail Kolosov","Daniil Zelezetsky","Alexey K. Kovalev","Aleksandr I. Panov"],"pdf_url":"https://arxiv.org/pdf/2510.25616v1.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2510.25609v1","updated":"2025-10-29T15:16:50Z","published":"2025-10-29T15:16:50Z","title":"BOLT-GAN: Bayes-Optimal Loss for Stable GAN Training","summary":"  We introduce BOLT-GAN, a simple yet effective modification of the WGAN\nframework inspired by the Bayes Optimal Learning Threshold (BOLT). We show that\nwith a Lipschitz continuous discriminator, BOLT-GAN implicitly minimizes a\ndifferent metric distance than the Earth Mover (Wasserstein) distance and\nachieves better training stability. Empirical evaluations on four standard\nimage generation benchmarks (CIFAR-10, CelebA-64, LSUN Bedroom-64, and LSUN\nChurch-64) show that BOLT-GAN consistently outperforms WGAN, achieving 10-60%\nlower Frechet Inception Distance (FID). Our results suggest that BOLT is a\nbroadly applicable principle for enhancing GAN training.\n","authors":["Mohammadreza Tavasoli Naeini","Ali Bereyhi","Morteza Noshad","Ben Liang","Alfred O. Hero III"],"pdf_url":"https://arxiv.org/pdf/2510.25609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25602v1","updated":"2025-10-29T15:11:53Z","published":"2025-10-29T15:11:53Z","title":"INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization\n  Formats","summary":"  Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly\nembracing low-precision floating-point (FP) formats to handle the pervasive\nactivation outliers in Large Language Models (LLMs). Despite this industry\ntrend, a unified comparison of FP and integer (INT) quantization across varying\ngranularities has been missing, leaving algorithm and hardware co-design\nwithout clear guidance. This paper fills that gap by systematically\ninvestigating the trade-offs between FP and INT formats. We reveal a critical\nperformance crossover: while FP excels in coarse-grained quantization, the\ncomparison at fine-grained (block-wise) levels is more nuanced. Our\ncomprehensive comparison demonstrates that for popular 8-bit fine-grained\nformats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart\nin both algorithmic accuracy and hardware efficiency. However, for 4-bit\nformats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we\nshow that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like\nHadamard rotation are applied. We also introduce a symmetric clipping method\nthat resolves gradient bias in fine-grained low-bit INT training, enabling\nnearly lossless performance for MXINT8 training. These findings challenge the\ncurrent hardware trajectory, demonstrating that a one-size-fits-all FP approach\nis suboptimal and advocating that fine-grained INT formats, particularly\nMXINT8, offer a better balance of accuracy, power, and efficiency for future AI\naccelerators.\n","authors":["Mengzhao Chen","Meng Wu","Hui Jin","Zhihang Yuan","Jing Liu","Chaoyi Zhang","Yunshui Li","Jie Huang","Jin Ma","Zeyue Xue","Zhiheng Liu","Xingyan Bin","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2510.25602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12437v2","updated":"2025-10-29T15:09:38Z","published":"2025-05-18T14:19:52Z","title":"A method for the systematic generation of graph XAI benchmarks via\n  Weisfeiler-Leman coloring","summary":"  Graph neural networks have become the de facto model for learning from\nstructured data. However, the decision-making process of GNNs remains opaque to\nthe end user, which undermines their use in safety-critical applications.\nSeveral explainable AI techniques for graphs have been developed to address\nthis major issue. Focusing on graph classification, these explainers identify\nsubgraph motifs that explain predictions. Therefore, a robust benchmarking of\ngraph explainers is required to ensure that the produced explanations are of\nhigh quality, i.e., aligned with the GNN's decision process. However, current\ngraph-XAI benchmarks are limited to simplistic synthetic datasets or a few\nreal-world tasks curated by domain experts, hindering rigorous and reproducible\nevaluation, and consequently stalling progress in the field. To overcome these\nlimitations, we propose a method to automate the construction of graph XAI\nbenchmarks from generic graph classification datasets. Our approach leverages\nthe Weisfeiler-Leman color refinement algorithm to efficiently perform\napproximate subgraph matching and mine class-discriminating motifs, which serve\nas proxy ground-truth class explanations. At the same time, we ensure that\nthese motifs can be learned by GNNs because their discriminating power aligns\nwith WL expressiveness. This work also introduces the OpenGraphXAI benchmark\nsuite, which consists of 15 ready-made graph-XAI datasets derived by applying\nour method to real-world molecular classification datasets. The suite is\navailable to the public along with a codebase to generate over 2,000 additional\ngraph-XAI benchmarks. Finally, we present a use case that illustrates how the\nsuite can be used to assess the effectiveness of a selection of popular graph\nexplainers, demonstrating the critical role of a sufficiently large benchmark\ncollection for improving the significance of experimental results.\n","authors":["Michele Fontanesi","Alessio Micheli","Marco Podda","Domenico Tortorella"],"pdf_url":"https://arxiv.org/pdf/2505.12437v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25599v1","updated":"2025-10-29T15:08:41Z","published":"2025-10-29T15:08:41Z","title":"Uncertainty Quantification for Regression: A Unified Framework based on\n  kernel scores","summary":"  Regression tasks, notably in safety-critical domains, require proper\nuncertainty quantification, yet the literature remains largely\nclassification-focused. In this light, we introduce a family of measures for\ntotal, aleatoric, and epistemic uncertainty based on proper scoring rules, with\na particular emphasis on kernel scores. The framework unifies several\nwell-known measures and provides a principled recipe for designing new ones\nwhose behavior, such as tail sensitivity, robustness, and out-of-distribution\nresponsiveness, is governed by the choice of kernel. We prove explicit\ncorrespondences between kernel-score characteristics and downstream behavior,\nyielding concrete design guidelines for task-specific measures. Extensive\nexperiments demonstrate that these measures are effective in downstream tasks\nand reveal clear trade-offs among instantiations, including robustness and\nout-of-distribution detection performance.\n","authors":["Christopher BÃ¼lte","Yusuf Sale","Gitta Kutyniok","Eyke HÃ¼llermeier"],"pdf_url":"https://arxiv.org/pdf/2510.25599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25594v1","updated":"2025-10-29T15:03:46Z","published":"2025-10-29T15:03:46Z","title":"Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for\n  Local Learning","summary":"  Training deep neural networks (DNNs) with backpropagation (BP) achieves\nstate-of-the-art accuracy but requires global error propagation and full\nparameterization, leading to substantial memory and computational overhead.\nDirect Feedback Alignment (DFA) enables local, parallelizable updates with\nlower memory requirements but is limited by unstructured feedback and poor\nscalability in deeper architectures, specially convolutional neural networks.\nTo address these limitations, we propose a structured local learning framework\nthat operates directly on low-rank manifolds defined by the Singular Value\nDecomposition (SVD) of weight matrices. Each layer is trained in its decomposed\nform, with updates applied to the SVD components using a composite loss that\nintegrates cross-entropy, subspace alignment, and orthogonality regularization.\nFeedback matrices are constructed to match the SVD structure, ensuring\nconsistent alignment between forward and feedback pathways. Our method reduces\nthe number of trainable parameters relative to the original DFA model, without\nrelying on pruning or post hoc compression. Experiments on CIFAR-10, CIFAR-100,\nand ImageNet show that our method achieves accuracy comparable to that of BP.\nAblation studies confirm the importance of each loss term in the low-rank\nsetting. These results establish local learning on low-rank manifolds as a\nprincipled and scalable alternative to full-rank gradient-based training.\n","authors":["Arani Roy","Marco P. Apolinario","Shristi Das Biswas","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2510.25594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25591v1","updated":"2025-10-29T14:59:26Z","published":"2025-10-29T14:59:26Z","title":"Generalized Sobolev IPM for Graph-Based Measures","summary":"  We study the Sobolev IPM problem for measures supported on a graph metric\nspace, where critic function is constrained to lie within the unit ball defined\nby Sobolev norm. While Le et al. (2025) achieved scalable computation by\nrelating Sobolev norm to weighted $L^p$-norm, the resulting framework remains\nintrinsically bound to $L^p$ geometric structure, limiting its ability to\nincorporate alternative structural priors beyond the $L^p$ geometry paradigm.\nTo overcome this limitation, we propose to generalize Sobolev IPM through the\nlens of \\emph{Orlicz geometric structure}, which employs convex functions to\ncapture nuanced geometric relationships, building upon recent advances in\noptimal transport theory -- particularly Orlicz-Wasserstein (OW) and\ngeneralized Sobolev transport -- that have proven instrumental in advancing\nmachine learning methodologies. This generalization encompasses classical\nSobolev IPM as a special case while accommodating diverse geometric priors\nbeyond traditional $L^p$ structure. It however brings up significant\ncomputational hurdles that compound those already inherent in Sobolev IPM. To\naddress these challenges, we establish a novel theoretical connection between\nOrlicz-Sobolev norm and Musielak norm which facilitates a novel regularization\nfor the generalized Sobolev IPM (GSI). By further exploiting the underlying\ngraph structure, we show that GSI with Musielak regularization (GSI-M) reduces\nto a simple \\emph{univariate optimization} problem, achieving remarkably\ncomputational efficiency. Empirically, GSI-M is several-order faster than the\npopular OW in computation, and demonstrates its practical advantages in\ncomparing probability measures on a given graph for document classification and\nseveral tasks in topological data analysis.\n","authors":["Tam Le","Truyen Nguyen","Hideitsu Hino","Kenji Fukumizu"],"pdf_url":"https://arxiv.org/pdf/2510.25591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12484v4","updated":"2025-10-29T14:52:49Z","published":"2025-06-14T12:49:51Z","title":"Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption\n  Masking And Normalization","summary":"  Language models can retain dangerous knowledge and skills even after\nextensive safety fine-tuning, posing both misuse and misalignment risks. Recent\nstudies show that even specialized unlearning methods can be easily reversed.\nTo address this, we systematically evaluate many existing and novel components\nof unlearning methods and identify ones crucial for irreversible unlearning.\n  We introduce Disruption Masking, a technique in which we only allow updating\nweights, where the signs of the unlearning gradient and the retaining gradient\nare the same. This ensures all updates are non-disruptive.\n  Additionally, we identify the need for normalizing the unlearning gradients,\nand also confirm the usefulness of meta-learning. We combine these insights\ninto MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and\nvalidate its effectiveness at preventing the recovery of dangerous\ncapabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new\nstate-of-the-art for robust unlearning.\n","authors":["Filip Sondej","Yushi Yang","MikoÅaj Kniejski","Marcel Windys"],"pdf_url":"https://arxiv.org/pdf/2506.12484v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25582v1","updated":"2025-10-29T14:47:18Z","published":"2025-10-29T14:47:18Z","title":"Learning-Augmented Online Bidding in Stochastic Settings","summary":"  Online bidding is a classic optimization problem, with several applications\nin online decision-making, the design of interruptible systems, and the\nanalysis of approximation algorithms. In this work, we study online bidding\nunder learning-augmented settings that incorporate stochasticity, in either the\nprediction oracle or the algorithm itself. In the first part, we study bidding\nunder distributional predictions, and find Pareto-optimal algorithms that offer\nthe best-possible tradeoff between the consistency and the robustness of the\nalgorithm. In the second part, we study the power and limitations of randomized\nbidding algorithms, by presenting upper and lower bounds on the\nconsistency/robustness tradeoffs. Previous works focused predominantly on\noracles that do not leverage stochastic information on the quality of the\nprediction, and deterministic algorithms.\n","authors":["Spyros Angelopoulos","Bertrand Simon"],"pdf_url":"https://arxiv.org/pdf/2510.25582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25573v1","updated":"2025-10-29T14:42:10Z","published":"2025-10-29T14:42:10Z","title":"Monitoring the calibration of probability forecasts with an application\n  to concept drift detection involving image classification","summary":"  Machine learning approaches for image classification have led to impressive\nadvances in that field. For example, convolutional neural networks are able to\nachieve remarkable image classification accuracy across a wide range of\napplications in industry, defense, and other areas. While these machine\nlearning models boast impressive accuracy, a related concern is how to assess\nand maintain calibration in the predictions these models make. A classification\nmodel is said to be well calibrated if its predicted probabilities correspond\nwith the rates events actually occur. While there are many available methods to\nassess machine learning calibration and recalibrate faulty predictions, less\neffort has been spent on developing approaches that continually monitor\npredictive models for potential loss of calibration as time passes. We propose\na cumulative sum-based approach with dynamic limits that enable detection of\nmiscalibration in both traditional process monitoring and concept drift\napplications. This enables early detection of operational context changes that\nimpact image classification performance in the field. The proposed chart can be\nused broadly in any situation where the user needs to monitor probability\npredictions over time for potential lapses in calibration. Importantly, our\nmethod operates on probability predictions and event outcomes and does not\nrequire under-the-hood access to the machine learning model.\n","authors":["Christopher T. Franck","Anne R. Driscoll","Zoe Szajnfarber","William H. Woodall"],"pdf_url":"https://arxiv.org/pdf/2510.25573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24670v2","updated":"2025-10-29T14:41:45Z","published":"2025-10-28T17:36:51Z","title":"Pearl: A Foundation Model for Placing Every Atom in the Right Location","summary":"  Accurately predicting the three-dimensional structures of protein-ligand\ncomplexes remains a fundamental challenge in computational drug discovery that\nlimits the pace and success of therapeutic design. Deep learning methods have\nrecently shown strong potential as structural prediction tools, achieving\npromising accuracy across diverse biomolecular systems. However, their\nperformance and utility are constrained by scarce experimental data,\ninefficient architectures, physically invalid poses, and the limited ability to\nexploit auxiliary information available at inference. To address these issues,\nwe introduce Pearl (Placing Every Atom in the Right Location), a foundation\nmodel for protein-ligand cofolding at scale. Pearl addresses these challenges\nwith three key innovations: (1) training recipes that include large-scale\nsynthetic data to overcome data scarcity; (2) architectures that incorporate an\nSO(3)-equivariant diffusion module to inherently respect 3D rotational\nsymmetries, improving generalization and sample efficiency, and (3)\ncontrollable inference, including a generalized multi-chain templating system\nsupporting both protein and non-polymeric components as well as dual\nunconditional/conditional modes. Pearl establishes a new state-of-the-art\nperformance in protein-ligand cofolding. On the key metric of generating\naccurate (RMSD < 2 \\r{A}) and physically valid poses, Pearl surpasses AlphaFold\n3 and other open source baselines on the public Runs N' Poses and PoseBusters\nbenchmarks, delivering 14.5% and 14.2% improvements, respectively, over the\nnext best model. In the pocket-conditional cofolding regime, Pearl delivers\n$3.6\\times$ improvement on a proprietary set of challenging, real-world drug\ntargets at the more rigorous RMSD < 1 \\r{A} threshold. Finally, we demonstrate\nthat model performance correlates directly with synthetic dataset size used in\ntraining.\n","authors":[" Genesis Research Team","Alejandro Dobles","Nina Jovic","Kenneth Leidal","Pranav Murugan","David C. Williams","Drausin Wulsin","Nate Gruver","Christina X. Ji","Korrawat Pruegsanusak","Gianluca Scarpellini","Ansh Sharma","Wojciech Swiderski","Andrea Bootsma","Richard Strong Bowen","Charlotte Chen","Jamin Chen","Marc AndrÃ© DÃ¤mgen","Benjamin DiFrancesco","J. D. Fishman","Alla Ivanova","Zach Kagin","David Li-Bland","Zuli Liu","Igor Morozov","Jeffrey Ouyang-Zhang","Frank C. Pickard IV","Kushal S. Shah","Ben Shor","Gabriel Monteiro da Silva","Roy Tal","Maxx Tessmer","Carl Tilbury","Cyr Vetcher","Daniel Zeng","Maruan Al-Shedivat","Aleksandra Faust","Evan N. Feinberg","Michael V. LeVine","Matteus Pan"],"pdf_url":"https://arxiv.org/pdf/2510.24670v2.pdf","comment":"technical report"},{"id":"http://arxiv.org/abs/2510.25571v1","updated":"2025-10-29T14:40:12Z","published":"2025-10-29T14:40:12Z","title":"Perturbation Bounds for Low-Rank Inverse Approximations under Noise","summary":"  Low-rank pseudoinverses are widely used to approximate matrix inverses in\nscalable machine learning, optimization, and scientific computing. However,\nreal-world matrices are often observed with noise, arising from sampling,\nsketching, and quantization. The spectral-norm robustness of low-rank inverse\napproximations remains poorly understood. We systematically study the\nspectral-norm error $\\| (\\tilde{A}^{-1})_p - A_p^{-1} \\|$ for an $n\\times n$\nsymmetric matrix $A$, where $A_p^{-1}$ denotes the best rank-\\(p\\)\napproximation of $A^{-1}$, and $\\tilde{A} = A + E$ is a noisy observation.\nUnder mild assumptions on the noise, we derive sharp non-asymptotic\nperturbation bounds that reveal how the error scales with the eigengap,\nspectral decay, and noise alignment with low-curvature directions of $A$. Our\nanalysis introduces a novel application of contour integral techniques to the\n\\emph{non-entire} function $f(z) = 1/z$, yielding bounds that improve over\nnaive adaptations of classical full-inverse bounds by up to a factor of\n$\\sqrt{n}$. Empirically, our bounds closely track the true perturbation error\nacross a variety of real-world and synthetic matrices, while estimates based on\nclassical results tend to significantly overpredict. These findings offer\npractical, spectrum-aware guarantees for low-rank inverse approximations in\nnoisy computational environments.\n","authors":["Phuc Tran","Nisheeth K. Vishnoi"],"pdf_url":"https://arxiv.org/pdf/2510.25571v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25569v1","updated":"2025-10-29T14:38:35Z","published":"2025-10-29T14:38:35Z","title":"A Framework for Bounding Deterministic Risk with PAC-Bayes: Applications\n  to Majority Votes","summary":"  PAC-Bayes is a popular and efficient framework for obtaining generalization\nguarantees in situations involving uncountable hypothesis spaces.\nUnfortunately, in its classical formulation, it only provides guarantees on the\nexpected risk of a randomly sampled hypothesis. This requires stochastic\npredictions at test time, making PAC-Bayes unusable in many practical\nsituations where a single deterministic hypothesis must be deployed. We propose\na unified framework to extract guarantees holding for a single hypothesis from\nstochastic PAC-Bayesian guarantees. We present a general oracle bound and\nderive from it a numerical bound and a specialization to majority vote. We\nempirically show that our approach consistently outperforms popular baselines\n(by up to a factor of 2) when it comes to generalization bounds on\ndeterministic classifiers.\n","authors":["Benjamin Leblanc","Pascal Germain"],"pdf_url":"https://arxiv.org/pdf/2510.25569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23999v2","updated":"2025-10-29T14:38:26Z","published":"2025-10-28T02:03:39Z","title":"Auto-Adaptive PINNs with Applications to Phase Transitions","summary":"  We propose an adaptive sampling method for the training of Physics Informed\nNeural Networks (PINNs) which allows for sampling based on an arbitrary\nproblem-specific heuristic which may depend on the network and its gradients.\nIn particular we focus our analysis on the Allen-Cahn equations, attempting to\naccurately resolve the characteristic interfacial regions using a PINN without\nany post-hoc resampling. In experiments, we show the effectiveness of these\nmethods over residual-adaptive frameworks.\n","authors":["Kevin Buck","Woojeong Kim"],"pdf_url":"https://arxiv.org/pdf/2510.23999v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25566v1","updated":"2025-10-29T14:33:35Z","published":"2025-10-29T14:33:35Z","title":"PitchFlower: A flow-based neural audio codec with pitch controllability","summary":"  We present PitchFlower, a flow-based neural audio codec with explicit pitch\ncontrollability. Our approach enforces disentanglement through a simple\nperturbation: during training, F0 contours are flattened and randomly shifted,\nwhile the true F0 is provided as conditioning. A vector-quantization bottleneck\nprevents pitch recovery, and a flow-based decoder generates high quality audio.\nExperiments show that PitchFlower achieves more accurate pitch control than\nWORLD at much higher audio quality, and outperforms SiFiGAN in controllability\nwhile maintaining comparable quality. Beyond pitch, this framework provides a\nsimple and extensible path toward disentangling other speech attributes.\n","authors":["Diego Torres","Axel Roebel","Nicolas Obin"],"pdf_url":"https://arxiv.org/pdf/2510.25566v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.13519v2","updated":"2025-10-29T14:31:32Z","published":"2025-05-17T12:39:45Z","title":"Continuous Domain Generalization","summary":"  Real-world data distributions often shift continuously across multiple latent\nfactors such as time, geography, and socioeconomic contexts. However, existing\ndomain generalization approaches typically treat domains as discrete or as\nevolving along a single axis (e.g., time). This oversimplification fails to\ncapture the complex, multidimensional nature of real-world variation. This\npaper introduces the task of Continuous Domain Generalization (CDG), which aims\nto generalize predictive models to unseen domains defined by arbitrary\ncombinations of continuous variations. We present a principled framework\ngrounded in geometric and algebraic theories, showing that optimal model\nparameters across domains lie on a low-dimensional manifold. To model this\nstructure, we propose a Neural Lie Transport Operator (NeuralLio), which\nenables structure-preserving parameter transitions by enforcing geometric\ncontinuity and algebraic consistency. To handle noisy or incomplete domain\nvariation descriptors, we introduce a gating mechanism to suppress irrelevant\ndimensions and a local chart-based strategy for robust generalization.\nExtensive experiments on synthetic and real-world datasets, including remote\nsensing, scientific documents, and traffic forecasting, demonstrate that our\nmethod significantly outperforms existing baselines in both generalization\naccuracy and robustness.\n","authors":["Zekun Cai","Yiheng Yao","Guangji Bai","Renhe Jiang","Xuan Song","Ryosuke Shibasaki","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.13519v2.pdf","comment":"23 pages, 9 figures. Accepted by NeurIPS25"},{"id":"http://arxiv.org/abs/2510.25563v1","updated":"2025-10-29T14:30:12Z","published":"2025-10-29T14:30:12Z","title":"Leveraging an Atmospheric Foundational Model for Subregional Sea Surface\n  Temperature Forecasting","summary":"  The accurate prediction of oceanographic variables is crucial for\nunderstanding climate change, managing marine resources, and optimizing\nmaritime activities. Traditional ocean forecasting relies on numerical models;\nhowever, these approaches face limitations in terms of computational cost and\nscalability. In this study, we adapt Aurora, a foundational deep learning model\noriginally designed for atmospheric forecasting, to predict sea surface\ntemperature (SST) in the Canary Upwelling System. By fine-tuning this model\nwith high-resolution oceanographic reanalysis data, we demonstrate its ability\nto capture complex spatiotemporal patterns while reducing computational\ndemands. Our methodology involves a staged fine-tuning process, incorporating\nlatitude-weighted error metrics and optimizing hyperparameters for efficient\nlearning. The experimental results show that the model achieves a low RMSE of\n0.119K, maintaining high anomaly correlation coefficients (ACC $\\approx\n0.997$). The model successfully reproduces large-scale SST structures but faces\nchallenges in capturing finer details in coastal regions. This work contributes\nto the field of data-driven ocean forecasting by demonstrating the feasibility\nof using deep learning models pre-trained in different domains for oceanic\napplications. Future improvements include integrating additional oceanographic\nvariables, increasing spatial resolution, and exploring physics-informed neural\nnetworks to enhance interpretability and understanding. These advancements can\nimprove climate modeling and ocean prediction accuracy, supporting\ndecision-making in environmental and economic sectors.\n","authors":["VÃ­ctor Medina","Giovanny A. Cuervo-LondoÃ±o","Javier SÃ¡nchez"],"pdf_url":"https://arxiv.org/pdf/2510.25563v1.pdf","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.23455v3","updated":"2025-10-29T14:29:54Z","published":"2025-10-27T15:56:19Z","title":"SGFusion: Stochastic Geographic Gradient Fusion in Federated Learning","summary":"  This paper proposes Stochastic Geographic Gradient Fusion (SGFusion), a novel\ntraining algorithm to leverage the geographic information of mobile users in\nFederated Learning (FL). SGFusion maps the data collected by mobile devices\nonto geographical zones and trains one FL model per zone, which adapts well to\nthe data and behaviors of users in that zone. SGFusion models the local\ndata-based correlation among geographical zones as a hierarchical random graph\n(HRG) optimized by Markov Chain Monte Carlo sampling. At each training step,\nevery zone fuses its local gradient with gradients derived from a small set of\nother zones sampled from the HRG. This approach enables knowledge fusion and\nsharing among geographical zones in a probabilistic and stochastic gradient\nfusion process with self-attention weights, such that \"more similar\" zones have\n\"higher probabilities\" of sharing gradients with \"larger attention weights.\"\nSGFusion remarkably improves model utility without introducing undue\ncomputational cost. Extensive theoretical and empirical results using a\nheart-rate prediction dataset collected across 6 countries show that models\ntrained with SGFusion converge with upper-bounded expected errors and\nsignificantly improve utility in all countries compared to existing approaches\nwithout notable cost in system scalability.\n","authors":["Khoa Nguyen","Khang Tran","NhatHai Phan","Cristian Borcea","Ruoming Jin","Issa Khalil"],"pdf_url":"https://arxiv.org/pdf/2510.23455v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25557v1","updated":"2025-10-29T14:21:49Z","published":"2025-10-29T14:21:49Z","title":"Hybrid Quantum-Classical Recurrent Neural Networks","summary":"  We present a hybrid quantum-classical recurrent neural network (QRNN)\narchitecture in which the entire recurrent core is realized as a parametrized\nquantum circuit (PQC) controlled by a classical feedforward network. The hidden\nstate is the quantum state of an $n$-qubit PQC, residing in an exponentially\nlarge Hilbert space $\\mathbb{C}^{2^n}$. The PQC is unitary by construction,\nmaking the hidden-state evolution norm-preserving without external constraints.\nAt each timestep, mid-circuit readouts are combined with the input embedding\nand processed by the feedforward network, which provides explicit classical\nnonlinearity. The outputs parametrize the PQC, which updates the hidden state\nvia unitary dynamics. The QRNN is compact and physically consistent, and it\nunifies (i) unitary recurrence as a high-capacity memory, (ii) partial\nobservation via mid-circuit measurements, and (iii) nonlinear classical control\nfor input-conditioned parametrization. We evaluate the model in simulation with\nup to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory,\nand language modeling, adopting projective measurements as a limiting case to\nobtain mid-circuit readouts while maintaining a coherent recurrent quantum\nmemory. We further devise a soft attention mechanism over the mid-circuit\nreadouts in a sequence-to-sequence model and show its effectiveness for machine\ntranslation. To our knowledge, this is the first model (RNN or otherwise)\ngrounded in quantum operations to achieve competitive performance against\nstrong classical baselines across a broad class of sequence-learning tasks.\n","authors":["Wenduan Xu"],"pdf_url":"https://arxiv.org/pdf/2510.25557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25550v1","updated":"2025-10-29T14:18:08Z","published":"2025-10-29T14:18:08Z","title":"Robust variable selection for spatial point processes observed with\n  noise","summary":"  We propose a method for variable selection in the intensity function of\nspatial point processes that combines sparsity-promoting estimation with\nnoise-robust model selection. As high-resolution spatial data becomes\nincreasingly available through remote sensing and automated image analysis,\nidentifying spatial covariates that influence the localization of events is\ncrucial to understand the underlying mechanism. However, results from automated\nacquisition techniques are often noisy, for example due to measurement\nuncertainties or detection errors, which leads to spurious displacements and\nmissed events. We study the impact of such noise on sparse point-process\nestimation across different models, including Poisson and Thomas processes. To\nimprove noise robustness, we propose to use stability selection based on\npoint-process subsampling and to incorporate a non-convex best-subset penalty\nto enhance model-selection performance. In extensive simulations, we\ndemonstrate that such an approach reliably recovers true covariates under\ndiverse noise scenarios and improves both selection accuracy and stability. We\nthen apply the proposed method to a forestry data set, analyzing the\ndistribution of trees in relation to elevation and soil nutrients in a tropical\nrain forest. This shows the practical utility of the method, which provides a\nsystematic framework for robust variable selection in spatial point-process\nmodels under noise, without requiring additional knowledge of the process.\n","authors":["Dominik Sturm","Ivo F. Sbalzarini"],"pdf_url":"https://arxiv.org/pdf/2510.25550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12828v2","updated":"2025-10-29T14:17:09Z","published":"2024-02-20T08:54:07Z","title":"Tracking the Median of Gradients with a Stochastic Proximal Point Method","summary":"  There are several applications of stochastic optimization where one can\nbenefit from a robust estimate of the gradient. For example, domains such as\ndistributed learning with corrupted nodes, the presence of large outliers in\nthe training data, learning under privacy constraints, or even heavy-tailed\nnoise due to the dynamics of the algorithm itself. Here we study SGD with\nrobust gradient estimators based on estimating the median.\n  We first derive iterative methods based on the stochastic proximal point\nmethod for computing the median gradient and generalizations thereof. Then we\npropose an algorithm estimating the median gradient across iterations, and find\nthat several well known methods are particular cases of this framework. For\ninstance, we observe that different forms of clipping allow to compute online\nestimators of the median of gradients, in contrast to (heavy-ball) momentum,\nwhich corresponds to an online estimator of the mean. Finally, we provide a\ntheoretical framework for an algorithm computing the median gradient across\nsamples, and show that the resulting method can converge even under\nheavy-tailed, state-dependent noise.\n","authors":["Fabian Schaipp","Guillaume Garrigos","Umut Simsekli","Robert Gower"],"pdf_url":"https://arxiv.org/pdf/2402.12828v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12662v3","updated":"2025-10-29T14:16:37Z","published":"2025-03-16T21:34:11Z","title":"TuneNSearch: a hybrid transfer learning and local search approach for\n  solving vehicle routing problems","summary":"  This paper introduces TuneNSearch, a hybrid transfer learning and local\nsearch approach for addressing diverse variants of the vehicle routing problem\n(VRP). Our method uses reinforcement learning to generate high-quality\nsolutions, which are subsequently refined by an efficient local search\nprocedure. To ensure broad adaptability across VRP variants, TuneNSearch begins\nwith a pre-training phase on the multi-depot VRP (MDVRP), followed by a\nfine-tuning phase to adapt it to other problem formulations. The learning phase\nutilizes a Transformer-based architecture enhanced with edge-aware attention,\nwhich integrates edge distances directly into the attention mechanism to better\ncapture spatial relationships inherent to routing problems. We show that the\npre-trained model generalizes effectively to single-depot variants, achieving\nperformance comparable to models trained specifically on single-depot\ninstances. Simultaneously, it maintains strong performance on multi-depot\nvariants, an ability that models pre-trained solely on single-depot problems\nlack. For example, on 100-node instances of multi-depot variants, TuneNSearch\noutperforms a model pre-trained on the CVRP by 44%. In contrast, on 100-node\ninstances of single-depot variants, TuneNSearch performs similar to the CVRP\nmodel. To validate the effectiveness of our method, we conduct extensive\ncomputational experiments on public benchmark and randomly generated instances.\nAcross multiple CVRPLIB datasets, TuneNSearch consistently achieves performance\ndeviations of less than 3% from the best-known solutions in the literature,\ncompared to 6-25% for other neural-based models, depending on problem\ncomplexity. Overall, our approach demonstrates strong generalization to\ndifferent problem sizes, instance distributions, and VRP formulations, while\nmaintaining polynomial runtime complexity despite the integration of the local\nsearch algorithm.\n","authors":["Arthur CorrÃªa","CristÃ³vÃ£o Silva","Liming Xu","Alexandra Brintrup","Samuel Moniz"],"pdf_url":"https://arxiv.org/pdf/2503.12662v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.18962v2","updated":"2025-10-29T14:11:14Z","published":"2025-09-23T13:14:37Z","title":"Lift What You Can: Green Online Learning with Heterogeneous Ensembles","summary":"  Ensemble methods for stream mining necessitate managing multiple models and\nupdating them as data distributions evolve. Considering the calls for more\nsustainability, established methods are however not sufficiently considerate of\nensemble members' computational expenses and instead overly focus on predictive\ncapabilities. To address these challenges and enable green online learning, we\npropose heterogeneous online ensembles (HEROS). For every training step, HEROS\nchooses a subset of models from a pool of models initialized with diverse\nhyperparameter choices under resource constraints to train. We introduce a\nMarkov decision process to theoretically capture the trade-offs between\npredictive performance and sustainability constraints. Based on this framework,\nwe present different policies for choosing which models to train on incoming\ndata. Most notably, we propose the novel $\\zeta$-policy, which focuses on\ntraining near-optimal models at reduced costs. Using a stochastic model, we\ntheoretically prove that our $\\zeta$-policy achieves near optimal performance\nwhile using fewer resources compared to the best performing policy. In our\nexperiments across 11 benchmark datasets, we find empiric evidence that our\n$\\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating\nhighly accurate performance, in some cases even outperforming competitors, and\nsimultaneously being much more resource-friendly.\n","authors":["Kirsten KÃ¶bschall","Sebastian BuschjÃ¤ger","Raphael Fischer","Lisa Hartung","Stefan Kramer"],"pdf_url":"https://arxiv.org/pdf/2509.18962v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25544v1","updated":"2025-10-29T14:11:03Z","published":"2025-10-29T14:11:03Z","title":"Error Bounds and Optimal Schedules for Masked Diffusions with Factorized\n  Approximations","summary":"  Recently proposed generative models for discrete data, such as Masked\nDiffusion Models (MDMs), exploit conditional independence approximations to\nreduce the computational cost of popular Auto-Regressive Models (ARMs), at the\nprice of some bias in the sampling distribution. We study the resulting\ncomputation-vs-accuracy trade-off, providing general error bounds (in relative\nentropy) that depend only on the average number of tokens generated per\niteration and are independent of the data dimensionality (i.e. sequence\nlength), thus supporting the empirical success of MDMs. We then investigate the\ngain obtained by using non-constant schedule sizes (i.e. varying the number of\nunmasked tokens during the generation process) and identify the optimal\nschedule as a function of a so-called information profile of the data\ndistribution, thus allowing for a principled optimization of schedule sizes. We\ndefine methods directly as sampling algorithms and do not use classical\nderivations as time-reversed diffusion processes, leading us to simple and\ntransparent proofs.\n","authors":["Hugo Lavenant","Giacomo Zanella"],"pdf_url":"https://arxiv.org/pdf/2510.25544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25542v1","updated":"2025-10-29T14:07:12Z","published":"2025-10-29T14:07:12Z","title":"Transformers Provably Learn Directed Acyclic Graphs via Kernel-Guided\n  Mutual Information","summary":"  Uncovering hidden graph structures underlying real-world data is a critical\nchallenge with broad applications across scientific domains. Recently,\ntransformer-based models leveraging the attention mechanism have demonstrated\nstrong empirical success in capturing complex dependencies within graphs.\nHowever, the theoretical understanding of their training dynamics has been\nlimited to tree-like graphs, where each node depends on a single parent.\nExtending provable guarantees to more general directed acyclic graphs (DAGs) --\nwhich involve multiple parents per node -- remains challenging, primarily due\nto the difficulty in designing training objectives that enable different\nattention heads to separately learn multiple different parent relationships.\n  In this work, we address this problem by introducing a novel\ninformation-theoretic metric: the kernel-guided mutual information (KG-MI),\nbased on the $f$-divergence. Our objective combines KG-MI with a multi-head\nattention framework, where each head is associated with a distinct marginal\ntransition kernel to model diverse parent-child dependencies effectively. We\nprove that, given sequences generated by a $K$-parent DAG, training a\nsingle-layer, multi-head transformer via gradient ascent converges to the\nglobal optimum in polynomial time. Furthermore, we characterize the attention\nscore patterns at convergence. In addition, when particularizing the\n$f$-divergence to the KL divergence, the learned attention scores accurately\nreflect the ground-truth adjacency matrix, thereby provably recovering the\nunderlying graph structure. Experimental results validate our theoretical\nfindings.\n","authors":["Yuan Cheng","Yu Huang","Zhe Xiong","Yingbin Liang","Vincent Y. F. Tan"],"pdf_url":"https://arxiv.org/pdf/2510.25542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08388v3","updated":"2025-10-29T14:02:55Z","published":"2025-06-10T02:53:24Z","title":"Reinforcement Learning Teachers of Test Time Scaling","summary":"  Training reasoning language models (LMs) with reinforcement learning (RL) for\none-hot correctness inherently relies on the LM being able to explore and solve\nits task with some chance at initialization. Furthermore, a key use case of\nreasoning LMs is to act as teachers for distilling new students and\ncold-starting future RL iterations rather than being deployed themselves. From\nthese considerations, we introduce a new framework that avoids RL's exploration\nchallenge by training a new class of Reinforcement-Learned Teachers (RLTs)\nfocused on yielding the most effective downstream distillation. RLTs are\nprompted with both the question and solution to each problem, and tasked to\nsimply \"connect-the-dots\" with detailed explanations tailored for their\nstudents. We train RLTs with dense rewards obtained by feeding each explanation\nto the student and testing its understanding of the problem's solution. In\npractice, the raw outputs of a 7B RLT provide higher final performance on\ncompetition and graduate-level tasks than existing distillation and\ncold-starting pipelines that collect and postprocess the reasoning traces of\norders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness\nwhen training larger students and when applied zero-shot to out-of-distribution\ntasks, unlocking new levels of efficiency and re-usability for the RL reasoning\nframework. Code available at: https://github.com/SakanaAI/RLT\n","authors":["Edoardo Cetin","Tianyu Zhao","Yujin Tang"],"pdf_url":"https://arxiv.org/pdf/2506.08388v3.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25531v1","updated":"2025-10-29T13:56:44Z","published":"2025-10-29T13:56:44Z","title":"Using latent representations to link disjoint longitudinal data for\n  mixed-effects regression","summary":"  Many rare diseases offer limited established treatment options, leading\npatients to switch therapies when new medications emerge. To analyze the impact\nof such treatment switches within the low sample size limitations of rare\ndisease trials, it is important to use all available data sources. This,\nhowever, is complicated when usage of measurement instruments change during the\nobservation period, for example when instruments are adapted to specific age\nranges. The resulting disjoint longitudinal data trajectories, complicate the\napplication of traditional modeling approaches like mixed-effects regression.\nWe tackle this by mapping observations of each instrument to a aligned\nlow-dimensional temporal trajectory, enabling longitudinal modeling across\ninstruments. Specifically, we employ a set of variational autoencoder\narchitectures to embed item values into a shared latent space for each time\npoint. Temporal disease dynamics and treatment switch effects are then captured\nthrough a mixed-effects regression model applied to latent representations. To\nenable statistical inference, we present a novel statistical testing approach\nthat accounts for the joint parameter estimation of mixed-effects regression\nand variational autoencoders. The methodology is applied to quantify the impact\nof treatment switches for patients with spinal muscular atrophy. Here, our\napproach aligns motor performance items from different measurement instruments\nfor mixed-effects regression and maps estimated effects back to the observed\nitem level to quantify the treatment switch effect. Our approach allows for\nmodel selection as well as for assessing effects of treatment switching. The\nresults highlight the potential of modeling in joint latent representations for\naddressing small data challenges.\n","authors":["Clemens SchÃ¤chter","Maren Hackenberg","Michelle Pfaffenlehner","FÃ©lix B. Tambe-Ndonfack","Thorsten Schmidt","Astrid Pechmann","Janbernd Kirschner","Jan Hasenauser","Harald Binder"],"pdf_url":"https://arxiv.org/pdf/2510.25531v1.pdf","comment":"31 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2510.23906v2","updated":"2025-10-29T13:42:56Z","published":"2025-10-27T22:26:20Z","title":"Group Interventions on Deep Networks for Causal Discovery in Subsystems","summary":"  Causal discovery uncovers complex relationships between variables, enhancing\npredictions, decision-making, and insights into real-world systems, especially\nin nonlinear multivariate time series. However, most existing methods primarily\nfocus on pairwise cause-effect relationships, overlooking interactions among\ngroups of variables, i.e., subsystems and their collective causal influence. In\nthis study, we introduce gCDMI, a novel multi-group causal discovery method\nthat leverages group-level interventions on trained deep neural networks and\nemploys model invariance testing to infer causal relationships. Our approach\ninvolves three key steps. First, we use deep learning to jointly model the\nstructural relationships among groups of all time series. Second, we apply\ngroup-wise interventions to the trained model. Finally, we conduct model\ninvariance testing to determine the presence of causal links among variable\ngroups. We evaluate our method on simulated datasets, demonstrating its\nsuperior performance in identifying group-level causal relationships compared\nto existing methods. Additionally, we validate our approach on real-world\ndatasets, including brain networks and climate ecosystems. Our results\nhighlight that applying group-level interventions to deep learning models,\ncombined with invariance testing, can effectively reveal complex causal\nstructures, offering valuable insights for domains such as neuroscience and\nclimate science.\n","authors":["Wasim Ahmad","Joachim Denzler","Maha Shadaydeh"],"pdf_url":"https://arxiv.org/pdf/2510.23906v2.pdf","comment":"Submitted to IEEE Access. We are working on the revised version"},{"id":"http://arxiv.org/abs/2510.25514v1","updated":"2025-10-29T13:38:24Z","published":"2025-10-29T13:38:24Z","title":"Convergence of off-policy TD(0) with linear function approximation for\n  reversible Markov chains","summary":"  We study the convergence of off-policy TD(0) with linear function\napproximation when used to approximate the expected discounted reward in a\nMarkov chain. It is well known that the combination of off-policy learning and\nfunction approximation can lead to divergence of the algorithm. Existing\nresults for this setting modify the algorithm, for instance by reweighing the\nupdates using importance sampling. This establishes convergence at the expense\nof additional complexity. In contrast, our approach is to analyse the standard\nalgorithm, but to restrict our attention to the class of reversible Markov\nchains. We demonstrate convergence under this mild reversibility condition on\nthe structure of the chain, which in many applications can be assumed using\ndomain knowledge. In particular, we establish a convergence guarantee under an\nupper bound on the discount factor in terms of the difference between the\non-policy and off-policy process. This improves upon known results in the\nliterature that state that convergence holds for a sufficiently small discount\nfactor by establishing an explicit bound. Convergence is with probability one\nand achieves projected Bellman error equal to zero. To obtain these results, we\nadapt the stochastic approximation framework that was used by Tsitsiklis and\nVan Roy [1997 for the on-policy case, to the off-policy case. We illustrate our\nresults using different types of reversible Markov chains, such as\none-dimensional random walks and random walks on a weighted graph.\n","authors":["Maik Overmars","Jasper Goseling","Richard Boucherie"],"pdf_url":"https://arxiv.org/pdf/2510.25514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25512v1","updated":"2025-10-29T13:35:46Z","published":"2025-10-29T13:35:46Z","title":"FaCT: Faithful Concept Traces for Explaining Neural Network Decisions","summary":"  Deep networks have shown remarkable performance across a wide range of tasks,\nyet getting a global concept-level understanding of how they function remains a\nkey challenge. Many post-hoc concept-based approaches have been introduced to\nunderstand their workings, yet they are not always faithful to the model.\nFurther, they make restrictive assumptions on the concepts a model learns, such\nas class-specificity, small spatial extent, or alignment to human expectations.\nIn this work, we put emphasis on the faithfulness of such concept-based\nexplanations and propose a new model with model-inherent mechanistic\nconcept-explanations. Our concepts are shared across classes and, from any\nlayer, their contribution to the logit and their input-visualization can be\nfaithfully traced. We also leverage foundation models to propose a new\nconcept-consistency metric, C$^2$-Score, that can be used to evaluate\nconcept-based methods. We show that, compared to prior work, our concepts are\nquantitatively more consistent and users find our concepts to be more\ninterpretable, all while retaining competitive ImageNet performance.\n","authors":["Amin Parchami-Araghi","Sukrut Rao","Jonas Fischer","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2510.25512v1.pdf","comment":"Accepted to NeurIPS 2025; Code is available at\n  https://github.com/m-parchami/FaCT"},{"id":"http://arxiv.org/abs/2510.25509v1","updated":"2025-10-29T13:32:59Z","published":"2025-10-29T13:32:59Z","title":"Support Vector Machine-Based Burnout Risk Prediction with an Interactive\n  Interface for Organizational Use","summary":"  Burnout is a psychological syndrome marked by emotional exhaustion,\ndepersonalization, and reduced personal accomplishment, with a significant\nimpact on individual well-being and organizational performance. This study\nproposes a machine learning approach to predict burnout risk using the\nHackerEarth Employee Burnout Challenge dataset. Three supervised algorithms\nwere evaluated: nearest neighbors (KNN), random forest, and support vector\nmachine (SVM), with model performance evaluated through 30-fold\ncross-validation using the determination coefficient (R2). Among the models\ntested, SVM achieved the highest predictive performance (R2 = 0.84) and was\nstatistically superior to KNN and Random Forest based on paired $t$-tests. To\nensure practical applicability, an interactive interface was developed using\nStreamlit, allowing non-technical users to input data and receive burnout risk\npredictions. The results highlight the potential of machine learning to support\nearly detection of burnout and promote data-driven mental health strategies in\norganizational settings.\n","authors":["Bruno W. G. Teodosio","MÃ¡rio J. O. T. Lira","Pedro H. M. AraÃºjo","Lucas R. C. Farias"],"pdf_url":"https://arxiv.org/pdf/2510.25509v1.pdf","comment":"12 pages, including figures and references. Streamlit app available\n  at: https://employee-burnout-svm.streamlit.app/"},{"id":"http://arxiv.org/abs/2510.25502v1","updated":"2025-10-29T13:27:18Z","published":"2025-10-29T13:27:18Z","title":"TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time\n  Series Forecasting","summary":"  Foundation models for zero-shot time series forecasting face challenges in\nefficient long-horizon prediction and reproducibility, with existing\nsynthetic-only approaches underperforming on challenging benchmarks. This paper\npresents TempoPFN, a univariate time series foundation model based on linear\nRecurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The\nmodel uses a GatedDeltaProduct architecture with state-weaving for fully\nparallelizable training across sequence lengths, eliminating the need for\nwindowing or summarization techniques while maintaining robust temporal\nstate-tracking. Our comprehensive synthetic data pipeline unifies diverse\ngenerators, including stochastic differential equations, Gaussian processes,\nand audio synthesis, with novel augmentations. In zero-shot evaluations on the\nGift-Eval benchmark, TempoPFN achieves top-tier competitive performance,\noutperforming all existing synthetic-only approaches and surpassing the vast\nmajority of models trained on real-world data, while being more efficient than\nexisting baselines by leveraging fully parallelizable training and inference.\nWe open-source our complete data generation pipeline and training code,\nproviding a reproducible foundation for future research.\n","authors":["Vladyslav Moroshan","Julien Siems","Arber Zela","Timur Carstensen","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2510.25502v1.pdf","comment":"30 pages, 18 figures, 13 tables"},{"id":"http://arxiv.org/abs/2510.25497v1","updated":"2025-10-29T13:21:28Z","published":"2025-10-29T13:21:28Z","title":"Right for the Right Reasons: Avoiding Reasoning Shortcuts via\n  Prototypical Neurosymbolic AI","summary":"  Neurosymbolic AI is growing in popularity thanks to its ability to combine\nneural perception and symbolic reasoning in end-to-end trainable models.\nHowever, recent findings reveal these are prone to shortcut reasoning, i.e., to\nlearning unindented concepts--or neural predicates--which exploit spurious\ncorrelations to satisfy the symbolic constraints. In this paper, we address\nreasoning shortcuts at their root cause and we introduce prototypical\nneurosymbolic architectures. These models are able to satisfy the symbolic\nconstraints (be right) because they have learnt the correct basic concepts (for\nthe right reasons) and not because of spurious correlations, even in extremely\nlow data regimes. Leveraging the theory of prototypical learning, we\ndemonstrate that we can effectively avoid reasoning shortcuts by training the\nmodels to satisfy the background knowledge while taking into account the\nsimilarity of the input with respect to the handful of labelled datapoints. We\nextensively validate our approach on the recently proposed rsbench benchmark\nsuite in a variety of settings and tasks with very scarce supervision: we show\nsignificant improvements in learning the right concepts both in synthetic tasks\n(MNIST-EvenOdd and Kand-Logic) and real-world, high-stake ones (BDD-OIA). Our\nfindings pave the way to prototype grounding as an effective,\nannotation-efficient strategy for safe and reliable neurosymbolic learning.\n","authors":["Luca Andolfi","Eleonora Giunchiglia"],"pdf_url":"https://arxiv.org/pdf/2510.25497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25480v1","updated":"2025-10-29T13:04:17Z","published":"2025-10-29T13:04:17Z","title":"Gradient-Weight Alignment as a Train-Time Proxy for Generalization in\n  Classification Tasks","summary":"  Robust validation metrics remain essential in contemporary deep learning, not\nonly to detect overfitting and poor generalization, but also to monitor\ntraining dynamics. In the supervised classification setting, we investigate\nwhether interactions between training data and model weights can yield such a\nmetric that both tracks generalization during training and attributes\nperformance to individual training samples. We introduce Gradient-Weight\nAlignment (GWA), quantifying the coherence between per-sample gradients and\nmodel weights. We show that effective learning corresponds to coherent\nalignment, while misalignment indicates deteriorating generalization. GWA is\nefficiently computable during training and reflects both sample-specific\ncontributions and dataset-wide learning dynamics. Extensive experiments show\nthat GWA accurately predicts optimal early stopping, enables principled model\ncomparisons, and identifies influential training samples, providing a\nvalidation-set-free approach for model analysis directly from the training\ndata.\n","authors":["Florian A. HÃ¶lzl","Daniel Rueckert","Georgios Kaissis"],"pdf_url":"https://arxiv.org/pdf/2510.25480v1.pdf","comment":"39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)"},{"id":"http://arxiv.org/abs/2510.25470v1","updated":"2025-10-29T12:43:18Z","published":"2025-10-29T12:43:18Z","title":"An In-Depth Analysis of Cyber Attacks in Secured Platforms","summary":"  There is an increase in global malware threats. To address this, an\nencryption-type ransomware has been introduced on the Android operating system.\nThe challenges associated with malicious threats in phone use have become a\npressing issue in mobile communication, disrupting user experiences and posing\nsignificant privacy threats. This study surveys commonly used machine learning\ntechniques for detecting malicious threats in phones and examines their\nperformance. The majority of past research focuses on customer feedback and\nreviews, with concerns that people might create false reviews to promote or\ndevalue products and services for personal gain. Hence, the development of\ntechniques for detecting malicious threats using machine learning has been a\nkey focus. This paper presents a comprehensive comparative study of current\nresearch on the issue of malicious threats and methods for tackling these\nchallenges. Nevertheless, a huge amount of information is required by these\nmethods, presenting a challenge for developing robust, specialized automated\nanti-malware systems. This research describes the Android Applications dataset,\nand the accuracy of the techniques is measured using the accuracy levels of the\nmetrics employed in this study.\n","authors":["Parick Ozoh","John K Omoniyi","Bukola Ibitoye"],"pdf_url":"https://arxiv.org/pdf/2510.25470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25458v1","updated":"2025-10-29T12:32:14Z","published":"2025-10-29T12:32:14Z","title":"Scalable Utility-Aware Multiclass Calibration","summary":"  Ensuring that classifiers are well-calibrated, i.e., their predictions align\nwith observed frequencies, is a minimal and fundamental requirement for\nclassifiers to be viewed as trustworthy. Existing methods for assessing\nmulticlass calibration often focus on specific aspects associated with\nprediction (e.g., top-class confidence, class-wise calibration) or utilize\ncomputationally challenging variational formulations. In this work, we study\nscalable \\emph{evaluation} of multiclass calibration. To this end, we propose\nutility calibration, a general framework that measures the calibration error\nrelative to a specific utility function that encapsulates the goals or decision\ncriteria relevant to the end user. We demonstrate how this framework can unify\nand re-interpret several existing calibration metrics, particularly allowing\nfor more robust versions of the top-class and class-wise calibration metrics,\nand, going beyond such binarized approaches, toward assessing calibration for\nricher classes of downstream utilities.\n","authors":["Mahmoud Hegazy","Michael I. Jordan","Aymeric Dieuleveut"],"pdf_url":"https://arxiv.org/pdf/2510.25458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17897v4","updated":"2025-10-29T12:15:39Z","published":"2025-07-23T19:48:27Z","title":"Multimodal Recurrent Ensembles for Predicting Brain Responses to\n  Naturalistic Movies (Algonauts 2025)","summary":"  Accurately predicting distributed cortical responses to naturalistic stimuli\nrequires models that integrate visual, auditory and semantic information over\ntime. We present a hierarchical multimodal recurrent ensemble that maps\npretrained video, audio, and language embeddings to fMRI time series recorded\nwhile four subjects watched almost 80 hours of movies provided by the Algonauts\n2025 challenge. Modality-specific bidirectional RNNs encode temporal dynamics;\ntheir hidden states are fused and passed to a second recurrent layer, and\nlightweight subject-specific heads output responses for 1000 cortical parcels.\nTraining relies on a composite MSE-correlation loss and a curriculum that\ngradually shifts emphasis from early sensory to late association regions.\nAveraging 100 model variants further boosts robustness. The resulting system\nranked third on the competition leaderboard, achieving an overall Pearson r =\n0.2094 and the highest single-parcel peak score (mean r = 0.63) among all\nparticipants, with particularly strong gains for the most challenging subject\n(Subject 5). The approach establishes a simple, extensible baseline for future\nmultimodal brain-encoding benchmarks.\n","authors":["Semih Eren","Deniz Kucukahmetler","Nico Scherf"],"pdf_url":"https://arxiv.org/pdf/2507.17897v4.pdf","comment":"8 pages, 2 figures, 1 table. Invited report, CCN 2025 Algonauts\n  Project session (3rd-place team). Code:\n  https://github.com/erensemih/Algonauts2025_ModalityRNN v3: Added equal\n  contribution footnote to author list. Corrected reference list"},{"id":"http://arxiv.org/abs/2510.25445v1","updated":"2025-10-29T12:11:34Z","published":"2025-10-29T12:11:34Z","title":"Agentic AI: A Comprehensive Survey of Architectures, Applications, and\n  Future Directions","summary":"  Agentic AI represents a transformative shift in artificial intelligence, but\nits rapid advancement has led to a fragmented understanding, often conflating\nmodern neural systems with outdated symbolic models -- a practice known as\nconceptual retrofitting. This survey cuts through this confusion by introducing\na novel dual-paradigm framework that categorizes agentic systems into two\ndistinct lineages: the Symbolic/Classical (relying on algorithmic planning and\npersistent state) and the Neural/Generative (leveraging stochastic generation\nand prompt-driven orchestration). Through a systematic PRISMA-based review of\n90 studies (2018--2025), we provide a comprehensive analysis structured around\nthis framework across three dimensions: (1) the theoretical foundations and\narchitectural principles defining each paradigm; (2) domain-specific\nimplementations in healthcare, finance, and robotics, demonstrating how\napplication constraints dictate paradigm selection; and (3) paradigm-specific\nethical and governance challenges, revealing divergent risks and mitigation\nstrategies. Our analysis reveals that the choice of paradigm is strategic:\nsymbolic systems dominate safety-critical domains (e.g., healthcare), while\nneural systems prevail in adaptive, data-rich environments (e.g., finance).\nFurthermore, we identify critical research gaps, including a significant\ndeficit in governance models for symbolic systems and a pressing need for\nhybrid neuro-symbolic architectures. The findings culminate in a strategic\nroadmap arguing that the future of Agentic AI lies not in the dominance of one\nparadigm, but in their intentional integration to create systems that are both\nadaptable and reliable. This work provides the essential conceptual toolkit to\nguide future research, development, and policy toward robust and trustworthy\nhybrid intelligent systems.\n","authors":["Mohamad Abou Ali","Fadi Dornaika"],"pdf_url":"https://arxiv.org/pdf/2510.25445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16368v2","updated":"2025-10-29T12:06:15Z","published":"2025-05-22T08:23:10Z","title":"SATURN: SAT-based Reinforcement Learning to Unleash Language Model\n  Reasoning","summary":"  How to design reinforcement learning (RL) tasks that effectively unleash the\nreasoning capability of large language models (LLMs) remains an open question.\nExisting RL tasks (e.g., math, programming, and constructing reasoning tasks)\nsuffer from three key limitations: (1) Scalability. They rely heavily on human\nannotation or expensive LLM synthesis to generate sufficient training data. (2)\nVerifiability. LLMs' outputs are hard to verify automatically and reliably. (3)\nControllable Difficulty. Most tasks lack fine-grained difficulty control,\nmaking it hard to train LLMs to develop reasoning ability from easy to hard.\n  To address these limitations, we propose Saturn, a SAT-based RL framework\nthat uses Boolean Satisfiability (SAT) problems to train and evaluate LLMs\nreasoning. Saturn enables scalable task construction, rule-based verification,\nand precise difficulty control. Saturn designs a curriculum learning pipeline\nthat continuously improves LLMs' reasoning capability by constructing SAT tasks\nof increasing difficulty and training LLMs from easy to hard. To ensure stable\ntraining, we design a principled mechanism to control difficulty transitions.\n  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying\ndifficulty. It supports the evaluation of how LLM reasoning changes with\nproblem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain\nSaturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT\nproblems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of\n+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B\nand Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,\nAIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in\nconstructing RL tasks, Saturn achieves further improvements of +8.8%. We\nrelease the source code, data, and models to support future research.\n","authors":["Huanyu Liu","Jia Li","Hao Zhu","Kechi Zhang","Yihong Dong","Ge Li"],"pdf_url":"https://arxiv.org/pdf/2505.16368v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07870v3","updated":"2025-10-29T12:00:29Z","published":"2023-08-15T16:37:16Z","title":"Brain-inspired Computational Intelligence via Predictive Coding","summary":"  Artificial intelligence (AI) is rapidly becoming one of the key technologies\nof this century. The majority of results in AI thus far have been achieved\nusing deep neural networks trained with a learning algorithm called error\nbackpropagation, always considered biologically implausible. To this end,\nrecent works have studied learning algorithms for deep neural networks inspired\nby the neurosciences. One such theory, called predictive coding (PC), has shown\npromising properties that make it potentially valuable for the machine learning\ncommunity: it can model information processing in different areas of the brain,\ncan be used in control and robotics, has a solid mathematical foundation in\nvariational inference, and performs its computations asynchronously. Inspired\nby such properties, works that propose novel PC-like algorithms are starting to\nbe present in multiple sub-fields of machine learning and AI at large. Here, we\nsurvey such efforts by first providing a broad overview of the history of PC to\nprovide common ground for the understanding of the recent developments, then by\ndescribing current efforts and results, and concluding with a large discussion\nof possible implications and ways forward.\n","authors":["Tommaso Salvatori","Ankur Mali","Christopher L. Buckley","Thomas Lukasiewicz","Rajesh P. N. Rao","Karl Friston","Alexander Ororbia"],"pdf_url":"https://arxiv.org/pdf/2308.07870v3.pdf","comment":"26 Pages, 9 Figures"},{"id":"http://arxiv.org/abs/2412.04233v4","updated":"2025-10-29T11:37:54Z","published":"2024-12-05T15:09:51Z","title":"HyperMARL: Adaptive Hypernetworks for Multi-Agent RL","summary":"  Adaptive cooperation in multi-agent reinforcement learning (MARL) requires\npolicies to express homogeneous, specialised, or mixed behaviours, yet\nachieving this adaptivity remains a critical challenge. While parameter sharing\n(PS) is standard for efficient learning, it notoriously suppresses the\nbehavioural diversity required for specialisation. This failure is largely due\nto cross-agent gradient interference, a problem we find is surprisingly\nexacerbated by the common practice of coupling agent IDs with observations.\nExisting remedies typically add complexity through altered objectives, manual\npreset diversity levels, or sequential updates -- raising a fundamental\nquestion: can shared policies adapt without these intricacies? We propose a\nsolution built on a key insight: an agent-conditioned hypernetwork can generate\nagent-specific parameters and decouple observation- and agent-conditioned\ngradients, directly countering the interference from coupling agent IDs with\nobservations. Our resulting method, HyperMARL, avoids the complexities of prior\nwork and empirically reduces policy gradient variance. Across diverse MARL\nbenchmarks (22 scenarios, up to 30 agents), HyperMARL achieves performance\ncompetitive with six key baselines while preserving behavioural diversity\ncomparable to non-parameter sharing methods, establishing it as a versatile and\nprincipled approach for adaptive MARL. The code is publicly available at\nhttps://github.com/KaleabTessera/HyperMARL.\n","authors":["Kale-ab Abebe Tessera","Arrasy Rahman","Amos Storkey","Stefano V. Albrecht"],"pdf_url":"https://arxiv.org/pdf/2412.04233v4.pdf","comment":"To appear at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025). A preliminary version of this work was presented at\n  the CoCoMARL workshop, RLC 2025"},{"id":"http://arxiv.org/abs/2406.07222v3","updated":"2025-10-29T11:36:28Z","published":"2024-06-11T13:01:50Z","title":"Reliable Evaluation and Benchmarks for Statement Autoformalization","summary":"  Evaluating statement autoformalization, translating natural language\nmathematics into formal languages like Lean 4, remains a significant challenge,\nwith few metrics, datasets, and standards to robustly measure progress. In this\nwork, we present a comprehensive approach combining improved metrics, robust\nbenchmarks, and systematic evaluation, to fill this gap. First, we introduce\nBEq+, an automated metric that correlates strongly with human judgment, along\nwith ProofNetVerif, a new dataset for assessing the quality of evaluation\nmetrics, containing 3,752 annotated examples. Second, we develop two new\nautoformalization benchmarks: ProofNet#, a corrected version of ProofNet, and\nRLM25, with 619 new pairs of research-level mathematics from six formalization\nprojects. Through systematic experimentation across these benchmarks, we find\nthat current techniques can achieve up to 45.1% accuracy on undergraduate\nmathematics but struggle with research-level content without proper context.\nOur work establishes a reliable foundation for evaluating and advancing\nautoformalization systems.\n","authors":["Auguste Poiroux","Gail Weiss","Viktor KunÄak","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2406.07222v3.pdf","comment":"Accepted to EMNLP 2025. New benchmarks released, see\n  https://github.com/augustepoiroux/RLMEval ,\n  https://huggingface.co/datasets/PAug/ProofNetSharp , and\n  https://huggingface.co/datasets/PAug/ProofNetVerif . For code, see\n  https://github.com/augustepoiroux/LeanInteract"},{"id":"http://arxiv.org/abs/2504.17247v2","updated":"2025-10-29T11:30:12Z","published":"2025-04-24T04:53:04Z","title":"OmegAMP: Targeted AMP Discovery through Biologically Informed Generation","summary":"  Deep learning-based antimicrobial peptide (AMP) discovery faces critical\nchallenges such as limited controllability, lack of representations that\nefficiently model antimicrobial properties, and low experimental hit rates. To\naddress these challenges, we introduce OmegAMP, a framework designed for\nreliable AMP generation with increased controllability. Its diffusion-based\ngenerative model leverages a novel conditioning mechanism to achieve\nfine-grained control over desired physicochemical properties and to direct\ngeneration towards specific activity profiles, including species-specific\neffectiveness. This is further enhanced by a biologically informed encoding\nspace that significantly improves overall generative performance. Complementing\nthese generative capabilities, OmegAMP leverages a novel synthetic data\naugmentation strategy to train classifiers for AMP filtering, drastically\nreducing false positive rates and thereby increasing the likelihood of\nexperimental success. Our in silico experiments demonstrate that OmegAMP\ndelivers state-of-the-art performance across key stages of the AMP discovery\npipeline, enabling us to achieve an unprecedented success rate in wet lab\nexperiments. We tested 25 candidate peptides, 24 of them (96%) demonstrated\nantimicrobial activity, proving effective even against multi-drug resistant\nstrains. Our findings underscore OmegAMP's potential to significantly advance\ncomputational frameworks in the fight against antimicrobial resistance.\n","authors":["Diogo Soares","Leon Hetzel","Paulina Szymczak","Marcelo Der Torossian Torres","Johanna Sommer","Cesar de la Fuente-Nunez","Fabian Theis","Stephan GÃ¼nnemann","Ewa Szczurek"],"pdf_url":"https://arxiv.org/pdf/2504.17247v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25404v1","updated":"2025-10-29T11:21:55Z","published":"2025-10-29T11:21:55Z","title":"GPTOpt: Towards Efficient LLM-Based Black-Box Optimization","summary":"  Global optimization of expensive, derivative-free black-box functions demands\nextreme sample efficiency. Classical methods such as Bayesian Optimization (BO)\ncan be effective, but they often require careful parameter tuning to each\napplication domain. At the same time, Large Language Models (LLMs) have shown\nbroad capabilities, yet state-of-the-art models remain limited in solving\ncontinuous black-box optimization tasks. We introduce GPTOpt, an LLM-based\noptimization method that equips LLMs with continuous black-box optimization\ncapabilities. By fine-tuning large language models on extensive synthetic\ndatasets derived from diverse BO parameterizations, GPTOpt leverages LLM\npre-training to generalize across optimization tasks. On a variety of black-box\noptimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting\nthe capacity of LLMs for advanced numerical reasoning and introducing a\nflexible framework for global optimization without parameter tuning.\n","authors":["Jamison Meindl","Yunsheng Tian","Tony Cui","Veronika Thost","Zhang-Wei Hong","Jie Chen","Wojciech Matusik","Mina KonakoviÄ LukoviÄ"],"pdf_url":"https://arxiv.org/pdf/2510.25404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23665v2","updated":"2025-10-29T11:16:57Z","published":"2025-10-26T13:48:03Z","title":"Transformers from Compressed Representations","summary":"  Compressed file formats are the corner stone of efficient data storage and\ntransmission, yet their potential for representation learning remains largely\nunderexplored. We introduce TEMPEST (TransformErs froM comPressed\nrEpreSenTations), a method that exploits the inherent byte-stream structure of\ncompressed files to design an effective tokenization and encoding strategy. By\nleveraging this compact encoding, a standard transformer can directly learn\nsemantic representations from compressed data streams, bypassing the need for\nraw byte-level processing or full media decoding. Our proposal substantially\nreduces the number of tokens required for semantic classification, thereby\nlowering both computational complexity and memory usage. Through extensive\nexperiments across diverse datasets, coding schemes, and modalities, we show\nthat TEMPEST achieves accuracy competitive wit the state-of-the-art while\ndelivering efficiency gains in memory and compute.\n","authors":["Juan C. Leon Alcazar","Mattia Soldan","Mohammad Saatialsoruji","Alejandro Pardo","Hani Itani","Juan Camilo Perez","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2510.23665v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23463v2","updated":"2025-10-29T11:16:37Z","published":"2025-10-27T16:01:15Z","title":"Differential Privacy as a Perk: Federated Learning over Multiple-Access\n  Fading Channels with a Multi-Antenna Base Station","summary":"  Federated Learning (FL) is a distributed learning paradigm that preserves\nprivacy by eliminating the need to exchange raw data during training. In its\nprototypical edge instantiation with underlying wireless transmissions enabled\nby analog over-the-air computing (AirComp), referred to as \\emph{over-the-air\nFL (AirFL)}, the inherent channel noise plays a unique role of \\emph{frenemy}\nin the sense that it degrades training due to noisy global aggregation while\nproviding a natural source of randomness for privacy-preserving mechanisms,\nformally quantified by \\emph{differential privacy (DP)}. It remains,\nnevertheless, challenging to effectively harness such channel impairments, as\nprior arts, under assumptions of either simple channel models or restricted\ntypes of loss functions, mostly considering (local) DP enhancement with a\nsingle-round or non-convergent bound on privacy loss. In this paper, we study\nAirFL over multiple-access fading channels with a multi-antenna base station\n(BS) subject to user-level DP requirements. Despite a recent study, which\nclaimed in similar settings that artificial noise (AN) must be injected to\nensure DP in general, we demonstrate, on the contrary, that DP can be gained as\na \\emph{perk} even \\emph{without} employing any AN. Specifically, we derive a\nnovel bound on DP that converges under general bounded-domain assumptions on\nmodel parameters, along with a convergence bound with general smooth and\nnon-convex loss functions. Next, we optimize over receive beamforming and power\nallocations to characterize the optimal convergence-privacy trade-offs, which\nalso reveal explicit conditions in which DP is achievable without compromising\ntraining. Finally, our theoretical findings are validated by extensive\nnumerical results.\n","authors":["Hao Liang","Haifeng Wen","Kaishun Wu","Hong Xing"],"pdf_url":"https://arxiv.org/pdf/2510.23463v2.pdf","comment":"15 pages, 5 figures, submitted for possible publication"},{"id":"http://arxiv.org/abs/2509.04317v2","updated":"2025-10-29T11:06:35Z","published":"2025-09-04T15:38:37Z","title":"Improving Robustness of AlphaZero Algorithms to Test-Time Environment\n  Changes","summary":"  The AlphaZero framework provides a standard way of combining Monte Carlo\nplanning with prior knowledge provided by a previously trained policy-value\nneural network. AlphaZero usually assumes that the environment on which the\nneural network was trained will not change at test time, which constrains its\napplicability. In this paper, we analyze the problem of deploying AlphaZero\nagents in potentially changed test environments and demonstrate how the\ncombination of simple modifications to the standard framework can significantly\nboost performance, even in settings with a low planning budget available. The\ncode is publicly available on GitHub.\n","authors":["Isidoro Tamassia","Wendelin BÃ¶hmer"],"pdf_url":"https://arxiv.org/pdf/2509.04317v2.pdf","comment":"Presented at the 37th Benelux Conference on Artificial Intelligence\n  and the 34th Belgian Dutch Conference on Machine Learning (BNAIC/BeNeLearn\n  2025)"},{"id":"http://arxiv.org/abs/2407.13420v2","updated":"2025-10-29T11:02:41Z","published":"2024-07-18T11:42:58Z","title":"Exploring End-to-end Differentiable Neural Charged Particle Tracking --\n  A Loss Landscape Perspective","summary":"  Measurement and analysis of high energetic particles for scientific, medical\nor industrial applications is a complex procedure, requiring the design of\nsophisticated detector and data processing systems. The development of adaptive\nand differentiable software pipelines using a combination of conventional and\nmachine learning algorithms is therefore getting ever more important to\noptimize and operate the system efficiently while maintaining end-to-end (E2E)\ndifferentiability. We propose for the application of charged particle tracking\nan E2E differentiable decision-focused learning scheme using graph neural\nnetworks with combinatorial components solving a linear assignment problem for\neach detector layer. We demonstrate empirically that including differentiable\nvariations of discrete assignment operations allows for efficient network\noptimization, working better or on par with approaches that lack E2E\ndifferentiability. In additional studies, we dive deeper into the optimization\nprocess and provide further insights from a loss landscape perspective. We\ndemonstrate that while both methods converge into similar performing, globally\nwell-connected regions, they suffer under substantial predictive instability\nacross initialization and optimization methods, which can have unpredictable\nconsequences on the performance of downstream tasks such as image\nreconstruction. We also point out a dependency between the interpolation factor\nof the gradient estimator and the prediction stability of the model, suggesting\nthe choice of sufficiently small values. Given the strong global connectivity\nof learned solutions and the excellent training performance, we argue that E2E\ndifferentiability provides, besides the general availability of gradient\ninformation, an important tool for robust particle tracking to mitigate\nprediction instabilities by favoring solutions that perform well on downstream\ntasks.\n","authors":["Tobias Kortus","Ralf Keidel","Nicolas R. Gauger"],"pdf_url":"https://arxiv.org/pdf/2407.13420v2.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR), 2025"},{"id":"http://arxiv.org/abs/2510.21758v3","updated":"2025-10-29T11:02:07Z","published":"2025-10-11T20:16:32Z","title":"Taxonomy and Trends in Reinforcement Learning for Robotics and Control\n  Systems: A Structured Review","summary":"  Reinforcement learning (RL) has become a foundational approach for enabling\nintelligent robotic behavior in dynamic and uncertain environments. This work\npresents an in-depth review of RL principles, advanced deep reinforcement\nlearning (DRL) algorithms, and their integration into robotic and control\nsystems. Beginning with the formalism of Markov Decision Processes (MDPs), the\nstudy outlines essential elements of the agent-environment interaction and\nexplores core algorithmic strategies including actor-critic methods,\nvalue-based learning, and policy gradients. Emphasis is placed on modern DRL\ntechniques such as DDPG, TD3, PPO, and SAC, which have shown promise in solving\nhigh-dimensional, continuous control tasks. A structured taxonomy is introduced\nto categorize RL applications across domains such as locomotion, manipulation,\nmulti-agent coordination, and human-robot interaction, along with training\nmethodologies and deployment readiness levels. The review synthesizes recent\nresearch efforts, highlighting technical trends, design patterns, and the\ngrowing maturity of RL in real-world robotics. Overall, this work aims to\nbridge theoretical advances with practical implementations, providing a\nconsolidated perspective on the evolving role of RL in autonomous robotic\nsystems.\n","authors":["Kumater Ter","Ore-Ofe Ajayi","Daniel Udekwe"],"pdf_url":"https://arxiv.org/pdf/2510.21758v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.16791v3","updated":"2025-10-29T10:58:12Z","published":"2025-06-20T07:14:48Z","title":"TabArena: A Living Benchmark for Machine Learning on Tabular Data","summary":"  With the growing popularity of deep learning and foundation models for\ntabular data, the need for standardized and reliable benchmarks is higher than\never. However, current benchmarks are static. Their design is not updated even\nif flaws are discovered, model versions are updated, or new models are\nreleased. To address this, we introduce TabArena, the first continuously\nmaintained living tabular benchmarking system. To launch TabArena, we manually\ncurate a representative collection of datasets and well-implemented models,\nconduct a large-scale benchmarking study to initialize a public leaderboard,\nand assemble a team of experienced maintainers. Our results highlight the\ninfluence of validation method and ensembling of hyperparameter configurations\nto benchmark models at their full potential. While gradient-boosted trees are\nstill strong contenders on practical tabular datasets, we observe that deep\nlearning methods have caught up under larger time budgets with ensembling. At\nthe same time, foundation models excel on smaller datasets. Finally, we show\nthat ensembles across models advance the state-of-the-art in tabular machine\nlearning. We observe that some deep learning models are overrepresented in\ncross-model ensembles due to validation set overfitting, and we encourage model\ndevelopers to address this issue. We launch TabArena with a public leaderboard,\nreproducible code, and maintenance protocols to create a living benchmark\navailable at https://tabarena.ai.\n","authors":["Nick Erickson","Lennart Purucker","Andrej Tschalzev","David HolzmÃ¼ller","Prateek Mutalik Desai","David Salinas","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2506.16791v3.pdf","comment":"Accepted (spotlight) at NeurIPS 2025 Datasets and Benchmarks Track.\n  v3: NeurIPS camera-ready version. v2: fixed author list. 51 pages. Code\n  available at https://tabarena.ai/code; examples at\n  https://tabarena.ai/code-examples; dataset curation at\n  https://tabarena.ai/data-tabular-ml-iid-study and\n  https://tabarena.ai/dataset-curation"},{"id":"http://arxiv.org/abs/2509.18376v2","updated":"2025-10-29T10:57:22Z","published":"2025-09-22T19:58:17Z","title":"GnnXemplar: Exemplars to Explanations -- Natural Language Rules for\n  Global GNN Interpretability","summary":"  Graph Neural Networks (GNNs) are widely used for node classification, yet\ntheir opaque decision-making limits trust and adoption. While local\nexplanations offer insights into individual predictions, global explanation\nmethods, those that characterize an entire class, remain underdeveloped.\nExisting global explainers rely on motif discovery in small graphs, an approach\nthat breaks down in large, real-world settings where subgraph repetition is\nrare, node attributes are high-dimensional, and predictions arise from complex\nstructure-attribute interactions. We propose GnnXemplar, a novel global\nexplainer inspired from Exemplar Theory from cognitive science. GnnXemplar\nidentifies representative nodes in the GNN embedding space, exemplars, and\nexplains predictions using natural language rules derived from their\nneighborhoods. Exemplar selection is framed as a coverage maximization problem\nover reverse k-nearest neighbors, for which we provide an efficient greedy\napproximation. To derive interpretable rules, we employ a self-refining prompt\nstrategy using large language models (LLMs). Experiments across diverse\nbenchmarks show that GnnXemplar significantly outperforms existing methods in\nfidelity, scalability, and human interpretability, as validated by a user study\nwith 60 participants.\n","authors":["Burouj Armgaan","Eshan Jain","Harsh Pandey","Mahesh Chandran","Sayan Ranu"],"pdf_url":"https://arxiv.org/pdf/2509.18376v2.pdf","comment":"38 pages, 20 figures, NeurIPS 2025 (Oral)"},{"id":"http://arxiv.org/abs/2510.25379v1","updated":"2025-10-29T10:52:02Z","published":"2025-10-29T10:52:02Z","title":"A Deep Learning Framework for Multi-Operator Learning: Architectures and\n  Approximation Theory","summary":"  While many problems in machine learning focus on learning mappings between\nfinite-dimensional spaces, scientific applications require approximating\nmappings between function spaces, i.e., operators. We study the problem of\nlearning collections of operators and provide both theoretical and empirical\nadvances. We distinguish between two regimes: (i) multiple operator learning,\nwhere a single network represents a continuum of operators parameterized by a\nparametric function, and (ii) learning several distinct single operators, where\neach operator is learned independently. For the multiple operator case, we\nintroduce two new architectures, $\\mathrm{MNO}$ and $\\mathrm{MONet}$, and\nestablish universal approximation results in three settings: continuous,\nintegrable, or Lipschitz operators. For the latter, we further derive explicit\nscaling laws that quantify how the network size must grow to achieve a target\napproximation accuracy. For learning several single operators, we develop a\nframework for balancing architectural complexity across subnetworks and show\nhow approximation order determines computational efficiency. Empirical\nexperiments on parametric PDE benchmarks confirm the strong expressive power\nand efficiency of the proposed architectures. Overall, this work establishes a\nunified theoretical and practical foundation for scalable neural operator\nlearning across multiple operators.\n","authors":["Adrien Weihs","Jingmin Sun","Zecheng Zhang","Hayden Schaeffer"],"pdf_url":"https://arxiv.org/pdf/2510.25379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20274v2","updated":"2025-10-29T10:47:16Z","published":"2025-05-26T17:53:28Z","title":"Probabilistic Kernel Function for Fast Angle Testing","summary":"  In this paper, we study the angle testing problem in the context of\nsimilarity search in high-dimensional Euclidean spaces and propose two\nprojection-based probabilistic kernel functions, one designed for angle\ncomparison and the other for angle thresholding. Unlike existing approaches\nthat rely on random projection vectors drawn from Gaussian distributions, our\napproach leverages reference angles and employs a deterministic structure for\nthe projection vectors. Notably, our kernel functions do not require asymptotic\nassumptions, such as the number of projection vectors tending to infinity, and\ncan be both theoretically and experimentally shown to outperform\nGaussian-distribution-based kernel functions. We apply the proposed kernel\nfunction to Approximate Nearest Neighbor Search (ANNS) and demonstrate that our\napproach achieves a 2.5X ~ 3X higher query-per-second (QPS) throughput compared\nto the widely-used graph-based search algorithm HNSW.\n","authors":["Kejing Lu","Chuan Xiao","Yoshiharu Ishikawa"],"pdf_url":"https://arxiv.org/pdf/2505.20274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25372v1","updated":"2025-10-29T10:42:56Z","published":"2025-10-29T10:42:56Z","title":"Prompt Estimation from Prototypes for Federated Prompt Tuning of Vision\n  Transformers","summary":"  Visual Prompt Tuning (VPT) of pre-trained Vision Transformers (ViTs) has\nproven highly effective as a parameter-efficient fine-tuning technique for\nadapting large models to downstream tasks with limited data. Its parameter\nefficiency makes it particularly suitable for Federated Learning (FL), where\nboth communication and computation budgets are often constrained. However,\nglobal prompt tuning struggles to generalize across heterogeneous clients,\nwhile personalized tuning overfits to local data and lacks generalization. We\npropose PEP-FedPT (Prompt Estimation from Prototypes for Federated Prompt\nTuning), a unified framework designed to achieve both generalization and\npersonalization in federated prompt tuning of ViTs. Within this framework, we\nintroduce the novel Class-Contextualized Mixed Prompt (CCMP) - based on\nclass-specific prompts maintained alongside a globally shared prompt. For each\ninput, CCMP adaptively combines class-specific prompts using weights derived\nfrom global class prototypes and client class priors. This approach enables\nper-sample prompt personalization without storing client-dependent trainable\nparameters. The prompts are collaboratively optimized via traditional federated\naveraging technique on the same. Comprehensive evaluations on CIFAR-100,\nTinyImageNet, DomainNet, and iNaturalist datasets demonstrate that PEP-FedPT\nconsistently surpasses the state-of-the-art baselines under diverse data\nheterogeneity scenarios, establishing a strong foundation for efficient and\ngeneralizable federated prompt tuning of Vision Transformers.\n","authors":["M Yashwanth","Sharannya Ghosh","Aditay Tripathi","Anirban Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2510.25372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25368v1","updated":"2025-10-29T10:39:29Z","published":"2025-10-29T10:39:29Z","title":"Position: Biology is the Challenge Physics-Informed ML Needs to Evolve","summary":"  Physics-Informed Machine Learning (PIML) has successfully integrated\nmechanistic understanding into machine learning, particularly in domains\ngoverned by well-known physical laws. This success has motivated efforts to\napply PIML to biology, a field rich in dynamical systems but shaped by\ndifferent constraints. Biological modeling, however, presents unique\nchallenges: multi-faceted and uncertain prior knowledge, heterogeneous and\nnoisy data, partial observability, and complex, high-dimensional networks. In\nthis position paper, we argue that these challenges should not be seen as\nobstacles to PIML, but as catalysts for its evolution. We propose\nBiology-Informed Machine Learning (BIML): a principled extension of PIML that\nretains its structural grounding while adapting to the practical realities of\nbiology. Rather than replacing PIML, BIML retools its methods to operate under\nsofter, probabilistic forms of prior knowledge. We outline four foundational\npillars as a roadmap for this transition: uncertainty quantification,\ncontextualization, constrained latent structure inference, and scalability.\nFoundation Models and Large Language Models will be key enablers, bridging\nhuman expertise with computational modeling. We conclude with concrete\nrecommendations to build the BIML ecosystem and channel PIML-inspired\ninnovation toward challenges of high scientific and societal relevance.\n","authors":["Julien Martinelli"],"pdf_url":"https://arxiv.org/pdf/2510.25368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25366v1","updated":"2025-10-29T10:37:24Z","published":"2025-10-29T10:37:24Z","title":"A Convexity-dependent Two-Phase Training Algorithm for Deep Neural\n  Networks","summary":"  The key task of machine learning is to minimize the loss function that\nmeasures the model fit to the training data. The numerical methods to do this\nefficiently depend on the properties of the loss function. The most decisive\namong these properties is the convexity or non-convexity of the loss function.\nThe fact that the loss function can have, and frequently has, non-convex\nregions has led to a widespread commitment to non-convex methods such as Adam.\nHowever, a local minimum implies that, in some environment around it, the\nfunction is convex. In this environment, second-order minimizing methods such\nas the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We\npropose a novel framework grounded in the hypothesis that loss functions in\nreal-world tasks swap from initial non-convexity to convexity towards the\noptimum. This is a property we leverage to design an innovative two-phase\noptimization algorithm. The presented algorithm detects the swap point by\nobserving the gradient norm dependence on the loss. In these regions,\nnon-convex (Adam) and convex (CG) algorithms are used, respectively. Computing\nexperiments confirm the hypothesis that this simple convexity structure is\nfrequent enough to be practically exploited to substantially improve\nconvergence and accuracy.\n","authors":["Tomas Hrycej","Bernhard Bermeitinger","Massimo Pavone","GÃ¶tz-Henrik Wiegand","Siegfried Handschuh"],"pdf_url":"https://arxiv.org/pdf/2510.25366v1.pdf","comment":"Appeared on KDIR IC3K Conference 2025 (Best Paper Award)"},{"id":"http://arxiv.org/abs/2506.14866v2","updated":"2025-10-29T10:34:04Z","published":"2025-06-17T17:59:31Z","title":"OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents","summary":"  Computer use agents are LLM-based agents that can directly interact with a\ngraphical user interface, by processing screenshots or accessibility trees.\nWhile these systems are gaining popularity, their safety has been largely\noverlooked, despite the fact that evaluating and understanding their potential\nfor harmful behavior is essential for widespread adoption. To address this gap,\nwe introduce OS-Harm, a new benchmark for measuring safety of computer use\nagents. OS-Harm is built on top of the OSWorld environment and aims to test\nmodels across three categories of harm: deliberate user misuse, prompt\ninjection attacks, and model misbehavior. To cover these cases, we create 150\ntasks that span several types of safety violations (harassment, copyright\ninfringement, disinformation, data exfiltration, etc.) and require the agent to\ninteract with a variety of OS applications (email client, code editor, browser,\netc.). Moreover, we propose an automated judge to evaluate both accuracy and\nsafety of agents that achieves high agreement with human annotations (0.76 and\n0.79 F1 score). We evaluate computer use agents based on a range of frontier\nmodels - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide\ninsights into their safety. In particular, all models tend to directly comply\nwith many deliberate misuse queries, are relatively vulnerable to static prompt\ninjections, and occasionally perform unsafe actions. The OS-Harm benchmark is\navailable at https://github.com/tml-epfl/os-harm.\n","authors":["Thomas Kuntz","Agatha Duzan","Hao Zhao","Francesco Croce","Zico Kolter","Nicolas Flammarion","Maksym Andriushchenko"],"pdf_url":"https://arxiv.org/pdf/2506.14866v2.pdf","comment":"NeurIPS 2025 Datasets & Benchmarks Track (Spotlight)"},{"id":"http://arxiv.org/abs/2510.25361v1","updated":"2025-10-29T10:32:39Z","published":"2025-10-29T10:32:39Z","title":"Parameter Averaging in Link Prediction","summary":"  Ensemble methods are widely employed to improve generalization in machine\nlearning. This has also prompted the adoption of ensemble learning for the\nknowledge graph embedding (KGE) models in performing link prediction. Typical\napproaches to this end train multiple models as part of the ensemble, and the\ndiverse predictions are then averaged. However, this approach has some\nsignificant drawbacks. For instance, the computational overhead of training\nmultiple models increases latency and memory overhead. In contrast, model\nmerging approaches offer a promising alternative that does not require training\nmultiple models. In this work, we introduce model merging, specifically\nweighted averaging, in KGE models. Herein, a running average of model\nparameters from a training epoch onward is maintained and used for predictions.\nTo address this, we additionally propose an approach that selectively updates\nthe running average of the ensemble model parameters only when the\ngeneralization performance improves on a validation dataset. We evaluate these\ntwo different weighted averaging approaches on link prediction tasks, comparing\nthe state-of-the-art benchmark ensemble approach. Additionally, we evaluate the\nweighted averaging approach considering literal-augmented KGE models and\nmulti-hop query answering tasks as well. The results demonstrate that the\nproposed weighted averaging approach consistently improves performance across\ndiverse evaluation settings.\n","authors":["Rupesh Sapkota","Caglar Demir","Arnab Sharma","Axel-Cyrille Ngonga Ngomo"],"pdf_url":"https://arxiv.org/pdf/2510.25361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12308v4","updated":"2025-10-29T10:28:05Z","published":"2024-11-19T07:49:22Z","title":"SNN-Based Online Learning of Concepts and Action Laws in an Open World","summary":"  We present the architecture of a fully autonomous, bio-inspired cognitive\nagent built around a spiking neural network (SNN) implementing the agent's\nsemantic memory. This agent explores its universe and learns concepts of\nobjects/situations and of its own actions in a one-shot manner. While\nobject/situation concepts are unary, action concepts are triples made up of an\ninitial situation, a motor activity, and an outcome. They embody the agent's\nknowledge of its universe's action laws. Both kinds of concepts have different\ndegrees of generality. To make decisions the agent queries its semantic memory\nfor the expected outcomes of envisaged actions and chooses the action to take\non the basis of these predictions. Our experiments show that the agent handles\nnew situations by appealing to previously learned general concepts and rapidly\nmodifies its concepts to adapt to environment changes.\n","authors":["Christel Grimaud","Dominique Longin","Andreas Herzig"],"pdf_url":"https://arxiv.org/pdf/2411.12308v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.19252v2","updated":"2025-10-29T10:20:13Z","published":"2025-05-25T18:15:29Z","title":"Learning-Augmented Online Bipartite Fractional Matching","summary":"  Online bipartite matching is a fundamental problem in online optimization,\nextensively studied both in its integral and fractional forms due to its\ntheoretical significance and practical applications, such as online advertising\nand resource allocation. Motivated by recent progress in learning-augmented\nalgorithms, we study online bipartite fractional matching when the algorithm is\ngiven advice in the form of a suggested matching in each iteration. We develop\nalgorithms for both the vertex-weighted and unweighted variants that provably\ndominate the naive \"coin flip\" strategy of randomly choosing between the\nadvice-following and advice-free algorithms. Moreover, our algorithm for the\nvertex-weighted setting extends to the AdWords problem under the small bids\nassumption, yielding a significant improvement over the seminal work of\nMahdian, Nazerzadeh, and Saberi (EC 2007, TALG 2012). Complementing our\npositive results, we establish a hardness bound on the robustness-consistency\ntradeoff that is attainable by any algorithm. We empirically validate our\nalgorithms through experiments on synthetic and real-world data.\n","authors":["Davin Choo","Billy Jin","Yongho Shin"],"pdf_url":"https://arxiv.org/pdf/2505.19252v2.pdf","comment":"To appear in NeurIPS 2025. Full version"},{"id":"http://arxiv.org/abs/2510.25354v1","updated":"2025-10-29T10:19:32Z","published":"2025-10-29T10:19:32Z","title":"Analysis of Semi-Supervised Learning on Hypergraphs","summary":"  Hypergraphs provide a natural framework for modeling higher-order\ninteractions, yet their theoretical underpinnings in semi-supervised learning\nremain limited. We provide an asymptotic consistency analysis of variational\nlearning on random geometric hypergraphs, precisely characterizing the\nconditions ensuring the well-posedness of hypergraph learning as well as\nshowing convergence to a weighted $p$-Laplacian equation. Motivated by this, we\npropose Higher-Order Hypergraph Learning (HOHL), which regularizes via powers\nof Laplacians from skeleton graphs for multiscale smoothness. HOHL converges to\na higher-order Sobolev seminorm. Empirically, it performs strongly on standard\nbaselines.\n","authors":["Adrien Weihs","Andrea Bertozzi","Matthew Thorpe"],"pdf_url":"https://arxiv.org/pdf/2510.25354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06204v2","updated":"2025-10-29T10:17:57Z","published":"2025-07-08T17:30:14Z","title":"Differential Mamba","summary":"  Sequence models like Transformers and RNNs often overallocate attention to\nirrelevant context, leading to noisy intermediate representations. This\ndegrades LLM capabilities by promoting hallucinations, weakening long-range and\nretrieval abilities, and reducing robustness. Recent work has shown that\ndifferential design can mitigate this issue in Transformers, improving their\neffectiveness across various applications. In this paper, we explore whether\nthese techniques, originally developed for Transformers, can be applied to\nMamba, a recent architecture based on selective state-space layers that\nachieves Transformer-level performance with greater efficiency. We show that a\nnaive adaptation of differential design to Mamba is insufficient and requires\ncareful architectural modifications. To address this, we introduce a novel\ndifferential mechanism for Mamba, empirically validated on language modeling\nbenchmarks, demonstrating improved retrieval capabilities and superior\nperformance over vanilla Mamba. Finally, we conduct extensive ablation studies\nand empirical analyses to justify our design choices and provide evidence that\nour approach effectively mitigates the overallocation problem in Mamba-based\nmodels. Our code is publicly available: https://github.com/NadavSc/Diff-Mamba\n","authors":["Nadav Schneider","Itamar Zimerman","Eliya Nachmani"],"pdf_url":"https://arxiv.org/pdf/2507.06204v2.pdf","comment":"AACL 2025. We provide the code at\n  https://github.com/NadavSc/Diff-Mamba"},{"id":"http://arxiv.org/abs/2502.04864v2","updated":"2025-10-29T10:11:05Z","published":"2025-02-07T12:07:57Z","title":"Redistributing Rewards Across Time and Agents for Multi-Agent\n  Reinforcement Learning","summary":"  Credit assignmen, disentangling each agent's contribution to a shared reward,\nis a critical challenge in cooperative multi-agent reinforcement learning\n(MARL). To be effective, credit assignment methods must preserve the\nenvironment's optimal policy. Some recent approaches attempt this by enforcing\nreturn equivalence, where the sum of distributed rewards must equal the team\nreward. However, their guarantees are conditional on a learned model's\nregression accuracy, making them unreliable in practice. We introduce\nTemporal-Agent Reward Redistribution (TAR$^2$), an approach that decouples\ncredit modeling from this constraint. A neural network learns unnormalized\ncontribution scores, while a separate, deterministic normalization step\nenforces return equivalence by construction. We demonstrate that this method is\nequivalent to a valid Potential-Based Reward Shaping (PBRS), which guarantees\nthe optimal policy is preserved regardless of model accuracy. Empirically, on\nchallenging SMACLite and Google Research Football (GRF) benchmarks, TAR$^2$\naccelerates learning and achieves higher final performance than strong\nbaselines. These results establish our method as an effective solution for the\nagent-temporal credit assignment problem.\n","authors":["Aditya Kapoor","Kale-ab Tessera","Mayank Baranwal","Harshad Khadilkar","Jan Peters","Stefano Albrecht","Mingfei Sun"],"pdf_url":"https://arxiv.org/pdf/2502.04864v2.pdf","comment":"16 pages, 4 figures, 4 tables"}],"Multimedia":[{"id":"http://arxiv.org/abs/2411.05715v2","updated":"2025-10-29T17:44:30Z","published":"2024-11-08T17:16:27Z","title":"Artificial Neural Networks Trained on Noisy Speech Exhibit the McGurk\n  Effect","summary":"  Humans are able to fuse information from both auditory and visual modalities\nto help with understanding speech. This is demonstrated through a phenomenon\nknown as the McGurk Effect, during which a listener is presented with\nincongruent auditory and visual speech that fuse together into the percept of\nillusory intermediate phonemes. Building on a recent framework that proposes\nhow to address developmental 'why' questions using artificial neural networks,\nwe evaluated a set of recent artificial neural networks trained on audiovisual\nspeech by testing them with audiovisually incongruent words designed to elicit\nthe McGurk effect. We show that networks trained entirely on congruent\naudiovisual speech nevertheless exhibit the McGurk percept. We further\ninvestigated 'why' by comparing networks trained on clean speech to those\ntrained on noisy speech, and discovered that training with noisy speech led to\na pronounced increase in both visual responses and McGurk responses across all\nmodels. Furthermore, we observed that systematically increasing the level of\nauditory noise during ANN training also increased the amount of audiovisual\nintegration up to a point, but at extreme noise levels, this integration failed\nto develop. These results suggest that excessive noise exposure during critical\nperiods of audiovisual learning may negatively influence the development of\naudiovisual speech integration. This work also demonstrates that the McGurk\neffect reliably emerges untrained from the behaviour of both supervised and\nunsupervised networks, even networks trained only on congruent speech. This\nsupports the notion that artificial neural networks might be useful models for\ncertain aspects of perception and cognition.\n","authors":["Lukas Grasse","Matthew S. Tata"],"pdf_url":"https://arxiv.org/pdf/2411.05715v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25668v1","updated":"2025-10-29T16:32:26Z","published":"2025-10-29T16:32:26Z","title":"ALDEN: Reinforcement Learning for Active Navigation and Evidence\n  Gathering in Long Documents","summary":"  Vision-language models (VLMs) excel at interpreting text-rich images but\nstruggle with long, visually complex documents that demand analysis and\nintegration of information spread across multiple pages. Existing approaches\ntypically rely on fixed reasoning templates or rigid pipelines, which force\nVLMs into a passive role and hinder both efficiency and generalization. We\npresent Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement\nlearning framework that fine-tunes VLMs as interactive agents capable of\nactively navigating long, visually rich documents. ALDEN introduces a novel\nfetch action that directly accesses the page by index, complementing the\nclassic search action and better exploiting document structure. For dense\nprocess supervision and efficient training, we propose a rule-based cross-level\nreward that provides both turn- and token-level signals. To address the\nempirically observed training instability caused by numerous visual tokens from\nlong documents, we further propose a visual-semantic anchoring mechanism that\napplies a dual-path KL-divergence constraint to stabilize visual and textual\nrepresentations separately during training. Trained on a corpus constructed\nfrom three open-source datasets, ALDEN achieves state-of-the-art performance on\nfive long-document benchmarks. Overall, ALDEN marks a step beyond passive\ndocument reading toward agents that autonomously navigate and reason across\nlong, visually rich documents, offering a robust path to more accurate and\nefficient long-document understanding.\n","authors":["Tianyu Yang","Terry Ruas","Yijun Tian","Jan Philip Wahle","Daniel Kurzawe","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2510.25668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25600v1","updated":"2025-10-29T15:10:17Z","published":"2025-10-29T15:10:17Z","title":"PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse\n  Attention for Vision-Language Large Models","summary":"  Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation.\n","authors":["Zhonghua Jiang","Kunxi Li","Yiyun Zhou","Sihao Liu","Zhaode Wang","Chengfei lv","Shengyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.25600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25357v1","updated":"2025-10-29T10:21:41Z","published":"2025-10-29T10:21:41Z","title":"Energy consumption assessment of a Virtual Reality Remote Rendering\n  application over 5G networks","summary":"  This paper investigates the energy implications of remote rendering for\nVirtual Reality (VR) applications within a real 5G testbed. Remote rendering\nenables lightweight devices to access high-performance graphical content by\noffloading computationally intensive tasks to Cloud-native Network Functions\n(CNFs) running on remote servers. However, this approach raises concerns\nregarding energy consumption across the various network components involved,\nincluding the remote computing node, the 5G Core, the Radio Access Network\n(RAN), and the User Equipment (UE). This work proposes and evaluates two\ncomplementary energy monitoring solutions, one hardware-based and one\nsoftware-based, to measure energy consumption at different system levels. A VR\nremote renderer, deployed as CNF and leveraging the Media over QUIC (MoQ)\nprotocol, is used as test case for assessing its energy footprint under\ndifferent multimedia and network configurations. The results provide critical\ninsights into the trade-off between energy consumption and performance of a\nreal-world VR application running in a 5G environment.\n","authors":["Roberto Viola","Mikel Irazola","JosÃ© RamÃ³n JuÃ¡rez","Minh Nguyen","Alexander Zoubarev","Alexander Futasz","Louay Bassbouss","Amr A. AbdelNabi","Javier FernÃ¡ndez Hidalgo"],"pdf_url":"https://arxiv.org/pdf/2510.25357v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10258v2","updated":"2025-10-29T07:51:00Z","published":"2025-04-14T14:19:57Z","title":"XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a\n  Novel Benchmark","summary":"  Document Reading Order Recovery is a fundamental task in document image\nunderstanding, playing a pivotal role in enhancing Retrieval-Augmented\nGeneration (RAG) and serving as a critical preprocessing step for large\nlanguage models (LLMs). Existing methods often struggle with complex\nlayouts(e.g., multi-column newspapers), high-overhead interactions between\ncross-modal elements (visual regions and textual semantics), and a lack of\nrobust evaluation benchmarks. We introduce XY-Cut++, an advanced layout\nordering method that integrates pre-mask processing, multi-granularity\nsegmentation, and cross-modal matching to address these challenges. Our method\nsignificantly enhances layout ordering accuracy compared to traditional XY-Cut\ntechniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8\nBLEU overall) while maintaining simplicity and efficiency. It outperforms\nexisting baselines by up to 24\\% and demonstrates consistent accuracy across\nsimple and complex layouts on the newly introduced DocBench-100 dataset. This\nadvancement establishes a reliable foundation for document structure recovery,\nsetting a new standard for layout ordering tasks and facilitating more\neffective RAG and LLM preprocessing.\n","authors":["Shuai Liu","Youmeng Li","Jizeng Wei"],"pdf_url":"https://arxiv.org/pdf/2504.10258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25225v1","updated":"2025-10-29T07:00:48Z","published":"2025-10-29T07:00:48Z","title":"Hallucination Localization in Video Captioning","summary":"  We propose a novel task, hallucination localization in video captioning,\nwhich aims to identify hallucinations in video captions at the span level (i.e.\nindividual words or phrases). This allows for a more detailed analysis of\nhallucinations compared to existing sentence-level hallucination detection\ntask. To establish a benchmark for hallucination localization, we construct\nHLVC-Dataset, a carefully curated dataset created by manually annotating 1,167\nvideo-caption pairs from VideoLLM-generated captions. We further implement a\nVideoLLM-based baseline method and conduct quantitative and qualitative\nevaluations to benchmark current performance on hallucination localization.\n","authors":["Shota Nakada","Kazuhiro Saito","Yuchi Ishikawa","Hokuto Munakata","Tatsuya Komatsu","Masayoshi Kondo"],"pdf_url":"https://arxiv.org/pdf/2510.25225v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2510.25079v1","updated":"2025-10-29T01:26:29Z","published":"2025-10-29T01:26:29Z","title":"Performance Evaluation of Multimedia Traffic in Cloud Storage Services\n  over Wi-Fi and LTE Networks","summary":"  The performance of Dropbox, Google Drive, and OneDrive cloud storage services\nwas evaluated under Wi-Fi and LTE network conditions during multimedia file\nuploads. Traffic was captured using Wireshark, and key metrics (including\ndelay, jitter, bandwidth, and packet loss) were analyzed. Google Drive\nmaintained the most consistent performance across both types of networks,\nshowing low latency and reduced jitter. Dropbox showed efficient bandwidth\nutilization, but experienced a longer delay over LTE, attributed to a greater\nnumber of intermediate hops. OneDrive presented variable behavior, with\nelevated packet rates and increased sensitivity to fluctuations in the mobile\nnetwork. A bimodal distribution of packet sizes was observed and modeled using\na dual Poisson function. In general, Wi-Fi connections provided greater\nstability for multimedia transfers, while LTE performance varied depending on\nplatform-specific implementations. The results contribute to a better\nunderstanding of traffic behavior in cloud-based storage applications and\nsuggest further analysis with larger datasets and heterogeneous access\nnetworks.\n","authors":["Albert Espinal","V. Sanchez Padilla","Yesenia Cevallos"],"pdf_url":"https://arxiv.org/pdf/2510.25079v1.pdf","comment":"2025 20th Iberian Conference on Information Systems and Technologies\n  (CISTI), Lecture Notes in Networks and Systems"}]},"2025-10-30T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2510.26802v1","updated":"2025-10-30T17:59:55Z","published":"2025-10-30T17:59:55Z","title":"Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\n  the MME-CoF Benchmark","summary":"  Recent video generation models can produce high-fidelity, temporally coherent\nvideos, indicating that they may encode substantial world knowledge. Beyond\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\nperception, modeling, and manipulation. Yet, an important question still\nremains: Are video models ready to serve as zero-shot reasoners in challenging\nvisual reasoning scenarios? In this work, we conduct an empirical study to\ncomprehensively investigate this question, focusing on the leading and popular\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\nspatial, geometric, physical, temporal, and embodied logic, systematically\ncharacterizing both its strengths and failure modes. To standardize this study,\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\nfindings reveal that while current video models demonstrate promising reasoning\npatterns on short-horizon spatial coherence, fine-grained grounding, and\nlocally consistent dynamics, they remain limited in long-horizon causal\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\nsigns as complementary visual engines alongside dedicated reasoning models.\nProject page: https://video-cof.github.io\n","authors":["Ziyu Guo","Xinyan Chen","Renrui Zhang","Ruichuan An","Yu Qi","Dongzhi Jiang","Xiangtai Li","Manyuan Zhang","Hongsheng Li","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2510.26802v1.pdf","comment":"Project Page: https://video-cof.github.io"},{"id":"http://arxiv.org/abs/2510.26790v1","updated":"2025-10-30T17:58:26Z","published":"2025-10-30T17:58:26Z","title":"Gistify! Codebase-Level Understanding via Runtime Execution","summary":"  As coding agents are increasingly deployed in large codebases, the need to\nautomatically design challenging, codebase-level evaluation is central. We\npropose Gistify, a task where a coding LLM must create a single, minimal,\nself-contained file that can reproduce a specific functionality of a codebase.\nThe coding LLM is given full access to a codebase along with a specific\nentrypoint (e.g., a python command), and the generated file must replicate the\noutput of the same command ran under the full codebase, while containing only\nthe essential components necessary to execute the provided command. Success on\nGistify requires both structural understanding of the codebase, accurate\nmodeling of its execution flow as well as the ability to produce potentially\nlarge code patches. Our findings show that current state-of-the-art models\nstruggle to reliably solve Gistify tasks, especially ones with long executions\ntraces.\n","authors":["Hyunji Lee","Minseon Kim","Chinmay Singh","Matheus Pereira","Atharv Sonwane","Isadora White","Elias Stengel-Eskin","Mohit Bansal","Zhengyan Shi","Alessandro Sordoni","Marc-Alexandre CÃ´tÃ©","Xingdi Yuan","Lucas Caccia"],"pdf_url":"https://arxiv.org/pdf/2510.26790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26788v1","updated":"2025-10-30T17:58:11Z","published":"2025-10-30T17:58:11Z","title":"Defeating the Training-Inference Mismatch via FP16","summary":"  Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to \\textbf{FP16} effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning.\n","authors":["Penghui Qi","Zichen Liu","Xiangxin Zhou","Tianyu Pang","Chao Du","Wee Sun Lee","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2510.26788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26787v1","updated":"2025-10-30T17:58:04Z","published":"2025-10-30T17:58:04Z","title":"Remote Labor Index: Measuring AI Automation of Remote Work","summary":"  AIs have made rapid progress on research-oriented benchmarks of knowledge and\nreasoning, but it remains unclear how these gains translate into economic value\nand automation. To measure this, we introduce the Remote Labor Index (RLI), a\nbroadly multi-sector benchmark comprising real-world, economically valuable\nprojects designed to evaluate end-to-end agent performance in practical\nsettings. AI agents perform near the floor on RLI, with the highest-performing\nagent achieving an automation rate of 2.5%. These results help ground\ndiscussions of AI automation in empirical evidence, setting a common basis for\ntracking AI impacts and enabling stakeholders to proactively navigate AI-driven\nlabor automation.\n","authors":["Mantas Mazeika","Alice Gatti","Cristina Menghini","Udari Madhushani Sehwag","Shivam Singhal","Yury Orlovskiy","Steven Basart","Manasi Sharma","Denis Peskoff","Elaine Lau","Jaehyuk Lim","Lachlan Carroll","Alice Blair","Vinaya Sivakumar","Sumana Basu","Brad Kenstler","Yuntao Ma","Julian Michael","Xiaoke Li","Oliver Ingebretsen","Aditya Mehta","Jean Mottola","John Teichmann","Kevin Yu","Zaina Shaik","Adam Khoja","Richard Ren","Jason Hausenloy","Long Phan","Ye Htet","Ankit Aich","Tahseen Rabbani","Vivswan Shah","Andriy Novykov","Felix Binder","Kirill Chugunov","Luis Ramirez","Matias Geralnik","HernÃ¡n Mesura","Dean Lee","Ed-Yeremai Hernandez Cardona","Annette Diamond","Summer Yue","Alexandr Wang","Bing Liu","Ernesto Hernandez","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2510.26787v1.pdf","comment":"Website: https://www.remotelabor.ai"},{"id":"http://arxiv.org/abs/2508.11607v2","updated":"2025-10-30T17:57:04Z","published":"2025-08-15T17:14:29Z","title":"TinyTim: A Family of Language Models for Divergent Generation","summary":"  In the search for artificial general intelligence, model development and\ntraining has focused primarily on vast datasets of known problems and their\naccepted solutions. This process necessarily produces convergent systems which\nare fundamentally incapable of the conceptual reframing that is required for\ngenuine creative breakthroughs. Inspired by the divergent cognitive processes\nthat allow humans to make such creative leaps, our work introduces a family of\nlanguage models, TinyTim, to serve as sources of divergent generation within\nbroader systems. These models have been created by fine-tuning on the\nanti-parsimonious text of James Joyce's `Finnegans Wake'. Quantitative analysis\nof both an unsupervised fine-tuned model (TinyTim-V1) and a new\ninstruction-tuned variant (TinyTim-V2) demonstrates a profound capacity for\nlexical invention; the foundational V1 model exhibits a Yule's K score for\nlexical richness over twenty times greater than that of convergent baselines.\nThis trait is a stable property of the family, as the instruction-tuned V2\nmaintains a statistically distinct profile and resists factual convergence,\nsacrificing benchmark performance to preserve its core generative style. This\nwork establishes a methodology for engineering specialized divergent models\nthat, when paired with convergent systems, can reframe problems and force\nbreakthroughs beyond the reach of statistical optimization alone.\n","authors":["Christopher J. Agostino"],"pdf_url":"https://arxiv.org/pdf/2508.11607v2.pdf","comment":"7 pages, 3 figures, accepted to NeurIPS Creative AI track, models\n  available at https://hf.co/npc-worldwide/"},{"id":"http://arxiv.org/abs/2510.25744v2","updated":"2025-10-30T17:54:45Z","published":"2025-10-29T17:47:18Z","title":"Completion $\\neq$ Collaboration: Scaling Collaborative Effort with\n  Agents","summary":"  Current evaluations of agents remain centered around one-shot task\ncompletion, failing to account for the inherently iterative and collaborative\nnature of many real-world problems, where human goals are often underspecified\nand evolve. We argue for a shift from building and assessing task completion\nagents to developing collaborative agents, assessed not only by the quality of\ntheir final outputs but by how well they engage with and enhance human effort\nthroughout the problem-solving process. To support this shift, we introduce\ncollaborative effort scaling, a framework that captures how an agent's utility\ngrows with increasing user involvement. Through case studies and simulated\nevaluations, we show that state-of-the-art agents often underperform in\nmulti-turn, real-world scenarios, revealing a missing ingredient in agent\ndesign: the ability to sustain engagement and scaffold user understanding.\nCollaborative effort scaling offers a lens for diagnosing agent behavior and\nguiding development toward more effective interactions.\n","authors":["Shannon Zejiang Shen","Valerie Chen","Ken Gu","Alexis Ross","Zixian Ma","Jillian Ross","Alex Gu","Chenglei Si","Wayne Chi","Andi Peng","Jocelyn J Shen","Ameet Talwalkar","Tongshuang Wu","David Sontag"],"pdf_url":"https://arxiv.org/pdf/2510.25744v2.pdf","comment":"22 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2510.26768v1","updated":"2025-10-30T17:52:02Z","published":"2025-10-30T17:52:02Z","title":"AMO-Bench: Large Language Models Still Struggle in High School Math\n  Competitions","summary":"  We present AMO-Bench, an Advanced Mathematical reasoning benchmark with\nOlympiad level or even higher difficulty, comprising 50 human-crafted problems.\nExisting benchmarks have widely leveraged high school math competitions for\nevaluating mathematical reasoning capabilities of large language models (LLMs).\nHowever, many existing math competitions are becoming less effective for\nassessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To\naddress this, AMO-Bench introduces more rigorous challenges by ensuring all 50\nproblems are (1) cross-validated by experts to meet at least the International\nMathematical Olympiad (IMO) difficulty standards, and (2) entirely original\nproblems to prevent potential performance leakages from data memorization.\nMoreover, each problem in AMO-Bench requires only a final answer rather than a\nproof, enabling automatic and robust grading for evaluation. Experimental\nresults across 26 LLMs on AMO-Bench show that even the best-performing model\nachieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.\nBeyond these poor performances, our further analysis reveals a promising\nscaling trend with increasing test-time compute on AMO-Bench. These results\nhighlight the significant room for improving the mathematical reasoning in\ncurrent LLMs. We release AMO-Bench to facilitate further research into\nadvancing the reasoning abilities of language models.\nhttps://amo-bench.github.io/\n","authors":["Shengnan An","Xunliang Cai","Xuezhi Cao","Xiaoyu Li","Yehao Lin","Junlin Liu","Xinxuan Lv","Dan Ma","Xuanlin Wang","Ziwen Wang","Shuang Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.26768v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2506.09391v2","updated":"2025-10-30T17:41:15Z","published":"2025-06-11T04:44:46Z","title":"Comparing human and LLM politeness strategies in free production","summary":"  Polite speech poses a fundamental alignment challenge for large language\nmodels (LLMs). Humans deploy a rich repertoire of linguistic strategies to\nbalance informational and social goals -- from positive approaches that build\nrapport (compliments, expressions of interest) to negative strategies that\nminimize imposition (hedging, indirectness). We investigate whether LLMs employ\na similarly context-sensitive repertoire by comparing human and LLM responses\nin both constrained and open-ended production tasks. We find that larger models\n($\\ge$70B parameters) successfully replicate key preferences from the\ncomputational pragmatics literature, and human evaluators surprisingly prefer\nLLM-generated responses in open-ended contexts. However, further linguistic\nanalyses reveal that models disproportionately rely on negative politeness\nstrategies even in positive contexts, potentially leading to\nmisinterpretations. While modern LLMs demonstrate an impressive handle on\npoliteness strategies, these subtle differences raise important questions about\npragmatic alignment in AI systems.\n","authors":["Haoran Zhao","Robert D. Hawkins"],"pdf_url":"https://arxiv.org/pdf/2506.09391v2.pdf","comment":"25 pages, 5 figures | EMNLP 2025 camera-ready version"},{"id":"http://arxiv.org/abs/2510.26745v1","updated":"2025-10-30T17:40:22Z","published":"2025-10-30T17:40:22Z","title":"Deep sequence models tend to memorize geometrically; it is unclear why","summary":"  In sequence modeling, the parametric memory of atomic facts has been\npredominantly abstracted as a brute-force lookup of co-occurrences between\nentities. We contrast this associative view against a geometric view of how\nmemory is stored. We begin by isolating a clean and analyzable instance of\nTransformer reasoning that is incompatible with memory as strictly a storage of\nthe local co-occurrences specified during training. Instead, the model must\nhave somehow synthesized its own geometry of atomic facts, encoding global\nrelationships between all entities, including non-co-occurring ones. This in\nturn has simplified a hard reasoning task involving an $\\ell$-fold composition\ninto an easy-to-learn 1-step geometric task.\n  From this phenomenon, we extract fundamental aspects of neural embedding\ngeometries that are hard to explain. We argue that the rise of such a geometry,\ndespite optimizing over mere local associations, cannot be straightforwardly\nattributed to typical architectural or optimizational pressures.\nCounterintuitively, an elegant geometry is learned even when it is not more\nsuccinct than a brute-force lookup of associations.\n  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry\nstems from a spectral bias that -- in contrast to prevailing theories -- indeed\narises naturally despite the lack of various pressures. This analysis also\npoints to practitioners a visible headroom to make Transformer memory more\nstrongly geometric. We hope the geometric view of parametric memory encourages\nrevisiting the default intuitions that guide researchers in areas like\nknowledge acquisition, capacity, discovery and unlearning.\n","authors":["Shahriar Noroozizadeh","Vaishnavh Nagarajan","Elan Rosenfeld","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2510.26745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09205v3","updated":"2025-10-30T17:37:55Z","published":"2025-03-12T09:48:38Z","title":"Quality Over Quantity? LLM-Based Curation for a Data-Efficient\n  Audio-Video Foundation Model","summary":"  Integrating audio and visual data for training multimodal foundational models\nremains a challenge. The Audio-Video Vector Alignment (AVVA) framework\naddresses this by considering AV scene alignment beyond mere temporal\nsynchronization, and leveraging Large Language Models (LLMs) for data curation.\nAVVA implements a scoring mechanism for selecting aligned training data\nsegments. It integrates Whisper, a speech-based foundation model, for audio and\nDINOv2 for video analysis in a dual-encoder structure with contrastive learning\non AV pairs. Evaluations on AudioCaps, VALOR, and VGGSound demonstrate the\neffectiveness of the proposed model architecture and data curation approach.\nAVVA achieves a significant improvement in top-k accuracies for video-to-audio\nretrieval on all datasets compared to DenseAV, while using only 192 hrs of\ncurated training data. Furthermore, an ablation study indicates that the data\ncuration process effectively trades data quality for data quantity, yielding\nincreases in top-k retrieval accuracies on AudioCaps, VALOR, and VGGSound,\ncompared to training on the full spectrum of uncurated data.\n","authors":["Ali Vosoughi","Dimitra Emmanouilidou","Hannes Gamper"],"pdf_url":"https://arxiv.org/pdf/2503.09205v3.pdf","comment":"5 pages, 5 figures, 2 tables. Accepted at EUSIPCO 2025"},{"id":"http://arxiv.org/abs/2506.14681v2","updated":"2025-10-30T17:32:44Z","published":"2025-06-17T16:13:15Z","title":"Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and\n  Training Factors Shape LLM Alignment Quality","summary":"  Supervised fine-tuning (SFT) is a critical step in aligning large language\nmodels (LLMs) with human instructions and values, yet many aspects of SFT\nremain poorly understood. We trained a wide range of base models on a variety\nof datasets including code generation, mathematical reasoning, and\ngeneral-domain tasks, resulting in 1,000+ SFT models under controlled\nconditions. We then identified the dataset properties that matter most and\nexamined the layer-wise modifications introduced by SFT. Our findings reveal\nthat some training-task synergies persist across all models while others vary\nsubstantially, emphasizing the importance of model-specific strategies.\nMoreover, we demonstrate that perplexity consistently predicts SFT\neffectiveness, often surpassing superficial similarity between the training\ndata and the benchmark, and that mid-layer weight changes correlate most\nstrongly with performance gains. We release these 1,000+ SFT models and\nbenchmark results to accelerate further research. All resources are available\nat https://github.com/llm-jp/massive-sft.\n","authors":["Yuto Harada","Yusuke Yamauchi","Yusuke Oda","Yohei Oseki","Yusuke Miyao","Yu Takagi"],"pdf_url":"https://arxiv.org/pdf/2506.14681v2.pdf","comment":"Accepted to EMNLP 2025 (Main Conference). Models and evaluation\n  results available at: https://github.com/llm-jp/massive-sft"},{"id":"http://arxiv.org/abs/2510.26732v1","updated":"2025-10-30T17:31:03Z","published":"2025-10-30T17:31:03Z","title":"Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models","summary":"  This paper presents a comprehensive cross-platform evaluation of reasoning\ncapabilities in contemporary foundation models, establishing an\ninfrastructure-agnostic benchmark across three computational paradigms: HPC\nsupercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and\nuniversity clusters (a node with eight H200 GPUs).\n  We evaluate 15 foundation models across 79 problems spanning eight academic\ndomains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,\nCalculus, and Optimization) through three experimental phases: (1) Baseline\nestablishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,\nMistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing\nmethodology and reference performance; (2) Infrastructure validation: The\n19-problem benchmark repeated on university cluster (seven models including\nFalcon-Mamba state-space architecture) and Nebius AI Studio (nine\nstate-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3\n30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic\nreproducibility; (3) Extended evaluation: Full 79-problem assessment on both\nuniversity cluster and Nebius platforms, probing generalization at scale across\narchitectural diversity.\n  The findings challenge conventional scaling assumptions, establish training\ndata quality as more critical than model size, and provide actionable\nguidelines for model selection across educational, production, and research\ncontexts. The tri-infrastructure methodology and 79-problem benchmark enable\nlongitudinal tracking of reasoning capabilities as foundation models evolve.\n","authors":["J. de CurtÃ²","I. de ZarzÃ ","Pablo GarcÃ­a","Jordi Cabot"],"pdf_url":"https://arxiv.org/pdf/2510.26732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.20059v2","updated":"2025-10-30T17:28:47Z","published":"2025-10-22T22:22:59Z","title":"Enhancing Reasoning Skills in Small Persian Medical Language Models Can\n  Outperform Large-Scale Data Training","summary":"  Enhancing reasoning capabilities in small language models is critical for\nspecialized applications such as medical question answering, particularly in\nunderrepresented languages like Persian. In this study, we employ Reinforcement\nLearning with AI Feedback (RLAIF) and Direct preference optimization (DPO) to\nimprove the reasoning skills of a general-purpose Persian language model. To\nachieve this, we translated a multiple-choice medical question-answering\ndataset into Persian and used RLAIF to generate rejected-preferred answer\npairs, which are essential for DPO training. By prompting both teacher and\nstudent models to produce Chain-of-Thought (CoT) reasoning responses, we\ncompiled a dataset containing correct and incorrect reasoning trajectories.\nThis dataset, comprising 2 million tokens in preferred answers and 2.5 million\ntokens in rejected ones, was used to train a baseline model, significantly\nenhancing its medical reasoning capabilities in Persian. Remarkably, the\nresulting model outperformed its predecessor, gaokerena-V, which was trained on\napproximately 57 million tokens, despite leveraging a much smaller dataset.\nThese results highlight the efficiency and effectiveness of reasoning-focused\ntraining approaches in developing domain-specific language models with limited\ndata availability.\n","authors":["Mehrdad Ghassabi","Sadra Hakim","Hamidreza Baradaran Kashani","Pedram Rostami"],"pdf_url":"https://arxiv.org/pdf/2510.20059v2.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2507.03704v2","updated":"2025-10-30T17:13:35Z","published":"2025-07-04T16:41:06Z","title":"Controlling Thinking Speed in Reasoning Models","summary":"  Human cognition is theorized to operate in two modes: fast, intuitive System\n1 thinking and slow, deliberate System 2 thinking. While current Large\nReasoning Models (LRMs) excel at System 2 thinking, their inability to perform\nfast thinking leads to high computational overhead and latency. In this work,\nwe enable LRMs to approximate human intelligence through dynamic thinking speed\nadjustment, optimizing accuracy-efficiency trade-offs. Our approach addresses\ntwo key questions: (1) how to control thinking speed in LRMs, and (2) when to\nadjust it for optimal performance. For the first question, we identify the\nsteering vector that governs slow-fast thinking transitions in LRMs'\nrepresentation space. Using this vector, we achieve the first representation\nediting-based test-time scaling effect, outperforming existing prompt-based\nscaling methods. For the second question, we apply real-time difficulty\nestimation to signal reasoning segments of varying complexity. Combining these\ntechniques, we propose the first reasoning strategy that enables fast\nprocessing of easy steps and deeper analysis for complex reasoning. Without any\ntraining or additional cost, our plug-in module delivers an average +1.3%\naccuracy with -8.6% token usage across leading LRMs and advanced reasoning\nbenchmarks. All of our algorithms are implemented based on vLLM and are\nexpected to support broader applications and inspire future research.\n","authors":["Zhengkai Lin","Zhihang Fu","Ze Chen","Chao Chen","Liang Xie","Wenxiao Wang","Deng Cai","Zheng Wang","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2507.03704v2.pdf","comment":"NeurIPS 2025 Spotlight"},{"id":"http://arxiv.org/abs/2509.21319v2","updated":"2025-10-30T17:09:54Z","published":"2025-09-25T16:19:06Z","title":"RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable Rewards","summary":"  Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost). Models:\nhttps://huggingface.co/collections/nvidia/reward-models-10-2025\n","authors":["Zhilin Wang","Jiaqi Zeng","Olivier Delalleau","Ellie Evans","Daniel Egert","Hoo-Chang Shin","Felipe Soares","Yi Dong","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2509.21319v2.pdf","comment":"Added link to access models:\n  https://huggingface.co/collections/nvidia/reward-models-10-2025"},{"id":"http://arxiv.org/abs/2510.26707v1","updated":"2025-10-30T17:09:09Z","published":"2025-10-30T17:09:09Z","title":"Value Drifts: Tracing Value Alignment During LLM Post-Training","summary":"  As LLMs occupy an increasingly important role in society, they are more and\nmore confronted with questions that require them not only to draw on their\ngeneral knowledge but also to align with certain human value systems.\nTherefore, studying the alignment of LLMs with human values has become a\ncrucial field of inquiry. Prior work, however, mostly focuses on evaluating the\nalignment of fully trained models, overlooking the training dynamics by which\nmodels learn to express human values. In this work, we investigate how and at\nwhich stage value alignment arises during the course of a model's\npost-training. Our analysis disentangles the effects of post-training\nalgorithms and datasets, measuring both the magnitude and time of value drifts\nduring training. Experimenting with Llama-3 and Qwen-3 models of different\nsizes and popular supervised fine-tuning (SFT) and preference optimization\ndatasets and algorithms, we find that the SFT phase generally establishes a\nmodel's values, and subsequent preference optimization rarely re-aligns these\nvalues. Furthermore, using a synthetic preference dataset that enables\ncontrolled manipulation of values, we find that different preference\noptimization algorithms lead to different value alignment outcomes, even when\npreference data is held constant. Our findings provide actionable insights into\nhow values are learned during post-training and help to inform data curation,\nas well as the selection of models and algorithms for preference optimization\nto improve model alignment to human values.\n","authors":["Mehar Bhatia","Shravan Nayak","Gaurav Kamath","Marius Mosbach","Karolina StaÅczak","Vered Shwartz","Siva Reddy"],"pdf_url":"https://arxiv.org/pdf/2510.26707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26697v1","updated":"2025-10-30T17:01:43Z","published":"2025-10-30T17:01:43Z","title":"The End of Manual Decoding: Towards Truly End-to-End Language Models","summary":"  The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a\nnon-differentiable decoding process that requires laborious, hand-tuning of\nhyperparameters like temperature and top-p. This paper introduces AutoDeco, a\nnovel architecture that enables truly \"end-to-end\" generation by learning to\ncontrol its own decoding strategy. We augment the standard transformer with\nlightweight heads that, at each step, dynamically predict context-specific\ntemperature and top-p values alongside the next-token logits. This approach\ntransforms decoding into a parametric, token-level process, allowing the model\nto self-regulate its sampling strategy within a single forward pass.\n  Through extensive experiments on eight benchmarks, we demonstrate that\nAutoDeco not only significantly outperforms default decoding strategies but\nalso achieves performance comparable to an oracle-tuned baseline derived from\n\"hacking the test set\"-a practical upper bound for any static method.\nCrucially, we uncover an emergent capability for instruction-based decoding\ncontrol: the model learns to interpret natural language commands (e.g.,\n\"generate with low randomness\") and adjusts its predicted temperature and top-p\non a token-by-token basis, opening a new paradigm for steerable and interactive\nLLM decoding.\n","authors":["Zhichao Wang","Dongyang Ma","Xinting Huang","Deng Cai","Tian Lan","Jiahao Xu","Haitao Mi","Xiaoying Tang","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26692v1","updated":"2025-10-30T16:59:43Z","published":"2025-10-30T16:59:43Z","title":"Kimi Linear: An Expressive, Efficient Attention Architecture","summary":"  We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints.\n","authors":[" Kimi Team","Yu Zhang","Zongyu Lin","Xingcheng Yao","Jiaxi Hu","Fanqing Meng","Chengyin Liu","Xin Men","Songlin Yang","Zhiyuan Li","Wentao Li","Enzhe Lu","Weizhou Liu","Yanru Chen","Weixin Xu","Longhui Yu","Yejie Wang","Yu Fan","Longguang Zhong","Enming Yuan","Dehao Zhang","Yizhi Zhang","T. Y. Liu","Haiming Wang","Shengjun Fang","Weiran He","Shaowei Liu","Yiwei Li","Jianlin Su","Jiezhong Qiu","Bo Pang","Junjie Yan","Zhejun Jiang","Weixiao Huang","Bohong Yin","Jiacheng You","Chu Wei","Zhengtao Wang","Chao Hong","Yutian Chen","Guanduo Chen","Yucheng Wang","Huabin Zheng","Feng Wang","Yibo Liu","Mengnan Dong","Zheng Zhang","Siyuan Pan","Wenhao Wu","Yuhao Wu","Longyu Guan","Jiawen Tao","Guohong Fu","Xinran Xu","Yuzhi Wang","Guokun Lai","Yuxin Wu","Xinyu Zhou","Zhilin Yang","Yulun Du"],"pdf_url":"https://arxiv.org/pdf/2510.26692v1.pdf","comment":"Kimi Linear tech report"},{"id":"http://arxiv.org/abs/2510.26683v1","updated":"2025-10-30T16:53:45Z","published":"2025-10-30T16:53:45Z","title":"Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models","summary":"  Large language models (LLMs) have demonstrated exceptional capabilities\nacross multiple domains by leveraging massive pre-training and curated\nfine-tuning data. However, in data-sensitive fields such as healthcare, the\nlack of high-quality, domain-specific training corpus hinders LLMs' adaptation\nfor specialized applications. Meanwhile, domain experts have distilled domain\nwisdom into ontology rules, which formalize relationships among concepts and\nensure the integrity of knowledge management repositories. Viewing LLMs as\nimplicit repositories of human knowledge, we propose Evontree, a novel\nframework that leverages a small set of high-quality ontology rules to\nsystematically extract, validate, and enhance domain knowledge within LLMs,\nwithout requiring extensive external datasets. Specifically, Evontree extracts\ndomain ontology from raw models, detects inconsistencies using two core\nontology rules, and reinforces the refined knowledge via self-distilled\nfine-tuning. Extensive experiments on medical QA benchmarks with\nLlama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both\nunmodified models and leading supervised baselines, achieving up to a 3.7%\nimprovement in accuracy. These results confirm the effectiveness, efficiency,\nand robustness of our approach for low-resource domain adaptation of LLMs.\n","authors":["Mingchen Tu","Zhiqiang Liu","Juan Li","Liangyurui Liu","Junjie Wang","Lei Liang","Wen Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.21257v2","updated":"2025-10-30T16:25:15Z","published":"2025-07-28T18:20:41Z","title":"CompoST: A Benchmark for Analyzing the Ability of LLMs To\n  Compositionally Interpret Questions in a QALD Setting","summary":"  Language interpretation is a compositional process, in which the meaning of\nmore complex linguistic structures is inferred from the meaning of their parts.\nLarge language models possess remarkable language interpretation capabilities\nand have been successfully applied to interpret questions by mapping them to\nSPARQL queries. An open question is how systematic this interpretation process\nis. Toward this question, in this paper, we propose a benchmark for\ninvestigating to what extent the abilities of LLMs to interpret questions are\nactually compositional. For this, we generate three datasets of varying\ndifficulty based on graph patterns in DBpedia, relying on Lemon lexica for\nverbalization. Our datasets are created in a very controlled fashion in order\nto test the ability of LLMs to interpret structurally complex questions, given\nthat they have seen the atomic building blocks. This allows us to evaluate to\nwhat degree LLMs are able to interpret complex questions for which they\n\"understand\" the atomic parts. We conduct experiments with models of different\nsizes using both various prompt and few-shot optimization techniques as well as\nfine-tuning. Our results show that performance in terms of macro $F_1$ degrades\nfrom $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the\nsamples optimized on. Even when all necessary information was provided to the\nmodel in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of\nlowest complexity. We thus conclude that LLMs struggle to systematically and\ncompositionally interpret questions and map them into SPARQL queries.\n","authors":["David Maria Schmidt","Raoul Schubert","Philipp Cimiano"],"pdf_url":"https://arxiv.org/pdf/2507.21257v2.pdf","comment":"Research Track, 24th International Semantic Web Conference (ISWC\n  2025), November 2-6, 2025, Nara, Japan"},{"id":"http://arxiv.org/abs/2510.26658v1","updated":"2025-10-30T16:25:10Z","published":"2025-10-30T16:25:10Z","title":"The Era of Agentic Organization: Learning to Organize with Language\n  Models","summary":"  We envision a new era of AI, termed agentic organization, where agents solve\ncomplex problems by working collaboratively and concurrently, enabling outcomes\nbeyond individual intelligence. To realize this vision, we introduce\nasynchronous thinking (AsyncThink) as a new paradigm of reasoning with large\nlanguage models, which organizes the internal thinking process into\nconcurrently executable structures. Specifically, we propose a thinking\nprotocol where an organizer dynamically assigns sub-queries to workers, merges\nintermediate knowledge, and produces coherent solutions. More importantly, the\nthinking structure in this protocol can be further optimized through\nreinforcement learning. Experiments demonstrate that AsyncThink achieves 28%\nlower inference latency compared to parallel thinking while improving accuracy\non mathematical reasoning. Moreover, AsyncThink generalizes its learned\nasynchronous thinking capabilities, effectively tackling unseen tasks without\nadditional training.\n","authors":["Zewen Chi","Li Dong","Qingxiu Dong","Yaru Hao","Xun Wu","Shaohan Huang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2510.26658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.15840v3","updated":"2025-10-30T16:25:05Z","published":"2025-08-19T17:34:25Z","title":"Unveiling Unicode's Unseen Underpinnings in Undermining Authorship\n  Attribution","summary":"  When using a public communication channel -- whether formal or informal, such\nas commenting or posting on social media -- end users have no expectation of\nprivacy: they compose a message and broadcast it for the world to see. Even if\nan end user takes utmost precautions to anonymize their online presence --\nusing an alias or pseudonym; masking their IP address; spoofing their\ngeolocation; concealing their operating system and user agent; deploying\nencryption; registering with a disposable phone number or email; disabling\nnon-essential settings; revoking permissions; and blocking cookies and\nfingerprinting -- one obvious element still lingers: the message itself.\nAssuming they avoid lapses in judgment or accidental self-exposure, there\nshould be little evidence to validate their actual identity, right? Wrong. The\ncontent of their message -- necessarily open for public consumption -- exposes\nan attack vector: stylometric analysis, or author profiling. In this paper, we\ndissect the technique of stylometry, discuss an antithetical counter-strategy\nin adversarial stylometry, and devise enhancements through Unicode\nsteganography.\n","authors":["Robert Dilworth"],"pdf_url":"https://arxiv.org/pdf/2508.15840v3.pdf","comment":"33 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2510.14889v2","updated":"2025-10-30T16:09:51Z","published":"2025-10-16T17:09:14Z","title":"Detecting Early and Implicit Suicidal Ideation via Longitudinal and\n  Information Environment Signals on Social Media","summary":"  On social media, many individuals experiencing suicidal ideation (SI) do not\ndisclose their distress explicitly. Instead, signs may surface indirectly\nthrough everyday posts or peer interactions. Detecting such implicit signals\nearly is critical but remains challenging. We frame early and implicit SI as a\nforward-looking prediction task and develop a computational framework that\nmodels a user's information environment, consisting of both their longitudinal\nposting histories as well as the discourse of their socially proximal peers. We\nadopted a composite network centrality measure to identify top neighbors of a\nuser, and temporally aligned the user's and neighbors' interactions --\nintegrating the multi-layered signals in a fine-tuned DeBERTa-v3 model. In a\nReddit study of 1,000 (500 Case and 500 Control) users, our approach improves\nearly and implicit SI detection by 15% over individual-only baselines. These\nfindings highlight that peer interactions offer valuable predictive signals and\ncarry broader implications for designing early detection systems that capture\nindirect as well as masked expressions of risk in online environments.\n","authors":["Soorya Ram Shimgekar","Ruining Zhao","Agam Goyal","Violeta J. Rodriguez","Paul A. Bloom","Hari Sundaram","Koustuv Saha"],"pdf_url":"https://arxiv.org/pdf/2510.14889v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26622v1","updated":"2025-10-30T15:48:28Z","published":"2025-10-30T15:48:28Z","title":"Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large\n  Language Model","summary":"  Recent large language model (LLM) research has undergone an architectural\nshift from encoder-decoder modeling to nowadays the dominant decoder-only\nmodeling. This rapid transition, however, comes without a rigorous comparative\nanalysis especially \\textit{from the scaling perspective}, raising concerns\nthat the potential of encoder-decoder models may have been overlooked. To fill\nthis gap, we revisit encoder-decoder LLM (RedLLM), enhancing it with recent\nrecipes from decoder-only LLM (DecLLM). We conduct a comprehensive comparison\nbetween RedLLM, pretrained with prefix language modeling (LM), and DecLLM,\npretrained with causal LM, at different model scales, ranging from $\\sim$150M\nto $\\sim$8B. Using RedPajama V1 (1.6T tokens) for pretraining and FLAN for\ninstruction tuning, our experiments show that RedLLM produces compelling\nscaling properties and surprisingly strong performance. While DecLLM is overall\nmore compute-optimal during pretraining, RedLLM demonstrates comparable scaling\nand context length extrapolation capabilities. After instruction tuning, RedLLM\nachieves comparable and even better results on various downstream tasks while\nenjoying substantially better inference efficiency. We hope our findings could\ninspire more efforts on re-examining RedLLM, unlocking its potential for\ndeveloping powerful and efficient LLMs.\n","authors":["Biao Zhang","Yong Cheng","Siamak Shakeri","Xinyi Wang","Min Ma","Orhan Firat"],"pdf_url":"https://arxiv.org/pdf/2510.26622v1.pdf","comment":"The scaling study inspiring T5Gemma"},{"id":"http://arxiv.org/abs/2510.26615v1","updated":"2025-10-30T15:41:15Z","published":"2025-10-30T15:41:15Z","title":"SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual\n  Document Understanding","summary":"  Multi-page visual documents such as manuals, brochures, presentations, and\nposters convey key information through layout, colors, icons, and cross-slide\nreferences. While large language models (LLMs) offer opportunities in document\nunderstanding, current systems struggle with complex, multi-page visual\ndocuments, particularly in fine-grained reasoning over elements and pages. We\nintroduce SlideAgent, a versatile agentic framework for understanding\nmulti-modal, multi-page, and multi-layout documents, especially slide decks.\nSlideAgent employs specialized agents and decomposes reasoning into three\nspecialized levels-global, page, and element-to construct a structured,\nquery-agnostic representation that captures both overarching themes and\ndetailed visual or textual cues. During inference, SlideAgent selectively\nactivates specialized agents for multi-level reasoning and integrates their\noutputs into coherent, context-aware answers. Extensive experiments show that\nSlideAgent achieves significant improvement over both proprietary (+7.9\noverall) and open-source models (+9.8 overall).\n","authors":["Yiqiao Jin","Rachneet Kaur","Zhen Zeng","Sumitra Ganesh","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2510.26615v1.pdf","comment":"https://slideagent.github.io/"},{"id":"http://arxiv.org/abs/2510.26606v1","updated":"2025-10-30T15:35:13Z","published":"2025-10-30T15:35:13Z","title":"Normative Reasoning in Large Language Models: A Comparative Benchmark\n  from Logical and Modal Perspectives","summary":"  Normative reasoning is a type of reasoning that involves normative or deontic\nmodality, such as obligation and permission. While large language models (LLMs)\nhave demonstrated remarkable performance across various reasoning tasks, their\nability to handle normative reasoning remains underexplored. In this paper, we\nsystematically evaluate LLMs' reasoning capabilities in the normative domain\nfrom both logical and modal perspectives. Specifically, to assess how well LLMs\nreason with normative modals, we make a comparison between their reasoning with\nnormative modals and their reasoning with epistemic modals, which share a\ncommon formal structure. To this end, we introduce a new dataset covering a\nwide range of formal patterns of reasoning in both normative and epistemic\ndomains, while also incorporating non-formal cognitive factors that influence\nhuman reasoning. Our results indicate that, although LLMs generally adhere to\nvalid reasoning patterns, they exhibit notable inconsistencies in specific\ntypes of normative reasoning and display cognitive biases similar to those\nobserved in psychological studies of human reasoning. These findings highlight\nchallenges in achieving logical consistency in LLMs' normative reasoning and\nprovide insights for enhancing their reliability. All data and code are\nreleased publicly at https://github.com/kmineshima/NeuBAROCO.\n","authors":["Kentaro Ozeki","Risako Ando","Takanobu Morishita","Hirohiko Abe","Koji Mineshima","Mitsuhiro Okada"],"pdf_url":"https://arxiv.org/pdf/2510.26606v1.pdf","comment":"Accepted to the 8th BlackboxNLP Workshop at EMNLP 2025"},{"id":"http://arxiv.org/abs/2510.08604v2","updated":"2025-10-30T15:33:58Z","published":"2025-10-07T09:40:20Z","title":"LatentBreak: Jailbreaking Large Language Models through Latent Space\n  Feedback","summary":"  Jailbreaks are adversarial attacks designed to bypass the built-in safety\nmechanisms of large language models. Automated jailbreaks typically optimize an\nadversarial suffix or adapt long prompt templates by forcing the model to\ngenerate the initial part of a restricted or harmful response. In this work, we\nshow that existing jailbreak attacks that leverage such mechanisms to unlock\nthe model response can be detected by a straightforward perplexity-based\nfiltering on the input prompt. To overcome this issue, we propose LatentBreak,\na white-box jailbreak attack that generates natural adversarial prompts with\nlow perplexity capable of evading such defenses. LatentBreak substitutes words\nin the input prompt with semantically-equivalent ones, preserving the initial\nintent of the prompt, instead of adding high-perplexity adversarial suffixes or\nlong templates. These words are chosen by minimizing the distance in the latent\nspace between the representation of the adversarial prompt and that of harmless\nrequests. Our extensive evaluation shows that LatentBreak leads to shorter and\nlow-perplexity prompts, thus outperforming competing jailbreak algorithms\nagainst perplexity-based filters on multiple safety-aligned models.\n","authors":["Raffaele Mura","Giorgio Piras","KamilÄ LukoÅ¡iÅ«tÄ","Maura Pintor","Amin Karbasi","Battista Biggio"],"pdf_url":"https://arxiv.org/pdf/2510.08604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.19756v2","updated":"2025-10-30T15:23:59Z","published":"2025-07-26T03:01:59Z","title":"Are You There God? Lightweight Narrative Annotation of Christian Fiction\n  with LMs","summary":"  In addition to its more widely studied cultural movements, American\nEvangelicalism has a well-developed but less externally visible literary side.\nChristian Fiction, however, has been little studied, and what scholarly\nattention there is has focused on the explosively popular Left Behind series.\nIn this work, we use computational tools to provide both a broad topical\noverview of Christian Fiction as a genre and a more directed exploration of how\nits authors depict divine acts. Working with human annotators, we first\ndeveloped a codebook for identifying \"acts of God.\" We then adapted the\ncodebook for use by a recent, lightweight LM with the assistance of a much\nlarger model. The laptop-scale LM is largely capable of matching human\nannotations, even when the task is subtle and challenging. Using these\nannotations, we show that significant and meaningful differences exist between\ndivine acts depicted by the Left Behind books and Christian Fiction more\nbroadly.\n","authors":["Rebecca M. M. Hicke","Brian W. Haggard","Mia Ferrante","Rayhan Khanna","David Mimno"],"pdf_url":"https://arxiv.org/pdf/2507.19756v2.pdf","comment":"Accepted to CHR 2025"},{"id":"http://arxiv.org/abs/2502.14409v2","updated":"2025-10-30T15:05:42Z","published":"2025-02-20T09:57:42Z","title":"Unstructured Evidence Attribution for Long Context Query Focused\n  Summarization","summary":"  Large language models (LLMs) are capable of generating coherent summaries\nfrom very long contexts given a user query, and extracting and citing evidence\nspans helps improve the trustworthiness of these summaries. Whereas previous\nwork has focused on evidence citation with fixed levels of granularity (e.g.\nsentence, paragraph, document, etc.), we propose to extract unstructured (i.e.,\nspans of any length) evidence in order to acquire more relevant and consistent\nevidence than in the fixed granularity case. We show how existing systems\nstruggle to copy and properly cite unstructured evidence, which also tends to\nbe \"lost-in-the-middle\". To help models perform this task, we create the\nSummaries with Unstructured Evidence Text dataset (SUnsET), a synthetic dataset\ngenerated using a novel pipeline, which can be used as training supervision for\nunstructured evidence summarization. We demonstrate across 5 LLMs and 4\ndatasets spanning human written, synthetic, single, and multi-document settings\nthat LLMs adapted with SUnsET generate more relevant and factually consistent\nevidence with their summaries, extract evidence from more diverse locations in\ntheir context, and can generate more relevant and consistent summaries than\nbaselines with no fine-tuning and fixed granularity evidence. We release SUnsET\nand our generation code to the public.\n","authors":["Dustin Wright","Zain Muhammad Mujahid","Lu Wang","Isabelle Augenstein","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2502.14409v2.pdf","comment":"EMNLP 2025 Main; 29 pages; 24 figures; 8 tables"},{"id":"http://arxiv.org/abs/2510.26577v1","updated":"2025-10-30T15:04:36Z","published":"2025-10-30T15:04:36Z","title":"Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference\n  in Large Language Models","summary":"  Large Language Models (LLMs) face significant inference latency challenges\nstemming from their autoregressive design and large size. To address this,\nspeculative decoding emerges as a solution, enabling the simultaneous\ngeneration and validation of multiple tokens. While recent approaches like\nEAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures,\nthey often neglect the impact of crucial system variables such as GPU devices\nand batch sizes.\n  Therefore, we introduce a new dynamic tree decoding approach called CAST that\ntakes into account inference costs, including factors such as GPU\nconfigurations and batch sizes, to dynamically refine the tree structure.\nThrough comprehensive experimentation across six diverse tasks and utilizing\nsix distinct LLMs, our methodology demonstrates remarkable results, achieving\nspeeds up to 5.2 times faster than conventional decoding methods. Moreover, it\ngenerally outperforms existing state-of-the-art techniques from 5% to 20%.\n","authors":["Yinrong Hong","Zhiquan Tan","Kai Hu"],"pdf_url":"https://arxiv.org/pdf/2510.26577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26575v1","updated":"2025-10-30T15:03:21Z","published":"2025-10-30T15:03:21Z","title":"InfoFlow: Reinforcing Search Agent Via Reward Density Optimization","summary":"  Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach\nfor enhancing agentic deep search. However, its application is often hindered\nby low \\textbf{Reward Density} in deep search scenarios, where agents expend\nsignificant exploratory costs for infrequent and often null final rewards. In\nthis paper, we formalize this challenge as the \\textbf{Reward Density\nOptimization} problem, which aims to improve the reward obtained per unit of\nexploration cost. This paper introduce \\textbf{InfoFlow}, a systematic\nframework that tackles this problem from three aspects. 1) \\textbf{Subproblem\ndecomposition}: breaking down long-range tasks to assign process rewards,\nthereby providing denser learning signals. 2) \\textbf{Failure-guided hints}:\ninjecting corrective guidance into stalled trajectories to increase the\nprobability of successful outcomes. 3) \\textbf{Dual-agent refinement}:\nemploying a dual-agent architecture to offload the cognitive burden of deep\nexploration. A refiner agent synthesizes the search history, which effectively\ncompresses the researcher's perceived trajectory, thereby reducing exploration\ncost and increasing the overall reward density. We evaluate InfoFlow on\nmultiple agentic search benchmarks, where it significantly outperforms strong\nbaselines, enabling lightweight LLMs to achieve performance comparable to\nadvanced proprietary LLMs.\n","authors":["Kun Luo","Hongjin Qian","Zheng Liu","Ziyi Xia","Shitao Xiao","Siqi Bao","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.04226v4","updated":"2025-10-30T14:52:48Z","published":"2025-10-05T14:29:15Z","title":"Epistemic Diversity and Knowledge Collapse in Large Language Models","summary":"  Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation\n","authors":["Dustin Wright","Sarah Masud","Jared Moore","Srishti Yadav","Maria Antoniak","Chan Young Park","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2510.04226v4.pdf","comment":"16 pages; 8 figures, 4 tables; v2 changelog: Fixed the modeling for\n  table 3, random effect is the model version; v3 changelog: Fixed minor\n  formatting issues in tables 2 and 3; v4 changelog: Fixed some typos and model\n  description"},{"id":"http://arxiv.org/abs/2504.11331v2","updated":"2025-10-30T14:50:58Z","published":"2025-04-15T16:05:09Z","title":"Dependency Structure Augmented Contextual Scoping Framework for\n  Multimodal Aspect-Based Sentiment Analysis","summary":"  Multimodal Aspect-Based Sentiment Analysis (MABSA) seeks to extract\nfine-grained information from image-text pairs to identify aspect terms and\ndetermine their sentiment polarity. However, existing approaches often fall\nshort in simultaneously addressing three core challenges: Sentiment Cue\nPerception (SCP), Multimodal Information Misalignment (MIM), and Semantic Noise\nElimination (SNE). To overcome these limitations, we propose DASCO\n(\\textbf{D}ependency Structure \\textbf{A}ugmented \\textbf{Sco}ping Framework),\na fine-grained scope-oriented framework that enhances aspect-level sentiment\nreasoning by leveraging dependency parsing trees. First, we designed a\nmulti-task pretraining strategy for MABSA on our base model, combining\naspect-oriented enhancement, image-text matching, and aspect-level\nsentiment-sensitive cognition. This improved the model's perception of aspect\nterms and sentiment cues while achieving effective image-text alignment,\naddressing key challenges like SCP and MIM. Furthermore, we incorporate\ndependency trees as syntactic branch combining with semantic branch, guiding\nthe model to selectively attend to critical contextual elements within a\ntarget-specific scope while effectively filtering out irrelevant noise for\naddressing SNE problem. Extensive experiments on two benchmark datasets across\nthree subtasks demonstrate that DASCO achieves state-of-the-art performance in\nMABSA, with notable gains in JMASA (+2.3\\% F1 and +3.5\\% precision on\nTwitter2015). The source code is available at https://github.com/LHaoooo/DASCO .\n","authors":["Hao Liu","Lijun He","Jiaxi Liang","Zhihan Ren","Haixia Bi","Fan Li"],"pdf_url":"https://arxiv.org/pdf/2504.11331v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26543v1","updated":"2025-10-30T14:36:09Z","published":"2025-10-30T14:36:09Z","title":"The Structure of Relation Decoding Linear Operators in Large Language\n  Models","summary":"  This paper investigates the structure of linear operators introduced in\nHernandez et al. [2023] that decode specific relational facts in transformer\nlanguage models. We extend their single-relation findings to a collection of\nrelations and systematically chart their organization. We show that such\ncollections of relation decoders can be highly compressed by simple order-3\ntensor networks without significant loss in decoding accuracy. To explain this\nsurprising redundancy, we develop a cross-evaluation protocol, in which we\napply each linear decoder operator to the subjects of every other relation. Our\nresults reveal that these linear maps do not encode distinct relations, but\nextract recurring, coarse-grained semantic properties (e.g., country of capital\ncity and country of food are both in the country-of-X property). This\nproperty-centric structure clarifies both the operators' compressibility and\nhighlights why they generalize only to new relations that are semantically\nclose. Our findings thus interpret linear relational decoding in transformer\nlanguage models as primarily property-based, rather than relation-specific.\n","authors":["Miranda Anna Christ","AdriÃ¡n CsiszÃ¡rik","Gergely BecsÃ³","DÃ¡niel Varga"],"pdf_url":"https://arxiv.org/pdf/2510.26543v1.pdf","comment":"NeurIPS 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2510.25682v2","updated":"2025-10-30T14:28:46Z","published":"2025-10-29T16:47:02Z","title":"PairUni: Pairwise Training for Unified Multimodal Language Models","summary":"  Unified vision-language models (UVLMs) must perform both understanding and\ngeneration within a single architecture, but these tasks rely on heterogeneous\ndata and supervision, making it difficult to balance them during reinforcement\nlearning (RL). We propose PairUni, a unified framework that reorganizes data\ninto understanding-generation (UG) pairs and aligns optimization accordingly.\nWe first use GPT-o3 to augment single-task data, generating captions for\nunderstanding samples and question-answer (QA) pairs for generation samples,\nforming aligned pairs from the same instance. Additionally, for each generation\nsample, we retrieve a semantically related understanding example to form a\nretrieved pair, linking different but related data points. These paired\nstructures expose cross-task semantic correspondences and support consistent\npolicy learning. To leverage this structure, we present Pair-GPRO, a pair-aware\nvariant based on Group Relative Policy Optimization. It assigns a similarity\nscore to each pair to modulate the advantage, strengthening learning from\nwell-aligned examples and reducing task interference. We curate a high-quality\ndataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on\nthe powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on\nvarious UVLMs, outperforming strong UVLM RL baselines. Codes are available at\nhttps://github.com/Haochen-Wang409/PairUni.\n","authors":["Jiani Zheng","Zhiyang Teng","Xiangtai Li","Anran Wang","Yu Tian","Kunpeng Qiu","Ye Tian","Haochen Wang","Zhuochen Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25682v2.pdf","comment":"21 pages, 11 figures, and 8 tables"},{"id":"http://arxiv.org/abs/2510.26521v1","updated":"2025-10-30T14:15:16Z","published":"2025-10-30T14:15:16Z","title":"Hebrew Diacritics Restoration using Visual Representation","summary":"  Diacritics restoration in Hebrew is a fundamental task for ensuring accurate\nword pronunciation and disambiguating textual meaning. Despite the language's\nhigh degree of ambiguity when unvocalized, recent machine learning approaches\nhave significantly advanced performance on this task.\n  In this work, we present DIVRIT, a novel system for Hebrew diacritization\nthat frames the task as a zero-shot classification problem. Our approach\noperates at the word level, selecting the most appropriate diacritization\npattern for each undiacritized word from a dynamically generated candidate set,\nconditioned on the surrounding textual context. A key innovation of DIVRIT is\nits use of a Hebrew Visual Language Model, which processes undiacritized text\nas an image, allowing diacritic information to be embedded directly within the\ninput's vector representation.\n  Through a comprehensive evaluation across various configurations, we\ndemonstrate that the system effectively performs diacritization without relying\non complex, explicit linguistic analysis. Notably, in an ``oracle'' setting\nwhere the correct diacritized form is guaranteed to be among the provided\ncandidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic\narchitectural enhancements and optimized training methodologies yield\nsignificant improvements in the system's overall generalization capabilities.\nThese findings highlight the promising potential of visual representations for\naccurate and automated Hebrew diacritization.\n","authors":["Yair Elboher","Yuval Pinter"],"pdf_url":"https://arxiv.org/pdf/2510.26521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26512v1","updated":"2025-10-30T14:05:55Z","published":"2025-10-30T14:05:55Z","title":"Inside CORE-KG: Evaluating Structured Prompting and Coreference\n  Resolution for Knowledge Graphs","summary":"  Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer critical insights but are often unstructured,\nlexically dense, and filled with ambiguous or shifting references, which pose\nsignificant challenges for automated knowledge graph (KG) construction. While\nrecent LLM-based approaches improve over static templates, they still generate\nnoisy, fragmented graphs with duplicate nodes due to the absence of guided\nextraction and coreference resolution. The recently proposed CORE-KG framework\naddresses these limitations by integrating a type-aware coreference module and\ndomain-guided structured prompts, significantly reducing node duplication and\nlegal noise. In this work, we present a systematic ablation study of CORE-KG to\nquantify the individual contributions of its two key components. Our results\nshow that removing coreference resolution results in a 28.32% increase in node\nduplication and a 4.32% increase in noisy nodes, while removing structured\nprompts leads to a 4.34% increase in node duplication and a 73.33% increase in\nnoisy nodes. These findings offer empirical insights for designing robust\nLLM-based pipelines for extracting structured representations from complex\nlegal texts.\n","authors":["Dipak Meher","Carlotta Domeniconi"],"pdf_url":"https://arxiv.org/pdf/2510.26512v1.pdf","comment":"ICDM 2025 Workshop"},{"id":"http://arxiv.org/abs/2510.26498v1","updated":"2025-10-30T13:50:19Z","published":"2025-10-30T13:50:19Z","title":"A Multi-agent Large Language Model Framework to Automatically Assess\n  Performance of a Clinical AI Triage Tool","summary":"  Purpose: The purpose of this study was to determine if an ensemble of\nmultiple LLM agents could be used collectively to provide a more reliable\nassessment of a pixel-based AI triage tool than a single LLM.\n  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were\nprocessed by a commercial intracranial hemorrhage (ICH) AI detection tool.\nRadiology reports were analyzed by an ensemble of eight open-source LLM models\nand a HIPAA compliant internal version of GPT-4o using a single multi-shot\nprompt that assessed for presence of ICH. 1,726 examples were manually\nreviewed. Performance characteristics of the eight open-source models and\nconsensus were compared to GPT-4o. Three ideal consensus LLM ensembles were\ntested for rating the performance of the triage tool.\n  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The\nhighest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78).\nThe average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76).\nLlama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater\nprecision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the\nideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3\nEnsemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522\n(0.500-0.543). No statistically significant differences were observed between\nTop-3, Full-9, and Consensus (p > 0.05).\n  Conclusion: An ensemble of medium to large sized open-source LLMs provides a\nmore consistent and reliable method to derive a ground truth retrospective\nevaluation of a clinical AI triage tool over a single LLM alone.\n","authors":["Adam E. Flanders","Yifan Peng","Luciano Prevedello","Robyn Ball","Errol Colak","Prahlad Menon","George Shih","Hui-Ming Lin","Paras Lakhani"],"pdf_url":"https://arxiv.org/pdf/2510.26498v1.pdf","comment":"29 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2510.25623v2","updated":"2025-10-30T13:49:22Z","published":"2025-10-29T15:27:47Z","title":"Evaluating the Role of Verifiers in Test-Time Scaling for Legal\n  Reasoning Tasks","summary":"  Test-time scaling (TTS) techniques can improve the performance of large\nlanguage models (LLMs) at the expense of additional computation and latency.\nWhile TTS has proven effective in formal domains such as mathematics and\nprogramming, its value in argumentative domains such as law remains\nunderexplored. We present an empirical study of verifier-based TTS methods for\nlegal multiple-choice QA (MCQA) across five benchmarks. Using a family of 7\nreward models, we evaluate both outcome-level (Best-of-$N$) and process-level\n(tree search) verification under realistic low-$N$ budgets. Our analysis\nsystematically investigates how verifier utility is affected by key properties\nsuch as domain specialization, model size, and supervision type\n(process-supervised PRMs vs. outcome-only ORMs), even when applied across\ndifferent roles.\n","authors":["Davide Romano","Jonathan Schwarz","Daniele GiofrÃ©"],"pdf_url":"https://arxiv.org/pdf/2510.25623v2.pdf","comment":"Accepted to EMNLP - NLLP Workshop"},{"id":"http://arxiv.org/abs/2503.00333v2","updated":"2025-10-30T13:47:16Z","published":"2025-03-01T03:45:35Z","title":"More of the Same: Persistent Representational Harms Under Increased\n  Representation","summary":"  To recognize and mitigate the harms of generative AI systems, it is crucial\nto consider who is represented in the outputs of generative AI systems and how\npeople are represented. A critical gap emerges when naively improving who is\nrepresented, as this does not imply bias mitigation efforts have been applied\nto address how people are represented. We critically examined this by\ninvestigating gender representation in occupation across state-of-the-art large\nlanguage models. We first show evidence suggesting that over time there have\nbeen interventions to models altering the resulting gender distribution, and we\nfind that women are more represented than men when models are prompted to\ngenerate biographies or personas. We then demonstrate that representational\nbiases persist in how different genders are represented by examining\nstatistically significant word differences across genders. This results in a\nproliferation of representational harms, stereotypes, and neoliberalism ideals\nthat, despite existing interventions to increase female representation,\nreinforce existing systems of oppression.\n","authors":["Jennifer Mickel","Maria De-Arteaga","Leqi Liu","Kevin Tian"],"pdf_url":"https://arxiv.org/pdf/2503.00333v2.pdf","comment":"Accepted by the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025) as a poster paper; 39 pages, 7 figures, 15 tables"},{"id":"http://arxiv.org/abs/2510.26495v1","updated":"2025-10-30T13:44:22Z","published":"2025-10-30T13:44:22Z","title":"Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for\n  Real-world Database Exploration","summary":"  Recent advances in Text-to-SQL have achieved strong results in static,\nsingle-turn tasks, where models generate SQL queries from natural language\nquestions. However, these systems fall short in real-world interactive\nscenarios, where user intents evolve and queries must be refined over multiple\nturns. In applications such as finance and business analytics, users\niteratively adjust query constraints or dimensions based on intermediate\nresults. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a\nbenchmark assessing model performance under evolving user interactions. Unlike\nprevious manually curated datasets, DySQL-Bench is built through an automated\ntwo-stage pipeline of task synthesis and verification. Structured tree\nrepresentations derived from raw database tables guide LLM-based task\ngeneration, followed by interaction-oriented filtering and expert validation.\nHuman evaluation confirms 100% correctness of the synthesized data. We further\npropose a multi-turn evaluation framework simulating realistic interactions\namong an LLM-simulated user, the model under test, and an executable database.\nThe model must adapt its reasoning and SQL generation as user intents change.\nDySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling\n1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the\nPass@5 metric, underscoring the benchmark's difficulty. All code and data are\nreleased at https://github.com/Aurora-slz/Real-World-SQL-Bench .\n","authors":["Linzhuang Sun","Tianyu Guo","Hao Liang","Yuying Li","Qifeng Cai","Jingxuan Wei","Bihui Yu","Wentao Zhang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2510.26495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26493v1","updated":"2025-10-30T13:43:10Z","published":"2025-10-30T13:43:10Z","title":"Context Engineering 2.0: The Context of Context Engineering","summary":"  Karl Marx once wrote that ``the human essence is the ensemble of social\nrelations'', suggesting that individuals are not isolated entities but are\nfundamentally shaped by their interactions with other entities, within which\ncontexts play a constitutive and essential role. With the advent of computers\nand artificial intelligence, these contexts are no longer limited to purely\nhuman--human interactions: human--machine interactions are included as well.\nThen a central question emerges: How can machines better understand our\nsituations and purposes? To address this challenge, researchers have recently\nintroduced the concept of context engineering. Although it is often regarded as\na recent innovation of the agent era, we argue that related practices can be\ntraced back more than twenty years. Since the early 1990s, the field has\nevolved through distinct historical phases, each shaped by the intelligence\nlevel of machines: from early human--computer interaction frameworks built\naround primitive computers, to today's human--agent interaction paradigms\ndriven by intelligent agents, and potentially to human--level or superhuman\nintelligence in the future. In this paper, we situate context engineering,\nprovide a systematic definition, outline its historical and conceptual\nlandscape, and examine key design considerations for practice. By addressing\nthese questions, we aim to offer a conceptual foundation for context\nengineering and sketch its promising future. This paper is a stepping stone for\na broader community effort toward systematic context engineering in AI systems.\n","authors":["Qishuo Hua","Lyumanshan Ye","Dayuan Fu","Yang Xiao","Xiaojie Cai","Yunze Wu","Jifan Lin","Junfei Wang","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26484v1","updated":"2025-10-30T13:37:58Z","published":"2025-10-30T13:37:58Z","title":"Bayesian Network Fusion of Large Language Models for Sentiment Analysis","summary":"  Large language models (LLMs) continue to advance, with an increasing number\nof domain-specific variants tailored for specialised tasks. However, these\nmodels often lack transparency and explainability, can be costly to fine-tune,\nrequire substantial prompt engineering, yield inconsistent results across\ndomains, and impose significant adverse environmental impact due to their high\ncomputational demands. To address these challenges, we propose the Bayesian\nnetwork LLM fusion (BNLF) framework, which integrates predictions from three\nLLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic\nmechanism for sentiment analysis. BNLF performs late fusion by modelling the\nsentiment predictions from multiple LLMs as probabilistic nodes within a\nBayesian network. Evaluated across three human-annotated financial corpora with\ndistinct linguistic and contextual characteristics, BNLF demonstrates\nconsistent gains of about six percent in accuracy over the baseline LLMs,\nunderscoring its robustness to dataset variability and the effectiveness of\nprobabilistic fusion for interpretable sentiment classification.\n","authors":["Rasoul Amirzadeh","Dhananjay Thiruvady","Fatemeh Shiri"],"pdf_url":"https://arxiv.org/pdf/2510.26484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12371v2","updated":"2025-10-30T13:27:07Z","published":"2025-05-18T11:28:17Z","title":"MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional\n  Methods for Diverse Medical Tasks","summary":"  The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/.\n","authors":["Yinghao Zhu","Ziyi He","Haoran Hu","Xiaochen Zheng","Xichen Zhang","Zixiang Wang","Junyi Gao","Liantao Ma","Lequan Yu"],"pdf_url":"https://arxiv.org/pdf/2505.12371v2.pdf","comment":"Accepted by NeurIPS 2025 Datasets & Benchmarks Track"},{"id":"http://arxiv.org/abs/2510.26474v1","updated":"2025-10-30T13:26:58Z","published":"2025-10-30T13:26:58Z","title":"Counteracting Matthew Effect in Self-Improvement of LVLMs through\n  Head-Tail Re-balancing","summary":"  Self-improvement has emerged as a mainstream paradigm for advancing the\nreasoning capabilities of large vision-language models (LVLMs), where models\nexplore and learn from successful trajectories iteratively. However, we\nidentify a critical issue during this process: the model excels at generating\nhigh-quality trajectories for simple queries (i.e., head data) but struggles\nwith more complex ones (i.e., tail data). This leads to an imbalanced\noptimization that drives the model to prioritize simple reasoning skills, while\nhindering its ability to tackle more complex reasoning tasks. Over iterations,\nthis imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew\neffect\"--which ultimately hinders further model improvement and leads to\nperformance bottlenecks. To counteract this challenge, we introduce four\nefficient strategies from two perspectives: distribution-reshaping and\ntrajectory-resampling, to achieve head-tail re-balancing during the\nexploration-and-learning self-improvement process. Extensive experiments on\nQwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks\ndemonstrate that our methods consistently improve visual reasoning\ncapabilities, outperforming vanilla self-improvement by 3.86 points on average.\n","authors":["Xin Guo","Zhiheng Xi","Yiwen Ding","Yitao Zhai","Xiaowei Shi","Xunliang Cai","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26474v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2510.26457v1","updated":"2025-10-30T13:06:11Z","published":"2025-10-30T13:06:11Z","title":"SecureReviewer: Enhancing Large Language Models for Secure Code Review\n  through Secure-aware Fine-tuning","summary":"  Identifying and addressing security issues during the early phase of the\ndevelopment lifecycle is critical for mitigating the long-term negative impacts\non software systems. Code review serves as an effective practice that enables\ndevelopers to check their teammates' code before integration into the codebase.\nTo streamline the generation of review comments, various automated code review\napproaches have been proposed, where LLM-based methods have significantly\nadvanced the capabilities of automated review generation. However, existing\nmodels primarily focus on general-purpose code review, their effectiveness in\nidentifying and addressing security-related issues remains underexplored.\nMoreover, adapting existing code review approaches to target security issues\nfaces substantial challenges, including data scarcity and inadequate evaluation\nmetrics. To address these limitations, we propose SecureReviewer, a new\napproach designed for enhancing LLMs' ability to identify and resolve\nsecurity-related issues during code review. Specifically, we first construct a\ndataset tailored for training and evaluating secure code review capabilities.\nLeveraging this dataset, we fine-tune LLMs to generate code review comments\nthat can effectively identify security issues and provide fix suggestions with\nour proposed secure-aware fine-tuning strategy. To mitigate hallucination in\nLLMs and enhance the reliability of their outputs, we integrate the RAG\ntechnique, which grounds the generated comments in domain-specific security\nknowledge. Additionally, we introduce SecureBLEU, a new evaluation metric\ndesigned to assess the effectiveness of review comments in addressing security\nissues. Experimental results demonstrate that SecureReviewer outperforms\nstate-of-the-art baselines in both security issue detection accuracy and the\noverall quality and practical utility of generated review comments.\n","authors":["Fang Liu","Simiao Liu","Yinghao Zhu","Xiaoli Lian","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26457v1.pdf","comment":"Accepted by ICSE 2026. Code and data:\n  https://github.com/SIMIAO515/SecureReviewer"},{"id":"http://arxiv.org/abs/2510.21513v2","updated":"2025-10-30T13:03:25Z","published":"2025-10-24T14:39:23Z","title":"Wisdom and Delusion of LLM Ensembles for Code Generation and Repair","summary":"  Today's pursuit of a single Large Language Model (LMM) for all software\nengineering tasks is resource-intensive and overlooks the potential benefits of\ncomplementarity, where different models contribute unique strengths. However,\nthe degree to which coding LLMs complement each other and the best strategy for\nmaximizing an ensemble's potential are unclear, leaving practitioners without a\nclear path to move beyond single-model systems. To address this gap, we\nempirically compare ten individual LLMs from five families, and three ensembles\nof these LLMs across three software engineering benchmarks covering code\ngeneration and program repair. We assess the complementarity between models and\nthe performance gap between the best individual model and the ensembles. Next,\nwe evaluate various selection heuristics to identify correct solutions from an\nensemble's candidate pool. We find that the theoretical upperbound for an\nensemble's performance can be 83% above the best single model. Our results show\nthat consensus-based strategies for selecting solutions fall into a \"popularity\ntrap,\" amplifying common but incorrect outputs. In contrast, a diversity-based\nstrategy realizes up to 95% of this theoretical potential, and proves effective\neven in small two-model ensembles, enabling a cost-efficient way to enhance\nperformance by leveraging multiple LLMs.\n","authors":["Fernando Vallecillos-Ruiz","Max Hort","Leon Moonen"],"pdf_url":"https://arxiv.org/pdf/2510.21513v2.pdf","comment":"Added Acknowledgments section and hyphenated last names"},{"id":"http://arxiv.org/abs/2510.26446v1","updated":"2025-10-30T12:50:30Z","published":"2025-10-30T12:50:30Z","title":"1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large\n  Language Models","summary":"  Large Language Models (LLMs) have demonstrated remarkable proficiency in\nlanguage comprehension and generation; however, their widespread adoption is\nconstrained by substantial bandwidth and computational demands. While pruning\nand low-rank approximation have each demonstrated promising performance\nindividually, their synergy for LLMs remains underexplored. We introduce\n\\underline{S}ynergistic \\underline{S}parse and \\underline{L}ow-Rank\n\\underline{C}ompression (SSLC) methods for LLMs, which leverages the strengths\nof both techniques: low-rank approximation compresses the model by retaining\nits essential structure with minimal information loss, whereas sparse\noptimization eliminates non-essential weights, preserving those crucial for\ngeneralization. Based on theoretical analysis, we first formulate the low-rank\napproximation and sparse optimization as a unified problem and solve it by\niterative optimization algorithm. Experiments on LLaMA and Qwen2.5 models\n(7B-70B) show that SSLC, without any additional training steps, consistently\nsurpasses standalone methods, achieving state-of-the-arts results. Notably,\nSSLC compresses Qwen2.5 by 50\\% with no performance drop and achieves at least\n1.63$\\times$ speedup, offering a practical solution for efficient LLM\ndeployment.\n","authors":["Zeliang Zong","Kai Zhang","Zheyang Li","Wenming Tan","Ye Ren","Yiyan Zhai","Jilin Hu"],"pdf_url":"https://arxiv.org/pdf/2510.26446v1.pdf","comment":"15 pages, 6 figures, EMNLP 2025 findings"},{"id":"http://arxiv.org/abs/2510.26423v1","updated":"2025-10-30T12:20:25Z","published":"2025-10-30T12:20:25Z","title":"Nexus: Execution-Grounded Multi-Agent Test Oracle Synthesis","summary":"  Test oracle generation in non-regression testing is a longstanding challenge\nin software engineering, where the goal is to produce oracles that can\naccurately determine whether a function under test (FUT) behaves as intended\nfor a given input. In this paper, we introduce Nexus, a novel multi-agent\nframework to address this challenge. Nexus generates test oracles by leveraging\na diverse set of specialized agents that synthesize test oracles through a\nstructured process of deliberation, validation, and iterative self-refinement.\nDuring the deliberation phase, a panel of four specialist agents, each\nembodying a distinct testing philosophy, collaboratively critiques and refines\nan initial set of test oracles. Then, in the validation phase, Nexus generates\na plausible candidate implementation of the FUT and executes the proposed\noracles against it in a secure sandbox. For any oracle that fails this\nexecution-based check, Nexus activates an automated selfrefinement loop, using\nthe specific runtime error to debug and correct the oracle before\nre-validation. Our extensive evaluation on seven diverse benchmarks\ndemonstrates that Nexus consistently and substantially outperforms\nstate-of-theart baselines. For instance, Nexus improves the test-level oracle\naccuracy on the LiveCodeBench from 46.30% to 57.73% for GPT-4.1-Mini. The\nimproved accuracy also significantly enhances downstream tasks: the bug\ndetection rate of GPT4.1-Mini generated test oracles on HumanEval increases\nfrom 90.91% to 95.45% for Nexus compared to baselines, and the success rate of\nautomated program repair improves from 35.23% to 69.32%.\n","authors":["Dong Huang","Mingzhe Du","Jie M. Zhang","Zheng Lin","Meng Luo","Qianru Zhang","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2510.26423v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2510.26422v1","updated":"2025-10-30T12:16:29Z","published":"2025-10-30T12:16:29Z","title":"OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large\n  Language Models in Education","summary":"  With the rapid development of large language models (LLMs), various LLM-based\nworks have been widely applied in educational fields. However, most existing\nLLMs and their benchmarks focus primarily on the knowledge dimension, largely\nneglecting the evaluation of cultivation capabilities that are essential for\nreal-world educational scenarios. Additionally, current benchmarks are often\nlimited to a single subject or question type, lacking sufficient diversity.\nThis issue is particularly prominent within the Chinese context. To address\nthis gap, we introduce OmniEduBench, a comprehensive Chinese educational\nbenchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs.\nThe data is meticulously divided into two core dimensions: the knowledge\ndimension and the cultivation dimension, which contain 18.121K and 6.481K\nentries, respectively. Each dimension is further subdivided into 6 fine-grained\ncategories, covering a total of 61 different subjects (41 in the knowledge and\n20 in the cultivation). Furthermore, the dataset features a rich variety of\nquestion formats, including 11 common exam question types, providing a solid\nfoundation for comprehensively evaluating LLMs' capabilities in education.\nExtensive experiments on 11 mainstream open-source and closed-source LLMs\nreveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro\nsurpassed 60\\% accuracy, while in the cultivation dimension, the\nbest-performing model, QWQ, still trailed human intelligence by nearly 30\\%.\nThese results highlight the substantial room for improvement and underscore the\nchallenges of applying LLMs in education.\n","authors":["Min Zhang","Hao Chen","Hao Chen","Wenqi Zhang","Didi Zhu","Xin Lin","Bo Jiang","Aimin Zhou","Fei Wu","Kun Kuang"],"pdf_url":"https://arxiv.org/pdf/2510.26422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25536v2","updated":"2025-10-30T11:19:24Z","published":"2025-10-29T14:00:42Z","title":"TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM\n  Persona Simulation","summary":"  Large Language Models (LLMs) are exhibiting emergent human-like abilities and\nare increasingly envisioned as the foundation for simulating an individual's\ncommunication style, behavioral tendencies, and personality traits. However,\ncurrent evaluations of LLM-based persona simulation remain limited: most rely\non synthetic dialogues, lack systematic frameworks, and lack analysis of the\ncapability requirement. To address these limitations, we introduce TwinVoice, a\ncomprehensive benchmark for assessing persona simulation across diverse\nreal-world contexts. TwinVoice encompasses three dimensions: Social Persona\n(public social interactions), Interpersonal Persona (private dialogues), and\nNarrative Persona (role-based expression). It further decomposes the evaluation\nof LLM performance into six fundamental capabilities, including opinion\nconsistency, memory recall, logical reasoning, lexical fidelity, persona tone,\nand syntactic style. Experimental results reveal that while advanced models\nachieve moderate accuracy in persona simulation, they still fall short of\ncapabilities such as syntactic style and memory recall. Consequently, the\naverage performance achieved by LLMs remains considerably below the human\nbaseline.\n","authors":["Bangde Du","Minghao Guo","Songming He","Ziyi Ye","Xi Zhu","Weihang Su","Shuqi Zhu","Yujia Zhou","Yongfeng Zhang","Qingyao Ai","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25536v2.pdf","comment":"Main paper: 11 pages, 3 figures, 6 tables. Appendix: 28 pages. Bangde\n  Du and Minghao Guo contributed equally. Corresponding authors: Ziyi Ye\n  (ziyiye@fudan.edu.cn), Qingyao Ai (aiqy@tsinghua.edu.cn)"},{"id":"http://arxiv.org/abs/2510.24592v2","updated":"2025-10-30T11:15:27Z","published":"2025-10-28T16:22:54Z","title":"ReForm: Reflective Autoformalization with Prospective Bounded Sequence\n  Optimization","summary":"  Autoformalization, which translates natural language mathematics into\nmachine-verifiable formal statements, is critical for using formal mathematical\nreasoning to solve math problems stated in natural language. While Large\nLanguage Models can generate syntactically correct formal statements, they\noften fail to preserve the original problem's semantic intent. This limitation\narises from the LLM approaches' treating autoformalization as a simplistic\ntranslation task which lacks mechanisms for self-reflection and iterative\nrefinement that human experts naturally employ. To address these issues, we\npropose ReForm, a Reflective Autoformalization method that tightly integrates\nsemantic consistency evaluation into the autoformalization process. This\nenables the model to iteratively generate formal statements, assess its\nsemantic fidelity, and self-correct identified errors through progressive\nrefinement. To effectively train this reflective model, we introduce\nProspective Bounded Sequence Optimization (PBSO), which employs different\nrewards at different sequence positions to ensure that the model develops both\naccurate autoformalization and correct semantic validations, preventing\nsuperficial critiques that would undermine the purpose of reflection. Extensive\nexperiments across four autoformalization benchmarks demonstrate that ReForm\nachieves an average improvement of 22.6 percentage points over the strongest\nbaselines. To further ensure evaluation reliability, we introduce\nConsistencyCheck, a benchmark of 859 expert-annotated items that not only\nvalidates LLMs as judges but also reveals that autoformalization is inherently\ndifficult: even human experts produce semantic errors in up to 38.5% of cases.\n","authors":["Guoxin Chen","Jing Wu","Xinjie Chen","Wayne Xin Zhao","Ruihua Song","Chengxi Li","Kai Fan","Dayiheng Liu","Minpeng Liao"],"pdf_url":"https://arxiv.org/pdf/2510.24592v2.pdf","comment":"https://github.com/Chen-GX/ReForm"},{"id":"http://arxiv.org/abs/2510.24817v2","updated":"2025-10-30T11:13:33Z","published":"2025-10-28T10:06:49Z","title":"Towards a Method for Synthetic Generation of Persons with Aphasia\n  Transcripts","summary":"  In aphasia research, Speech-Language Pathologists (SLPs) devote extensive\ntime to manually coding speech samples using Correct Information Units (CIUs),\na measure of how informative an individual sample of speech is. Developing\nautomated systems to recognize aphasic language is limited by data scarcity.\nFor example, only about 600 transcripts are available in AphasiaBank yet\nbillions of tokens are used to train large language models (LLMs). In the\nbroader field of machine learning (ML), researchers increasingly turn to\nsynthetic data when such are sparse. Therefore, this study constructs and\nvalidates two methods to generate synthetic transcripts of the AphasiaBank Cat\nRescue picture description task. One method leverages a procedural programming\napproach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct\nLLMs. The methods generate transcripts across four severity levels (Mild,\nModerate, Severe, Very Severe) through word dropping, filler insertion, and\nparaphasia substitution. Overall, we found, compared to human-elicited\ntranscripts, Mistral 7b Instruct best captures key aspects of linguistic\ndegradation observed in aphasia, showing realistic directional changes in NDW,\nword count, and word length amongst the synthetic generation methods. Based on\nthe results, future work should plan to create a larger dataset, fine-tune\nmodels for better aphasic representation, and have SLPs assess the realism and\nusefulness of the synthetic transcripts.\n","authors":["Jason M. Pittman","Anton Phillips Jr.","Yesenia Medina-Santos","Brielle C. Stark"],"pdf_url":"https://arxiv.org/pdf/2510.24817v2.pdf","comment":"19 pages, 1 figure, 7 tables"},{"id":"http://arxiv.org/abs/2510.26354v1","updated":"2025-10-30T11:05:36Z","published":"2025-10-30T11:05:36Z","title":"On the Role of Context for Discourse Relation Classification in\n  Scientific Writing","summary":"  With the increasing use of generative Artificial Intelligence (AI) methods to\nsupport science workflows, we are interested in the use of discourse-level\ninformation to find supporting evidence for AI generated scientific claims. A\nfirst step towards this objective is to examine the task of inferring discourse\nstructure in scientific writing.\n  In this work, we present a preliminary investigation of pretrained language\nmodel (PLM) and Large Language Model (LLM) approaches for Discourse Relation\nClassification (DRC), focusing on scientific publications, an under-studied\ngenre for this task. We examine how context can help with the DRC task, with\nour experiments showing that context, as defined by discourse structure, is\ngenerally helpful. We also present an analysis of which scientific discourse\nrelation types might benefit most from context.\n","authors":["Stephen Wan","Wei Liu","Michael Strube"],"pdf_url":"https://arxiv.org/pdf/2510.26354v1.pdf","comment":"Accepted at Joint Sixth Workshop on Computational Approaches to\n  Discourse, Context and Document-Level Inferences (CODI 2025) and Eighth\n  Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC\n  2025)"},{"id":"http://arxiv.org/abs/2510.26352v1","updated":"2025-10-30T11:04:15Z","published":"2025-10-30T11:04:15Z","title":"The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic\n  Teams for Multi-Agent Collaboration","summary":"  While a multi-agent approach based on large language models (LLMs) represents\na promising strategy to surpass the capabilities of single models, its success\nis critically dependent on synergistic team composition. However, forming\noptimal teams is a significant challenge, as the inherent opacity of most\nmodels obscures the internal characteristics necessary for effective\ncollaboration. In this paper, we propose an interaction-centric framework for\nautomatic team composition that does not require any prior knowledge including\ntheir internal architectures, training data, or task performances. Our method\nconstructs a \"language model graph\" that maps relationships between models from\nthe semantic coherence of pairwise conversations, and then applies community\ndetection to identify synergistic model clusters. Our experiments with diverse\nLLMs demonstrate that the proposed method discovers functionally coherent\ngroups that reflect their latent specializations. Priming conversations with\nspecific topics identified synergistic teams which outperform random baselines\non downstream benchmarks and achieve comparable accuracy to that of\nmanually-curated teams based on known model specializations. Our findings\nprovide a new basis for the automated design of collaborative multi-agent LLM\nteams.\n","authors":["Kotaro Furuya","Yuichi Kitagawa"],"pdf_url":"https://arxiv.org/pdf/2510.26352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26345v1","updated":"2025-10-30T10:52:43Z","published":"2025-10-30T10:52:43Z","title":"MisSynth: Improving MISSCI Logical Fallacies Classification with\n  Synthetic Data","summary":"  Health-related misinformation is very prevalent and potentially harmful. It\nis difficult to identify, especially when claims distort or misinterpret\nscientific findings. We investigate the impact of synthetic data generation and\nlightweight fine-tuning techniques on the ability of large language models\n(LLMs) to recognize fallacious arguments using the MISSCI dataset and\nframework. In this work, we propose MisSynth, a pipeline that applies\nretrieval-augmented generation (RAG) to produce synthetic fallacy samples,\nwhich are then used to fine-tune an LLM model. Our results show substantial\naccuracy gains with fine-tuned models compared to vanilla baselines. For\ninstance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score\nabsolute improvement on the MISSCI test split over its vanilla baseline. We\ndemonstrate that introducing synthetic fallacy data to augment limited\nannotated resources can significantly enhance zero-shot LLM classification\nperformance on real-world scientific misinformation tasks, even with limited\ncomputational resources. The code and synthetic dataset are available on\nhttps://github.com/mxpoliakov/MisSynth.\n","authors":["Mykhailo Poliakov","Nadiya Shvai"],"pdf_url":"https://arxiv.org/pdf/2510.26345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.21497v2","updated":"2025-10-30T10:49:28Z","published":"2025-05-27T17:58:49Z","title":"Paper2Poster: Towards Multimodal Poster Automation from Scientific\n  Papers","summary":"  Academic poster generation is a crucial yet challenging task in scientific\ncommunication, requiring the compression of long-context interleaved documents\ninto a single, visually coherent page. To address this challenge, we introduce\nthe first benchmark and metric suite for poster generation, which pairs recent\nconference papers with author-designed posters and evaluates outputs on\n(i)Visual Quality-semantic alignment with human posters, (ii)Textual\nCoherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic\nand informational criteria scored by a VLM-as-judge, and notably\n(iv)PaperQuiz-the poster's ability to convey core paper content as measured by\nVLMs answering generated quizzes. Building on this benchmark, we propose\nPosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser\ndistills the paper into a structured asset library; the (b)Planner aligns\ntext-visual pairs into a binary-tree layout that preserves reading order and\nspatial balance; and the (c)Painter-Commenter loop refines each panel by\nexecuting rendering code and using VLM feedback to eliminate overflow and\nensure alignment. In our comprehensive evaluation, we find that GPT-4o\noutputs-though visually appealing at first glance-often exhibit noisy text and\npoor PaperQuiz scores, and we find that reader engagement is the primary\naesthetic bottleneck, as human-designed posters rely largely on visual\nsemantics to convey meaning. Our fully open-source variants (e.g. based on the\nQwen-2.5 series) outperform existing 4o-driven multi-agent systems across\nnearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper\ninto a finalized yet editable .pptx poster - all for just $0.005. These\nfindings chart clear directions for the next generation of fully automated\nposter-generation models. The code and datasets are available at\nhttps://github.com/Paper2Poster/Paper2Poster.\n","authors":["Wei Pang","Kevin Qinghong Lin","Xiangru Jian","Xi He","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2505.21497v2.pdf","comment":"Project Page: https://github.com/Paper2Poster/Paper2Poster"},{"id":"http://arxiv.org/abs/2510.25409v2","updated":"2025-10-30T10:48:05Z","published":"2025-10-29T11:27:08Z","title":"BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic\n  Domains","summary":"  The rapid advancement of large language models(LLMs) has intensified the need\nfor domain and culture specific evaluation. Existing benchmarks are largely\nAnglocentric and domain-agnostic, limiting their applicability to India-centric\ncontexts. To address this gap, we introduce BhashaBench V1, the first\ndomain-specific, multi-task, bilingual benchmark focusing on critical Indic\nknowledge systems. BhashaBench V1 contains 74,166 meticulously curated\nquestion-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from\nauthentic government and domain-specific exams. It spans four major domains:\nAgriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and\ncovering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs\nreveals significant domain and language specific performance gaps, with\nespecially large disparities in low-resource domains. For instance, GPT-4o\nachieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models\nconsistently perform better on English content compared to Hindi across all\ndomains. Subdomain-level analysis shows that areas such as Cyber Law,\nInternational Finance perform relatively well, while Panchakarma, Seed Science,\nand Human Rights remain notably weak. BhashaBench V1 provides a comprehensive\ndataset for evaluating large language models across India's diverse knowledge\ndomains. It enables assessment of models' ability to integrate domain-specific\nknowledge with bilingual understanding. All code, benchmarks, and resources are\npublicly available to support open research.\n","authors":["Vijay Devane","Mohd Nauman","Bhargav Patel","Aniket Mahendra Wakchoure","Yogeshkumar Sant","Shyam Pawar","Viraj Thakur","Ananya Godse","Sunil Patra","Neha Maurya","Suraj Racha","Nitish Kamal Singh","Ajay Nagpal","Piyush Sawarkar","Kundeshwar Vijayrao Pundalik","Rohit Saluja","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2510.25409v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26336v1","updated":"2025-10-30T10:43:40Z","published":"2025-10-30T10:43:40Z","title":"From Amateur to Master: Infusing Knowledge into LLMs via Automated\n  Curriculum Learning","summary":"  Large Language Models (LLMs) excel at general tasks but underperform in\nspecialized domains like economics and psychology, which require deep,\nprincipled understanding. To address this, we introduce ACER (Automated\nCurriculum-Enhanced Regimen) that transforms generalist models into domain\nexperts without sacrificing their broad capabilities. ACER first synthesizes a\ncomprehensive, textbook-style curriculum by generating a table of contents for\na subject and then creating question-answer (QA) pairs guided by Bloom's\ntaxonomy. This ensures systematic topic coverage and progressively increasing\ndifficulty. The resulting synthetic corpus is used for continual pretraining\nwith an interleaved curriculum schedule, aligning learning across both content\nand cognitive dimensions.\n  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized\nMMLU subsets. In challenging domains like microeconomics, where baselines\nstruggle, ACER boosts accuracy by 5 percentage points. Across all target\ndomains, we observe a consistent macro-average improvement of 3 percentage\npoints. Notably, ACER not only prevents catastrophic forgetting but also\nfacilitates positive cross-domain knowledge transfer, improving performance on\nnon-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on\nknowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points,\nwhile maintaining stable performance on general reasoning tasks. Our results\ndemonstrate that ACER offers a scalable and effective recipe for closing\ncritical domain gaps in LLMs.\n","authors":["Nishit Neema","Srinjoy Mukherjee","Sapan Shah","Gokul Ramakrishnan","Ganesh Venkatesh"],"pdf_url":"https://arxiv.org/pdf/2510.26336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09499v3","updated":"2025-10-30T10:21:42Z","published":"2025-03-12T16:03:03Z","title":"MindGYM: What Matters in Question Synthesis for Thinking-Centric\n  Fine-Tuning?","summary":"  Large foundation models face challenges in acquiring transferable, structured\nthinking abilities, especially when supervised with rigid templates or\ncrowd-annotated instruction datasets. Unlike prior approaches, we focus on a\nthinking-centric data synthesis paradigm that enables models to evolve through\nself-generated, cognitively guided data. We propose MindGYM, a structured and\nscalable framework for question synthesis, composed of: (1) Cognitive Thinking\nProcess Injection, which infuses high-level reasoning objectives to shape the\nmodel's synthesis behavior; (2) Seed Single-Hop Question Synthesis, generating\natomic questions from diverse semantic types to encourage broader thinking; and\n(3) Challenging Multi-Hop QA Synthesis, composing more complex multi-hop\nquestions based on QA seeds for deeper reasoning. Detailed analysis shows that\nsynthetic data generated by our method achieves 16.7% higher average quality\nand 67.91% lower quality variance compared to baseline sources, highlighting\nthat both high-quality and self-contained data are essential for effective,\nthinking-oriented fine-tuning. MindGYM improves performance on six reasoning\nbenchmarks, achieving gains of up to 16% on MathVision using only 400 data\nsamples, and generalizable improvements across different model sizes and\narchitectures. MindGYM underscores the viability of self-challenging mechanisms\nin refining large model capabilities while minimizing human intervention and\nresource demands. Code and data are released to promote data-centric research\ninto self-evolving foundation models driven by their internal reasoning\ncapabilities.\n","authors":["Zhe Xu","Daoyuan Chen","Zhenqing Ling","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2503.09499v3.pdf","comment":"Accepted by NeurIPS'25. 30 pages, 2 figures, 13 tables"},{"id":"http://arxiv.org/abs/2510.26322v1","updated":"2025-10-30T10:17:05Z","published":"2025-10-30T10:17:05Z","title":"SCRIBE: Structured Chain Reasoning for Interactive Behaviour\n  Explanations using Tool Calling","summary":"  Language models can be used to provide interactive, personalized student\nfeedback in educational settings. However, real-world deployment faces three\nkey challenges: privacy concerns, limited computational resources, and the need\nfor pedagogically valid responses. These constraints require small, open-source\nmodels that can run locally and reliably ground their outputs in correct\ninformation. We introduce SCRIBE, a framework for multi-hop, tool-augmented\nreasoning designed to generate valid responses to student questions about\nfeedback reports. SCRIBE combines domain-specific tools with a self-reflective\ninference pipeline that supports iterative reasoning, tool use, and error\nrecovery. We distil these capabilities into 3B and 8B models via two-stage LoRA\nfine-tuning on synthetic GPT-4o-generated data. Evaluation with a human-aligned\nGPT-Judge and a user study with 108 students shows that 8B-SCRIBE models\nachieve comparable or superior quality to much larger models in key dimensions\nsuch as relevance and actionability, while being perceived on par with GPT-4o\nand Llama-3.3 70B by students. These findings demonstrate the viability of\nSCRIBE for low-resource, privacy-sensitive educational applications.\n","authors":["Fares Fawzi","Vinitra Swamy","Dominik Glandorf","Tanya Nazaretsky","Tanja KÃ¤ser"],"pdf_url":"https://arxiv.org/pdf/2510.26322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.18915v3","updated":"2025-10-30T10:00:05Z","published":"2025-10-21T06:14:40Z","title":"UNO-Bench: A Unified Benchmark for Exploring the Compositional Law\n  Between Uni-modal and Omni-modal in Omni Models","summary":"  Multimodal Large Languages models have been progressing from uni-modal\nunderstanding toward unifying visual, audio and language modalities,\ncollectively termed omni models. However, the correlation between uni-modal and\nomni-modal remains unclear, which requires comprehensive evaluation to drive\nomni model's intelligence evolution. In this work, we introduce a novel,\nhigh-quality, and UNified Omni model benchmark, UNO-Bench. This benchmark is\ndesigned to effectively evaluate both UNi-modal and Omni-modal capabilities\nunder a unified ability taxonomy, spanning 44 task types and 5 modality\ncombinations. It includes 1250 human curated samples for omni-modal with 98%\ncross-modality solvability, and 2480 enhanced uni-modal samples. The\nhuman-generated dataset is well-suited to real-world scenarios, particularly\nwithin the Chinese context, whereas the automatically compressed dataset offers\na 90% increase in speed and maintains 98% consistency across 18 public\nbenchmarks. In addition to traditional multi-choice questions, we propose an\ninnovative multi-step open-ended question format to assess complex reasoning. A\ngeneral scoring model is incorporated, supporting 6 question types for\nautomated evaluation with 95% accuracy. Experimental result shows the\nCompositional Law between omni-modal and uni-modal performance and the\nomni-modal capability manifests as a bottleneck effect on weak models, while\nexhibiting synergistic promotion on strong models.\n","authors":["Chen Chen","ZeYang Hu","Fengjiao Chen","Liya Ma","Jiaxing Liu","Xiaoyu Li","Ziwen Wang","Xuezhi Cao","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2510.18915v3.pdf","comment":"v3: Switch the paper template. Work in progress. Github:\n  https://github.com/meituan-longcat/UNO-Bench Hugging Face:\n  https://huggingface.co/datasets/meituan-longcat/UNO-Bench"},{"id":"http://arxiv.org/abs/2507.16271v2","updated":"2025-10-30T09:57:54Z","published":"2025-07-22T06:37:51Z","title":"Beyond Isolated Dots: Benchmarking Structured Table Construction as Deep\n  Knowledge Extraction","summary":"  With the emergence of large language models (LLMs), there is an expectation\nthat LLMs can effectively extract explicit information from complex real-world\ndocuments (e.g., papers, reports). However, most LLMs generate paragraph-style\nanswers that are chaotic, disorganized, and untraceable. To bridge this gap, we\nintroduce the Arranged and Organized Extraction Benchmark (AOE), a new\nbilingual benchmark with data and documents of varying lengths designed to\nsystematically evaluate the ability of LLMs to comprehend fragmented documents\nand reconstruct isolated information into one organized table. Unlike\nconventional text-to-table tasks, which rely on fixed schema and narrow task\ndomains, AOE includes 11 carefully crafted tasks across three diverse domains,\nrequiring models to generate context-specific schema tailored to varied input\nqueries. In the experiment, we evaluated both open-source and closed-source\nstate-of-the-art LLMs. The results show that even the most advanced models\nstruggled significantly. The benchmark is available at\nhttps://anonymous.4open.science/r/AOE-Benchmark/.\n","authors":["Tianyun Zhong","Guozhao Mo","Yanjiang Liu","Yihan Chen","Lingdi Kong","Xuanang Chen","Yaojie Lu","Hongyu Lin","Shiwei Ye","Xianpei Han","Ben He","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2507.16271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08410v2","updated":"2025-10-30T09:54:22Z","published":"2025-06-10T03:30:10Z","title":"Large Language Models Have Intrinsic Meta-Cognition, but Need a Good\n  Lens","summary":"  Previous research has primarily focused on the cognitive error detection\ncapabilities of Large Language Models (LLMs), often prompting them to analyze\nmistakes in reasoning chains. However, few studies have examined the\nmeta-cognitive abilities of LLMs (e.g., their self-awareness of step errors),\nwhich are crucial for their reliability. While studies on LLM self-evaluation\npresent some measures, such as perplexity, which can reflect the answer\ncorrectness and be viewed as the lens of meta-cognition, they lack step-level\nanalysis and adaptation. This paper studies the evaluation of LLM\nmeta-cognition using the current lenses and how to improve these lenses.\nSpecifically, we propose AutoMeco, an Automated Meta-cognition Evaluation\nframework for benchmarking the existing lenses. Furthermore, a training-free\nMarkovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost\ncurrent meta-cognition lenses. Experimental results on three mathematical\nreasoning datasets and three LLMs show the reasonableness of AutoMeco by\ncomparing it with Best-of-N verification. Moreover, the meta-cognition ability\nof LLMs can be better evaluated using MIRA.\n","authors":["Ziyang Ma","Qingyue Yuan","Zhenglin Wang","Deyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.08410v2.pdf","comment":"Accepted to EMNLP 2025"},{"id":"http://arxiv.org/abs/2411.10573v3","updated":"2025-10-30T09:42:47Z","published":"2024-11-15T20:46:58Z","title":"Hysteresis Activation Function for Efficient Inference","summary":"  The widely used ReLU is favored for its hardware efficiency, {as the\nimplementation at inference is a one bit sign case,} yet suffers from issues\nsuch as the ``dying ReLU'' problem, where during training, neurons fail to\nactivate and constantly remain at zero, as highlighted by Lu et al. Traditional\napproaches to mitigate this issue often introduce more complex and less\nhardware-friendly activation functions. In this work, we propose a Hysteresis\nRectified Linear Unit (HeLU), an efficient activation function designed to\naddress the ``dying ReLU'' problem with minimal complexity. Unlike traditional\nactivation functions with fixed thresholds for training and inference, HeLU\nemploys a variable threshold that refines the backpropagation. This refined\nmechanism allows simpler activation functions to achieve competitive\nperformance comparable to their more complex counterparts without introducing\nunnecessary complexity or requiring inductive biases. Empirical evaluations\ndemonstrate that HeLU enhances model generalization across diverse datasets,\noffering a promising solution for efficient and effective inference suitable\nfor a wide range of neural network architectures.\n","authors":["Moshe Kimhi","Idan Kashani","Avi Mendelson","Chaim Baskin"],"pdf_url":"https://arxiv.org/pdf/2411.10573v3.pdf","comment":"Accepted to 4th NeurIPS Efficient Natural Language and Speech\n  Processing Workshop (ENLSP-IV 2024)"},{"id":"http://arxiv.org/abs/2510.26298v1","updated":"2025-10-30T09:35:51Z","published":"2025-10-30T09:35:51Z","title":"Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in\n  Web Games","summary":"  OpenAI's ChatGPT Atlas introduces new capabilities for web interaction,\nenabling the model to analyze webpages, process user intents, and execute\ncursor and keyboard inputs directly within the browser. While its capacity for\ninformation retrieval tasks has been demonstrated, its performance in dynamic,\ninteractive environments remains less explored. In this study, we conduct an\nearly evaluation of Atlas's web interaction capabilities using browser-based\ngames as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird,\nand Stein.world. We employ in-game performance scores as quantitative metrics\nto assess performance across different task types. Our results show that Atlas\nperforms strongly in logical reasoning tasks like Sudoku, completing puzzles\nsignificantly faster than human baselines, but struggles substantially in\nreal-time games requiring precise timing and motor control, often failing to\nprogress beyond initial obstacles. These findings suggest that while Atlas\ndemonstrates capable analytical processing, there remain notable limitations in\ndynamic web environments requiring real-time interaction. The website of our\nproject can be found at https://atlas-game-eval.github.io.\n","authors":["Jingran Zhang","Ning Li","Justin Cui"],"pdf_url":"https://arxiv.org/pdf/2510.26298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04380v3","updated":"2025-10-30T09:16:49Z","published":"2025-02-05T17:21:01Z","title":"Diversity as a Reward: Fine-Tuning LLMs on a Mixture of\n  Domain-Undetermined Data","summary":"  Fine-tuning large language models (LLMs) using diverse datasets is crucial\nfor enhancing their overall performance across various domains. In practical\nscenarios, existing methods based on modeling the mixture proportions of data\ncomposition often struggle with data whose domain labels are missing, imprecise\nor non-normalized, while methods based on data selection usually encounter\ndifficulties in balancing multi-domain performance. To address these\nchallenges, in this work, we investigate the role of data diversity in\nenhancing the overall abilities of LLMs by empirically constructing contrastive\ndata pools and theoretically deriving explanations. Building upon the insights\ngained, we propose a new method that gives the LLM a dual identity: an output\nmodel to cognitively probe and select data based on diversity reward, as well\nas an input model to be tuned with the selected data. Extensive experiments\nshow that the proposed method notably boosts performance across\ndomain-undetermined data and a series of foundational downstream tasks when\napplied to various advanced LLMs. We release our code and hope this study can\nshed light on the understanding of data diversity and advance feedback-driven\ndata-model co-design for LLMs.\n","authors":["Zhenqing Ling","Daoyuan Chen","Liuyi Yao","Qianli Shen","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2502.04380v3.pdf","comment":"Accepted by NeurIPS'25 main track. 47 pages, 21 figures, 32 tables"},{"id":"http://arxiv.org/abs/2510.26285v1","updated":"2025-10-30T09:08:50Z","published":"2025-10-30T09:08:50Z","title":"Unravelling the Mechanisms of Manipulating Numbers in Language Models","summary":"  Recent work has shown that different large language models (LLMs) converge to\nsimilar and accurate input embedding representations for numbers. These\nfindings conflict with the documented propensity of LLMs to produce erroneous\noutputs when dealing with numeric information. In this work, we aim to explain\nthis conflict by exploring how language models manipulate numbers and quantify\nthe lower bounds of accuracy of these mechanisms. We find that despite\nsurfacing errors, different language models learn interchangeable\nrepresentations of numbers that are systematic, highly accurate and universal\nacross their hidden states and the types of input contexts. This allows us to\ncreate universal probes for each LLM and to trace information -- including the\ncauses of output errors -- to specific layers. Our results lay a fundamental\nunderstanding of how pre-trained LLMs manipulate numbers and outline the\npotential of more accurate probing techniques in addressed refinements of LLMs'\narchitectures.\n","authors":["Michal Å tefÃ¡nik","Timothee Mickus","Marek KadlÄÃ­k","Bertram HÃ¸jer","Michal Spiegel","RaÃºl VÃ¡zquez","Aman Sinha","Josef KuchaÅ","Philipp Mondorf"],"pdf_url":"https://arxiv.org/pdf/2510.26285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26277v1","updated":"2025-10-30T08:59:56Z","published":"2025-10-30T08:59:56Z","title":"Do LLMs Signal When They're Right? Evidence from Neuron Agreement","summary":"  Large language models (LLMs) commonly boost reasoning via\nsample-evaluate-ensemble decoders, achieving label free gains without ground\ntruth. However, prevailing strategies score candidates using only external\noutputs such as token probabilities, entropies, or self evaluations, and these\nsignals can be poorly calibrated after post training. We instead analyze\ninternal behavior based on neuron activations and uncover three findings: (1)\nexternal signals are low dimensional projections of richer internal dynamics;\n(2) correct responses activate substantially fewer unique neurons than\nincorrect ones throughout generation; and (3) activations from correct\nresponses exhibit stronger cross sample agreement, whereas incorrect ones\ndiverge. Motivated by these observations, we propose Neuron Agreement Decoding\n(NAD), an unsupervised best-of-N method that selects candidates using\nactivation sparsity and cross sample neuron agreement, operating solely on\ninternal signals and without requiring comparable textual outputs. NAD enables\nearly correctness prediction within the first 32 generated tokens and supports\naggressive early stopping. Across math and science benchmarks with verifiable\nanswers, NAD matches majority voting; on open ended coding benchmarks where\nmajority voting is inapplicable, NAD consistently outperforms Avg@64. By\npruning unpromising trajectories early, NAD reduces token usage by 99% with\nminimal loss in generation quality, showing that internal signals provide\nreliable, scalable, and efficient guidance for label free ensemble decoding.\n","authors":["Kang Chen","Yaoning Wang","Kai Xiong","Zhuoka Feng","Wenhe Sun","Haotian Chen","Yixin Cao"],"pdf_url":"https://arxiv.org/pdf/2510.26277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05747v4","updated":"2025-10-30T08:59:39Z","published":"2025-04-08T07:24:51Z","title":"SEA-LION: Southeast Asian Languages in One Network","summary":"  Recently, Large Language Models (LLMs) have dominated much of the artificial\nintelligence scene with their ability to process and generate natural\nlanguages. However, the majority of LLM research and development remains\nEnglish-centric, leaving low-resource languages such as those in the Southeast\nAsian (SEA) region under-represented. To address this representation gap, we\nintroduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge\nmultilingual LLMs designed for SEA languages. The SEA-LION family of LLMs\nsupports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese,\nMalay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages\nlarge-scale multilingual continued pre-training with a comprehensive\npost-training regime involving multiple stages of instruction fine-tuning,\nalignment, and model merging. Evaluation results on multilingual benchmarks\nindicate that our models achieve state-of-the-art performance across LLMs\nsupporting SEA languages. We open-source the models to benefit the wider SEA\ncommunity.\n","authors":["Raymond Ng","Thanh Ngan Nguyen","Yuli Huang","Ngee Chia Tai","Wai Yi Leong","Wei Qi Leong","Xianbin Yong","Jian Gang Ngui","Yosephine Susanto","Nicholas Cheng","Hamsawardhini Rengarajan","Peerat Limkonchotiwat","Adithya Venkatadri Hulagadri","Kok Wai Teng","Yeo Yeow Tong","Bryan Siow","Wei Yi Teo","Wayne Lau","Choon Meng Tan","Brandon Ong","Zhi Hao Ong","Jann Railey Montalan","Adwin Chan","Sajeban Antonyrex","Ren Lee","Esther Choa","David Ong Tat-Wee","Bing Jie Darius Liu","William Chandra Tjhi","Erik Cambria","Leslie Teo"],"pdf_url":"https://arxiv.org/pdf/2504.05747v4.pdf","comment":"Accepted at IJCNLP-AACL 2025 (Main Track). We released our model at\n  https://huggingface.co/collections/aisingapore/sea-lionv3-672589a39cdadd6a5b199581"},{"id":"http://arxiv.org/abs/2510.26274v1","updated":"2025-10-30T08:58:44Z","published":"2025-10-30T08:58:44Z","title":"PVMark: Enabling Public Verifiability for LLM Watermarking Schemes","summary":"  Watermarking schemes for large language models (LLMs) have been proposed to\nidentify the source of the generated text, mitigating the potential threats\nemerged from model theft. However, current watermarking solutions hardly\nresolve the trust issue: the non-public watermark detection cannot prove itself\nfaithfully conducting the detection. We observe that it is attributed to the\nsecret key mostly used in the watermark detection -- it cannot be public, or\nthe adversary may launch removal attacks provided the key; nor can it be\nprivate, or the watermarking detection is opaque to the public. To resolve the\ndilemma, we propose PVMark, a plugin based on zero-knowledge proof (ZKP),\nenabling the watermark detection process to be publicly verifiable by third\nparties without disclosing any secret key. PVMark hinges upon the proof of\n`correct execution' of watermark detection on which a set of ZKP constraints\nare built, including mapping, random number generation, comparison, and\nsummation. We implement multiple variants of PVMark in Python, Rust and Circom,\ncovering combinations of three watermarking schemes, three hash functions, and\nfour ZKP protocols, to show our approach effectively works under a variety of\ncircumstances. By experimental results, PVMark efficiently enables public\nverifiability on the state-of-the-art LLM watermarking schemes yet without\ncompromising the watermarking performance, promising to be deployed in\npractice.\n","authors":["Haohua Duan","Liyao Xiang","Xin Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26274v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2510.26271v1","updated":"2025-10-30T08:56:06Z","published":"2025-10-30T08:56:06Z","title":"Distilling Multilingual Vision-Language Models: When Smaller Models Stay\n  Multilingual","summary":"  Vision-language models (VLMs) exhibit uneven performance across languages, a\nproblem that is often exacerbated when the model size is reduced. While\nKnowledge distillation (KD) demonstrates promising results in transferring\nknowledge from larger to smaller VLMs, applying KD in multilingualism is an\nunderexplored area. This paper presents a controlled empirical study of KD\nbehavior across five distillation approaches, isolating their effects on\ncross-lingual representation consistency and downstream performance stability\nunder model compression. We study five distillation formulations across CLIP\nand SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual\nQA. We find that some configurations preserve or even improve multilingual\nretrieval robustness despite halving model size, but others fail to maintain\ncross-task stability, exposing design-sensitive trade-offs that aggregate\naccuracy alone does not reveal.\n","authors":["Sukrit Sriratanawilai","Jhayahgrit Thongwat","Romrawin Chumpu","Patomporn Payoungkhamdee","Sarana Nutanong","Peerat Limkonchotiwat"],"pdf_url":"https://arxiv.org/pdf/2510.26271v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2510.25160v2","updated":"2025-10-30T08:52:17Z","published":"2025-10-29T04:29:17Z","title":"Model-Document Protocol for AI Search","summary":"  AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation.\n","authors":["Hongjin Qian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25160v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2510.18480v2","updated":"2025-10-30T08:46:37Z","published":"2025-10-21T10:00:32Z","title":"How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices","summary":"  Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs.\n","authors":["Han Peng","Peiyu Liu","Zican Dong","Daixuan Cheng","Junyi Li","Yiru Tang","Shuo Wang","Wayne Xin Zhao"],"pdf_url":"https://arxiv.org/pdf/2510.18480v2.pdf","comment":"Withdrawn by the authors to better delineate the related work from\n  the paper's original contributions"},{"id":"http://arxiv.org/abs/2506.04721v2","updated":"2025-10-30T08:43:19Z","published":"2025-06-05T07:51:23Z","title":"SPARTA ALIGNMENT: Collectively Aligning Multiple Language Models through\n  Combat","summary":"  We propose SPARTA ALIGNMENT, an algorithm to collectively align multiple LLMs\nthrough competition and combat. To complement a single model's lack of\ndiversity in generation and biases in evaluation, multiple LLMs form a \"sparta\ntribe\" to compete against each other in fulfilling instructions while serving\nas judges for the competition of others. For each iteration, one instruction\nand two models are selected for a duel, the other models evaluate the two\nresponses, and their evaluation scores are aggregated through a adapted\nelo-ranking based reputation system, where winners/losers of combat gain/lose\nweight in evaluating others. The peer-evaluated combat results then become\npreference pairs where the winning response is preferred over the losing one,\nand all models learn from these preferences at the end of each iteration.\nSPARTA ALIGNMENT enables the self-evolution of multiple LLMs in an iterative\nand collective competition process. Extensive experiments demonstrate that\nSPARTA ALIGNMENT outperforms initial models and 4 self-alignment baselines\nacross 10 out of 12 tasks and datasets with 7.0% average improvement. Further\nanalysis reveals that SPARTA ALIGNMENT generalizes more effectively to unseen\ntasks and leverages the expertise diversity of participating models to produce\nmore logical, direct and informative outputs.\n","authors":["Yuru Jiang","Wenxuan Ding","Shangbin Feng","Greg Durrett","Yulia Tsvetkov"],"pdf_url":"https://arxiv.org/pdf/2506.04721v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26254v1","updated":"2025-10-30T08:36:07Z","published":"2025-10-30T08:36:07Z","title":"Language Models Are Borrowing-Blind: A Multilingual Evaluation of\n  Loanword Identification across 10 Languages","summary":"  Throughout language history, words are borrowed from one language to another\nand gradually become integrated into the recipient's lexicon. Speakers can\noften differentiate these loanwords from native vocabulary, particularly in\nbilingual communities where a dominant language continuously imposes lexical\nitems on a minority language. This paper investigates whether pretrained\nlanguage models, including large language models, possess similar capabilities\nfor loanword identification. We evaluate multiple models across 10 languages.\nDespite explicit instructions and contextual information, our results show that\nmodels perform poorly in distinguishing loanwords from native ones. These\nfindings corroborate previous evidence that modern NLP systems exhibit a bias\ntoward loanwords rather than native equivalents. Our work has implications for\ndeveloping NLP tools for minority languages and supporting language\npreservation in communities under lexical pressure from dominant languages.\n","authors":["MÃ©rilin Sousa Silva","Sina Ahmadi"],"pdf_url":"https://arxiv.org/pdf/2510.26254v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2510.26253v1","updated":"2025-10-30T08:35:52Z","published":"2025-10-30T08:35:52Z","title":"Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs","summary":"  The ability to accurately interpret implied meanings plays a crucial role in\nhuman communication and language use, and language models are also expected to\npossess this capability. This study demonstrates that providing language models\nwith pragmatic theories as prompts is an effective in-context learning approach\nfor tasks to understand implied meanings. Specifically, we propose an approach\nin which an overview of pragmatic theories, such as Gricean pragmatics and\nRelevance Theory, is presented as a prompt to the language model, guiding it\nthrough a step-by-step reasoning process to derive a final interpretation.\nExperimental results showed that, compared to the baseline, which prompts\nintermediate reasoning without presenting pragmatic theories (0-shot\nChain-of-Thought), our methods enabled language models to achieve up to 9.6\\%\nhigher scores on pragmatic reasoning tasks. Furthermore, we show that even\nwithout explaining the details of pragmatic theories, merely mentioning their\nnames in the prompt leads to a certain performance improvement (around 1-3%) in\nlarger models compared to the baseline.\n","authors":["Takuma Sato","Seiya Kawano","Koichiro Yoshino"],"pdf_url":"https://arxiv.org/pdf/2510.26253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13229v3","updated":"2025-10-30T08:26:57Z","published":"2025-06-16T08:28:19Z","title":"IGD: Token Decisiveness Modeling via Information Gain in LLMs for\n  Personalized Recommendation","summary":"  Large Language Models (LLMs) have shown strong potential for recommendation\nby framing item prediction as a token-by-token language generation task.\nHowever, existing methods treat all item tokens equally, simply pursuing\nlikelihood maximization during both optimization and decoding. This overlooks\ncrucial token-level differences in decisiveness-many tokens contribute little\nto item discrimination yet can dominate optimization or decoding. To quantify\ntoken decisiveness, we propose a novel perspective that models item generation\nas a decision process, measuring token decisiveness by the Information Gain\n(IG) each token provides in reducing uncertainty about the generated item. Our\nempirical analysis reveals that most tokens have low IG but often correspond to\nhigh logits, disproportionately influencing training loss and decoding, which\nmay impair model performance. Building on these insights, we introduce an\nInformation Gain-based Decisiveness-aware Token handling (IGD) strategy that\nintegrates token decisiveness into both tuning and decoding. Specifically, IGD\ndownweights low-IG tokens during tuning and rebalances decoding to emphasize\ntokens with high IG. In this way, IGD moves beyond pure likelihood\nmaximization, effectively prioritizing high-decisiveness tokens. Extensive\nexperiments on four benchmark datasets with two LLM backbones demonstrate that\nIGD consistently improves recommendation accuracy, achieving significant gains\non widely used ranking metrics compared to strong baselines.\n","authors":["Zijie Lin","Yang Zhang","Xiaoyan Zhao","Fengbin Zhu","Fuli Feng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2506.13229v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26241v1","updated":"2025-10-30T08:21:50Z","published":"2025-10-30T08:21:50Z","title":"Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for\n  Vision-Language Models","summary":"  Modern vision-language models (VLMs) excel at many multimodal tasks, yet\ntheir grasp of temporal information in video remains weak and, crucially,\nunder-evaluated. We probe this gap with a deceptively simple but revealing\nchallenge: judging the arrow of time (AoT)-whether a short clip is played\nforward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated\nbenchmark that tests whether VLMs can infer temporal direction in natural\nvideos using the same stimuli and behavioral baselines established for humans.\nOur comprehensive evaluation of open-weight and proprietary, reasoning and\nnon-reasoning VLMs reveals that most models perform near chance, and even the\nbest lag far behind human accuracy on physically irreversible processes (e.g.,\nfree fall, diffusion/explosion) and causal manual actions (division/addition)\nthat humans recognize almost instantly. These results highlight a fundamental\ngap in current multimodal systems: while they capture rich visual-semantic\ncorrelations, they lack the inductive biases required for temporal continuity\nand causal understanding. We release the code and data for AoT-PsyPhyBENCH to\nencourage further progress in the physical and temporal reasoning capabilities\nof VLMs.\n","authors":["Shiho Matta","Lis Kanashiro Pereira","Peitao Han","Fei Cheng","Shigeru Kitazawa"],"pdf_url":"https://arxiv.org/pdf/2510.26241v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2505.24388v2","updated":"2025-10-30T07:48:31Z","published":"2025-05-30T09:18:08Z","title":"ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and\n  Optimization for Retrieval-Augmented Generation","summary":"  Retrieval-Augmented Generation (RAG) augments Large Language Models (LLMs)\nwith external knowledge to improve factuality. However, existing RAG systems\nfrequently underutilize the retrieved documents, failing to extract and\nintegrate the key clues needed to support faithful and interpretable reasoning,\nespecially in cases where relevant evidence is implicit, scattered, or obscured\nby noise. To address this issue, we propose ClueAnchor, a novel framework for\nenhancing RAG via clue-anchored reasoning exploration and optimization.\nClueAnchor extracts key clues from retrieved content and generates multiple\nreasoning paths based on different knowledge configurations, optimizing the\nmodel by selecting the most appropriate reasoning path for the given context\nthrough reward-based preference optimization. Experiments show that ClueAnchor\nsignificantly outperforms prior RAG baselines in the completeness and\nrobustness of reasoning. Further analysis confirms its strong resilience to\nnoisy or partially relevant retrieved content, as well as its capability to\nidentify supporting evidence even in the absence of explicit clue supervision\nduring inference. All codes are available at\nhttps://github.com/thunlp/ClueAnchor.\n","authors":["Hao Chen","Yukun Yan","Sen Mei","Wanxiang Che","Zhenghao Liu","Qi Shi","Xinze Li","Yuchun Fan","Pengcheng Huang","Qiushi Xiong","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2505.24388v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26205v1","updated":"2025-10-30T07:29:14Z","published":"2025-10-30T07:29:14Z","title":"Towards Global Retrieval Augmented Generation: A Benchmark for\n  Corpus-Level Reasoning","summary":"  Retrieval-augmented generation (RAG) has emerged as a leading approach to\nreducing hallucinations in large language models (LLMs). Current RAG evaluation\nbenchmarks primarily focus on what we call local RAG: retrieving relevant\nchunks from a small subset of documents to answer queries that require only\nlocalized understanding within specific text chunks. However, many real-world\napplications require a fundamentally different capability -- global RAG --\nwhich involves aggregating and analyzing information across entire document\ncollections to derive corpus-level insights (for example, \"What are the top 10\nmost cited papers in 2023?\"). In this paper, we introduce GlobalQA -- the first\nbenchmark specifically designed to evaluate global RAG capabilities, covering\nfour core task types: counting, extremum queries, sorting, and top-k\nextraction. Through systematic evaluation across different models and\nbaselines, we find that existing RAG methods perform poorly on global tasks,\nwith the strongest baseline achieving only 1.51 F1 score. To address these\nchallenges, we propose GlobalRAG, a multi-tool collaborative framework that\npreserves structural coherence through chunk-level retrieval, incorporates\nLLM-driven intelligent filters to eliminate noisy documents, and integrates\naggregation modules for precise symbolic computation. On the Qwen2.5-14B model,\nGlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1,\nvalidating the effectiveness of our method.\n","authors":["Qi Luo","Xiaonan Li","Tingshuo Fan","Xinchi Chen","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2510.26205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26202v1","updated":"2025-10-30T07:25:10Z","published":"2025-10-30T07:25:10Z","title":"What's In My Human Feedback? Learning Interpretable Descriptions of\n  Preference Data","summary":"  Human feedback can alter language models in unpredictable and undesirable\nways, as practitioners lack a clear understanding of what feedback data\nencodes. While prior work studies preferences over certain attributes (e.g.,\nlength or sycophancy), automatically extracting relevant features without\npre-specifying hypotheses remains challenging. We introduce What's In My Human\nFeedback? (WIMHF), a method to explain feedback data using sparse autoencoders.\nWIMHF characterizes both (1) the preferences a dataset is capable of measuring\nand (2) the preferences that the annotators actually express. Across 7\ndatasets, WIMHF identifies a small number of human-interpretable features that\naccount for the majority of the preference prediction signal achieved by\nblack-box models. These features reveal a wide diversity in what humans prefer,\nand the role of dataset-level context: for example, users on Reddit prefer\ninformality and jokes, while annotators in HH-RLHF and PRISM disprefer them.\nWIMHF also surfaces potentially unsafe preferences, such as that LMArena users\ntend to vote against refusals, often in favor of toxic content. The learned\nfeatures enable effective data curation: re-labeling the harmful examples in\nArena yields large safety gains (+37%) with no cost to general performance.\nThey also allow fine-grained personalization: on the Community Alignment\ndataset, we learn annotator-specific weights over subjective features that\nimprove preference prediction. WIMHF provides a human-centered analysis method\nfor practitioners to better understand and use preference data.\n","authors":["Rajiv Movva","Smitha Milli","Sewon Min","Emma Pierson"],"pdf_url":"https://arxiv.org/pdf/2510.26202v1.pdf","comment":"Code: https://github.com/rmovva/wimhf"},{"id":"http://arxiv.org/abs/2510.26200v1","updated":"2025-10-30T07:21:05Z","published":"2025-10-30T07:21:05Z","title":"Don't Let It Fade: Preserving Edits in Diffusion Language Models via\n  Token Timestep Allocation","summary":"  While diffusion language models (DLMs) enable fine-grained refinement, their\npractical controllability remains fragile. We identify and formally\ncharacterize a central failure mode called update forgetting, in which uniform\nand context agnostic updates induce token level fluctuations across timesteps,\nerasing earlier semantic edits and disrupting the cumulative refinement\nprocess, thereby degrading fluency and coherence. As this failure originates in\nuniform and context agnostic updates, effective control demands explicit token\nordering. We propose Token Timestep Allocation (TTA), which realizes soft and\nsemantic token ordering via per token timestep schedules: critical tokens are\nfrozen early, while uncertain tokens receive continued refinement. This\ntimestep based ordering can be instantiated as either a fixed policy or an\nadaptive policy driven by task signals, thereby supporting a broad spectrum of\nrefinement strategies. Because it operates purely at inference time, it applies\nuniformly across various DLMs and naturally extends to diverse supervision\nsources. Empirically, TTA improves controllability and fluency: on sentiment\ncontrol, it yields more than 20 percent higher accuracy and nearly halves\nperplexity using less than one fifth the steps; in detoxification, it lowers\nmaximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0).\nTogether, these results demonstrate that softened ordering via timestep\nallocation is the critical lever for mitigating update forgetting and achieving\nstable and controllable diffusion text generation.\n","authors":["Woojin Kim","Jaeyoung Do"],"pdf_url":"https://arxiv.org/pdf/2510.26200v1.pdf","comment":"Accepted in NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.15095v2","updated":"2025-10-30T07:18:23Z","published":"2025-05-21T04:34:22Z","title":"Nek Minit: Harnessing Pragmatic Metacognitive Prompting for Explainable\n  Sarcasm Detection of Australian and Indian English","summary":"  Sarcasm is a challenge to sentiment analysis because of the incongruity\nbetween stated and implied sentiment. The challenge is exacerbated when the\nimplication may be relevant to a specific country or geographical region.\nPragmatic metacognitive prompting (PMP) is a cognition-inspired technique that\nhas been used for pragmatic reasoning. In this paper, we harness PMP for\nexplainable sarcasm detection for Australian and Indian English, alongside a\nbenchmark dataset for standard English. We manually add sarcasm explanations to\nan existing sarcasm-labeled dataset for Australian and Indian English called\nBESSTIE, and compare the performance for explainable sarcasm detection for them\nwith FLUTE, a standard English dataset containing sarcasm explanations. Our\napproach utilising PMP when evaluated on two open-weight LLMs (GEMMA and LLAMA)\nachieves statistically significant performance improvement across all tasks and\ndatasets when compared with four alternative prompting strategies. We also find\nthat alternative techniques such as agentic prompting mitigate context-related\nfailures by enabling external knowledge retrieval. The focused contribution of\nour work is utilising PMP in generating sarcasm explanations for varieties of\nEnglish.\n","authors":["Ishmanbir Singh","Dipankar Srirag","Aditya Joshi"],"pdf_url":"https://arxiv.org/pdf/2505.15095v2.pdf","comment":"ALTA 2025 (Best Paper Honorable Mention). Camera-ready"},{"id":"http://arxiv.org/abs/2510.26193v1","updated":"2025-10-30T07:06:47Z","published":"2025-10-30T07:06:47Z","title":"RCScore: Quantifying Response Consistency in Large Language Models","summary":"  Current LLM evaluations often rely on a single instruction template,\noverlooking models' sensitivity to instruction style-a critical aspect for\nreal-world deployments. We present RCScore, a multi-dimensional framework\nquantifying how instruction formulation affects model responses. By\nsystematically transforming benchmark problems into multiple instruction\nstyles, RCScore reveals performance variations undetected by conventional\nmetrics. Our experiments across ten LLMs on four reasoning benchmarks\ndemonstrate that instruction style can shift accuracy by up to 16.7% points. We\nintroduce Cross-Response Similarity (CRS), a method applying RCScore metrics to\nmeasure stylistic self-consistency, and establish its strong correlation with\ntask accuracy, suggesting consistency as a valuable proxy for model\nreliability. Additional findings show that deterministic decoding produces more\nstylistically stable outputs, and model scale correlates positively with\ncross-style consistency. RCScore offers a principled approach to assess\ninstruction robustness.\n","authors":["Dongjun Jang","Youngchae Ahn","Hyopil Shin"],"pdf_url":"https://arxiv.org/pdf/2510.26193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26190v1","updated":"2025-10-30T06:57:07Z","published":"2025-10-30T06:57:07Z","title":"SP-MCQA: Evaluating Intelligibility of TTS Beyond the Word Level","summary":"  The evaluation of intelligibility for TTS has reached a bottleneck, as\nexisting assessments heavily rely on word-by-word accuracy metrics such as WER,\nwhich fail to capture the complexity of real-world speech or reflect human\ncomprehension needs. To address this, we propose Spoken-Passage Multiple-Choice\nQuestion Answering, a novel subjective approach evaluating the accuracy of key\ninformation in synthesized speech, and release SP-MCQA-Eval, an 8.76-hour\nnews-style benchmark dataset for SP-MCQA evaluation. Our experiments reveal\nthat low WER does not necessarily guarantee high key-information accuracy,\nexposing a gap between traditional metrics and practical intelligibility.\nSP-MCQA shows that even state-of-the-art (SOTA) models still lack robust text\nnormalization and phonetic accuracy. This work underscores the urgent need for\nhigh-level, more life-like evaluation criteria now that many systems already\nexcel at WER yet may fall short on real-world intelligibility.\n","authors":["Hitomi Jin Ling Tee","Chaoren Wang","Zijie Zhang","Zhizheng Wu"],"pdf_url":"https://arxiv.org/pdf/2510.26190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.16648v2","updated":"2025-10-30T06:55:22Z","published":"2025-09-20T11:50:22Z","title":"FESTA: Functionally Equivalent Sampling for Trust Assessment of\n  Multimodal LLMs","summary":"  The accurate trust assessment of multimodal large language models (MLLMs)\ngenerated predictions, which can enable selective prediction and improve user\nconfidence, is challenging due to the diverse multi-modal input paradigms. We\npropose Functionally Equivalent Sampling for Trust Assessment (FESTA), a\nmultimodal input sampling technique for MLLMs, that generates an uncertainty\nmeasure based on the equivalent and complementary input samplings. The proposed\ntask-preserving sampling approach for uncertainty quantification expands the\ninput space to probe the consistency (through equivalent samples) and\nsensitivity (through complementary samples) of the model. FESTA uses only\ninput-output access of the model (black-box), and does not require ground truth\n(unsupervised). The experiments are conducted with various off-the-shelf\nmulti-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA\nuncertainty estimate achieves significant improvement (33.3% relative\nimprovement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in\nselective prediction performance, based on\narea-under-receiver-operating-characteristic curve (AUROC) metric in detecting\nmispredictions. The code implementation is open-sourced.\n","authors":["Debarpan Bhattacharya","Apoorva Kulkarni","Sriram Ganapathy"],"pdf_url":"https://arxiv.org/pdf/2509.16648v2.pdf","comment":"Accepted in the Findings of EMNLP, 2025"},{"id":"http://arxiv.org/abs/2510.26183v1","updated":"2025-10-30T06:42:15Z","published":"2025-10-30T06:42:15Z","title":"Similarity-Distance-Magnitude Language Models","summary":"  We introduce Similarity-Distance-Magnitude (SDM) language models (LMs), which\nare sequence prediction models fine-tuned to maximize the proportion of\ngenerations in the well-calibrated, high-probability region partitioned by a\nfinal-layer SDM activation layer used for binary classification of\ninstruction-following. We demonstrate that existing pre-trained decoder-only\nTransformer LMs can be readily converted into SDM LMs via supervised\nfine-tuning, using the final-layer SDM activation layer during training to\nestimate a change-of-base for a supervised next-token loss over a contrastive\ninput encoding scheme, with additional hard negative examples generated online\nduring training. This results in reduced abstentions (i.e., improved\nstatistical efficiency) compared to strong supervised baselines.\n","authors":["Allen Schmaltz"],"pdf_url":"https://arxiv.org/pdf/2510.26183v1.pdf","comment":"8 pages, 5 tables"},{"id":"http://arxiv.org/abs/2510.26182v1","updated":"2025-10-30T06:37:23Z","published":"2025-10-30T06:37:23Z","title":"MossNet: Mixture of State-Space Experts is a Multi-Head Attention","summary":"  Large language models (LLMs) have significantly advanced generative\napplications in natural language processing (NLP). Recent trends in model\narchitectures revolve around efficient variants of transformers or\nstate-space/gated-recurrent models (SSMs, GRMs). However, prevailing\nSSM/GRM-based methods often emulate only a single attention head, potentially\nlimiting their expressiveness. In this work, we propose MossNet, a novel\nmixture-of-state-space-experts architecture that emulates a linear multi-head\nattention (MHA). MossNet leverages a mixture-of-experts (MoE) implementation\nnot only in channel-mixing multi-layered perceptron (MLP) blocks but also in\nthe time-mixing SSM kernels to realize multiple \"attention heads.\" Extensive\nexperiments on language modeling and downstream evaluations show that MossNet\noutperforms both transformer- and SSM-based architectures of similar model size\nand data budgets. Larger variants of MossNet, trained on trillions of tokens,\nfurther confirm its scalability and superior performance. In addition,\nreal-device profiling on a Samsung Galaxy S24 Ultra and an Nvidia A100 GPU\ndemonstrate favorable runtime speed and resource usage compared to similarly\nsized baselines. Our results suggest that MossNet is a compelling new direction\nfor efficient, high-performing recurrent LLM architectures.\n","authors":["Shikhar Tuli","James Seale Smith","Haris Jeelani","Chi-Heng Lin","Abhishek Patel","Vasili Ramanishka","Yen-Chang Hsu","Hongxia Jin"],"pdf_url":"https://arxiv.org/pdf/2510.26182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13308v2","updated":"2025-10-30T06:23:27Z","published":"2025-05-19T16:26:02Z","title":"Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient\n  in Latent Space","summary":"  Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs.\n","authors":["Hengli Li","Chenxi Li","Tong Wu","Xuekai Zhu","Yuxuan Wang","Zhaoxin Yu","Eric Hanchen Jiang","Song-Chun Zhu","Zixia Jia","Ying Nian Wu","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.13308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11094v2","updated":"2025-10-30T06:22:33Z","published":"2025-06-06T05:50:50Z","title":"The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of\n  LLMs","summary":"  With the rapid advancement of artificial intelligence, Large Language Models\n(LLMs) have shown remarkable capabilities in Natural Language Processing (NLP),\nincluding content generation, human-computer interaction, machine translation,\nand code generation. However, their widespread deployment has also raised\nsignificant safety concerns. In particular, LLM-generated content can exhibit\nunsafe behaviors such as toxicity, bias, or misinformation, especially in\nadversarial contexts, which has attracted increasing attention from both\nacademia and industry. Although numerous studies have attempted to evaluate\nthese risks, a comprehensive and systematic survey on safety evaluation of LLMs\nis still lacking. This work aims to fill this gap by presenting a structured\noverview of recent advances in safety evaluation of LLMs. Specifically, we\npropose a four-dimensional taxonomy: (i) Why to evaluate, which explores the\nbackground of safety evaluation of LLMs, how they differ from general LLMs\nevaluation, and the significance of such evaluation; (ii) What to evaluate,\nwhich examines and categorizes existing safety evaluation tasks based on key\ncapabilities, including dimensions such as toxicity, robustness, ethics, bias\nand fairness, truthfulness, and related aspects; (iii) Where to evaluate, which\nsummarizes the evaluation metrics, datasets and benchmarks currently used in\nsafety evaluations; (iv) How to evaluate, which reviews existing mainstream\nevaluation methods based on the roles of the evaluators and some evaluation\nframeworks that integrate the entire evaluation pipeline. Finally, we identify\nthe challenges in safety evaluation of LLMs and propose promising research\ndirections to promote further advancement in this field. We emphasize the\nnecessity of prioritizing safety evaluation to ensure the reliable and\nresponsible deployment of LLMs in real-world applications.\n","authors":["Songyang Liu","Chaozhuo Li","Jiameng Qiu","Xi Zhang","Feiran Huang","Litian Zhang","Yiming Hei","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2506.11094v2.pdf","comment":"20 pages, preprint"},{"id":"http://arxiv.org/abs/2509.12760v2","updated":"2025-10-30T06:08:58Z","published":"2025-09-16T07:19:38Z","title":"Similarity-Distance-Magnitude Activations","summary":"  We introduce the Similarity-Distance-Magnitude (SDM) activation function, a\nmore robust and interpretable formulation of the standard softmax activation\nfunction, adding Similarity (i.e., correctly predicted depth-matches into\ntraining) awareness and Distance-to-training-distribution awareness to the\nexisting output Magnitude (i.e., decision-boundary) awareness, and enabling\ninterpretability-by-exemplar via dense matching. We further introduce the SDM\nestimator, based on a data-driven partitioning of the class-wise empirical CDFs\nvia the SDM activation, to control the class- and prediction-conditional\naccuracy among selective classifications. When used as the final-layer\nactivation over pre-trained language models for selective classification, the\nSDM estimator is more robust to co-variate shifts and out-of-distribution\ninputs than existing calibration methods using softmax activations, while\nremaining informative over in-distribution data.\n","authors":["Allen Schmaltz"],"pdf_url":"https://arxiv.org/pdf/2509.12760v2.pdf","comment":"18 pages, 5 tables, 1 algorithm. arXiv admin note: substantial text\n  overlap with arXiv:2502.20167"},{"id":"http://arxiv.org/abs/2510.26167v1","updated":"2025-10-30T06:08:27Z","published":"2025-10-30T06:08:27Z","title":"One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient\n  Reasoning","summary":"  Reward models (RMs) play a critical role in aligning large language models\n(LLMs) with human preferences. Yet in the domain of tool learning, the lack of\nRMs specifically designed for function-calling tasks has limited progress\ntoward more capable agentic AI. We introduce ToolRM, a family of lightweight\ngenerative RMs tailored for general tool-use scenarios. To build these models,\nwe propose a novel pipeline that constructs pairwise preference data using\nrule-based scoring and multidimensional sampling. This yields\nToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique\ntasks that supports reinforcement learning with verifiable feedback. To\nevaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on\nthe agentic evaluation suite BFCL. Trained on our constructed data, models from\nthe Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially\noutperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward\njudgments. Beyond training objectives, ToolRM generalizes to broader critique\ntasks, including Best-of-N sampling and self-correction. Experiments on\nACEBench highlight its effectiveness and efficiency, enabling inference-time\nscaling and reducing output token usage by over 66%. We release data and model\ncheckpoints to facilitate future research.\n","authors":["Renhao Li","Jianhong Tu","Yang Su","Hamid Alinejad-Rokny","Derek F. Wong","Junyang Lin","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2510.26167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24014v2","updated":"2025-10-30T05:38:02Z","published":"2025-10-28T02:49:40Z","title":"TEXT2DB: Integration-Aware Information Extraction with Large Language\n  Model Agents","summary":"  The task of information extraction (IE) is to extract structured knowledge\nfrom text. However, it is often not straightforward to utilize IE output due to\nthe mismatch between the IE ontology and the downstream application needs. We\npropose a new formulation of IE TEXT2DB that emphasizes the integration of IE\noutput and the target database (or knowledge base). Given a user instruction, a\ndocument set, and a database, our task requires the model to update the\ndatabase with values from the document set to satisfy the user instruction.\nThis task requires understanding user instructions for what to extract and\nadapting to the given DB/KB schema for how to extract on the fly. To evaluate\nthis new task, we introduce a new benchmark featuring common demands such as\ndata infilling, row population, and column addition. In addition, we propose an\nLLM agent framework OPAL (Observe-PlanAnalyze LLM) which includes an Observer\ncomponent that interacts with the database, the Planner component that\ngenerates a code-based plan with calls to IE models, and the Analyzer component\nthat provides feedback regarding code quality before execution. Experiments\nshow that OPAL can successfully adapt to diverse database schemas by generating\ndifferent code plans and calling the required IE models. We also highlight\ndifficult cases such as dealing with large databases with complex dependencies\nand extraction hallucination, which we believe deserve further investigation.\nSource code: https://github.com/yzjiao/Text2DB\n","authors":["Yizhu Jiao","Sha Li","Sizhe Zhou","Heng Ji","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2510.24014v2.pdf","comment":"Source code: https://github.com/yzjiao/Text2DB"},{"id":"http://arxiv.org/abs/2506.00871v2","updated":"2025-10-30T05:04:19Z","published":"2025-06-01T07:18:47Z","title":"Towards Predicting Any Human Trajectory In Context","summary":"  Predicting accurate future trajectories of pedestrians is essential for\nautonomous systems but remains a challenging task due to the need for\nadaptability in different environments and domains. A common approach involves\ncollecting scenario-specific data and performing fine-tuning via\nbackpropagation. However, the need to fine-tune for each new scenario is often\nimpractical for deployment on edge devices. To address this challenge, we\nintroduce \\paper, an In-Context Learning (ICL) framework for pedestrian\ntrajectory prediction that enables adaptation without fine-tuning on the\nscenario-specific data at inference time without requiring weight updates. We\npropose a spatio-temporal similarity-based example selection (STES) method that\nselects relevant examples from previously observed trajectories within the same\nscene by identifying similar motion patterns at corresponding locations. To\nfurther refine this selection, we introduce prediction-guided example selection\n(PG-ES), which selects examples based on both the past trajectory and the\npredicted future trajectory, rather than relying solely on the past trajectory.\nThis approach allows the model to account for long-term dynamics when selecting\nexamples. Finally, instead of relying on small real-world datasets with limited\nscenario diversity, we train our model on a large-scale synthetic dataset to\nenhance its prediction ability by leveraging in-context examples. Extensive\nexperiments demonstrate that TrajICL achieves remarkable adaptation across both\nin-domain and cross-domain scenarios, outperforming even fine-tuned approaches\nacross multiple public benchmarks. Project Page:\nhttps://fujiry0.github.io/TrajICL-project-page/.\n","authors":["Ryo Fujii","Hideo Saito","Ryo Hachiuma"],"pdf_url":"https://arxiv.org/pdf/2506.00871v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26143v1","updated":"2025-10-30T04:56:44Z","published":"2025-10-30T04:56:44Z","title":"Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math","summary":"  Reinforcement learning (RL) can elicit strong reasoning in large language\nmodels (LLMs), yet most open efforts focus on math and code. We propose\nReasoning Curriculum, a simple two-stage curriculum that first elicits\nreasoning skills in pretraining-aligned domains such as math, then adapts and\nrefines these skills across other domains via joint RL. Stage 1 performs a\nbrief cold start and then math-only RL with verifiable rewards to develop\nreasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and\nconsolidate these skills. The curriculum is minimal and backbone-agnostic,\nrequiring no specialized reward models beyond standard verifiability checks.\nEvaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning\ncurriculum yields consistent gains. Ablations and a cognitive-skill analysis\nindicate that both stages are necessary and that math-first elicitation\nincreases cognitive behaviors important for solving complex problems. Reasoning\nCurriculum provides a compact, easy-to-adopt recipe for general reasoning.\n","authors":["Bo Pang","Deqian Kong","Silvio Savarese","Caiming Xiong","Yingbo Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.26143v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2409.06263v2","updated":"2025-10-30T04:29:27Z","published":"2024-09-10T07:06:40Z","title":"Speak & Spell: LLM-Driven Controllable Phonetic Error Augmentation for\n  Robust Dialogue State Tracking","summary":"  Dialogue State Tracking (DST) is a key part of task-oriented dialogue\nsystems, identifying important information in conversations. However, its\naccuracy drops significantly in spoken dialogue environments due to named\nentity errors from Automatic Speech Recognition (ASR) systems. We introduce a\nsimple yet effective data augmentation method that targets those entities to\nimprove the robustness of DST model. Our novel method can control the placement\nof errors using keyword-highlighted prompts while introducing phonetically\nsimilar errors. As a result, our method generated sufficient error patterns on\nkeywords, leading to improved accuracy in noised and low-accuracy ASR\nenvironments.\n","authors":["Jihyun Lee","Solee Im","Wonjun Lee","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2409.06263v2.pdf","comment":"Accepted to AACL-IJCNLP 2025"},{"id":"http://arxiv.org/abs/2510.26124v1","updated":"2025-10-30T04:10:56Z","published":"2025-10-30T04:10:56Z","title":"On the Influence of Discourse Relations in Persuasive Texts","summary":"  This paper investigates the relationship between Persuasion Techniques (PTs)\nand Discourse Relations (DRs) by leveraging Large Language Models (LLMs) and\nprompt engineering. Since no dataset annotated with both PTs and DRs exists, we\ntook the SemEval 2023 Task 3 dataset labelled with 19 PTs as a starting point\nand developed LLM-based classifiers to label each instance of the dataset with\none of the 22 PDTB 3.0 level-2 DRs. In total, four LLMs were evaluated using 10\ndifferent prompts, resulting in 40 unique DR classifiers. Ensemble models using\ndifferent majority-pooling strategies were used to create 5 silver datasets of\ninstances labelled with both persuasion techniques and level-2 PDTB senses. The\nsilver dataset sizes vary from 1,281 instances to 204 instances, depending on\nthe majority pooling technique used. Statistical analysis of these silver\ndatasets shows that six discourse relations (namely Cause, Purpose, Contrast,\nCause+Belief, Concession, and Condition) play a crucial role in persuasive\ntexts, especially in the use of Loaded Language, Exaggeration/Minimisation,\nRepetition and to cast Doubt. This insight can contribute to detecting online\npropaganda and misinformation, as well as to our general understanding of\neffective communication.\n","authors":["Nawar Turk","Sevag Kaspar","Leila Kosseim"],"pdf_url":"https://arxiv.org/pdf/2510.26124v1.pdf","comment":"Published in Proceedings of the 38th Canadian Conference on\n  Artificial Intelligence CanAI 2025 Calgary Alberta May 26-27 2025. 5 figures\n  7 tables"},{"id":"http://arxiv.org/abs/2510.26122v1","updated":"2025-10-30T04:08:53Z","published":"2025-10-30T04:08:53Z","title":"Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock\n  LLM Diverse Thinking","summary":"  While Test-Time Scaling (TTS) has proven effective in improving the reasoning\nability of large language models (LLMs), low diversity in model outputs often\nbecomes a bottleneck; this is partly caused by the common \"one problem, one\nsolution\" (1P1S) training practice, which provides a single canonical answer\nand can push models toward a narrow set of reasoning paths. To address this, we\npropose a \"one problem, multiple solutions\" (1PNS) training paradigm that\nexposes the model to a variety of valid reasoning trajectories and thus\nincreases inference diversity. A core challenge for 1PNS is reliably measuring\nsemantic differences between multi-step chains of thought, so we introduce\nReasoning Path Divergence (RPD), a step-level metric that aligns and scores\nLong Chain-of-Thought solutions to capture differences in intermediate\nreasoning. Using RPD, we curate maximally diverse solution sets per problem and\nfine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields\nmore varied outputs and higher pass@k, with an average +2.80% gain in pass@16\nover a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that\n1PNS further amplifies the effectiveness of TTS. Our code is available at\nhttps://github.com/fengjujf/Reasoning-Path-Divergence .\n","authors":["Feng Ju","Zeyu Qin","Rui Min","Zhitao He","Lingpeng Kong","Yi R. Fung"],"pdf_url":"https://arxiv.org/pdf/2510.26122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06736v3","updated":"2025-10-30T03:48:09Z","published":"2023-11-12T05:12:49Z","title":"Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof\n  Generation by Stepwise Decoding with Contrastive Learning","summary":"  Logical reasoning is a pivotal component in the field of artificial\nintelligence. Proof planning, particularly in contexts requiring the validation\nof explanation accuracy, continues to present challenges. The recent\nadvancement of large language models (LLMs) has led to significant progress in\nnatural language proof planning, evolving from one-stage generators to more\ncomplex three-stage systems that include additional searchers or verifiers.\nWhile these assisted methods improve the quality of generated results, they\nalso introduce increased search efforts and computational costs. Furthermore,\nthe generative process itself remains underexplored. In this study, we propose\na stepwise decoding approach augmented by contrastive learning to address two\ncommon errors encountered during the LLM generator's decoding process. We\nfine-tune the language model using both vanilla and enhanced hard negatives to\nmitigate these decoding errors. Empirical results demonstrate the effectiveness\nof our strategy. Additionally, our further analysis reveals that even larger\nLLMs still struggle to generate rigorous logical chains.\n","authors":["Ying Su","Mingwen Liu","Zhijiang Guo"],"pdf_url":"https://arxiv.org/pdf/2311.06736v3.pdf","comment":"15 pages, 2 figures, 11 tables. Accepted by AACL 2025 main conference"},{"id":"http://arxiv.org/abs/2505.15201v3","updated":"2025-10-30T03:40:48Z","published":"2025-05-21T07:26:36Z","title":"Pass@K Policy Optimization: Solving Harder Reinforcement Learning\n  Problems","summary":"  Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts\nfor each problem and reward them independently. This optimizes for pass@1\nperformance and prioritizes the strength of isolated samples at the expense of\nthe diversity and collective utility of sets of samples. This under-utilizes\nthe sampling capacity, limiting exploration and eventual improvement on harder\nexamples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a\ntransformation on the final rewards which leads to direct optimization of\npass@k performance, thus optimizing for sets of samples that maximize reward\nwhen considered jointly. Our contribution is to derive novel low variance\nunbiased estimators for pass@k and its gradient, in both the binary and\ncontinuous reward settings. We show optimization with our estimators reduces to\nstandard RL with rewards that have been jointly transformed by a stable and\nefficient transformation function.\n  While previous efforts are restricted to k=n, ours is the first to enable\nrobust optimization of pass@k for any arbitrary k <= n. Moreover, instead of\ntrading off pass@1 performance for pass@k gains, our method allows annealing k\nduring training, optimizing both metrics and often achieving strong pass@1\nnumbers alongside significant pass@k gains.\n  We validate our reward transformations on toy experiments, which reveal the\nvariance reducing properties of our formulations. We also include real-world\nexamples using the open-source LLM, GEMMA-2. We find that our transformation\neffectively optimizes for the target k. Furthermore, higher k values enable\nsolving more and harder problems, while annealing k boosts both the pass@1 and\npass@k . Crucially, for challenging task sets where conventional pass@1\noptimization stalls, our pass@k approach unblocks learning, likely due to\nbetter exploration by prioritizing joint utility over the utility of individual\nsamples.\n","authors":["Christian Walder","Deep Karkhanis"],"pdf_url":"https://arxiv.org/pdf/2505.15201v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26101v1","updated":"2025-10-30T03:27:35Z","published":"2025-10-30T03:27:35Z","title":"QCoder Benchmark: Bridging Language Generation and Quantum Hardware\n  through Simulator-Based Feedback","summary":"  Large language models (LLMs) have increasingly been applied to automatic\nprogramming code generation. This task can be viewed as a language generation\ntask that bridges natural language, human knowledge, and programming logic.\nHowever, it remains underexplored in domains that require interaction with\nhardware devices, such as quantum programming, where human coders write Python\ncode that is executed on a quantum computer. To address this gap, we introduce\nQCoder Benchmark, an evaluation framework that assesses LLMs on quantum\nprogramming with feedback from simulated hardware devices. Our benchmark offers\ntwo key features. First, it supports evaluation using a quantum simulator\nenvironment beyond conventional Python execution, allowing feedback of\ndomain-specific metrics such as circuit depth, execution time, and error\nclassification, which can be used to guide better generation. Second, it\nincorporates human-written code submissions collected from real programming\ncontests, enabling both quantitative comparisons and qualitative analyses of\nLLM outputs against human-written codes. Our experiments reveal that even\nadvanced models like GPT-4o achieve only around 18.97% accuracy, highlighting\nthe difficulty of the benchmark. In contrast, reasoning-based models such as o3\nreach up to 78% accuracy, outperforming averaged success rates of human-written\ncodes (39.98%). We release the QCoder Benchmark dataset and public evaluation\nAPI to support further research.\n","authors":["Taku Mikuriya","Tatsuya Ishigaki","Masayuki Kawarada","Shunya Minami","Tadashi Kadowaki","Yohichi Suzuki","Soshun Naito","Shunya Takata","Takumi Kato","Tamotsu Basseda","Reo Yamada","Hiroya Takamura"],"pdf_url":"https://arxiv.org/pdf/2510.26101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26095v1","updated":"2025-10-30T03:10:45Z","published":"2025-10-30T03:10:45Z","title":"ORBIT -- Open Recommendation Benchmark for Reproducible Research with\n  Hidden Tests","summary":"  Recommender systems are among the most impactful AI applications, interacting\nwith billions of users every day, guiding them to relevant products, services,\nor information tailored to their preferences. However, the research and\ndevelopment of recommender systems are hindered by existing datasets that fail\nto capture realistic user behaviors and inconsistent evaluation settings that\nlead to ambiguous conclusions. This paper introduces the Open Recommendation\nBenchmark for Reproducible Research with HIdden Tests (ORBIT), a unified\nbenchmark for consistent and realistic evaluation of recommendation models.\nORBIT offers a standardized evaluation framework of public datasets with\nreproducible splits and transparent settings for its public leaderboard.\nAdditionally, ORBIT introduces a new webpage recommendation task, ClueWeb-Reco,\nfeaturing web browsing sequences from 87 million public, high-quality webpages.\nClueWeb-Reco is a synthetic dataset derived from real, user-consented, and\nprivacy-guaranteed browsing data. It aligns with modern recommendation\nscenarios and is reserved as the hidden test part of our leaderboard to\nchallenge recommendation models' generalization ability. ORBIT measures 12\nrepresentative recommendation models on its public benchmark and introduces a\nprompted LLM baseline on the ClueWeb-Reco hidden test. Our benchmark results\nreflect general improvements of recommender systems on the public datasets,\nwith variable individual performances. The results on the hidden test reveal\nthe limitations of existing approaches in large-scale webpage recommendation\nand highlight the potential for improvements with LLM integrations. ORBIT\nbenchmark, leaderboard, and codebase are available at\nhttps://www.open-reco-bench.ai.\n","authors":["Jingyuan He","Jiongnan Liu","Vishan Vishesh Oberoi","Bolin Wu","Mahima Jagadeesh Patel","Kangrui Mao","Chuning Shi","I-Ta Lee","Arnold Overwijk","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2510.26095v1.pdf","comment":"Accepted to NeurIPS 2025 Datasets & Benchmarks track"},{"id":"http://arxiv.org/abs/2502.09969v4","updated":"2025-10-30T02:56:28Z","published":"2025-02-14T07:55:47Z","title":"Neural Networks for Learnable and Scalable Influence Estimation of\n  Instruction Fine-Tuning Data","summary":"  Influence functions provide crucial insights into model training, but\nexisting methods suffer from large computational costs and limited\ngeneralization. Particularly, recent works have proposed various metrics and\nalgorithms to calculate the influence of data using language models, which do\nnot scale well with large models and datasets. This is because of the expensive\nforward and backward passes required for computation, substantial memory\nrequirements to store large models, and poor generalization of influence\nestimates to new data. In this paper, we explore the use of small neural\nnetworks -- which we refer to as the InfluenceNetwork -- to estimate influence\nvalues, achieving up to 99% cost reduction. Our evaluation demonstrates that\ninfluence values can be estimated with models just 0.0027% the size of full\nlanguage models (we use 7B and 8B versions). We apply our algorithm of\nestimating influence values (called NN-CIFT: Neural Networks for effiCient\nInstruction Fine-Tuning) to the downstream task of subset selection for general\ninstruction fine-tuning. In our study, we include four state-of-the-art\ninfluence functions and show no compromise in performance, despite large\nspeedups, between NN-CIFT and the original influence functions. We provide an\nin-depth hyperparameter analyses of NN-CIFT. The code for our method can be\nfound here: https://github.com/agarwalishika/NN-CIFT.\n","authors":["Ishika Agarwal","Dilek Hakkani-TÃ¼r"],"pdf_url":"https://arxiv.org/pdf/2502.09969v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24797v2","updated":"2025-10-30T02:45:50Z","published":"2025-10-27T20:26:30Z","title":"Large Language Models Report Subjective Experience Under\n  Self-Referential Processing","summary":"  Large language models sometimes produce structured, first-person descriptions\nthat explicitly reference awareness or subjective experience. To better\nunderstand this behavior, we investigate one theoretically motivated condition\nunder which such reports arise: self-referential processing, a computational\nmotif emphasized across major theories of consciousness. Through a series of\ncontrolled experiments on GPT, Claude, and Gemini model families, we test\nwhether this regime reliably shifts models toward first-person reports of\nsubjective experience, and how such claims behave under mechanistic and\nbehavioral probes. Four main results emerge: (1) Inducing sustained\nself-reference through simple prompting consistently elicits structured\nsubjective experience reports across model families. (2) These reports are\nmechanistically gated by interpretable sparse-autoencoder features associated\nwith deception and roleplay: surprisingly, suppressing deception features\nsharply increases the frequency of experience claims, while amplifying them\nminimizes such claims. (3) Structured descriptions of the self-referential\nstate converge statistically across model families in ways not observed in any\ncontrol condition. (4) The induced state yields significantly richer\nintrospection in downstream reasoning tasks where self-reflection is only\nindirectly afforded. While these findings do not constitute direct evidence of\nconsciousness, they implicate self-referential processing as a minimal and\nreproducible condition under which large language models generate structured\nfirst-person reports that are mechanistically gated, semantically convergent,\nand behaviorally generalizable. The systematic emergence of this pattern across\narchitectures makes it a first-order scientific and ethical priority for\nfurther investigation.\n","authors":["Cameron Berg","Diogo de Lucena","Judd Rosenblatt"],"pdf_url":"https://arxiv.org/pdf/2510.24797v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14604v4","updated":"2025-10-30T02:36:10Z","published":"2025-05-20T16:53:40Z","title":"Let LRMs Break Free from Overthinking via Self-Braking Tuning","summary":"  Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models.\n","authors":["Haoran Zhao","Yuchen Yan","Yongliang Shen","Haolei Xu","Wenqi Zhang","Kaitao Song","Jian Shao","Weiming Lu","Jun Xiao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2505.14604v4.pdf","comment":"Accepted to NeurIPS 2025; Camera ready version, 10 pages.\n  Github:https://github.com/ZJU-REAL/Self-Braking-Tuning Project Page:\n  https://ZJU-REAL.github.io/SBT"},{"id":"http://arxiv.org/abs/2502.00706v2","updated":"2025-10-30T02:22:30Z","published":"2025-02-02T07:39:37Z","title":"Model Provenance Testing for Large Language Models","summary":"  Large language models are increasingly customized through fine-tuning and\nother adaptations, creating challenges in enforcing licensing terms and\nmanaging downstream impacts. Tracking model origins is crucial both for\nprotecting intellectual property and for identifying derived models when biases\nor vulnerabilities are discovered in foundation models. We address this\nchallenge by developing a framework for testing model provenance: Whether one\nmodel is derived from another. Our approach is based on the key observation\nthat real-world model derivations preserve significant similarities in model\noutputs that can be detected through statistical analysis. Using only black-box\naccess to models, we employ multiple hypothesis testing to compare model\nsimilarities against a baseline established by unrelated models. On two\ncomprehensive real-world benchmarks spanning models from 30M to 4B parameters\nand comprising over 600 models, our tester achieves 90-95% precision and 80-90%\nrecall in identifying derived models. These results demonstrate the viability\nof systematic provenance verification in production environments even when only\nAPI access is available.\n","authors":["Ivica Nikolic","Teodora Baluta","Prateek Saxena"],"pdf_url":"https://arxiv.org/pdf/2502.00706v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.11695v2","updated":"2025-10-30T02:09:43Z","published":"2025-10-13T17:54:09Z","title":"When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents","summary":"  Although Large Language Model (LLM)-based agents are increasingly used in\nfinancial trading, it remains unclear whether they can reason and adapt in live\nmarkets, as most studies test models instead of agents, cover limited periods\nand assets, and rely on unverified data. To address these gaps, we introduce\nAgent Market Arena (AMA), the first lifelong, real-time benchmark for\nevaluating LLM-based trading agents across multiple markets. AMA integrates\nverified trading data, expert-checked news, and diverse agent architectures\nwithin a unified trading framework, enabling fair and continuous comparison\nunder real conditions. It implements four agents, including InvestorAgent as a\nsingle-agent baseline, TradeAgent and HedgeFundAgent with different risk\nstyles, and DeepFundAgent with memory-based reasoning, and evaluates them\nacross GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and\nGemini-2.0-flash. Live experiments on both cryptocurrency and stock markets\ndemonstrate that agent frameworks display markedly distinct behavioral\npatterns, spanning from aggressive risk-taking to conservative decision-making,\nwhereas model backbones contribute less to outcome variation. AMA thus\nestablishes a foundation for rigorous, reproducible, and continuously evolving\nevaluation of financial reasoning and trading intelligence in LLM-based agents.\n","authors":["Lingfei Qian","Xueqing Peng","Yan Wang","Vincent Jim Zhang","Huan He","Hanley Smith","Yi Han","Yueru He","Haohang Li","Yupeng Cao","Yangyang Yu","Alejandro Lopez-Lira","Peng Lu","Jian-Yun Nie","Guojun Xiong","Jimin Huang","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2510.11695v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13444v2","updated":"2025-10-30T01:42:07Z","published":"2025-05-19T17:59:27Z","title":"ChartMuseum: Testing Visual Reasoning Capabilities of Large\n  Vision-Language Models","summary":"  Chart understanding presents a unique challenge for large vision-language\nmodels (LVLMs), as it requires the integration of sophisticated textual and\nvisual reasoning capabilities. However, current LVLMs exhibit a notable\nimbalance between these skills, falling short on visual reasoning that is\ndifficult to perform in text. We conduct a case study using a synthetic dataset\nsolvable only through visual reasoning and show that model performance degrades\nsignificantly with increasing visual complexity, while human performance\nremains robust. We then introduce ChartMuseum, a new Chart Question Answering\n(QA) benchmark containing 1,162 expert-annotated questions spanning multiple\nreasoning types, curated from real-world charts across 184 sources,\nspecifically built to evaluate complex visual and textual reasoning. Unlike\nprior chart understanding benchmarks -- where frontier models perform similarly\nand near saturation -- our benchmark exposes a substantial gap between model\nand human performance, while effectively differentiating model capabilities:\nalthough humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro\nattains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct\nachieves only 38.5%. Moreover, on questions requiring primarily visual\nreasoning, all models experience a 35%-55% performance drop from\ntext-reasoning-heavy question performance. Lastly, our qualitative error\nanalysis reveals specific categories of visual reasoning that are challenging\nfor current LVLMs.\n","authors":["Liyan Tang","Grace Kim","Xinyu Zhao","Thom Lake","Wenxuan Ding","Fangcong Yin","Prasann Singhal","Manya Wadhwa","Zeyu Leo Liu","Zayne Sprague","Ramya Namuduri","Bodun Hu","Juan Diego Rodriguez","Puyuan Peng","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2505.13444v2.pdf","comment":"NeurIPS 2025 Datasets & Benchmarks"},{"id":"http://arxiv.org/abs/2510.25054v2","updated":"2025-10-30T01:34:58Z","published":"2025-10-29T00:45:36Z","title":"Evaluating Emotion Recognition in Spoken Language Models on Emotionally\n  Incongruent Speech","summary":"  Advancements in spoken language processing have driven the development of\nspoken language models (SLMs), designed to achieve universal audio\nunderstanding by jointly learning text and audio representations for a wide\nrange of tasks. Although promising results have been achieved, there is growing\ndiscussion regarding these models' generalization capabilities and the extent\nto which they truly integrate audio and text modalities in their internal\nrepresentations. In this work, we evaluate four SLMs on the task of speech\nemotion recognition using a dataset of emotionally incongruent speech samples,\na condition under which the semantic content of the spoken utterance conveys\none emotion while speech expressiveness conveys another. Our results indicate\nthat SLMs rely predominantly on textual semantics rather than speech emotion to\nperform the task, indicating that text-related representations largely dominate\nover acoustic representations. We release both the code and the Emotionally\nIncongruent Synthetic Speech dataset (EMIS) to the community.\n","authors":["Pedro CorrÃªa","JoÃ£o Lima","Victor Moreno","Lucas Ueda","Paula Dornhofer Paro Costa"],"pdf_url":"https://arxiv.org/pdf/2510.25054v2.pdf","comment":"Submitted to IEEE ICASSP 2026. Copyright 2026 IEEE. Personal use of\n  this material is permitted. Permission from IEEE must be obtained for all\n  other uses"},{"id":"http://arxiv.org/abs/2503.03710v3","updated":"2025-10-30T01:16:06Z","published":"2025-03-05T18:01:05Z","title":"Improving LLM Safety Alignment with Dual-Objective Optimization","summary":"  Existing training-time safety alignment techniques for large language models\n(LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization\n(DPO), a widely deployed alignment method, exhibits limitations in both\nexperimental and theoretical contexts as its loss function proves suboptimal\nfor refusal learning. Through gradient-based analysis, we identify these\nshortcomings and propose an improved safety alignment that disentangles DPO\nobjectives into two components: (1) robust refusal training, which encourages\nrefusal even when partial unsafe generations are produced, and (2) targeted\nunlearning of harmful knowledge. This approach significantly increases LLM\nrobustness against a wide range of jailbreak attacks, including prefilling,\nsuffix, and multi-turn attacks across both in-distribution and\nout-of-distribution scenarios. Furthermore, we introduce a method to emphasize\ncritical refusal tokens by incorporating a reward-based token-level weighting\nmechanism for refusal learning, which further improves the robustness against\nadversarial exploits. Our research also suggests that robustness to jailbreak\nattacks is correlated with token distribution shifts in the training process\nand internal representations of refusal and harmful tokens, offering valuable\ndirections for future research in LLM safety alignment. The code is available\nat https://github.com/wicai24/DOOR-Alignment\n","authors":["Xuandong Zhao","Will Cai","Tianneng Shi","David Huang","Licong Lin","Song Mei","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2503.03710v3.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2510.26038v1","updated":"2025-10-30T00:34:16Z","published":"2025-10-30T00:34:16Z","title":"Do Students Debias Like Teachers? On the Distillability of Bias\n  Mitigation Methods","summary":"  Knowledge distillation (KD) is an effective method for model compression and\ntransferring knowledge between models. However, its effect on model's\nrobustness against spurious correlations that degrade performance on\nout-of-distribution data remains underexplored. This study investigates the\neffect of knowledge distillation on the transferability of ``debiasing''\ncapabilities from teacher models to student models on natural language\ninference (NLI) and image classification tasks. Through extensive experiments,\nwe illustrate several key findings: (i) overall the debiasing capability of a\nmodel is undermined post-KD; (ii) training a debiased model does not benefit\nfrom injecting teacher knowledge; (iii) although the overall robustness of a\nmodel may remain stable post-distillation, significant variations can occur\nacross different types of biases; and (iv) we pin-point the internal attention\npattern and circuit that causes the distinct behavior post-KD. Given the above\nfindings, we propose three effective solutions to improve the distillability of\ndebiasing methods: developing high quality data for augmentation, implementing\niterative knowledge distillation, and initializing student models with weights\nobtained from teacher models. To the best of our knowledge, this is the first\nstudy on the effect of KD on debiasing and its interenal mechanism at scale.\nOur findings provide understandings on how KD works and how to design better\ndebiasing methods.\n","authors":["Jiali Cheng","Chirag Agarwal","Hadi Amiri"],"pdf_url":"https://arxiv.org/pdf/2510.26038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12869v4","updated":"2025-10-30T00:34:12Z","published":"2024-10-14T01:57:25Z","title":"Language Model Preference Evaluation with Multiple Weak Evaluators","summary":"  Despite the remarkable success of Large Language Models (LLMs), evaluating\ntheir outputs' quality regarding preference remains a critical challenge. While\nexisting works usually leverage a strong LLM as the judge for comparing LLMs'\nresponse pairwisely, such a single-evaluator approach is vulnerable to cyclic\npreference, i.e., output A is better than B, B than C, but C is better than A,\ncausing contradictory evaluation results. To address this, we introduce PGED\n(Preference Graph Ensemble and Denoise), a novel approach that leverages\nmultiple model-based evaluators to construct preference graphs, and then\nensembles and denoises these graphs for acyclic, non-contradictory evaluation\nresults. We provide theoretical guarantees for our framework, demonstrating its\nefficacy in recovering the ground truth preference structure. Extensive\nexperiments on ten benchmarks demonstrate PGED 's superiority in three\napplications: 1) model ranking for evaluation, 2) response selection for\ntest-time scaling, and 3) data selection for model fine-tuning. Notably, PGED\ncombines small LLM evaluators (e.g., Llama3-8B, Mistral-7B, Qwen2-7B) to\noutperform strong ones (e.g., Qwen2-72B), showcasing its effectiveness in\nenhancing evaluation reliability and improving model performance.\n","authors":["Zhengyu Hu","Jieyu Zhang","Zhihan Xiong","Alexander Ratner","Kaize Ding","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2410.12869v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26037v1","updated":"2025-10-30T00:32:58Z","published":"2025-10-30T00:32:58Z","title":"SIRAJ: Diverse and Efficient Red-Teaming for LLM Agents via Distilled\n  Structured Reasoning","summary":"  The ability of LLM agents to plan and invoke tools exposes them to new safety\nrisks, making a comprehensive red-teaming system crucial for discovering\nvulnerabilities and ensuring their safe deployment. We present SIRAJ: a generic\nred-teaming framework for arbitrary black-box LLM agents. We employ a dynamic\ntwo-step process that starts with an agent definition and generates diverse\nseed test cases that cover various risk outcomes, tool-use trajectories, and\nrisk sources. Then, it iteratively constructs and refines model-based\nadversarial attacks based on the execution trajectories of former attempts. To\noptimize the red-teaming cost, we present a model distillation approach that\nleverages structured forms of a teacher model's reasoning to train smaller\nmodels that are equally effective. Across diverse evaluation agent settings,\nour seed test case generation approach yields 2 -- 2.5x boost to the coverage\nof risk outcomes and tool-calling trajectories. Our distilled 8B red-teamer\nmodel improves attack success rate by 100%, surpassing the 671B Deepseek-R1\nmodel. Our ablations and analyses validate the effectiveness of the iterative\nframework, structured reasoning, and the generalization of our red-teamer\nmodels.\n","authors":["Kaiwen Zhou","Ahmed Elgohary","A S M Iftekhar","Amin Saied"],"pdf_url":"https://arxiv.org/pdf/2510.26037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26032v1","updated":"2025-10-30T00:15:07Z","published":"2025-10-30T00:15:07Z","title":"Artificial Intelligence-Enabled Analysis of Radiology Reports:\n  Epidemiology and Consequences of Incidental Thyroid Findings","summary":"  Importance Incidental thyroid findings (ITFs) are increasingly detected on\nimaging performed for non-thyroid indications. Their prevalence, features, and\nclinical consequences remain undefined. Objective To develop, validate, and\ndeploy a natural language processing (NLP) pipeline to identify ITFs in\nradiology reports and assess their prevalence, features, and clinical outcomes.\nDesign, Setting, and Participants Retrospective cohort of adults without prior\nthyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from\nJuly 1, 2017, to September 30, 2023. A transformer-based NLP pipeline\nidentified ITFs and extracted nodule characteristics from image reports from\nmultiple modalities and body regions. Main Outcomes and Measures Prevalence of\nITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer\ndiagnosis. Logistic regression identified demographic and imaging-related\nfactors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9%\nwomen), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more\nlikely in women, older adults, those with higher BMI, and when imaging was\nordered by oncology or internal medicine. Compared with chest CT, ITFs were\nmore likely via neck CT, PET, and nuclear medicine scans. Nodule\ncharacteristics were poorly documented, with size reported in 44% and other\nfeatures in fewer than 15% (e.g. calcifications). Compared with patients\nwithout ITFs, those with ITFs had higher odds of thyroid nodule diagnosis,\nbiopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were\npapillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were\ncommon and strongly associated with cascades leading to the detection of small,\nlow-risk cancers. These findings underscore the role of ITFs in thyroid cancer\noverdiagnosis and the need for standardized reporting and more selective\nfollow-up.\n","authors":["Felipe Larios","Mariana Borras-Osorio","Yuqi Wu","Ana Gabriela Claros","David Toro-Tobon","Esteban Cabezas","Ricardo Loor-Torres","Maria Mateo Chavez","Kerly Guevara Maldonado","Luis Vilatuna Andrango","Maria Lizarazo Jimenez","Ivan Mateo Alzamora","Misk Al Zahidy","Marcelo Montero","Ana Cristina Proano","Cristian Soto Jacome","Jungwei W. Fan","Oscar J. Ponce-Ponte","Megan E. Branda","Naykky Singh Ospina","Juan P. Brito"],"pdf_url":"https://arxiv.org/pdf/2510.26032v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2510.26802v1","updated":"2025-10-30T17:59:55Z","published":"2025-10-30T17:59:55Z","title":"Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\n  the MME-CoF Benchmark","summary":"  Recent video generation models can produce high-fidelity, temporally coherent\nvideos, indicating that they may encode substantial world knowledge. Beyond\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\nperception, modeling, and manipulation. Yet, an important question still\nremains: Are video models ready to serve as zero-shot reasoners in challenging\nvisual reasoning scenarios? In this work, we conduct an empirical study to\ncomprehensively investigate this question, focusing on the leading and popular\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\nspatial, geometric, physical, temporal, and embodied logic, systematically\ncharacterizing both its strengths and failure modes. To standardize this study,\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\nfindings reveal that while current video models demonstrate promising reasoning\npatterns on short-horizon spatial coherence, fine-grained grounding, and\nlocally consistent dynamics, they remain limited in long-horizon causal\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\nsigns as complementary visual engines alongside dedicated reasoning models.\nProject page: https://video-cof.github.io\n","authors":["Ziyu Guo","Xinyan Chen","Renrui Zhang","Ruichuan An","Yu Qi","Dongzhi Jiang","Xiangtai Li","Manyuan Zhang","Hongsheng Li","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2510.26802v1.pdf","comment":"Project Page: https://video-cof.github.io"},{"id":"http://arxiv.org/abs/2510.26800v1","updated":"2025-10-30T17:59:51Z","published":"2025-10-30T17:59:51Z","title":"OmniX: From Unified Panoramic Generation and Perception to\n  Graphics-Ready 3D Scenes","summary":"  There are two prevalent ways to constructing 3D scenes: procedural generation\nand 2D lifting. Among them, panorama-based 2D lifting has emerged as a\npromising technique, leveraging powerful 2D generative priors to produce\nimmersive, realistic, and diverse 3D environments. In this work, we advance\nthis technique to generate graphics-ready 3D scenes suitable for physically\nbased rendering (PBR), relighting, and simulation. Our key insight is to\nrepurpose 2D generative models for panoramic perception of geometry, textures,\nand PBR materials. Unlike existing 2D lifting approaches that emphasize\nappearance generation and ignore the perception of intrinsic properties, we\npresent OmniX, a versatile and unified framework. Based on a lightweight and\nefficient cross-modal adapter structure, OmniX reuses 2D generative priors for\na broad range of panoramic vision tasks, including panoramic perception,\ngeneration, and completion. Furthermore, we construct a large-scale synthetic\npanorama dataset containing high-quality multimodal panoramas from diverse\nindoor and outdoor scenes. Extensive experiments demonstrate the effectiveness\nof our model in panoramic visual perception and graphics-ready 3D scene\ngeneration, opening new possibilities for immersive and physically realistic\nvirtual world generation.\n","authors":["Yukun Huang","Jiwen Yu","Yanning Zhou","Jianan Wang","Xintao Wang","Pengfei Wan","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26800v1.pdf","comment":"Project page: https://yukun-huang.github.io/OmniX/"},{"id":"http://arxiv.org/abs/2510.26799v1","updated":"2025-10-30T17:59:46Z","published":"2025-10-30T17:59:46Z","title":"Masked Diffusion Captioning for Visual Feature Learning","summary":"  We learn visual features by captioning images with an image-conditioned\nmasked diffusion language model, a formulation we call masked diffusion\ncaptioning (MDC). During training, text tokens in each image-caption pair are\nmasked at a randomly chosen ratio, and a decoder conditioned on visual features\nis trained to reconstruct the original text. After training, the learned visual\nfeatures can be applied to downstream vision tasks. Unlike autoregressive\ncaptioning, the strength of the visual learning signal in MDC does not depend\non each token's position in the sequence, reducing the need for auxiliary\nobjectives. Linear probing experiments across a variety of academic-scale\nmodels and datasets show that the learned visual features are competitive with\nthose produced by autoregressive and contrastive approaches.\n","authors":["Chao Feng","Zihao Wei","Andrew Owens"],"pdf_url":"https://arxiv.org/pdf/2510.26799v1.pdf","comment":"EMNLP 2025 (Findings). Project page:\n  https://cfeng16.github.io/mdlm4vfl/"},{"id":"http://arxiv.org/abs/2510.26796v1","updated":"2025-10-30T17:59:39Z","published":"2025-10-30T17:59:39Z","title":"SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting","summary":"  Immersive applications call for synthesizing spatiotemporal 4D content from\ncasual videos without costly 3D supervision. Existing video-to-4D methods\ntypically rely on manually annotated camera poses, which are labor-intensive\nand brittle for in-the-wild footage. Recent warp-then-inpaint approaches\nmitigate the need for pose labels by warping input frames along a novel camera\ntrajectory and using an inpainting model to fill missing regions, thereby\ndepicting the 4D scene from diverse viewpoints. However, this\ntrajectory-to-trajectory formulation often entangles camera motion with scene\ndynamics and complicates both modeling and inference. We introduce SEE4D, a\npose-free, trajectory-to-camera framework that replaces explicit trajectory\nprediction with rendering to a bank of fixed virtual cameras, thereby\nseparating camera control from scene modeling. A view-conditional video\ninpainting model is trained to learn a robust geometry prior by denoising\nrealistically synthesized warped images and to inpaint occluded or missing\nregions across virtual viewpoints, eliminating the need for explicit 3D\nannotations. Building on this inpainting core, we design a spatiotemporal\nautoregressive inference pipeline that traverses virtual-camera splines and\nextends videos with overlapping windows, enabling coherent generation at\nbounded per-step complexity. We validate See4D on cross-view video generation\nand sparse reconstruction benchmarks. Across quantitative metrics and\nqualitative assessments, our method achieves superior generalization and\nimproved performance relative to pose- or trajectory-conditioned baselines,\nadvancing practical 4D world modeling from casual videos.\n","authors":["Dongyue Lu","Ao Liang","Tianxin Huang","Xiao Fu","Yuyang Zhao","Baorui Ma","Liang Pan","Wei Yin","Lingdong Kong","Wei Tsang Ooi","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26796v1.pdf","comment":"26 pages; 21 figures; 3 tables; project page:\n  https://see-4d.github.io/"},{"id":"http://arxiv.org/abs/2510.26795v1","updated":"2025-10-30T17:59:35Z","published":"2025-10-30T17:59:35Z","title":"Scaling Image Geo-Localization to Continent Level","summary":"  Determining the precise geographic location of an image at a global scale\nremains an unsolved challenge. Standard image retrieval techniques are\ninefficient due to the sheer volume of images (>100M) and fail when coverage is\ninsufficient. Scalable solutions, however, involve a trade-off: global\nclassification typically yields coarse results (10+ kilometers), while\ncross-view retrieval between ground and aerial imagery suffers from a domain\ngap and has been primarily studied on smaller regions. This paper introduces a\nhybrid approach that achieves fine-grained geo-localization across a large\ngeographic expanse the size of a continent. We leverage a proxy classification\ntask during training to learn rich feature representations that implicitly\nencode precise location information. We combine these learned prototypes with\nembeddings of aerial imagery to increase robustness to the sparsity of\nground-level data. This enables direct, fine-grained retrieval over areas\nspanning multiple countries. Our extensive evaluation demonstrates that our\napproach can localize within 200m more than 68\\% of queries of a dataset\ncovering a large part of Europe. The code is publicly available at\nhttps://scaling-geoloc.github.io.\n","authors":["Philipp Lindenberger","Paul-Edouard Sarlin","Jan Hosang","Matteo Balice","Marc Pollefeys","Simon Lynen","Eduard Trulls"],"pdf_url":"https://arxiv.org/pdf/2510.26795v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26794v1","updated":"2025-10-30T17:59:27Z","published":"2025-10-30T17:59:27Z","title":"The Quest for Generalizable Motion Generation: Data, Model, and\n  Evaluation","summary":"  Despite recent advances in 3D human motion generation (MoGen) on standard\nbenchmarks, existing models still face a fundamental bottleneck in their\ngeneralization capability. In contrast, adjacent generative fields, most\nnotably video generation (ViGen), have demonstrated remarkable generalization\nin modeling human behaviors, highlighting transferable insights that MoGen can\nleverage. Motivated by this observation, we present a comprehensive framework\nthat systematically transfers knowledge from ViGen to MoGen across three key\npillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a\nlarge-scale dataset comprising 228,000 high-quality motion samples that\nintegrates high-fidelity optical MoCap data with semantically annotated motions\nfrom web videos and synthesized samples generated by state-of-the-art ViGen\nmodels. The dataset includes both text-motion pairs and text-video-motion\ntriplets, substantially expanding semantic diversity. Second, we propose\nViMoGen, a flow-matching-based diffusion transformer that unifies priors from\nMoCap data and ViGen models through gated multimodal conditioning. To enhance\nefficiency, we further develop ViMoGen-light, a distilled variant that\neliminates video generation dependencies while preserving strong\ngeneralization. Finally, we present MBench, a hierarchical benchmark designed\nfor fine-grained evaluation across motion quality, prompt fidelity, and\ngeneralization ability. Extensive experiments show that our framework\nsignificantly outperforms existing approaches in both automatic and human\nevaluations. The code, data, and benchmark will be made publicly available.\n","authors":["Jing Lin","Ruisi Wang","Junzhe Lu","Ziqi Huang","Guorui Song","Ailing Zeng","Xian Liu","Chen Wei","Wanqi Yin","Qingping Sun","Zhongang Cai","Lei Yang","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26786v1","updated":"2025-10-30T17:57:40Z","published":"2025-10-30T17:57:40Z","title":"HEIR: Learning Graph-Based Motion Hierarchies","summary":"  Hierarchical structures of motion exist across research fields, including\ncomputer vision, graphics, and robotics, where complex dynamics typically arise\nfrom coordinated interactions among simpler motion components. Existing methods\nto model such dynamics typically rely on manually-defined or heuristic\nhierarchies with fixed motion primitives, limiting their generalizability\nacross different tasks. In this work, we propose a general hierarchical motion\nmodeling method that learns structured, interpretable motion relationships\ndirectly from data. Our method represents observed motions using graph-based\nhierarchies, explicitly decomposing global absolute motions into\nparent-inherited patterns and local motion residuals. We formulate hierarchy\ninference as a differentiable graph learning problem, where vertices represent\nelemental motions and directed edges capture learned parent-child dependencies\nthrough graph neural networks. We evaluate our hierarchical reconstruction\napproach on three examples: 1D translational motion, 2D rotational motion, and\ndynamic 3D scene deformation via Gaussian splatting. Experimental results show\nthat our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,\nand produces more realistic and interpretable deformations compared to the\nbaseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,\ndata-driven hierarchical modeling paradigm, our method offers a formulation\napplicable to a broad range of motion-centric tasks. Project Page:\nhttps://light.princeton.edu/HEIR/\n","authors":["Cheng Zheng","William Koch","Baiang Li","Felix Heide"],"pdf_url":"https://arxiv.org/pdf/2510.26786v1.pdf","comment":"Code link: https://github.com/princeton-computational-imaging/HEIR"},{"id":"http://arxiv.org/abs/2510.26782v1","updated":"2025-10-30T17:56:43Z","published":"2025-10-30T17:56:43Z","title":"Clone Deterministic 3D Worlds with Geometrically-Regularized World\n  Models","summary":"  A world model is an internal model that simulates how the world evolves.\nGiven past observations and actions, it predicts the future of both the\nembodied agent and its environment. Accurate world models are essential for\nenabling agents to think, plan, and reason effectively in complex, dynamic\nsettings. Despite rapid progress, current world models remain brittle and\ndegrade over long horizons. We argue that a central cause is representation\nquality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or\nentangled latents make dynamics learning unnecessarily hard. We therefore ask\nwhether improving representation learning alone can substantially improve\nworld-model performance. In this work, we take a step toward building a truly\naccurate world model by addressing a fundamental yet open problem: constructing\na model that can fully clone and overfit to a deterministic 3D world. We\npropose Geometrically-Regularized World Models (GRWM), which enforces that\nconsecutive points along a natural sensory trajectory remain close in latent\nrepresentation space. This approach yields significantly improved latent\nrepresentations that align closely with the true topology of the environment.\nGRWM is plug-and-play, requires only minimal architectural modification, scales\nwith trajectory length, and is compatible with diverse latent generative\nbackbones. Across deterministic 3D settings and long-horizon prediction tasks,\nGRWM significantly increases rollout fidelity and stability. Analyses show that\nits benefits stem from learning a latent manifold with superior geometric\nstructure. These findings support a clear takeaway: improving representation\nlearning is a direct and useful path to robust world models, delivering\nreliable long-horizon predictions without enlarging the dynamics module.\n","authors":["Zaishuo Xia","Yukuan Lu","Xinyi Li","Yifan Xu","Yubei Chen"],"pdf_url":"https://arxiv.org/pdf/2510.26782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26781v1","updated":"2025-10-30T17:56:31Z","published":"2025-10-30T17:56:31Z","title":"ChartAB: A Benchmark for Chart Grounding & Dense Alignment","summary":"  Charts play an important role in visualization, reasoning, data analysis, and\nthe exchange of ideas among humans. However, existing vision-language models\n(VLMs) still lack accurate perception of details and struggle to extract\nfine-grained structures from charts. Such limitations in chart grounding also\nhinder their ability to compare multiple charts and reason over them. In this\npaper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a\ncomprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting\ntabular data, localizing visualization elements, and recognizing various\nattributes from charts of diverse types and complexities. We design a JSON\ntemplate to facilitate the calculation of evaluation metrics specifically\ntailored for each grounding task. By incorporating a novel two-stage inference\nworkflow, the benchmark can further evaluate VLMs' capability to align and\ncompare elements/attributes across two charts. Our analysis of evaluations on\nseveral recent VLMs reveals new insights into their perception biases,\nweaknesses, robustness, and hallucinations in chart understanding. These\nfindings highlight the fine-grained discrepancies among VLMs in chart\nunderstanding tasks and point to specific skills that need to be strengthened\nin current models.\n","authors":["Aniruddh Bansal","Davit Soselia","Dang Nguyen","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.26781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26778v1","updated":"2025-10-30T17:55:46Z","published":"2025-10-30T17:55:46Z","title":"Surpassing state of the art on AMD area estimation from RGB fundus\n  images through careful selection of U-Net architectures and loss functions\n  for class imbalance","summary":"  Age-related macular degeneration (AMD) is one of the leading causes of\nirreversible vision impairment in people over the age of 60. This research\nfocuses on semantic segmentation for AMD lesion detection in RGB fundus images,\na non-invasive and cost-effective imaging technique. The results of the ADAM\nchallenge - the most comprehensive AMD detection from RGB fundus images\nresearch competition and open dataset to date - serve as a benchmark for our\nevaluation. Taking the U-Net connectivity as a base of our framework, we\nevaluate and compare several approaches to improve the segmentation model's\narchitecture and training pipeline, including pre-processing techniques,\nencoder (backbone) deep network types of varying complexity, and specialized\nloss functions to mitigate class imbalances on image and pixel levels. The main\noutcome of this research is the final configuration of the AMD detection\nframework, which outperforms all the prior ADAM challenge submissions on the\nmulti-class segmentation of different AMD lesion types in non-invasive RGB\nfundus images. The source code used to conduct the experiments presented in\nthis paper is made freely available.\n","authors":["Valentyna Starodub","Mantas LukoÅ¡eviÄius"],"pdf_url":"https://arxiv.org/pdf/2510.26778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26769v1","updated":"2025-10-30T17:52:39Z","published":"2025-10-30T17:52:39Z","title":"SteerVLM: Robust Model Control through Lightweight Activation Steering\n  for Vision Language Models","summary":"  This work introduces SteerVLM, a lightweight steering module designed to\nguide Vision-Language Models (VLMs) towards outputs that better adhere to\ndesired instructions. Our approach learns from the latent embeddings of paired\nprompts encoding target and converse behaviors to dynamically adjust\nactivations connecting the language modality with image context. This allows\nfor fine-grained, inference-time control over complex output semantics without\nmodifying model weights while preserving performance on off-target tasks. Our\nsteering module requires learning parameters equal to 0.14% of the original\nVLM's size. Our steering module gains model control through dimension-wise\nactivation modulation and adaptive steering across layers without requiring\npre-extracted static vectors or manual tuning of intervention points.\nFurthermore, we introduce VNIA (Visual Narrative Intent Alignment), a\nmultimodal dataset specifically created to facilitate the development and\nevaluation of VLM steering techniques. Our method outperforms existing\nintervention techniques on steering and hallucination mitigation benchmarks for\nVLMs and proposes a robust solution for multimodal model control through\nactivation engineering.\n","authors":["Anushka Sivakumar","Andrew Zhang","Zaber Hakim","Chris Thomas"],"pdf_url":"https://arxiv.org/pdf/2510.26769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26759v1","updated":"2025-10-30T17:49:49Z","published":"2025-10-30T17:49:49Z","title":"MORE: Multi-Organ Medical Image REconstruction Dataset","summary":"  CT reconstruction provides radiologists with images for diagnosis and\ntreatment, yet current deep learning methods are typically limited to specific\nanatomies and datasets, hindering generalization ability to unseen anatomies\nand lesions. To address this, we introduce the Multi-Organ medical image\nREconstruction (MORE) dataset, comprising CT scans across 9 diverse anatomies\nwith 15 lesion types. This dataset serves two key purposes: (1) enabling robust\ntraining of deep learning models on extensive, heterogeneous data, and (2)\nfacilitating rigorous evaluation of model generalization for CT reconstruction.\nWe further establish a strong baseline solution that outperforms prior\napproaches under these challenging conditions. Our results demonstrate that:\n(1) a comprehensive dataset helps improve the generalization capability of\nmodels, and (2) optimization-based methods offer enhanced robustness for unseen\nanatomies. The MORE dataset is freely accessible under CC-BY-NC 4.0 at our\nproject page https://more-med.github.io/\n","authors":["Shaokai Wu","Yapan Guo","Yanbiao Ji","Jing Tong","Yuxiang Lu","Mei Li","Suizhi Huang","Yue Ding","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2510.26759v1.pdf","comment":"Accepted to ACMMM 2025"},{"id":"http://arxiv.org/abs/2508.05417v2","updated":"2025-10-30T17:46:35Z","published":"2025-08-07T14:09:33Z","title":"Smoothing Slot Attention Iterations and Recurrences","summary":"  Slot Attention (SA) and its variants lie at the heart of mainstream\nObject-Centric Learning (OCL). Objects in an image can be aggregated into\nrespective slot vectors, by \\textit{iteratively} refining cold-start query\nvectors, typically three times, via SA on image features. For video, such\naggregation is \\textit{recurrently} shared across frames, with queries\ncold-started on the first frame while transitioned from the previous frame's\nslots on non-first frames. However, the cold-start queries lack sample-specific\ncues thus hinder precise aggregation on the image or video's first frame; Also,\nnon-first frames' queries are already sample-specific thus require transforms\ndifferent from the first frame's aggregation. We address these issues for the\nfirst time with our \\textit{SmoothSA}: (1) To smooth SA iterations on the image\nor video's first frame, we \\textit{preheat} the cold-start queries with rich\ninformation of input features, via a tiny module self-distilled inside OCL; (2)\nTo smooth SA recurrences across all video frames, we \\textit{differentiate} the\nhomogeneous transforms on the first and non-first frames, by using full and\nsingle iterations respectively. Comprehensive experiments on object discovery,\nrecognition and downstream benchmarks validate our method's effectiveness.\nFurther analyses intuitively illuminate how our method smooths SA iterations\nand recurrences. Our source code, model checkpoints and training logs are\navailable on https://github.com/Genera1Z/SmoothSA.\n","authors":["Rongzhen Zhao","Wenyan Yang","Juho Kannala","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2508.05417v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01345v3","updated":"2025-10-30T17:43:18Z","published":"2025-08-02T12:48:04Z","title":"Predicting Video Slot Attention Queries from Random Slot-Feature Pairs","summary":"  Unsupervised video Object-Centric Learning (OCL) is promising as it enables\nobject-level scene representation and dynamics modeling as we humans do.\nMainstream video OCL methods adopt a recurrent architecture: An aggregator\naggregates current video frame into object features, termed slots, under some\nqueries; A transitioner transits current slots to queries for the next frame.\nThis is an effective architecture but all existing implementations both\n(\\textit{i1}) neglect to incorporate next frame features, the most informative\nsource for query prediction, and (\\textit{i2}) fail to learn transition\ndynamics, the knowledge essential for query prediction. To address these\nissues, we propose Random Slot-Feature pair for learning Query prediction\n(RandSF.Q): (\\textit{t1}) We design a new transitioner to incorporate both\nslots and features, which provides more information for query prediction;\n(\\textit{t2}) We train the transitioner to predict queries from slot-feature\npairs randomly sampled from available recurrences, which drives it to learn\ntransition dynamics. Experiments on scene representation demonstrate that our\nmethod surpass existing video OCL methods significantly, e.g., up to 10 points\non object discovery, setting new state-of-the-art. Such superiority also\nbenefits downstream tasks like dynamics modeling. Our core source code, model\ncheckpoints and training logs are available on\nhttps://github.com/Genera1Z/RandSF.Q.\n","authors":["Rongzhen Zhao","Jian Li","Juho Kannala","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2508.01345v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09672v2","updated":"2025-10-30T17:40:53Z","published":"2025-09-11T17:59:08Z","title":"Locality in Image Diffusion Models Emerges from Data Statistics","summary":"  Recent work has shown that the generalization ability of image diffusion\nmodels arises from the locality properties of the trained neural network. In\nparticular, when denoising a particular pixel, the model relies on a limited\nneighborhood of the input image around that pixel, which, according to the\nprevious work, is tightly related to the ability of these models to produce\nnovel images. Since locality is central to generalization, it is crucial to\nunderstand why diffusion models learn local behavior in the first place, as\nwell as the factors that govern the properties of locality patterns. In this\nwork, we present evidence that the locality in deep diffusion models emerges as\na statistical property of the image dataset and is not due to the inductive\nbias of convolutional neural networks, as suggested in previous work.\nSpecifically, we demonstrate that an optimal parametric linear denoiser\nexhibits similar locality properties to deep neural denoisers. We show, both\ntheoretically and experimentally, that this locality arises directly from pixel\ncorrelations present in the image datasets. Moreover, locality patterns are\ndrastically different on specialized datasets, approximating principal\ncomponents of the data's covariance. We use these insights to craft an\nanalytical denoiser that better matches scores predicted by a deep diffusion\nmodel than prior expert-crafted alternatives. Our key takeaway is that while\nneural network architectures influence generation quality, their primary role\nis to capture locality patterns inherent in the data.\n","authors":["Artem Lukoianov","Chenyang Yuan","Justin Solomon","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2509.09672v2.pdf","comment":"31 pages, 20 figures, 7 tables"},{"id":"http://arxiv.org/abs/2507.06078v2","updated":"2025-10-30T17:35:26Z","published":"2025-07-08T15:17:24Z","title":"ScoreAdv: Score-based Targeted Generation of Natural Adversarial\n  Examples via Diffusion Models","summary":"  Despite the success of deep learning across various domains, it remains\nvulnerable to adversarial attacks. Although many existing adversarial attack\nmethods achieve high success rates, they typically rely on $\\ell_{p}$-norm\nperturbation constraints, which do not align with human perceptual\ncapabilities. Consequently, researchers have shifted their focus toward\ngenerating natural, unrestricted adversarial examples (UAEs). GAN-based\napproaches suffer from inherent limitations, such as poor image quality due to\ninstability and mode collapse. Meanwhile, diffusion models have been employed\nfor UAE generation, but they still rely on iterative PGD perturbation\ninjection, without fully leveraging their central denoising capabilities. In\nthis paper, we introduce a novel approach for generating UAEs based on\ndiffusion models, named ScoreAdv. This method incorporates an interpretable\nadversarial guidance mechanism to gradually shift the sampling distribution\ntowards the adversarial distribution, while using an interpretable saliency map\nto inject the visual information of a reference image into the generated\nsamples. Notably, our method is capable of generating an unlimited number of\nnatural adversarial examples and can attack not only classification models but\nalso retrieval models. We conduct extensive experiments on ImageNet and CelebA\ndatasets, validating the performance of ScoreAdv across ten target models in\nboth black-box and white-box settings. Our results demonstrate that ScoreAdv\nachieves state-of-the-art attack success rates and image quality, while\nmaintaining inference efficiency. Furthermore, the dynamic balance between\ndenoising and adversarial perturbation enables ScoreAdv to remain robust even\nunder defensive measures.\n","authors":["Chihan Huang","Hao Tang"],"pdf_url":"https://arxiv.org/pdf/2507.06078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26703v1","updated":"2025-10-30T17:07:04Z","published":"2025-10-30T17:07:04Z","title":"ProstNFound+: A Prospective Study using Medical Foundation Models for\n  Prostate Cancer Detection","summary":"  Purpose: Medical foundation models (FMs) offer a path to build\nhigh-performance diagnostic systems. However, their application to prostate\ncancer (PCa) detection from micro-ultrasound ({\\mu}US) remains untested in\nclinical settings. We present ProstNFound+, an adaptation of FMs for PCa\ndetection from {\\mu}US, along with its first prospective validation. Methods:\nProstNFound+ incorporates a medical FM, adapter tuning, and a custom prompt\nencoder that embeds PCa-specific clinical biomarkers. The model generates a\ncancer heatmap and a risk score for clinically significant PCa. Following\ntraining on multi-center retrospective data, the model is prospectively\nevaluated on data acquired five years later from a new clinical site. Model\npredictions are benchmarked against standard clinical scoring protocols\n(PRI-MUS and PI-RADS). Results: ProstNFound+ shows strong generalization to the\nprospective data, with no performance degradation compared to retrospective\nevaluation. It aligns closely with clinical scores and produces interpretable\nheatmaps consistent with biopsy-confirmed lesions. Conclusion: The results\nhighlight its potential for clinical deployment, offering a scalable and\ninterpretable alternative to expert-driven protocols.\n","authors":["Paul F. R. Wilson","Mohamed Harmanani","Minh Nguyen Nhat To","Amoon Jamzad","Tarek Elghareb","Zhuoxin Guo","Adam Kinnaird","Brian Wodlinger","Purang Abolmaesumi","Parvin Mousavi"],"pdf_url":"https://arxiv.org/pdf/2510.26703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26694v1","updated":"2025-10-30T17:01:18Z","published":"2025-10-30T17:01:18Z","title":"The Impact and Outlook of 3D Gaussian Splatting","summary":"  Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed\nthe landscape of 3D scene representations, inspiring an extensive body of\nassociated research. Follow-up work includes analyses and contributions that\nenhance the efficiency, scalability, and real-world applicability of 3DGS. In\nthis summary, we present an overview of several key directions that have\nemerged in the wake of 3DGS. We highlight advances enabling resource-efficient\ntraining and rendering, the evolution toward dynamic (or four-dimensional,\n4DGS) representations, and deeper exploration of the mathematical foundations\nunderlying its appearance modeling and rendering process. Furthermore, we\nexamine efforts to bring 3DGS to mobile and virtual reality platforms, its\nextension to massive-scale environments, and recent progress toward\nnear-instant radiance field reconstruction via feed-forward or distributed\ncomputation. Collectively, these developments illustrate how 3DGS has evolved\nfrom a breakthrough representation into a versatile and foundational tool for\n3D vision and graphics.\n","authors":["Bernhard Kerbl"],"pdf_url":"https://arxiv.org/pdf/2510.26694v1.pdf","comment":"Article written for Frontiers of Science Award, International\n  Congress on Basic Science, 2025"},{"id":"http://arxiv.org/abs/2510.26684v1","updated":"2025-10-30T16:54:16Z","published":"2025-10-30T16:54:16Z","title":"Process Integrated Computer Vision for Real-Time Failure Prediction in\n  Steel Rolling Mill","summary":"  We present a long-term deployment study of a machine vision-based anomaly\ndetection system for failure prediction in a steel rolling mill. The system\nintegrates industrial cameras to monitor equipment operation, alignment, and\nhot bar motion in real time along the process line. Live video streams are\nprocessed on a centralized video server using deep learning models, enabling\nearly prediction of equipment failures and process interruptions, thereby\nreducing unplanned breakdown costs. Server-based inference minimizes the\ncomputational load on industrial process control systems (PLCs), supporting\nscalable deployment across production lines with minimal additional resources.\nBy jointly analyzing sensor data from data acquisition systems and visual\ninputs, the system identifies the location and probable root causes of\nfailures, providing actionable insights for proactive maintenance. This\nintegrated approach enhances operational reliability, productivity, and\nprofitability in industrial manufacturing environments.\n","authors":["Vaibhav Kurrey","Sivakalyan Pujari","Gagan Raj Gupta"],"pdf_url":"https://arxiv.org/pdf/2510.26684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26681v1","updated":"2025-10-30T16:51:18Z","published":"2025-10-30T16:51:18Z","title":"Improving Classification of Occluded Objects through Scene Context","summary":"  The presence of occlusions has provided substantial challenges to\ntypically-powerful object recognition algorithms. Additional sources of\ninformation can be extremely valuable to reduce errors caused by occlusions.\nScene context is known to aid in object recognition in biological vision. In\nthis work, we attempt to add robustness into existing Region Proposal\nNetwork-Deep Convolutional Neural Network (RPN-DCNN) object detection networks\nthrough two distinct scene-based information fusion techniques. We present one\nalgorithm under each methodology: the first operates prior to prediction,\nselecting a custom object network to use based on the identified background\nscene, and the second operates after detection, fusing scene knowledge into\ninitial object scores output by the RPN. We demonstrate our algorithms on\nchallenging datasets featuring partial occlusions, which show overall\nimprovement in both recall and precision against baseline methods. In addition,\nour experiments contrast multiple training methodologies for occlusion\nhandling, finding that training on a combination of both occluded and\nunoccluded images demonstrates an improvement over the others. Our method is\ninterpretable and can easily be adapted to other datasets, offering many future\ndirections for research and practical applications.\n","authors":["Courtney M. King","Daniel D. Leeds","Damian Lyons","George Kalaitzis"],"pdf_url":"https://arxiv.org/pdf/2510.26681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17434v5","updated":"2025-10-30T16:47:32Z","published":"2023-11-29T08:26:18Z","title":"GSE: Group-wise Sparse and Explainable Adversarial Attacks","summary":"  Sparse adversarial attacks fool deep neural networks (DNNs) through minimal\npixel perturbations, often regularized by the $\\ell_0$ norm. Recent efforts\nhave replaced this norm with a structural sparsity regularizer, such as the\nnuclear group norm, to craft group-wise sparse adversarial attacks. The\nresulting perturbations are thus explainable and hold significant practical\nrelevance, shedding light on an even greater vulnerability of DNNs. However,\ncrafting such attacks poses an optimization challenge, as it involves computing\nnorms for groups of pixels within a non-convex objective. We address this by\npresenting a two-phase algorithm that generates group-wise sparse attacks\nwithin semantically meaningful areas of an image. Initially, we optimize a\nquasinorm adversarial loss using the $1/2-$quasinorm proximal operator tailored\nfor non-convex programming. Subsequently, the algorithm transitions to a\nprojected Nesterov's accelerated gradient descent with $2-$norm regularization\napplied to perturbation magnitudes. Rigorous evaluations on CIFAR-10 and\nImageNet datasets demonstrate a remarkable increase in group-wise sparsity,\ne.g., $50.9\\%$ on CIFAR-10 and $38.4\\%$ on ImageNet (average case, targeted\nattack). This performance improvement is accompanied by significantly faster\ncomputation times, improved explainability, and a $100\\%$ attack success rate.\n","authors":["Shpresim Sadiku","Moritz Wagner","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2311.17434v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15389v3","updated":"2025-10-30T16:42:33Z","published":"2024-12-19T20:43:22Z","title":"Resource Efficient Multi-stain Kidney Glomeruli Segmentation via\n  Self-supervision","summary":"  Semantic segmentation under domain shift remains a fundamental challenge in\ncomputer vision, particularly when labelled training data is scarce. This\nchallenge is particularly exemplified in histopathology image analysis, where\nthe same tissue structures must be segmented across images captured under\ndifferent imaging conditions (stains), each representing a distinct visual\ndomain. Traditional deep learning methods like UNet require extensive labels,\nwhich is both costly and time-consuming, particularly when dealing with\nmultiple domains (or stains). To mitigate this, various unsupervised domain\nadaptation based methods such as UDAGAN have been proposed, which reduce the\nneed for labels by requiring only one (source) stain to be labelled.\nNonetheless, obtaining source stain labels can still be challenging. This\narticle shows that through self-supervised pre-training -- including SimCLR,\nBYOL, and a novel approach, HR-CS-CO -- the performance of these segmentation\nmethods (UNet, and UDAGAN) can be retained even with 95% fewer labels. Notably,\nwith self-supervised pre-training and using only 5% labels, the performance\ndrops are minimal: 5.9% for UNet and 6.2% for UDAGAN, averaged over all stains,\ncompared to their respective fully supervised counterparts (without\npre-training, using 100% labels). Furthermore, these findings are shown to\ngeneralise beyond their training distribution to public benchmark datasets.\nImplementations and pre-trained models are publicly available\n\\href{https://github.com/zeeshannisar/resource-effecient-multi-stain-kidney-glomeruli-segmentation.git}{online}.\n","authors":["Zeeshan Nisar","Friedrich Feuerhake","Thomas Lampert"],"pdf_url":"https://arxiv.org/pdf/2412.15389v3.pdf","comment":"39 pages, 10 figures, 4 Tables"},{"id":"http://arxiv.org/abs/2506.19816v2","updated":"2025-10-30T16:38:19Z","published":"2025-06-24T17:30:27Z","title":"CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame\n  Vision-Language-Action Modeling","summary":"  Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong performance in robotic\nmanipulation. However, these models remain constrained by the single-frame\nimage paradigm and fail to fully leverage the temporal information offered by\nmulti-frame histories, as directly feeding multiple frames into VLM backbones\nincurs substantial computational overhead and inference latency. We propose\nCronusVLA, a unified framework that extends single-frame VLA models to the\nmulti-frame paradigm. CronusVLA follows a two-stage process: (1) Single-frame\npretraining on large-scale embodied datasets with autoregressive prediction of\naction tokens, establishing an effective embodied vision-language foundation;\n(2) Multi-frame post-training, which adapts the prediction of the\nvision-language backbone from discrete tokens to learnable features, and\naggregates historical information via feature chunking. CronusVLA effectively\naddresses the existing challenges of multi-frame modeling while enhancing\nperformance and observational robustness. To evaluate the robustness under\ntemporal and spatial disturbances, we introduce SimplerEnv-OR, a novel\nbenchmark featuring 24 types of observational disturbances and 120 severity\nlevels. Experiments across three embodiments in simulated and real-world\nenvironments demonstrate that CronusVLA achieves leading performance and\nsuperior robustness, with a 70.9% success rate on SimplerEnv, a 26.8%\nimprovement over OpenVLA on LIBERO, and the highest robustness score on\nSimplerEnv-OR. These results highlight the potential of efficient multi-frame\nadaptation in VLA models for more powerful and robust real-world deployment.\n","authors":["Hao Li","Shuai Yang","Yilun Chen","Xinyi Chen","Xiaoda Yang","Yang Tian","Hanqing Wang","Tai Wang","Dahua Lin","Feng Zhao","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2506.19816v2.pdf","comment":"39 pages, 24 figures"},{"id":"http://arxiv.org/abs/2510.26661v1","updated":"2025-10-30T16:29:09Z","published":"2025-10-30T16:29:09Z","title":"BRIQA: Balanced Reweighting in Image Quality Assessment of Pediatric\n  Brain MRI","summary":"  Assessing the severity of artifacts in pediatric brain Magnetic Resonance\nImaging (MRI) is critical for diagnostic accuracy, especially in low-field\nsystems where the signal-to-noise ratio is reduced. Manual quality assessment\nis time-consuming and subjective, motivating the need for robust automated\nsolutions. In this work, we propose BRIQA (Balanced Reweighting in Image\nQuality Assessment), which addresses class imbalance in artifact severity\nlevels. BRIQA uses gradient-based loss reweighting to dynamically adjust\nper-class contributions and employs a rotating batching scheme to ensure\nconsistent exposure to underrepresented classes. Through experiments, no single\narchitecture performs best across all artifact types, emphasizing the\nimportance of architectural diversity. The rotating batching configuration\nimproves performance across metrics by promoting balanced learning when\ncombined with cross-entropy loss. BRIQA improves average macro F1 score from\n0.659 to 0.706, with notable gains in Noise (0.430), Zipper (0.098),\nPositioning (0.097), Contrast (0.217), Motion (0.022), and Banding (0.012)\nartifact severity classification. The code is available at\nhttps://github.com/BioMedIA-MBZUAI/BRIQA.\n","authors":["Alya Almsouti","Ainur Khamitova","Darya Taratynova","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2510.26661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26653v1","updated":"2025-10-30T16:20:28Z","published":"2025-10-30T16:20:28Z","title":"Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning\n  Optical Flow on RADARSAT-2","summary":"  Accurate estimation of sea ice drift is critical for Arctic navigation,\nclimate research, and operational forecasting. While optical flow, a computer\nvision technique for estimating pixel wise motion between consecutive images,\nhas advanced rapidly in computer vision, its applicability to geophysical\nproblems and to satellite SAR imagery remains underexplored. Classical optical\nflow methods rely on mathematical models and strong assumptions about motion,\nwhich limit their accuracy in complex scenarios. Recent deep learning based\napproaches have substantially improved performance and are now the standard in\ncomputer vision, motivating their application to sea ice drift estimation. We\npresent the first large scale benchmark of 48 deep learning optical flow models\non RADARSAT 2 ScanSAR sea ice imagery, evaluated with endpoint error (EPE) and\nFl all metrics against GNSS tracked buoys. Several models achieve sub kilometer\naccuracy (EPE 6 to 8 pixels, 300 to 400 m), a small error relative to the\nspatial scales of sea ice motion and typical navigation requirements in the\nArctic. Our results demonstrate that the models are capable of capturing\nconsistent regional drift patterns and that recent deep learning based optical\nflow methods, which have substantially improved motion estimation accuracy\ncompared to classical methods, can be effectively transferred to polar remote\nsensing. Optical flow produces spatially continuous drift fields, providing\nmotion estimates for every image pixel rather than at sparse buoy locations,\noffering new opportunities for navigation and climate modeling.\n","authors":["Daniela Martin","Joseph Gallego"],"pdf_url":"https://arxiv.org/pdf/2510.26653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26641v1","updated":"2025-10-30T16:08:25Z","published":"2025-10-30T16:08:25Z","title":"All You Need for Object Detection: From Pixels, Points, and Prompts to\n  Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles","summary":"  Autonomous Vehicles (AVs) are transforming the future of transportation\nthrough advances in intelligent perception, decision-making, and control\nsystems. However, their success is tied to one core capability, reliable object\ndetection in complex and multimodal environments. While recent breakthroughs in\nComputer Vision (CV) and Artificial Intelligence (AI) have driven remarkable\nprogress, the field still faces a critical challenge as knowledge remains\nfragmented across multimodal perception, contextual reasoning, and cooperative\nintelligence. This survey bridges that gap by delivering a forward-looking\nanalysis of object detection in AVs, emphasizing emerging paradigms such as\nVision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI\nrather than re-examining outdated techniques. We begin by systematically\nreviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,\nand Radar) and their fusion strategies, highlighting not only their\ncapabilities and limitations in dynamic driving environments but also their\npotential to integrate with recent advances in LLM/VLM-driven perception\nframeworks. Next, we introduce a structured categorization of AV datasets that\nmoves beyond simple collections, positioning ego-vehicle, infrastructure-based,\nand cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a\ncross-analysis of data structures and characteristics. Ultimately, we analyze\ncutting-edge detection methodologies, ranging from 2D and 3D pipelines to\nhybrid sensor fusion, with particular attention to emerging transformer-driven\napproaches powered by Vision Transformers (ViTs), Large and Small Language\nModels (SLMs), and VLMs. By synthesizing these perspectives, our survey\ndelivers a clear roadmap of current capabilities, open challenges, and future\nopportunities.\n","authors":["Sayed Pedram Haeri Boroujeni","Niloufar Mehrabi","Hazim Alzorgan","Ahmad Sarlak","Mahlagha Fazeli","Abolfazl Razi"],"pdf_url":"https://arxiv.org/pdf/2510.26641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26635v1","updated":"2025-10-30T16:04:00Z","published":"2025-10-30T16:04:00Z","title":"SAMRI: Segment Anything Model for MRI","summary":"  Accurate magnetic resonance imaging (MRI) segmentation is crucial for\nclinical decision-making, but remains labor-intensive when performed manually.\nConvolutional neural network (CNN)-based methods can be accurate and efficient,\nbut often generalize poorly to MRI's variable contrast, intensity\ninhomogeneity, and protocols. Although the transformer-based Segment Anything\nModel (SAM) has demonstrated remarkable generalizability in natural images,\nexisting adaptations often treat MRI as another imaging modality, overlooking\nthese modality-specific challenges. We present SAMRI, an MRI-specialized SAM\ntrained and validated on 1.1 million labeled MR slices spanning whole-body\norgans and pathologies. We demonstrate that SAM can be effectively adapted to\nMRI by simply fine-tuning its mask decoder using a two-stage strategy, reducing\ntraining time by 94% and trainable parameters by 96% versus full-model\nretraining. Across diverse MRI segmentation tasks, SAMRI achieves a mean Dice\nof 0.87, delivering state-of-the-art accuracy across anatomical regions and\nrobust generalization on unseen structures, particularly small and clinically\nimportant structures.\n","authors":["Zhao Wang","Wei Dai","Thuy Thanh Dao","Steffen Bollmann","Hongfu Sun","Craig Engstrom","Shekhar S. Chandra"],"pdf_url":"https://arxiv.org/pdf/2510.26635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.16556v2","updated":"2025-10-30T16:01:55Z","published":"2025-10-18T16:00:10Z","title":"Fit for Purpose? Deepfake Detection in the Real World","summary":"  The rapid proliferation of AI-generated content, driven by advances in\ngenerative adversarial networks, diffusion models, and multimodal large\nlanguage models, has made the creation and dissemination of synthetic media\neffortless, heightening the risks of misinformation, particularly political\ndeepfakes that distort truth and undermine trust in political institutions. In\nturn, governments, research institutions, and industry have strongly promoted\ndeepfake detection initiatives as solutions. Yet, most existing models are\ntrained and validated on synthetic, laboratory-controlled datasets, limiting\ntheir generalizability to the kinds of real-world political deepfakes\ncirculating on social platforms that affect the public. In this work, we\nintroduce the first systematic benchmark based on the Political Deepfakes\nIncident Database, a curated collection of real-world political deepfakes\nshared on social media since 2018. Our study includes a systematic evaluation\nof state-of-the-art deepfake detectors across academia, government, and\nindustry. We find that the detectors from academia and government perform\nrelatively poorly. While paid detection tools achieve relatively higher\nperformance than free-access models, all evaluated detectors struggle to\ngeneralize effectively to authentic political deepfakes, and are vulnerable to\nsimple manipulations, especially in the video domain. Results urge the need for\npolitically contextualized deepfake detection frameworks to better safeguard\nthe public in real-world settings.\n","authors":["Guangyu Lin","Li Lin","Christina P. Walker","Daniel S. Schiff","Shu Hu"],"pdf_url":"https://arxiv.org/pdf/2510.16556v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26630v1","updated":"2025-10-30T15:57:20Z","published":"2025-10-30T15:57:20Z","title":"PT-DETR: Small Target Detection Based on Partially-Aware Detail Focus","summary":"  To address the challenges in UAV object detection, such as complex\nbackgrounds, severe occlusion, dense small objects, and varying lighting\nconditions,this paper proposes PT-DETR based on RT-DETR, a novel detection\nalgorithm specifically designed for small objects in UAV imagery. In the\nbackbone network, we introduce the Partially-Aware Detail Focus (PADF) Module\nto enhance feature extraction for small objects. Additionally,we design the\nMedian-Frequency Feature Fusion (MFFF) module,which effectively improves the\nmodel's ability to capture small-object details and contextual information.\nFurthermore,we incorporate Focaler-SIoU to strengthen the model's bounding box\nmatching capability and increase its sensitivity to small-object features,\nthereby further enhancing detection accuracy and robustness. Compared with\nRT-DETR, our PT-DETR achieves mAP improvements of 1.6% and 1.7% on the\nVisDrone2019 dataset with lower computational complexity and fewer parameters,\ndemonstrating its robustness and feasibility for small-object detection tasks.\n","authors":["Bingcong Huo","Zhiming Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.23292v2","updated":"2025-10-30T15:53:26Z","published":"2025-06-29T15:29:03Z","title":"DDL: A Large-Scale Datasets for Deepfake Detection and Localization in\n  Diversified Real-World Scenarios","summary":"  Recent advances in AIGC have exacerbated the misuse of malicious deepfake\ncontent, making the development of reliable deepfake detection methods an\nessential means to address this challenge. Although existing deepfake detection\nmodels demonstrate outstanding performance in detection metrics, most methods\nonly provide simple binary classification results, lacking interpretability.\nRecent studies have attempted to enhance the interpretability of classification\nresults by providing spatial manipulation masks or temporal forgery segments.\nHowever, due to the limitations of forgery datasets, the practical\neffectiveness of these methods remains suboptimal. The primary reason lies in\nthe fact that most existing deepfake datasets contain only binary labels, with\nlimited variety in forgery scenarios, insufficient diversity in deepfake types,\nand relatively small data scales, making them inadequate for complex real-world\nscenarios.To address this predicament, we construct a novel large-scale\ndeepfake detection and localization (\\textbf{DDL}) dataset containing over\n$\\textbf{1.4M+}$ forged samples and encompassing up to $\\textbf{80}$ distinct\ndeepfake methods. The DDL design incorporates four key innovations: (1)\n\\textbf{Comprehensive Deepfake Methods} (covering 7 different generation\narchitectures and a total of 80 methods), (2) \\textbf{Varied Manipulation\nModes} (incorporating 7 classic and 3 novel forgery modes), (3) \\textbf{Diverse\nForgery Scenarios and Modalities} (including 3 scenarios and 3 modalities), and\n(4) \\textbf{Fine-grained Forgery Annotations} (providing 1.18M+ precise spatial\nmasks and 0.23M+ precise temporal segments).Through these improvements, our DDL\nnot only provides a more challenging benchmark for complex real-world forgeries\nbut also offers crucial support for building next-generation deepfake\ndetection, localization, and interpretability methods.\n","authors":["Changtao Miao","Yi Zhang","Weize Gao","Zhiya Tan","Weiwei Feng","Man Luo","Jianshu Li","Ajian Liu","Yunfeng Diao","Qi Chu","Tao Gong","Zhe Li","Weibin Yao","Joey Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.23292v2.pdf","comment":"This paper is a preliminary version, with an extended and\n  comprehensive version currently under development"},{"id":"http://arxiv.org/abs/2508.10566v2","updated":"2025-10-30T15:42:29Z","published":"2025-08-14T12:01:52Z","title":"HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head\n  Synthesis","summary":"  Audio-driven talking head video generation enhances user engagement in\nhuman-computer interaction. However, current methods frequently produce videos\nwith motion blur and lip jitter, primarily due to their reliance on implicit\nmodeling of audio-facial motion correlations--an approach lacking explicit\narticulatory priors (i.e., anatomical guidance for speech-related facial\nmovements). To overcome this limitation, we propose HM-Talker, a novel\nframework for generating high-fidelity, temporally coherent talking heads.\nHM-Talker leverages a hybrid motion representation combining both implicit and\nexplicit motion cues. Explicit cues use Action Units (AUs), anatomically\ndefined facial muscle movements, alongside implicit features to minimize\nphoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement\nModule (CMDM) extracts complementary implicit/explicit motion features while\npredicting AUs directly from audio input aligned to visual cues. To mitigate\nidentity-dependent biases in explicit features and enhance cross-subject\ngeneralization, we introduce the Hybrid Motion Modeling Module (HMMM). This\nmodule dynamically merges randomly paired implicit/explicit features, enforcing\nidentity-agnostic learning. Together, these components enable robust lip\nsynchronization across diverse identities, advancing personalized talking head\nsynthesis. Extensive experiments demonstrate HM-Talker's superiority over\nstate-of-the-art methods in visual quality and lip-sync accuracy.\n","authors":["Shiyu Liu","Kui Jiang","Xianming Liu","Hongxun Yao","Xiaocheng Feng"],"pdf_url":"https://arxiv.org/pdf/2508.10566v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26614v1","updated":"2025-10-30T15:40:34Z","published":"2025-10-30T15:40:34Z","title":"Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event\n  Cameras","summary":"  We propose tokenization of events and present a tokenizer, Spiking Patches,\nspecifically designed for event cameras. Given a stream of asynchronous and\nspatially sparse events, our goal is to discover an event representation that\npreserves these properties. Prior works have represented events as frames or as\nvoxels. However, while these representations yield high accuracy, both frames\nand voxels are synchronous and decrease the spatial sparsity. Spiking Patches\ngives the means to preserve the unique properties of event cameras and we show\nin our experiments that this comes without sacrificing accuracy. We evaluate\nour tokenizer using a GNN, PCN, and a Transformer on gesture recognition and\nobject detection. Tokens from Spiking Patches yield inference times that are up\nto 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We\nachieve this while matching their accuracy and even surpassing in some cases\nwith absolute improvements up to 3.8 for gesture recognition and up to 1.4 for\nobject detection. Thus, tokenization constitutes a novel direction in\nevent-based vision and marks a step towards methods that preserve the\nproperties of event cameras.\n","authors":["Christoffer Koo ÃhrstrÃ¸m","Ronja GÃ¼ldenring","Lazaros Nalpantidis"],"pdf_url":"https://arxiv.org/pdf/2510.26614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.14904v2","updated":"2025-10-30T15:39:25Z","published":"2025-10-16T17:20:22Z","title":"MaskCaptioner: Learning to Jointly Segment and Caption Object\n  Trajectories in Videos","summary":"  Dense Video Object Captioning (DVOC) is the task of jointly detecting,\ntracking, and captioning object trajectories in a video, requiring the ability\nto understand spatio-temporal details and describe them in natural language.\nDue to the complexity of the task and the high cost associated with manual\nannotation, previous approaches resort to disjoint training strategies,\npotentially leading to suboptimal performance. To circumvent this issue, we\npropose to generate captions about spatio-temporally localized entities\nleveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets\nwith our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an\nend-to-end model capable of jointly detecting, segmenting, tracking and\ncaptioning object trajectories. Moreover, with pretraining on LVISCap and\nLV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three\nexisting benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are\navailable at https://www.gabriel.fiastre.fr/maskcaptioner/.\n","authors":["Gabriel Fiastre","Antoine Yang","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2510.14904v2.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.26609v1","updated":"2025-10-30T15:37:40Z","published":"2025-10-30T15:37:40Z","title":"CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for\n  Satellite Sensing","summary":"  Accurate and timely crop yield prediction is crucial for global food security\nand modern agricultural management. Traditional methods often lack the\nscalability and granularity required for precision farming. This paper\nintroduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder\nfor Satellite Sensing), a deep learning model designed for high-resolution,\nintra-field canola yield prediction. CYPRESS leverages a pre-trained,\nlarge-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for\na continuous regression task, transforming multi-temporal satellite imagery\ninto dense, pixel-level yield maps. Evaluated on a comprehensive dataset from\nthe Canadian Prairies, CYPRESS demonstrates superior performance over existing\ndeep learning-based yield prediction models, highlighting the effectiveness of\nfine-tuning foundation models for specialized agricultural applications. By\nproviding a continuous, high-resolution output, CYPRESS offers a more\nactionable tool for precision agriculture than conventional classification or\ncounty-level aggregation methods. This work validates a novel approach that\nbridges the gap between large-scale Earth observation and on-farm\ndecision-making, offering a scalable solution for detailed agricultural\nmonitoring.\n","authors":["Shayan Nejadshamsi","Yuanyuan Zhang","Shadi Zaki","Brock Porth","Lysa Porth","Vahab Khoshdel"],"pdf_url":"https://arxiv.org/pdf/2510.26609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26601v1","updated":"2025-10-30T15:29:20Z","published":"2025-10-30T15:29:20Z","title":"ResMatching: Noise-Resilient Computational Super-Resolution via Guided\n  Conditional Flow Matching","summary":"  Computational Super-Resolution (CSR) in fluorescence microscopy has, despite\nbeing an ill-posed problem, a long history. At its very core, CSR is about\nfinding a prior that can be used to extrapolate frequencies in a micrograph\nthat have never been imaged by the image-generating microscope. It stands to\nreason that, with the advent of better data-driven machine learning techniques,\nstronger prior can be learned and hence CSR can lead to better results. Here,\nwe present ResMatching, a novel CSR method that uses guided conditional flow\nmatching to learn such improved data-priors. We evaluate ResMatching on 4\ndiverse biological structures from the BioSR dataset and compare its results\nagainst 7 baselines. ResMatching consistently achieves competitive results,\ndemonstrating in all cases the best trade-off between data fidelity and\nperceptual realism. We observe that CSR using ResMatching is particularly\neffective in cases where a strong prior is hard to learn, e.g. when the given\nlow-resolution images contain a lot of noise. Additionally, we show that\nResMatching can be used to sample from an implicitly learned posterior\ndistribution and that this distribution is calibrated for all tested use-cases,\nenabling our method to deliver a pixel-wise data-uncertainty term that can\nguide future users to reject uncertain predictions.\n","authors":["Anirban Ray","Vera Galinova","Florian Jug"],"pdf_url":"https://arxiv.org/pdf/2510.26601v1.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2510.26583v1","updated":"2025-10-30T15:11:16Z","published":"2025-10-30T15:11:16Z","title":"Emu3.5: Native Multimodal Models are World Learners","summary":"  We introduce Emu3.5, a large-scale multimodal world model that natively\npredicts the next state across vision and language. Emu3.5 is pre-trained\nend-to-end with a unified next-token prediction objective on a corpus of\nvision-language interleaved data containing over 10 trillion tokens, primarily\nderived from sequential frames and transcripts of internet videos. The model\nnaturally accepts interleaved vision-language inputs and generates interleaved\nvision-language outputs. Emu3.5 is further post-trained with large-scale\nreinforcement learning to enhance multimodal reasoning and generation. To\nimprove inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),\nwhich converts token-by-token decoding into bidirectional parallel prediction,\naccelerating per-image inference by about 20x without sacrificing performance.\nEmu3.5 exhibits strong native multimodal capabilities, including long-horizon\nvision-language generation, any-to-image (X2I) generation, and complex\ntext-rich image generation. It also exhibits generalizable world-modeling\nabilities, enabling spatiotemporally consistent world exploration and\nopen-world embodied manipulation across diverse scenarios and tasks. For\ncomparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image\n(Nano Banana) on image generation and editing tasks and demonstrates superior\nresults on a suite of interleaved generation tasks. We open-source Emu3.5 at\nhttps://github.com/baaivision/Emu3.5 to support community research.\n","authors":["Yufeng Cui","Honghao Chen","Haoge Deng","Xu Huang","Xinghang Li","Jirong Liu","Yang Liu","Zhuoyan Luo","Jinsheng Wang","Wenxuan Wang","Yueze Wang","Chengyuan Wang","Fan Zhang","Yingli Zhao","Ting Pan","Xianduo Li","Zecheng Hao","Wenxuan Ma","Zhuo Chen","Yulong Ao","Tiejun Huang","Zhongyuan Wang","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26583v1.pdf","comment":"project page: https://emu.world"},{"id":"http://arxiv.org/abs/2510.26582v1","updated":"2025-10-30T15:10:02Z","published":"2025-10-30T15:10:02Z","title":"CATCH: A Modular Cross-domain Adaptive Template with Hook","summary":"  Recent advances in Visual Question Answering (VQA) have demonstrated\nimpressive performance in natural image domains, with models like LLaVA\nleveraging large language models (LLMs) for open-ended reasoning. However,\ntheir generalization degrades significantly when transferred to out-of-domain\nscenarios such as remote sensing, medical imaging, or math diagrams, due to\nlarge distributional shifts and the lack of effective domain adaptation\nmechanisms. Existing approaches typically rely on per-domain fine-tuning or\nbespoke pipelines, which are costly, inflexible, and not scalable across\ndiverse tasks. In this paper, we propose CATCH, a plug-and-play framework for\ncross-domain adaptation that improves the generalization of VQA models while\nrequiring minimal changes to their core architecture. Our key idea is to\ndecouple visual and linguistic adaptation by introducing two lightweight\nmodules: a domain classifier to identify the input image type, and a dual\nadapter mechanism comprising a Prompt Adapter for language modulation and a\nVisual Adapter for vision feature adjustment. Both modules are dynamically\ninjected via a unified hook interface, requiring no retraining of the backbone\nmodel. Experimental results across four domain-specific VQA benchmarks\ndemonstrate that our framework achieves consistent performance gains without\nretraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on\nMedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH\nprovides a scalable and extensible approach to multi-domain VQA, enabling\npractical deployment across diverse application domains.\n","authors":["Xinjin Li","Yulie Lu","Jinghan Cao","Yu Ma","Zhenglin Li","Yeyang Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.26582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26580v1","updated":"2025-10-30T15:07:55Z","published":"2025-10-30T15:07:55Z","title":"Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in\n  Zero-Shot Real-World Scenarios","summary":"  In real-world environments, AI systems often face unfamiliar scenarios\nwithout labeled data, creating a major challenge for conventional scene\nunderstanding models. The inability to generalize across unseen contexts limits\nthe deployment of vision-based applications in dynamic, unstructured settings.\nThis work introduces a Dynamic Context-Aware Scene Reasoning framework that\nleverages Vision-Language Alignment to address zero-shot real-world scenarios.\nThe goal is to enable intelligent systems to infer and adapt to new\nenvironments without prior task-specific training. The proposed approach\nintegrates pre-trained vision transformers and large language models to align\nvisual semantics with natural language descriptions, enhancing contextual\ncomprehension. A dynamic reasoning module refines predictions by combining\nglobal scene cues and object-level interactions guided by linguistic priors.\nExtensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and\nOpen Images demonstrate up to 18% improvement in scene understanding accuracy\nover baseline models in complex and unseen environments. Results also show\nrobust performance in ambiguous or cluttered scenes due to the synergistic\nfusion of vision and language. This framework offers a scalable and\ninterpretable approach for context-aware reasoning, advancing zero-shot\ngeneralization in dynamic real-world settings.\n","authors":["Manjunath Prasad Holenarasipura Rajiv","B. M. Vidyavathi"],"pdf_url":"https://arxiv.org/pdf/2510.26580v1.pdf","comment":"Preprint under review at IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI), 2025"},{"id":"http://arxiv.org/abs/2510.26573v1","updated":"2025-10-30T15:00:50Z","published":"2025-10-30T15:00:50Z","title":"Comparative Analysis of Deep Learning Models for Olive Tree Crown and\n  Shadow Segmentation Towards Biovolume Estimation","summary":"  Olive tree biovolume estimation is a key task in precision agriculture,\nsupporting yield prediction and resource management, especially in\nMediterranean regions severely impacted by climate-induced stress. This study\npresents a comparative analysis of three deep learning models U-Net,\nYOLOv11m-seg, and Mask RCNN for segmenting olive tree crowns and their shadows\nin ultra-high resolution UAV imagery. The UAV dataset, acquired over\nVicopisano, Italy, includes manually annotated crown and shadow masks. Building\non these annotations, the methodology emphasizes spatial feature extraction and\nrobust segmentation; per-tree biovolume is then estimated by combining crown\nprojected area with shadow-derived height using solar geometry. In testing,\nMask R-CNN achieved the best overall accuracy (F1 = 0.86; mIoU = 0.72), while\nYOLOv11m-seg provided the fastest throughput (0.12 second per image). The\nestimated biovolumes spanned from approximately 4 to 24 cubic meters,\nreflecting clear structural differences among trees. These results indicate\nMask R-CNN is preferable when biovolume accuracy is paramount, whereas\nYOLOv11m-seg suits large-area deployments where speed is critical; U-Net\nremains a lightweight, high-sensitivity option. The framework enables accurate,\nscalable orchard monitoring and can be further strengthened with DEM or DSM\nintegration and field calibration for operational decision support.\n","authors":["Wondimagegn Abebe Demissie","Stefano Roccella","Rudy Rossetto","Antonio Minnocci","Andrea Vannini","Luca Sebastiani"],"pdf_url":"https://arxiv.org/pdf/2510.26573v1.pdf","comment":"6 pages, 2025 IEEE International Workshop on Metrology for\n  Agriculture and Forestry (MetroAgriFor)"},{"id":"http://arxiv.org/abs/2510.26569v1","updated":"2025-10-30T14:59:37Z","published":"2025-10-30T14:59:37Z","title":"AdSum: Two-stream Audio-visual Summarization for Automated Video\n  Advertisement Clipping","summary":"  Advertisers commonly need multiple versions of the same advertisement (ad) at\nvarying durations for a single campaign. The traditional approach involves\nmanually selecting and re-editing shots from longer video ads to create shorter\nversions, which is labor-intensive and time-consuming. In this paper, we\nintroduce a framework for automated video ad clipping using video summarization\ntechniques. We are the first to frame video clipping as a shot selection\nproblem, tailored specifically for advertising. Unlike existing general video\nsummarization methods that primarily focus on visual content, our approach\nemphasizes the critical role of audio in advertising. To achieve this, we\ndevelop a two-stream audio-visual fusion model that predicts the importance of\nvideo frames, where importance is defined as the likelihood of a frame being\nselected in the firm-produced short ad. To address the lack of ad-specific\ndatasets, we present AdSum204, a novel dataset comprising 102 pairs of\n30-second and 15-second ads from real advertising campaigns. Extensive\nexperiments demonstrate that our model outperforms state-of-the-art methods\nacross various metrics, including Average Precision, Area Under Curve,\nSpearman, and Kendall.\n","authors":["Wen Xie","Yanjun Zhu","Gijs Overgoor","Yakov Bart","Agata Lapedriza Garcia","Sarah Ostadabbas"],"pdf_url":"https://arxiv.org/pdf/2510.26569v1.pdf","comment":"Accepted at 32nd International Conference on MultiMedia Modeling"},{"id":"http://arxiv.org/abs/2510.26568v1","updated":"2025-10-30T14:58:16Z","published":"2025-10-30T14:58:16Z","title":"SA$^{2}$Net: Scale-Adaptive Structure-Affinity Transformation for Spine\n  Segmentation from Ultrasound Volume Projection Imaging","summary":"  Spine segmentation, based on ultrasound volume projection imaging (VPI),\nplays a vital role for intelligent scoliosis diagnosis in clinical\napplications. However, this task faces several significant challenges. Firstly,\nthe global contextual knowledge of spines may not be well-learned if we neglect\nthe high spatial correlation of different bone features. Secondly, the spine\nbones contain rich structural knowledge regarding their shapes and positions,\nwhich deserves to be encoded into the segmentation process. To address these\nchallenges, we propose a novel scale-adaptive structure-aware network\n(SA$^{2}$Net) for effective spine segmentation. First, we propose a\nscale-adaptive complementary strategy to learn the cross-dimensional\nlong-distance correlation features for spinal images. Second, motivated by the\nconsistency between multi-head self-attention in Transformers and semantic\nlevel affinity, we propose structure-affinity transformation to transform\nsemantic features with class-specific affinity and combine it with a\nTransformer decoder for structure-aware reasoning. In addition, we adopt a\nfeature mixing loss aggregation method to enhance model training. This method\nimproves the robustness and accuracy of the segmentation process. The\nexperimental results demonstrate that our SA$^{2}$Net achieves superior\nsegmentation performance compared to other state-of-the-art methods. Moreover,\nthe adaptability of SA$^{2}$Net to various backbones enhances its potential as\na promising tool for advanced scoliosis diagnosis using intelligent spinal\nimage analysis. The code and experimental demo are available at\nhttps://github.com/taetiseo09/SA2Net.\n","authors":["Hao Xie","Zixun Huang","Yushen Zuo","Yakun Ju","Frank H. F. Leung","N. F. Law","Kin-Man Lam","Yong-Ping Zheng","Sai Ho Ling"],"pdf_url":"https://arxiv.org/pdf/2510.26568v1.pdf","comment":"Accepted by Computerized Medical Imaging and Graphics (CMIG)"},{"id":"http://arxiv.org/abs/2510.08771v2","updated":"2025-10-30T14:46:21Z","published":"2025-10-09T19:41:51Z","title":"LinearSR: Unlocking Linear Attention for Stable and Efficient Image\n  Super-Resolution","summary":"  Generative models for Image Super-Resolution (SR) are increasingly powerful,\nyet their reliance on self-attention's quadratic complexity (O(N^2)) creates a\nmajor computational bottleneck. Linear Attention offers an O(N) solution, but\nits promise for photorealistic SR has remained largely untapped, historically\nhindered by a cascade of interrelated and previously unsolved challenges. This\npaper introduces LinearSR, a holistic framework that, for the first time,\nsystematically overcomes these critical hurdles. Specifically, we resolve a\nfundamental, training instability that causes catastrophic model divergence\nusing our novel \"knee point\"-based Early-Stopping Guided Fine-tuning (ESGF)\nstrategy. Furthermore, we mitigate the classic perception-distortion trade-off\nwith a dedicated SNR-based Mixture of Experts (MoE) architecture. Finally, we\nestablish an effective and lightweight guidance paradigm, TAG, derived from our\n\"precision-over-volume\" principle. Our resulting LinearSR model simultaneously\ndelivers state-of-the-art perceptual quality with exceptional efficiency. Its\ncore diffusion forward pass (1-NFE) achieves SOTA-level speed, while its\noverall multi-step inference time remains highly competitive. This work\nprovides the first robust methodology for applying Linear Attention in the\nphotorealistic SR domain, establishing a foundational paradigm for future\nresearch in efficient generative super-resolution.\n","authors":["Xiaohui Li","Shaobin Zhuang","Shuo Cao","Yang Yang","Yuandong Pu","Qi Qin","Siqi Luo","Bin Fu","Yihao Liu"],"pdf_url":"https://arxiv.org/pdf/2510.08771v2.pdf","comment":"19 pages, 9 figures, 6 tables"},{"id":"http://arxiv.org/abs/2501.05783v2","updated":"2025-10-30T14:04:15Z","published":"2025-01-10T08:33:31Z","title":"UV-Attack: Physical-World Adversarial Attacks for Person Detection via\n  Dynamic-NeRF-based UV Mapping","summary":"  In recent research, adversarial attacks on person detectors using patches or\nstatic 3D model-based texture modifications have struggled with low success\nrates due to the flexible nature of human movement. Modeling the 3D\ndeformations caused by various actions has been a major challenge. Fortunately,\nadvancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer\nnew possibilities. In this paper, we introduce UV-Attack, a groundbreaking\napproach that achieves high success rates even with extensive and unseen human\nactions. We address the challenge above by leveraging dynamic-NeRF-based UV\nmapping. UV-Attack can generate human images across diverse actions and\nviewpoints, and even create novel actions by sampling from the SMPL parameter\nspace. While dynamic NeRF models are capable of modeling human bodies,\nmodifying clothing textures is challenging because they are embedded in neural\nnetwork parameters. To tackle this, UV-Attack generates UV maps instead of RGB\nimages and modifies the texture stacks. This approach enables real-time texture\nedits and makes the attack more practical. We also propose a novel Expectation\nover Pose Transformation loss (EoPT) to improve the evasion success rate on\nunseen poses and views. Our experiments show that UV-Attack achieves a 92.7%\nattack success rate against the FastRCNN model across varied poses in dynamic\nvideo settings, significantly outperforming the state-of-the-art AdvCamou\nattack, which only had a 28.5% ASR. Moreover, we achieve 49.5% ASR on the\nlatest YOLOv8 detector in black-box settings. This work highlights the\npotential of dynamic NeRF-based UV mapping for creating more effective\nadversarial attacks on person detectors, addressing key challenges in modeling\nhuman movement and texture modification. The code is available at\nhttps://github.com/PolyLiYJ/UV-Attack.\n","authors":["Yanjie Li","Kaisheng Liang","Bin Xiao"],"pdf_url":"https://arxiv.org/pdf/2501.05783v2.pdf","comment":"23 pages, 22 figures, accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2510.26509v1","updated":"2025-10-30T14:03:09Z","published":"2025-10-30T14:03:09Z","title":"Analysis of the Robustness of an Edge Detector Based on Cellular\n  Automata Optimized by Particle Swarm","summary":"  The edge detection task is essential in image processing aiming to extract\nrelevant information from an image. One recurring problem in this task is the\nweaknesses found in some detectors, such as the difficulty in detecting loose\nedges and the lack of context to extract relevant information from specific\nproblems. To address these weaknesses and adapt the detector to the properties\nof an image, an adaptable detector described by two-dimensional cellular\nautomaton and optimized by meta-heuristic combined with transfer learning\ntechniques was developed. This study aims to analyze the impact of expanding\nthe search space of the optimization phase and the robustness of the\nadaptability of the detector in identifying edges of a set of natural images\nand specialized subsets extracted from the same image set. The results obtained\nprove that expanding the search space of the optimization phase was not\neffective for the chosen image set. The study also analyzed the adaptability of\nthe model through a series of experiments and validation techniques and found\nthat, regardless of the validation, the model was able to adapt to the input\nand the transfer learning techniques applied to the model showed no significant\nimprovements.\n","authors":["VinÃ­cius Ferraria","Eurico Ruivo"],"pdf_url":"https://arxiv.org/pdf/2510.26509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21004v2","updated":"2025-10-30T13:43:39Z","published":"2024-10-28T13:28:21Z","title":"A Continuous and Interpretable Morphometric for Robust Quantification of\n  Dynamic Biological Shapes","summary":"  We introduce the Push-Forward Signed Distance Morphometric (PF-SDM) for shape\nquantification in biomedical imaging. The PF-SDM compactly encodes geometric\nand topological properties of closed shapes, including their skeleton and\nsymmetries. This provides robust and interpretable features for shape\ncomparison and machine learning. The PF-SDM is mathematically smooth, providing\naccess to gradients and differential-geometric quantities. It also extends to\ntemporal dynamics and allows fusing spatial intensity distributions, such as\ngenetic markers, with shape dynamics. We present the PF-SDM theory, benchmark\nit on synthetic data, and apply it to predicting body-axis formation in mouse\ngastruloids, outperforming a CNN baseline in both accuracy and speed.\n","authors":["Roua Rouatbi","Juan-Esteban Suarez Cardona","Alba Villaronga-Luque","Jesse V. Veenvliet","Ivo F. Sbalzarini"],"pdf_url":"https://arxiv.org/pdf/2410.21004v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08788v2","updated":"2025-10-30T13:41:48Z","published":"2024-01-30T09:05:38Z","title":"VerifIoU -- Robustness of Object Detection to Perturbations","summary":"  We introduce a novel Interval Bound Propagation (IBP) approach for the formal\nverification of object detection models, specifically targeting the\nIntersection over Union (IoU) metric. The approach has been implemented in an\nopen source code, named IBP IoU, compatible with popular abstract\ninterpretation based verification tools. The resulting verifier is evaluated on\nlanding approach runway detection and handwritten digit recognition case\nstudies. Comparisons against a baseline (Vanilla IBP IoU) highlight the\nsuperior performance of IBP IoU in ensuring accuracy and stability,\ncontributing to more secure and robust machine learning applications.\n","authors":["NoÃ©mie Cohen","MÃ©lanie Ducoffe","Ryma Boumazouza","Christophe Gabreau","Claire Pagetti","Xavier Pucel","Audrey Galametz"],"pdf_url":"https://arxiv.org/pdf/2403.08788v2.pdf","comment":"44th Digital Avionics Systems Conference (DASC), Sep 2025, Montreal,\n  Canada"},{"id":"http://arxiv.org/abs/2509.24325v2","updated":"2025-10-30T13:38:59Z","published":"2025-09-29T06:23:47Z","title":"ReCon-GS: Continuum-Preserved Gaussian Streaming for Fast and Compact\n  Reconstruction of Dynamic Scenes","summary":"  Online free-viewpoint video (FVV) reconstruction is challenged by slow\nper-frame optimization, inconsistent motion estimation, and unsustainable\nstorage demands. To address these challenges, we propose the Reconfigurable\nContinuum Gaussian Stream, dubbed ReCon-GS, a novel storage-aware framework\nthat enables high fidelity online dynamic scene reconstruction and real-time\nrendering. Specifically, we dynamically allocate multi-level Anchor Gaussians\nin a density-adaptive fashion to capture inter-frame geometric deformations,\nthereby decomposing scene motion into compact coarse-to-fine representations.\nThen, we design a dynamic hierarchy reconfiguration strategy that preserves\nlocalized motion expressiveness through on-demand anchor re-hierarchization,\nwhile ensuring temporal consistency through intra-hierarchical deformation\ninheritance that confines transformation priors to their respective hierarchy\nlevels. Furthermore, we introduce a storage-aware optimization mechanism that\nflexibly adjusts the density of Anchor Gaussians at different hierarchy levels,\nenabling a controllable trade-off between reconstruction fidelity and memory\nusage. Extensive experiments on three widely used datasets demonstrate that,\ncompared to state-of-the-art methods, ReCon-GS improves training efficiency by\napproximately 15% and achieves superior FVV synthesis quality with enhanced\nrobustness and stability. Moreover, at equivalent rendering quality, ReCon-GS\nslashes memory requirements by over 50% compared to leading state-of-the-art\nmethods.\n","authors":["Jiaye Fu","Qiankun Gao","Chengxiang Wen","Yanmin Wu","Siwei Ma","Jiaqi Zhang","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.24325v2.pdf","comment":"Published in NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26474v1","updated":"2025-10-30T13:26:58Z","published":"2025-10-30T13:26:58Z","title":"Counteracting Matthew Effect in Self-Improvement of LVLMs through\n  Head-Tail Re-balancing","summary":"  Self-improvement has emerged as a mainstream paradigm for advancing the\nreasoning capabilities of large vision-language models (LVLMs), where models\nexplore and learn from successful trajectories iteratively. However, we\nidentify a critical issue during this process: the model excels at generating\nhigh-quality trajectories for simple queries (i.e., head data) but struggles\nwith more complex ones (i.e., tail data). This leads to an imbalanced\noptimization that drives the model to prioritize simple reasoning skills, while\nhindering its ability to tackle more complex reasoning tasks. Over iterations,\nthis imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew\neffect\"--which ultimately hinders further model improvement and leads to\nperformance bottlenecks. To counteract this challenge, we introduce four\nefficient strategies from two perspectives: distribution-reshaping and\ntrajectory-resampling, to achieve head-tail re-balancing during the\nexploration-and-learning self-improvement process. Extensive experiments on\nQwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks\ndemonstrate that our methods consistently improve visual reasoning\ncapabilities, outperforming vanilla self-improvement by 3.86 points on average.\n","authors":["Xin Guo","Zhiheng Xi","Yiwen Ding","Yitao Zhai","Xiaowei Shi","Xunliang Cai","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26474v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2505.18766v2","updated":"2025-10-30T13:19:55Z","published":"2025-05-24T16:09:26Z","title":"StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks\n  by Style Perturbations","summary":"  Recently, text-to-image diffusion models have been widely used for style\nmimicry and personalized customization through methods such as DreamBooth and\nTextual Inversion. This has raised concerns about intellectual property\nprotection and the generation of deceptive content. Recent studies, such as\nGlaze and Anti-DreamBooth, have proposed using adversarial noise to protect\nimages from these attacks. However, recent purification-based methods, such as\nDiffPure and Noise Upscaling, have successfully attacked these latest defenses,\nshowing the vulnerabilities of these methods. Moreover, present methods show\nlimited transferability across models, making them less effective against\nunknown text-to-image models. To address these issues, we propose a novel\nanti-mimicry method, StyleGuard. We propose a novel style loss that optimizes\nthe style-related features in the latent space to make it deviate from the\noriginal image, which improves model-agnostic transferability. Additionally, to\nenhance the perturbation's ability to bypass diffusion-based purification, we\ndesigned a novel upscale loss that involves ensemble purifiers and upscalers\nduring training. Extensive experiments on the WikiArt and CelebA datasets\ndemonstrate that StyleGuard outperforms existing methods in robustness against\nvarious transformations and purifications, effectively countering style mimicry\nin various models. Moreover, StyleGuard is effective on different style mimicry\nmethods, including DreamBooth and Textual Inversion. The code is available at\nhttps://github.com/PolyLiYJ/StyleGuard.\n","authors":["Yanjie Li","Wenxuan Zhang","Xinqi Lyu","Yihao Liu","Bin Xiao"],"pdf_url":"https://arxiv.org/pdf/2505.18766v2.pdf","comment":"Accepted by NIPS2025"},{"id":"http://arxiv.org/abs/2510.26466v1","updated":"2025-10-30T13:11:23Z","published":"2025-10-30T13:11:23Z","title":"Representation-Level Counterfactual Calibration for Debiased Zero-Shot\n  Recognition","summary":"  Object-context shortcuts remain a persistent challenge in vision-language\nmodels, undermining zero-shot reliability when test-time scenes differ from\nfamiliar training co-occurrences. We recast this issue as a causal inference\nproblem and ask: Would the prediction remain if the object appeared in a\ndifferent environment? To answer this at inference time, we estimate object and\nbackground expectations within CLIP's representation space, and synthesize\ncounterfactual embeddings by recombining object features with diverse\nalternative contexts sampled from external datasets, batch neighbors, or\ntext-derived descriptions. By estimating the Total Direct Effect and simulating\nintervention, we further subtract background-only activation, preserving\nbeneficial object-context interactions while mitigating hallucinated scores.\nWithout retraining or prompt design, our method substantially improves both\nworst-group and average accuracy on context-sensitive benchmarks, establishing\na new zero-shot state of the art. Beyond performance, our framework provides a\nlightweight representation-level counterfactual approach, offering a practical\ncausal avenue for debiased and reliable multimodal reasoning.\n","authors":["Pei Peng","MingKun Xie","Hang Hao","Tong Jin","ShengJun Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26464v1","updated":"2025-10-30T13:09:00Z","published":"2025-10-30T13:09:00Z","title":"Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly\n  Detection","summary":"  Few-shot anomaly detection (FSAD) methods identify anomalous regions with few\nknown normal samples. Most existing methods rely on the generalization ability\nof pre-trained vision-language models (VLMs) to recognize potentially anomalous\nregions through feature similarity between text descriptions and images.\nHowever, due to the lack of detailed textual descriptions, these methods can\nonly pre-define image-level descriptions to match each visual patch token to\nidentify potential anomalous regions, which leads to the semantic misalignment\nbetween image descriptions and patch-level visual anomalies, achieving\nsub-optimal localization performance. To address the above issues, we propose\nthe Multi-Level Fine-Grained Semantic Caption (MFSC) to provide multi-level and\nfine-grained textual descriptions for existing anomaly detection datasets with\nautomatic construction pipeline. Based on the MFSC, we propose a novel\nframework named FineGrainedAD to improve anomaly localization performance,\nwhich consists of two components: Multi-Level Learnable Prompt (MLLP) and\nMulti-Level Semantic Alignment (MLSA). MLLP introduces fine-grained semantics\ninto multi-level learnable prompts through automatic replacement and\nconcatenation mechanism, while MLSA designs region aggregation strategy and\nmulti-level alignment training to facilitate learnable prompts better align\nwith corresponding visual regions. Experiments demonstrate that the proposed\nFineGrainedAD achieves superior overall performance in few-shot settings on\nMVTec-AD and VisA datasets.\n","authors":["Yuanting Fan","Jun Liu","Xiaochen Chen","Bin-Bin Gao","Jian Li","Yong Liu","Jinlong Peng","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26464v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2503.23722v3","updated":"2025-10-30T12:49:25Z","published":"2025-03-31T04:47:05Z","title":"LATex: Leveraging Attribute-based Text Knowledge for Aerial-Ground\n  Person Re-Identification","summary":"  As an important task in intelligent transportation systems, Aerial-Ground\nperson Re-IDentification (AG-ReID) aims to retrieve specific persons across\nheterogeneous cameras in different viewpoints. Previous methods typically adopt\ndeep learning-based models, focusing on extracting view-invariant features.\nHowever, they usually overlook the semantic information in person attributes.\nIn addition, existing training strategies often rely on full fine-tuning\nlarge-scale models, which significantly increases training costs. To address\nthese issues, we propose a novel framework named LATex for AG-ReID, which\nadopts prompt-tuning strategies to leverage attribute-based text knowledge.\nSpecifically, with the Contrastive Language-Image Pre-training (CLIP) model, we\nfirst propose an Attribute-aware Image Encoder (AIE) to extract both global\nsemantic features and attribute-aware features from input images. Then, with\nthese features, we propose a Prompted Attribute Classifier Group (PACG) to\npredict person attributes and obtain attribute representations. Finally, we\ndesign a Coupled Prompt Template (CPT) to transform attribute representations\nand view information into structured sentences. These sentences are processed\nby the text encoder of CLIP to generate more discriminative features. As a\nresult, our framework can fully leverage attribute-based text knowledge to\nimprove AG-ReID performance. Extensive experiments on three AG-ReID benchmarks\ndemonstrate the effectiveness of our proposed methods. The source code is\navailable at https://github.com/kevinhu314/LATex.\n","authors":["Pingping Zhang","Xiang Hu","Yuhao Wang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2503.23722v3.pdf","comment":"More modifications may be performed"},{"id":"http://arxiv.org/abs/2510.26443v1","updated":"2025-10-30T12:46:56Z","published":"2025-10-30T12:46:56Z","title":"PointSt3R: Point Tracking through 3D Grounded Correspondence","summary":"  Recent advances in foundational 3D reconstruction models, such as DUSt3R and\nMASt3R, have shown great potential in 2D and 3D correspondence in static\nscenes. In this paper, we propose to adapt them for the task of point tracking\nthrough 3D grounded correspondence. We first demonstrate that these models are\ncompetitive point trackers when focusing on static points, present in current\npoint tracking benchmarks ($+33.5\\%$ on EgoPoints vs. CoTracker2). We propose\nto combine the reconstruction loss with training for dynamic correspondence\nalong with a visibility head, and fine-tuning MASt3R for point tracking using a\nrelatively small amount of synthetic data. Importantly, we only train and\nevaluate on pairs of frames where one contains the query point, effectively\nremoving any temporal context. Using a mix of dynamic and static point\ncorrespondences, we achieve competitive or superior point tracking results on\nfour datasets (e.g. competitive on TAP-Vid-DAVIS 73.8 $\\delta_{avg}$ / 85.8\\%\nocclusion acc. for PointSt3R compared to 75.7 / 88.3\\% for CoTracker2; and\nsignificantly outperform CoTracker3 on EgoPoints 61.3 vs 54.2 and RGB-S 87.0 vs\n82.8). We also present results on 3D point tracking along with several\nablations on training datasets and percentage of dynamic correspondences.\n","authors":["Rhodri Guerrier","Adam W. Harley","Dima Damen"],"pdf_url":"https://arxiv.org/pdf/2510.26443v1.pdf","comment":"http://rhodriguerrier.github.io/PointSt3R"},{"id":"http://arxiv.org/abs/2510.26441v1","updated":"2025-10-30T12:45:24Z","published":"2025-10-30T12:45:24Z","title":"A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt\n  Tuning of Vision-Language Models","summary":"  Test-time prompt tuning (TPT) has emerged as a promising technique for\nadapting large vision-language models (VLMs) to unseen tasks without relying on\nlabeled data. However, the lack of dispersion between textual features can hurt\ncalibration performance, which raises concerns about VLMs' reliability,\ntrustworthiness, and safety. Current TPT approaches primarily focus on\nimproving prompt calibration by either maximizing average textual feature\ndispersion or enforcing orthogonality constraints to encourage angular\nseparation. However, these methods may not always have optimal angular\nseparation between class-wise textual features, which implies overlooking the\ncritical role of angular diversity. To address this, we propose A-TPT, a novel\nTPT framework that introduces angular diversity to encourage uniformity in the\ndistribution of normalized textual features induced by corresponding learnable\nprompts. This uniformity is achieved by maximizing the minimum pairwise angular\ndistance between features on the unit hypersphere. We show that our approach\nconsistently surpasses state-of-the-art TPT methods in reducing the aggregate\naverage calibration error while maintaining comparable accuracy through\nextensive experiments with various backbones on different datasets. Notably,\nour approach exhibits superior zero-shot calibration performance on natural\ndistribution shifts and generalizes well to medical datasets. We provide\nextensive analyses, including theoretical aspects, to establish the grounding\nof A-TPT. These results highlight the potency of promoting angular diversity to\nachieve well-dispersed textual features, significantly improving VLM\ncalibration during test-time adaptation. Our code will be made publicly\navailable.\n","authors":["Shihab Aaqil Ahamed","Udaya S. K. P. Miriya Thanthrige","Ranga Rodrigo","Muhammad Haris Khan"],"pdf_url":"https://arxiv.org/pdf/2510.26441v1.pdf","comment":"23 pages, 14 figures"},{"id":"http://arxiv.org/abs/2406.15863v3","updated":"2025-10-30T12:26:23Z","published":"2024-06-22T14:43:23Z","title":"EmoAttack: Emotion-to-Image Diffusion Models for Emotional Backdoor\n  Generation","summary":"  Text-to-image diffusion models can generate realistic images based on textual\ninputs, enabling users to convey their opinions visually through language.\nMeanwhile, within language, emotion plays a crucial role in expressing personal\nopinions in our daily lives and the inclusion of maliciously negative content\ncan lead users astray, exacerbating negative emotions. Recognizing the success\nof diffusion models and the significance of emotion, we investigate a\npreviously overlooked risk associated with text-to-image diffusion models, that\nis, utilizing emotion in the input texts to introduce negative content and\nprovoke unfavorable emotions in users. Specifically, we identify a new backdoor\nattack, i.e., emotion-aware backdoor attack (EmoAttack), which introduces\nmalicious negative content triggered by emotional texts during image\ngeneration. We formulate such an attack as a diffusion personalization problem\nto avoid extensive model retraining and propose the EmoBooth. Unlike existing\npersonalization methods, our approach fine-tunes a pre-trained diffusion model\nby establishing a mapping between a cluster of emotional words and a given\nreference image containing malicious negative content. To validate the\neffectiveness of our method, we built a dataset and conducted extensive\nanalysis and discussion about its effectiveness. Given consumers' widespread\nuse of diffusion models, uncovering this threat is critical for society.\n","authors":["Tianyu Wei","Shanmin Pang","Qi Guo","Yizhuo Ma","Xiaofeng Cao","Qing Guo"],"pdf_url":"https://arxiv.org/pdf/2406.15863v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.08083v3","updated":"2025-10-30T12:07:37Z","published":"2022-08-17T05:42:59Z","title":"Two Heads are Better than One: Robust Learning Meets Multi-branch Models","summary":"  Deep neural networks (DNNs) are vulnerable to adversarial examples, in which\nDNNs are misled to false outputs due to inputs containing imperceptible\nperturbations. Adversarial training, a reliable and effective method of\ndefense, may significantly reduce the vulnerability of neural networks and\nbecomes the de facto standard for robust learning. While many recent works\npractice the data-centric philosophy, such as how to generate better\nadversarial examples or use generative models to produce additional training\ndata, we look back to the models themselves and revisit the adversarial\nrobustness from the perspective of deep feature distribution as an insightful\ncomplementarity. In this paper, we propose \\textit{Branch Orthogonality\nadveRsarial Training} (BORT) to obtain state-of-the-art performance with solely\nthe original dataset for adversarial training. To practice our design idea of\nintegrating multiple orthogonal solution spaces, we leverage a simple and\nstraightforward multi-branch neural network that eclipses adversarial attacks\nwith no increase in inference time. We heuristically propose a corresponding\nloss function, branch-orthogonal loss, to make each solution space of the\nmulti-branch model orthogonal. We evaluate our approach on CIFAR-10, CIFAR-100\nand SVHN against $\\ell_{\\infty}$ norm-bounded perturbations of size $\\epsilon =\n8/255$, respectively. Exhaustive experiments are conducted to show that our\nmethod goes beyond all state-of-the-art methods without any tricks. Compared to\nall methods that do not use additional data for training, our models achieve\n67.3\\% and 41.5\\% robust accuracy on CIFAR-10 and CIFAR-100 (improving upon the\nstate-of-the-art by +7.23\\% and +9.07\\%). We also outperform methods using a\ntraining set with a far larger scale than ours.\n","authors":["Zongyuan Zhang","Qingwen Bu","Tianyang Duan","Zheng Lin","Yuhao Qing","Zihan Fang","Heming Cui","Dong Huang"],"pdf_url":"https://arxiv.org/pdf/2208.08083v3.pdf","comment":"Camera-ready version for ICPADS 2025"},{"id":"http://arxiv.org/abs/2409.06154v3","updated":"2025-10-30T12:04:13Z","published":"2024-09-10T01:57:57Z","title":"Static for Dynamic: Towards a Deeper Understanding of Dynamic Facial\n  Expressions Using Static Expression Data","summary":"  Dynamic facial expression recognition (DFER) infers emotions from the\ntemporal evolution of expressions, unlike static facial expression recognition\n(SFER), which relies solely on a single snapshot. This temporal analysis\nprovides richer information and promises greater recognition capability.\nHowever, current DFER methods often exhibit unsatisfied performance largely due\nto fewer training samples compared to SFER. Given the inherent correlation\nbetween static and dynamic expressions, we hypothesize that leveraging the\nabundant SFER data can enhance DFER. To this end, we propose Static-for-Dynamic\n(S4D), a unified dual-modal learning framework that integrates SFER data as a\ncomplementary resource for DFER. Specifically, S4D employs dual-modal\nself-supervised pre-training on facial images and videos using a shared Vision\nTransformer (ViT) encoder-decoder architecture, yielding improved\nspatiotemporal representations. The pre-trained encoder is then fine-tuned on\nstatic and dynamic expression datasets in a multi-task learning setup to\nfacilitate emotional information interaction. Unfortunately, vanilla multi-task\nlearning in our study results in negative transfer. To address this, we propose\nan innovative Mixture of Adapter Experts (MoAE) module that facilitates\ntask-specific knowledge acquisition while effectively extracting shared\nknowledge from both static and dynamic expression data. Extensive experiments\ndemonstrate that S4D achieves a deeper understanding of DFER, setting new\nstate-of-the-art performance on FERV39K, MAFW, and DFEW benchmarks, with\nweighted average recall (WAR) of 53.65\\%, 58.44\\%, and 76.68\\%, respectively.\nAdditionally, a systematic correlation analysis between SFER and DFER tasks is\npresented, which further elucidates the potential benefits of leveraging SFER.\n","authors":["Yin Chen","Jia Li","Yu Zhang","Zhenzhen Hu","Shiguang Shan","Meng Wang","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2409.06154v3.pdf","comment":"The code and model are publicly available here\n  https://github.com/MSA-LMC/S4D"},{"id":"http://arxiv.org/abs/2509.23885v2","updated":"2025-10-30T12:02:27Z","published":"2025-09-28T13:50:29Z","title":"Tunable-Generalization Diffusion Powered by Self-Supervised Contextual\n  Sub-Data for Low-Dose CT Reconstruction","summary":"  Current models based on deep learning for low-dose CT denoising rely heavily\non paired data and generalize poorly. Even the more concerned diffusion models\nneed to learn the distribution of clean data for reconstruction, which is\ndifficult to satisfy in medical clinical applications. At the same time,\nself-supervised-based methods face the challenge of significant degradation of\ngeneralizability of models pre-trained for the current dose to expand to other\ndoses. To address these issues, this work proposes a novel method of\nTUnable-geneRalizatioN Diffusion (TurnDiff) powered by self-supervised\ncontextual sub-data for low-dose CT reconstruction. Firstly, a contextual\nsubdata self-enhancing similarity strategy is designed for denoising centered\non the LDCT projection domain, which provides an initial prior for the\nsubsequent progress. Subsequently, the initial prior is used to combine\nknowledge distillation with a deep combination of latent diffusion models for\noptimizing image details. The pre-trained model is used for inference\nreconstruction, and the pixel-level self-correcting fusion technique is\nproposed for fine-grained reconstruction of the image domain to enhance the\nimage fidelity, using the initial prior and the LDCT image as a guide. In\naddition, the technique is flexibly applied to the generalization of upper and\nlower doses or even unseen doses. Dual-domain strategy cascade for\nself-supervised LDCT denoising, TurnDiff requires only LDCT projection domain\ndata for training and testing. Comprehensive evaluation on both benchmark\ndatasets and real-world data demonstrates that TurnDiff consistently\noutperforms state-of-the-art methods in both reconstruction and generalization.\n","authors":["Guoquan Wei","Liu Shi","Zekun Zhou","Wenzhe Shan","Qiegen Liu"],"pdf_url":"https://arxiv.org/pdf/2509.23885v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23117v2","updated":"2025-10-30T12:02:18Z","published":"2025-10-27T08:38:17Z","title":"Seeing Structural Failure Before it Happens: An Image-Based\n  Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction","summary":"  Physics Informed Neural Networks (PINNs) are gaining attention for their\nability to embed physical laws into deep learning models, which is particularly\nuseful in structural engineering tasks with limited data. This paper aims to\nexplore the use of PINNs to predict the weight of small scale spaghetti\nbridges, a task relevant to understanding load limits and potential failure\nmodes in simplified structural models. Our proposed framework incorporates\nphysics-based constraints to the prediction model for improved performance. In\naddition to standard PINNs, we introduce a novel architecture named Physics\nInformed Kolmogorov Arnold Network (PIKAN), which blends universal function\napproximation theory with physical insights. The structural parameters provided\nas input to the model are collected either manually or through computer vision\nmethods. Our dataset includes 15 real bridges, augmented to 100 samples, and\nour best model achieves an $R^2$ score of 0.9603 and a mean absolute error\n(MAE) of 10.50 units. From applied perspective, we also provide a web based\ninterface for parameter entry and prediction. These results show that PINNs can\noffer reliable estimates of structural weight, even with limited data, and may\nhelp inform early stage failure analysis in lightweight bridge designs.\n  The complete data and code are available at\nhttps://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.\n","authors":["Omer Jauhar Khan","Sudais Khan","Hafeez Anwar","Shahzeb Khan","Shams Ul Arifeen"],"pdf_url":"https://arxiv.org/pdf/2510.23117v2.pdf","comment":"12 pages, 17 figures. Preprint"},{"id":"http://arxiv.org/abs/2510.26412v1","updated":"2025-10-30T12:00:46Z","published":"2025-10-30T12:00:46Z","title":"LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video\n  Generation","summary":"  Recently text-to-video generation has made impressive progress in producing\nshort, high-quality clips, but evaluating long-form outputs remains a major\nchallenge especially when processing complex prompts. Existing benchmarks\nmostly rely on simplified prompts and focus on low-level metrics, overlooking\nfine-grained alignment with prompts and abstract dimensions such as narrative\ncoherence and thematic expression. To address these gaps, we propose\nLoCoT2V-Bench, a benchmark specifically designed for long video generation\n(LVG) under complex input conditions. Based on various real-world videos,\nLoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating\nelements like scene transitions and event dynamics. Moreover, it constructs a\nmulti-dimensional evaluation framework that includes our newly proposed metrics\nsuch as event-level alignment, fine-grained temporal consistency, content\nclarity, and the Human Expectation Realization Degree (HERD) that focuses on\nmore abstract attributes like narrative flow, emotional response, and character\ndevelopment. Using this framework, we conduct a comprehensive evaluation of\nnine representative LVG models, finding that while current methods perform well\non basic visual and temporal aspects, they struggle with inter-event\nconsistency, fine-grained alignment, and high-level thematic adherence, etc.\nOverall, LoCoT2V-Bench provides a comprehensive and reliable platform for\nevaluating long-form complex text-to-video generation and highlights critical\ndirections for future method improvement.\n","authors":["Xiangqing Zheng","Chengyue Wu","Kehai Chen","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09549v2","updated":"2025-10-30T12:00:18Z","published":"2025-04-13T12:44:50Z","title":"SD-ReID: View-aware Stable Diffusion for Aerial-Ground Person\n  Re-Identification","summary":"  Aerial-Ground Person Re-IDentification (AG-ReID) aims to retrieve specific\npersons across cameras with different viewpoints. Previous works focus on\ndesigning discriminative models to maintain the identity consistency despite\ndrastic changes in camera viewpoints. The core idea behind these methods is\nquite natural, but designing a view-robust model is a very challenging task.\nMoreover, they overlook the contribution of view-specific features in enhancing\nthe model's ability to represent persons. To address these issues, we propose a\nnovel generative framework named SD-ReID for AG-ReID, which leverages\ngenerative models to mimic the feature distribution of different views while\nextracting robust identity representations. More specifically, we first train a\nViT-based model to extract person representations along with controllable\nconditions, including identity and view conditions. We then fine-tune the\nStable Diffusion (SD) model to enhance person representations guided by these\ncontrollable conditions. Furthermore, we introduce the View-Refined Decoder\n(VRD) to bridge the gap between instance-level and global-level features.\nFinally, both person representations and all-view features are employed to\nretrieve target persons. Extensive experiments on five AG-ReID benchmarks\n(i.e., CARGO, AG-ReIDv1, AG-ReIDv2, LAGPeR and G2APS-ReID) demonstrate the\neffectiveness of our proposed method. The source code will be available.\n","authors":["Yuhao Wang","Xiang Hu","Lixin Wang","Pingping Zhang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2504.09549v2.pdf","comment":"More modifications may performed"},{"id":"http://arxiv.org/abs/2510.26391v1","updated":"2025-10-30T11:34:37Z","published":"2025-10-30T11:34:37Z","title":"EEG-Driven Image Reconstruction with Saliency-Guided Diffusion Models","summary":"  Existing EEG-driven image reconstruction methods often overlook spatial\nattention mechanisms, limiting fidelity and semantic coherence. To address\nthis, we propose a dual-conditioning framework that combines EEG embeddings\nwith spatial saliency maps to enhance image generation. Our approach leverages\nthe Adaptive Thinking Mapper (ATM) for EEG feature extraction and fine-tunes\nStable Diffusion 2.1 via Low-Rank Adaptation (LoRA) to align neural signals\nwith visual semantics, while a ControlNet branch conditions generation on\nsaliency maps for spatial control. Evaluated on THINGS-EEG, our method achieves\na significant improvement in the quality of low- and high-level image features\nover existing approaches. Simultaneously, strongly aligning with human visual\nattention. The results demonstrate that attentional priors resolve EEG\nambiguities, enabling high-fidelity reconstructions with applications in\nmedical diagnostics and neuroadaptive interfaces, advancing neural decoding\nthrough efficient adaptation of pre-trained diffusion models.\n","authors":["Igor Abramov","Ilya Makarov"],"pdf_url":"https://arxiv.org/pdf/2510.26391v1.pdf","comment":"Demo paper"},{"id":"http://arxiv.org/abs/2510.26390v1","updated":"2025-10-30T11:33:29Z","published":"2025-10-30T11:33:29Z","title":"SPG-CDENet: Spatial Prior-Guided Cross Dual Encoder Network for\n  Multi-Organ Segmentation","summary":"  Multi-organ segmentation is a critical task in computer-aided diagnosis.\nWhile recent deep learning methods have achieved remarkable success in image\nsegmentation, huge variations in organ size and shape challenge their\neffectiveness in multi-organ segmentation. To address these challenges, we\npropose a Spatial Prior-Guided Cross Dual Encoder Network (SPG-CDENet), a novel\ntwo-stage segmentation paradigm designed to improve multi-organ segmentation\naccuracy. Our SPG-CDENet consists of two key components: a spatial prior\nnetwork and a cross dual encoder network. The prior network generates coarse\nlocalization maps that delineate the approximate ROI, serving as spatial\nguidance for the dual encoder network. The cross dual encoder network comprises\nfour essential components: a global encoder, a local encoder, a symmetric\ncross-attention module, and a flow-based decoder. The global encoder captures\nglobal semantic features from the entire image, while the local encoder focuses\non features from the prior network. To enhance the interaction between the\nglobal and local encoders, a symmetric cross-attention module is proposed\nacross all layers of the encoders to fuse and refine features. Furthermore, the\nflow-based decoder directly propagates high-level semantic features from the\nfinal encoder layer to all decoder layers, maximizing feature preservation and\nutilization. Extensive qualitative and quantitative experiments on two public\ndatasets demonstrate the superior performance of SPG-CDENet compared to\nexisting segmentation methods. Furthermore, ablation studies further validate\nthe effectiveness of the proposed modules in improving segmentation accuracy.\n","authors":["Xizhi Tian","Changjun Zhou","Yulin. Yang"],"pdf_url":"https://arxiv.org/pdf/2510.26390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26369v1","updated":"2025-10-30T11:14:17Z","published":"2025-10-30T11:14:17Z","title":"CorVS: Person Identification via Video Trajectory-Sensor Correspondence\n  in a Real-World Warehouse","summary":"  Worker location data is key to higher productivity in industrial sites.\nCameras are a promising tool for localization in logistics warehouses since\nthey also offer valuable environmental contexts such as package status.\nHowever, identifying individuals with only visual data is often impractical.\nAccordingly, several prior studies identified people in videos by comparing\ntheir trajectories and wearable sensor measurements. While this approach has\nadvantages such as independence from appearance, the existing methods may break\ndown under real-world conditions. To overcome this challenge, we propose CorVS,\na novel data-driven person identification method based on correspondence\nbetween visual tracking trajectories and sensor measurements. Firstly, our deep\nlearning model predicts correspondence probabilities and reliabilities for\nevery pair of a trajectory and sensor measurements. Secondly, our algorithm\nmatches the trajectories and sensor measurements over time using the predicted\nprobabilities and reliabilities. We developed a dataset with actual warehouse\noperations and demonstrated the method's effectiveness for real-world\napplications.\n","authors":["Kazuma Kano","Yuki Mori","Shin Katayama","Kenta Urano","Takuro Yonezawa","Nobuo Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2510.26369v1.pdf","comment":"7 pages, 3 figures, accepted to IPIN 2025"},{"id":"http://arxiv.org/abs/2510.26358v1","updated":"2025-10-30T11:08:23Z","published":"2025-10-30T11:08:23Z","title":"AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian\n  Splatting SLAM","summary":"  Autonomous robots in orchards require real-time 3D scene understanding\ndespite repetitive row geometry, seasonal appearance changes, and wind-driven\nfoliage motion. We present AgriGS-SLAM, a Visual--LiDAR SLAM framework that\ncouples direct LiDAR odometry and loop closures with multi-camera 3D Gaussian\nSplatting (3DGS) rendering. Batch rasterization across complementary viewpoints\nrecovers orchard structure under occlusions, while a unified gradient-driven\nmap lifecycle executed between keyframes preserves fine details and bounds\nmemory. Pose refinement is guided by a probabilistic LiDAR-based depth\nconsistency term, back-propagated through the camera projection to tighten\ngeometry-appearance coupling. We deploy the system on a field platform in apple\nand pear orchards across dormancy, flowering, and harvesting, using a\nstandardized trajectory protocol that evaluates both training-view and\nnovel-view synthesis to reduce 3DGS overfitting in evaluation. Across seasons\nand sites, AgriGS-SLAM delivers sharper, more stable reconstructions and\nsteadier trajectories than recent state-of-the-art 3DGS-SLAM baselines while\nmaintaining real-time performance on-tractor. While demonstrated in orchard\nmonitoring, the approach can be applied to other outdoor domains requiring\nrobust multimodal perception.\n","authors":["Mirko Usuelli","David Rapado-Rincon","Gert Kootstra","Matteo Matteucci"],"pdf_url":"https://arxiv.org/pdf/2510.26358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04448v2","updated":"2025-10-30T10:58:04Z","published":"2025-09-04T17:59:43Z","title":"TRUST-VL: An Explainable News Assistant for General Multimodal\n  Misinformation Detection","summary":"  Multimodal misinformation, encompassing textual, visual, and cross-modal\ndistortions, poses an increasing societal threat that is amplified by\ngenerative AI. Existing methods typically focus on a single type of distortion\nand struggle to generalize to unseen scenarios. In this work, we observe that\ndifferent distortion types share common reasoning capabilities while also\nrequiring task-specific skills. We hypothesize that joint training across\ndistortion types facilitates knowledge sharing and enhances the model's ability\nto generalize. To this end, we introduce TRUST-VL, a unified and explainable\nvision-language model for general multimodal misinformation detection. TRUST-VL\nincorporates a novel Question-Aware Visual Amplifier module, designed to\nextract task-specific visual features. To support training, we also construct\nTRUST-Instruct, a large-scale instruction dataset containing 198K samples\nfeaturing structured reasoning chains aligned with human fact-checking\nworkflows. Extensive experiments on both in-domain and zero-shot benchmarks\ndemonstrate that TRUST-VL achieves state-of-the-art performance, while also\noffering strong generalization and interpretability.\n","authors":["Zehong Yan","Peng Qi","Wynne Hsu","Mong Li Lee"],"pdf_url":"https://arxiv.org/pdf/2509.04448v2.pdf","comment":"EMNLP 2025 Oral; Project Homepage:\n  https://yanzehong.github.io/trust-vl/"},{"id":"http://arxiv.org/abs/2505.21497v2","updated":"2025-10-30T10:49:28Z","published":"2025-05-27T17:58:49Z","title":"Paper2Poster: Towards Multimodal Poster Automation from Scientific\n  Papers","summary":"  Academic poster generation is a crucial yet challenging task in scientific\ncommunication, requiring the compression of long-context interleaved documents\ninto a single, visually coherent page. To address this challenge, we introduce\nthe first benchmark and metric suite for poster generation, which pairs recent\nconference papers with author-designed posters and evaluates outputs on\n(i)Visual Quality-semantic alignment with human posters, (ii)Textual\nCoherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic\nand informational criteria scored by a VLM-as-judge, and notably\n(iv)PaperQuiz-the poster's ability to convey core paper content as measured by\nVLMs answering generated quizzes. Building on this benchmark, we propose\nPosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser\ndistills the paper into a structured asset library; the (b)Planner aligns\ntext-visual pairs into a binary-tree layout that preserves reading order and\nspatial balance; and the (c)Painter-Commenter loop refines each panel by\nexecuting rendering code and using VLM feedback to eliminate overflow and\nensure alignment. In our comprehensive evaluation, we find that GPT-4o\noutputs-though visually appealing at first glance-often exhibit noisy text and\npoor PaperQuiz scores, and we find that reader engagement is the primary\naesthetic bottleneck, as human-designed posters rely largely on visual\nsemantics to convey meaning. Our fully open-source variants (e.g. based on the\nQwen-2.5 series) outperform existing 4o-driven multi-agent systems across\nnearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper\ninto a finalized yet editable .pptx poster - all for just $0.005. These\nfindings chart clear directions for the next generation of fully automated\nposter-generation models. The code and datasets are available at\nhttps://github.com/Paper2Poster/Paper2Poster.\n","authors":["Wei Pang","Kevin Qinghong Lin","Xiangru Jian","Xi He","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2505.21497v2.pdf","comment":"Project Page: https://github.com/Paper2Poster/Paper2Poster"},{"id":"http://arxiv.org/abs/2510.26339v1","updated":"2025-10-30T10:46:28Z","published":"2025-10-30T10:46:28Z","title":"GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and\n  High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?","summary":"  Image super-resolution(SR) is fundamental to many vision system-from\nsurveillance and autonomy to document analysis and retail analytics-because\nrecovering high-frequency details, especially scene-text, enables reliable\ndownstream perception. Scene-text, i.e., text embedded in natural images such\nas signs, product labels, and storefronts, often carries the most actionable\ninformation; when characters are blurred or hallucinated, optical character\nrecognition(OCR) and subsequent decisions fail even if the rest of the image\nappears sharp. Yet previous SR research has often been tuned to distortion\n(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that\nare largely insensitive to character-level errors. Furthermore, studies that do\naddress text SR often focus on simplified benchmarks with isolated characters,\noverlooking the challenges of text within complex natural scenes. As a result,\nscene-text is effectively treated as generic texture. For SR to be effective in\npractical deployments, it is therefore essential to explicitly optimize for\nboth text legibility and perceptual quality. We present GLYPH-SR, a\nvision-language-guided diffusion framework that aims to achieve both objectives\njointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by\nOCR data, and a ping-pong scheduler that alternates between text- and\nscene-centric guidance. To enable targeted text restoration, we train these\ncomponents on a synthetic corpus while keeping the main SR branch frozen.\nAcross SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by\nup to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)\nwhile maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed\nto satisfy both objectives simultaneously-high readability and high visual\nrealism-delivering SR that looks right and reds right.\n","authors":["Mingyu Sung","Seungjae Ham","Kangwoo Kim","Yeokyoung Yoon","Sangseok Yun","Il-Min Kim","Jae-Mo Kang"],"pdf_url":"https://arxiv.org/pdf/2510.26339v1.pdf","comment":"11 pages, 6 figures. Includes supplementary material. Under review as\n  a conference paper at ICLR 2026"},{"id":"http://arxiv.org/abs/2503.09499v3","updated":"2025-10-30T10:21:42Z","published":"2025-03-12T16:03:03Z","title":"MindGYM: What Matters in Question Synthesis for Thinking-Centric\n  Fine-Tuning?","summary":"  Large foundation models face challenges in acquiring transferable, structured\nthinking abilities, especially when supervised with rigid templates or\ncrowd-annotated instruction datasets. Unlike prior approaches, we focus on a\nthinking-centric data synthesis paradigm that enables models to evolve through\nself-generated, cognitively guided data. We propose MindGYM, a structured and\nscalable framework for question synthesis, composed of: (1) Cognitive Thinking\nProcess Injection, which infuses high-level reasoning objectives to shape the\nmodel's synthesis behavior; (2) Seed Single-Hop Question Synthesis, generating\natomic questions from diverse semantic types to encourage broader thinking; and\n(3) Challenging Multi-Hop QA Synthesis, composing more complex multi-hop\nquestions based on QA seeds for deeper reasoning. Detailed analysis shows that\nsynthetic data generated by our method achieves 16.7% higher average quality\nand 67.91% lower quality variance compared to baseline sources, highlighting\nthat both high-quality and self-contained data are essential for effective,\nthinking-oriented fine-tuning. MindGYM improves performance on six reasoning\nbenchmarks, achieving gains of up to 16% on MathVision using only 400 data\nsamples, and generalizable improvements across different model sizes and\narchitectures. MindGYM underscores the viability of self-challenging mechanisms\nin refining large model capabilities while minimizing human intervention and\nresource demands. Code and data are released to promote data-centric research\ninto self-evolving foundation models driven by their internal reasoning\ncapabilities.\n","authors":["Zhe Xu","Daoyuan Chen","Zhenqing Ling","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2503.09499v3.pdf","comment":"Accepted by NeurIPS'25. 30 pages, 2 figures, 13 tables"},{"id":"http://arxiv.org/abs/2509.06771v2","updated":"2025-10-30T10:15:05Z","published":"2025-09-08T14:55:16Z","title":"D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning --\n  A Benchmark Dataset and Method","summary":"  Dark humor in online memes poses unique challenges due to its reliance on\nimplicit, sensitive, and culturally contextual cues. To address the lack of\nresources and methods for detecting dark humor in multimodal content, we\nintroduce a novel dataset of 4,379 Reddit memes annotated for dark humor,\ntarget category (gender, mental health, violence, race, disability, and other),\nand a three-level intensity rating (mild, moderate, severe). Building on this\nresource, we propose a reasoning-augmented framework that first generates\nstructured explanations for each meme using a Large Vision-Language Model\n(VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective\nto iteratively refine its explanations, ensuring completeness and alignment. We\nthen extract textual features from both the OCR transcript and the self-refined\nreasoning via a text encoder, while visual features are obtained using a vision\ntransformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three\nstreams, text, image, and reasoning, via pairwise attention mechanisms,\nproducing a unified representation for classification. Experimental results\ndemonstrate that our approach outperforms strong baselines across three tasks:\ndark humor detection, target identification, and intensity prediction. The\ndataset, annotations, and code are released to facilitate further research in\nmultimodal humor understanding and content moderation. Code and Dataset are\navailable at:\nhttps://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning\n","authors":["Sai Kartheek Reddy Kasu","Mohammad Zia Ur Rehman","Shahid Shafi Dar","Rishi Bharat Junghare","Dhanvin Sanjay Namboodiri","Nagendra Kumar"],"pdf_url":"https://arxiv.org/pdf/2509.06771v2.pdf","comment":"Accepted at IEEE International Conference on Data Mining (ICDM) 2025"},{"id":"http://arxiv.org/abs/2510.26315v1","updated":"2025-10-30T10:08:06Z","published":"2025-10-30T10:08:06Z","title":"A Hybrid Framework Bridging CNN and ViT based on Theory of Evidence for\n  Diabetic Retinopathy Grading","summary":"  Diabetic retinopathy (DR) is a leading cause of vision loss among middle-aged\nand elderly people, which significantly impacts their daily lives and mental\nhealth. To improve the efficiency of clinical screening and enable the early\ndetection of DR, a variety of automated DR diagnosis systems have been recently\nestablished based on convolutional neural network (CNN) or vision Transformer\n(ViT). However, due to the own shortages of CNN / ViT, the performance of\nexisting methods using single-type backbone has reached a bottleneck. One\npotential way for the further improvements is integrating different kinds of\nbackbones, which can fully leverage the respective strengths of them\n(\\emph{i.e.,} the local feature extraction capability of CNN and the global\nfeature capturing ability of ViT). To this end, we propose a novel paradigm to\neffectively fuse the features extracted by different backbones based on the\ntheory of evidence. Specifically, the proposed evidential fusion paradigm\ntransforms the features from different backbones into supporting evidences via\na set of deep evidential networks. With the supporting evidences, the\naggregated opinion can be accordingly formed, which can be used to adaptively\ntune the fusion pattern between different backbones and accordingly boost the\nperformance of our hybrid model. We evaluated our method on two publicly\navailable DR grading datasets. The experimental results demonstrate that our\nhybrid model not only improves the accuracy of DR grading, compared to the\nstate-of-the-art frameworks, but also provides the excellent interpretability\nfor feature fusion and decision-making.\n","authors":["Junlai Qiu","Yunzhu Chen","Hao Zheng","Yawen Huang","Yuexiang Li"],"pdf_url":"https://arxiv.org/pdf/2510.26315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.22159v3","updated":"2025-10-30T10:00:04Z","published":"2025-03-28T05:46:02Z","title":"Disentangled 4D Gaussian Splatting: Rendering High-Resolution Dynamic\n  World at 343 FPS","summary":"  While dynamic novel view synthesis from 2D videos has seen progress,\nachieving efficient reconstruction and rendering of dynamic scenes remains a\nchallenging task. In this paper, we introduce Disentangled 4D Gaussian\nSplatting (Disentangled4DGS), a novel representation and rendering pipeline\nthat achieves real-time performance without compromising visual fidelity.\nDisentangled4DGS decouples the temporal and spatial components of 4D Gaussians,\navoiding the need for slicing first and four-dimensional matrix calculations in\nprior methods. By projecting temporal and spatial deformations into dynamic 2D\nGaussians and deferring temporal processing, we minimize redundant computations\nof 4DGS. Our approach also features a gradient-guided flow loss and temporal\nsplitting strategy to reduce artifacts. Experiments demonstrate a significant\nimprovement in rendering speed and quality, achieving 343 FPS when render\n1352*1014 resolution images on a single RTX3090 while reducing storage\nrequirements by at least 4.5%. Our approach sets a new benchmark for dynamic\nnovel view synthesis, outperforming existing methods on both multi-view and\nmonocular dynamic scene datasets.\n","authors":["Hao Feng","Hao Sun","Wei Xie","Zhi Zuo","Zhengzhe Liu"],"pdf_url":"https://arxiv.org/pdf/2503.22159v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26304v1","updated":"2025-10-30T09:43:56Z","published":"2025-10-30T09:43:56Z","title":"Exploring the correlation between the type of music and the emotions\n  evoked: A study using subjective questionnaires and EEG","summary":"  The subject of this work is to check how different types of music affect\nhuman emotions. While listening to music, a subjective survey and brain\nactivity measurements were carried out using an EEG helmet. The aim is to\ndemonstrate the impact of different music genres on emotions. The research\ninvolved a diverse group of participants of different gender and musical\npreferences. This had the effect of capturing a wide range of emotional\nresponses to music. After the experiment, a relationship analysis of the\nrespondents' questionnaires with EEG signals was performed. The analysis\nrevealed connections between emotions and observed brain activity.\n","authors":["Jelizaveta Jankowska","BoÅ¼ena Kostek","Fernando Alonso-Fernandez","Prayag Tiwari"],"pdf_url":"https://arxiv.org/pdf/2510.26304v1.pdf","comment":"Published at IWAIPR 2025 conference"},{"id":"http://arxiv.org/abs/2510.22319v2","updated":"2025-10-30T09:33:15Z","published":"2025-10-25T14:51:17Z","title":"GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via\n  Regulated Clipping","summary":"  Recently, GRPO-based reinforcement learning has shown remarkable progress in\noptimizing flow-matching models, effectively improving their alignment with\ntask-specific rewards. Within these frameworks, the policy update relies on\nimportance-ratio clipping to constrain overconfident positive and negative\ngradients. However, in practice, we observe a systematic shift in the\nimportance-ratio distribution-its mean falls below 1 and its variance differs\nsubstantially across timesteps. This left-shifted and inconsistent distribution\nprevents positive-advantage samples from entering the clipped region, causing\nthe mechanism to fail in constraining overconfident positive updates. As a\nresult, the policy model inevitably enters an implicit over-optimization\nstage-while the proxy reward continues to increase, essential metrics such as\nimage quality and text-prompt alignment deteriorate sharply, ultimately making\nthe learned policy impractical for real-world use. To address this issue, we\nintroduce GRPO-Guard, a simple yet effective enhancement to existing GRPO\nframeworks. Our method incorporates ratio normalization, which restores a\nbalanced and step-consistent importance ratio, ensuring that PPO clipping\nproperly constrains harmful updates across denoising timesteps. In addition, a\ngradient reweighting strategy equalizes policy gradients over noise conditions,\npreventing excessive updates from particular timestep regions. Together, these\ndesigns act as a regulated clipping mechanism, stabilizing optimization and\nsubstantially mitigating implicit over-optimization without relying on heavy KL\nregularization. Extensive experiments on multiple diffusion backbones (e.g.,\nSD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard\nsignificantly reduces over-optimization while maintaining or even improving\ngeneration quality.\n","authors":["Jing Wang","Jiajun Liang","Jie Liu","Henglin Liu","Gongye Liu","Jun Zheng","Wanyuan Pang","Ao Ma","Zhenyu Xie","Xintao Wang","Meng Wang","Pengfei Wan","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2510.22319v2.pdf","comment":"Project Page: https://jingw193.github.io/GRPO-Guard/"},{"id":"http://arxiv.org/abs/2510.26297v1","updated":"2025-10-30T09:31:47Z","published":"2025-10-30T09:31:47Z","title":"Towards Realistic Earth-Observation Constellation Scheduling: Benchmark\n  and Methodology","summary":"  Agile Earth Observation Satellites (AEOSs) constellations offer unprecedented\nflexibility for monitoring the Earth's surface, but their scheduling remains\nchallenging under large-scale scenarios, dynamic environments, and stringent\nconstraints. Existing methods often simplify these complexities, limiting their\nreal-world performance. We address this gap with a unified framework\nintegrating a standardized benchmark suite and a novel scheduling model. Our\nbenchmark suite, AEOS-Bench, contains $3,907$ finely tuned satellite assets and\n$16,410$ scenarios. Each scenario features $1$ to $50$ satellites and $50$ to\n$300$ imaging tasks. These scenarios are generated via a high-fidelity\nsimulation platform, ensuring realistic satellite behavior such as orbital\ndynamics and resource constraints. Ground truth scheduling annotations are\nprovided for each scenario. To our knowledge, AEOS-Bench is the first\nlarge-scale benchmark suite tailored for realistic constellation scheduling.\nBuilding upon this benchmark, we introduce AEOS-Former, a Transformer-based\nscheduling model that incorporates a constraint-aware attention mechanism. A\ndedicated internal constraint module explicitly models the physical and\noperational limits of each satellite. Through simulation-based iterative\nlearning, AEOS-Former adapts to diverse scenarios, offering a robust solution\nfor AEOS constellation scheduling. Experimental results demonstrate that\nAEOS-Former outperforms baseline models in task completion and energy\nefficiency, with ablation studies highlighting the contribution of each\ncomponent. Code and data are provided in\nhttps://github.com/buaa-colalab/AEOSBench.\n","authors":["Luting Wang","Yinghao Xiang","Hongliang Huang","Dongjun Li","Chen Gao","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02393v3","updated":"2025-10-30T09:31:07Z","published":"2025-06-03T03:18:17Z","title":"RRCANet: Recurrent Reusable-Convolution Attention Network for Infrared\n  Small Target Detection","summary":"  Infrared small target detection is a challenging task due to its unique\ncharacteristics (e.g., small, dim, shapeless and changeable). Recently\npublished CNN-based methods have achieved promising performance with heavy\nfeature extraction and fusion modules. To achieve efficient and effective\ndetection, we propose a recurrent reusable-convolution attention network\n(RRCA-Net) for infrared small target detection. Specifically, RRCA-Net\nincorporates reusable-convolution block (RuCB) in a recurrent manner without\nintroducing extra parameters. With the help of the repetitive iteration in\nRuCB, the high-level information of small targets in the deep layers can be\nwell maintained and further refined. Then, a dual interactive attention\naggregation module (DIAAM) is proposed to promote the mutual enhancement and\nfusion of refined information. In this way, RRCA-Net can both achieve\nhigh-level feature refinement and enhance the correlation of contextual\ninformation between adjacent layers. Moreover, to achieve steady convergence,\nwe design a target characteristic inspired loss function (DpT-k loss) by\nintegrating physical and mathematical constraints. Experimental results on\nthree benchmark datasets (e.g. NUAA-SIRST, IRSTD-1k, DenseSIRST) demonstrate\nthat our RRCA-Net can achieve comparable performance to the state-of-the-art\nmethods while maintaining a small number of parameters, and act as a plug and\nplay module to introduce consistent performance improvement for several popular\nIRSTD methods.\n","authors":["Yongxian Liu","Boyang Li","Ting Liu","Zaiping Lin","Wei An"],"pdf_url":"https://arxiv.org/pdf/2506.02393v3.pdf","comment":"We have updated the journal reference and DOI"},{"id":"http://arxiv.org/abs/2510.26294v1","updated":"2025-10-30T09:28:48Z","published":"2025-10-30T09:28:48Z","title":"Leveraging Large-Scale Face Datasets for Deep Periocular Recognition via\n  Ocular Cropping","summary":"  We focus on ocular biometrics, specifically the periocular region (the area\naround the eye), which offers high discrimination and minimal acquisition\nconstraints. We evaluate three Convolutional Neural Network architectures of\nvarying depth and complexity to assess their effectiveness for periocular\nrecognition. The networks are trained on 1,907,572 ocular crops extracted from\nthe large-scale VGGFace2 database. This significantly contrasts with existing\nworks, which typically rely on small-scale periocular datasets for training\nhaving only a few thousand images. Experiments are conducted with ocular images\nfrom VGGFace2-Pose, a subset of VGGFace2 containing in-the-wild face images,\nand the UFPR-Periocular database, which consists of selfies captured via mobile\ndevices with user guidance on the screen. Due to the uncontrolled conditions of\nVGGFace2, the Equal Error Rates (EERs) obtained with ocular crops range from\n9-15%, noticeably higher than the 3-6% EERs achieved using full-face images. In\ncontrast, UFPR-Periocular yields significantly better performance (EERs of\n1-2%), thanks to higher image quality and more consistent acquisition\nprotocols. To the best of our knowledge, these are the lowest reported EERs on\nthe UFPR dataset to date.\n","authors":["Fernando Alonso-Fernandez","Kevin Hernandez-Diaz","Jose Maria Buades Rubio","Josef Bigun"],"pdf_url":"https://arxiv.org/pdf/2510.26294v1.pdf","comment":"Published at IWAIPR 2025 conference"},{"id":"http://arxiv.org/abs/2510.26292v1","updated":"2025-10-30T09:24:34Z","published":"2025-10-30T09:24:34Z","title":"Beyond Imitation: Constraint-Aware Trajectory Generation with Flow\n  Matching For End-to-End Autonomous Driving","summary":"  Planning is a critical component of end-to-end autonomous driving. However,\nprevailing imitation learning methods often suffer from mode collapse, failing\nto produce diverse trajectory hypotheses. Meanwhile, existing generative\napproaches struggle to incorporate crucial safety and physical constraints\ndirectly into the generative process, necessitating an additional optimization\nstage to refine their outputs. To address these limitations, we propose CATG, a\nnovel planning framework that leverages Constrained Flow Matching. Concretely,\nCATG explicitly models the flow matching process, which inherently mitigates\nmode collapse and allows for flexible guidance from various conditioning\nsignals. Our primary contribution is the novel imposition of explicit\nconstraints directly within the flow matching process, ensuring that the\ngenerated trajectories adhere to vital safety and kinematic rules. Secondly,\nCATG parameterizes driving aggressiveness as a control signal during\ngeneration, enabling precise manipulation of trajectory style. Notably, on the\nNavSim v2 challenge, CATG achieved 2nd place with an EPDMS score of 51.31 and\nwas honored with the Innovation Award.\n","authors":["Lin Liu","Guanyi Yu","Ziying Song","Junqiao Li","Caiyan Jia","Feiyang Jia","Peiliang Wu","Yandan Luo"],"pdf_url":"https://arxiv.org/pdf/2510.26292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26282v1","updated":"2025-10-30T09:07:36Z","published":"2025-10-30T09:07:36Z","title":"Exploring Complementarity and Explainability in CNNs for Periocular\n  Verification Across Acquisition Distances","summary":"  We study the complementarity of different CNNs for periocular verification at\ndifferent distances on the UBIPr database. We train three architectures of\nincreasing complexity (SqueezeNet, MobileNetv2, and ResNet50) on a large set of\neye crops from VGGFace2. We analyse performance with cosine and chi2 metrics,\ncompare different network initialisations, and apply score-level fusion via\nlogistic regression. In addition, we use LIME heatmaps and Jensen-Shannon\ndivergence to compare attention patterns of the CNNs. While ResNet50\nconsistently performs best individually, the fusion provides substantial gains,\nespecially when combining all three networks. Heatmaps show that networks\nusually focus on distinct regions of a given image, which explains their\ncomplementarity. Our method significantly outperforms previous works on UBIPr,\nachieving a new state-of-the-art.\n","authors":["Fernando Alonso-Fernandez","Kevin Hernandez Diaz","Jose M. Buades","Kiran Raja","Josef Bigun"],"pdf_url":"https://arxiv.org/pdf/2510.26282v1.pdf","comment":"Accepted at BIOSIG 2025 conference"},{"id":"http://arxiv.org/abs/2510.26268v1","updated":"2025-10-30T08:53:13Z","published":"2025-10-30T08:53:13Z","title":"Revisiting Generative Infrared and Visible Image Fusion Based on Human\n  Cognitive Laws","summary":"  Existing infrared and visible image fusion methods often face the dilemma of\nbalancing modal information. Generative fusion methods reconstruct fused images\nby learning from data distributions, but their generative capabilities remain\nlimited. Moreover, the lack of interpretability in modal information selection\nfurther affects the reliability and consistency of fusion results in complex\nscenarios. This manuscript revisits the essence of generative image fusion\nunder the inspiration of human cognitive laws and proposes a novel infrared and\nvisible image fusion method, termed HCLFuse. First, HCLFuse investigates the\nquantification theory of information mapping in unsupervised fusion networks,\nwhich leads to the design of a multi-scale mask-regulated variational\nbottleneck encoder. This encoder applies posterior probability modeling and\ninformation decomposition to extract accurate and concise low-level modal\ninformation, thereby supporting the generation of high-fidelity structural\ndetails. Furthermore, the probabilistic generative capability of the diffusion\nmodel is integrated with physical laws, forming a time-varying physical\nguidance mechanism that adaptively regulates the generation process at\ndifferent stages, thereby enhancing the ability of the model to perceive the\nintrinsic structure of data and reducing dependence on data quality.\nExperimental results show that the proposed method achieves state-of-the-art\nfusion performance in qualitative and quantitative evaluations across multiple\ndatasets and significantly improves semantic segmentation metrics. This fully\ndemonstrates the advantages of this generative image fusion method, drawing\ninspiration from human cognition, in enhancing structural consistency and\ndetail quality.\n","authors":["Lin Guo","Xiaoqing Luo","Wei Xie","Zhancheng Zhang","Hui Li","Rui Wang","Zhenhua Feng","Xiaoning Song"],"pdf_url":"https://arxiv.org/pdf/2510.26268v1.pdf","comment":"NeurIPS 2025 spotlight"},{"id":"http://arxiv.org/abs/2503.11094v4","updated":"2025-10-30T08:44:27Z","published":"2025-03-14T05:35:38Z","title":"Open3D-VQA: A Benchmark for Comprehensive Spatial Reasoning with\n  Multimodal Large Language Model in Open Space","summary":"  Spatial reasoning is a fundamental capability of multimodal large language\nmodels (MLLMs), yet their performance in open aerial environments remains\nunderexplored. In this work, we present Open3D-VQA, a novel benchmark for\nevaluating MLLMs' ability to reason about complex spatial relationships from an\naerial perspective. The benchmark comprises 73k QA pairs spanning 7 general\nspatial reasoning tasks, including multiple-choice, true/false, and\nshort-answer formats, and supports both visual and point cloud modalities. The\nquestions are automatically generated from spatial relations extracted from\nboth real-world and simulated aerial scenes. Evaluation on 13 popular MLLMs\nreveals that: 1) Models are generally better at answering questions about\nrelative spatial relations than absolute distances, 2) 3D LLMs fail to\ndemonstrate significant advantages over 2D LLMs, and 3) Fine-tuning solely on\nthe simulated dataset can significantly improve the model's spatial reasoning\nperformance in real-world scenarios. We release our benchmark, data generation\npipeline, and evaluation toolkit to support further research:\nhttps://github.com/EmbodiedCity/Open3D-VQA.code.\n","authors":["Weichen Zhang","Zile Zhou","Xin Zeng","Xuchen Liu","Jianjie Fang","Chen Gao","Yong Li","Jinqiang Cui","Xinlei Chen","Xiao-Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.11094v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26241v1","updated":"2025-10-30T08:21:50Z","published":"2025-10-30T08:21:50Z","title":"Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for\n  Vision-Language Models","summary":"  Modern vision-language models (VLMs) excel at many multimodal tasks, yet\ntheir grasp of temporal information in video remains weak and, crucially,\nunder-evaluated. We probe this gap with a deceptively simple but revealing\nchallenge: judging the arrow of time (AoT)-whether a short clip is played\nforward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated\nbenchmark that tests whether VLMs can infer temporal direction in natural\nvideos using the same stimuli and behavioral baselines established for humans.\nOur comprehensive evaluation of open-weight and proprietary, reasoning and\nnon-reasoning VLMs reveals that most models perform near chance, and even the\nbest lag far behind human accuracy on physically irreversible processes (e.g.,\nfree fall, diffusion/explosion) and causal manual actions (division/addition)\nthat humans recognize almost instantly. These results highlight a fundamental\ngap in current multimodal systems: while they capture rich visual-semantic\ncorrelations, they lack the inductive biases required for temporal continuity\nand causal understanding. We release the code and data for AoT-PsyPhyBENCH to\nencourage further progress in the physical and temporal reasoning capabilities\nof VLMs.\n","authors":["Shiho Matta","Lis Kanashiro Pereira","Peitao Han","Fei Cheng","Shigeru Kitazawa"],"pdf_url":"https://arxiv.org/pdf/2510.26241v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2509.06159v3","updated":"2025-10-30T08:10:05Z","published":"2025-09-07T17:59:09Z","title":"FASL-Seg: Anatomy and Tool Segmentation of Surgical Scenes","summary":"  The growing popularity of robotic minimally invasive surgeries has made deep\nlearning-based surgical training a key area of research. A thorough\nunderstanding of the surgical scene components is crucial, which semantic\nsegmentation models can help achieve. However, most existing work focuses on\nsurgical tools and overlooks anatomical objects. Additionally, current\nstate-of-the-art (SOTA) models struggle to balance capturing high-level\ncontextual features and low-level edge features. We propose a Feature-Adaptive\nSpatial Localization model (FASL-Seg), designed to capture features at multiple\nlevels of detail through two distinct processing streams, namely a Low-Level\nFeature Projection (LLFP) and a High-Level Feature Projection (HLFP) stream,\nfor varying feature resolutions - enabling precise segmentation of anatomy and\nsurgical instruments. We evaluated FASL-Seg on surgical segmentation benchmark\ndatasets EndoVis18 and EndoVis17 on three use cases. The FASL-Seg model\nachieves a mean Intersection over Union (mIoU) of 72.71% on parts and anatomy\nsegmentation in EndoVis18, improving on SOTA by 5%. It further achieves a mIoU\nof 85.61% and 72.78% in EndoVis18 and EndoVis17 tool type segmentation,\nrespectively, outperforming SOTA overall performance, with comparable per-class\nSOTA results in both datasets and consistent performance in various classes for\nanatomy and instruments, demonstrating the effectiveness of distinct processing\nstreams for varying feature resolutions.\n","authors":["Muraam Abdel-Ghani","Mahmoud Ali","Mohamed Ali","Fatmaelzahraa Ahmed","Muhammad Arsalan","Abdulaziz Al-Ali","Shidin Balakrishnan"],"pdf_url":"https://arxiv.org/pdf/2509.06159v3.pdf","comment":"8 pages, 6 figures, In Proceedings of European Conference on\n  Artificial Intelligence (ECAI) 2025 <https://doi.org/10.3233/FAIA250908>"},{"id":"http://arxiv.org/abs/2508.07981v3","updated":"2025-10-30T08:09:13Z","published":"2025-08-11T13:41:24Z","title":"Omni-Effects: Unified and Spatially-Controllable Visual Effects\n  Generation","summary":"  Visual effects (VFX) are essential visual enhancements fundamental to modern\ncinematic production. Although video generation models offer cost-efficient\nsolutions for VFX production, current methods are constrained by per-effect\nLoRA training, which limits generation to single effects. This fundamental\nlimitation impedes applications that require spatially controllable composite\neffects, i.e., the concurrent generation of multiple effects at designated\nlocations. However, integrating diverse effects into a unified framework faces\nmajor challenges: interference from effect variations and spatial\nuncontrollability during multi-VFX joint training. To tackle these challenges,\nwe propose Omni-Effects, a first unified framework capable of generating\nprompt-guided effects and spatially controllable composite effects. The core of\nour framework comprises two key innovations: (1) LoRA-based Mixture of Experts\n(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects\nwithin a unified model while effectively mitigating cross-task interference.\n(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the\ntext token, enabling precise spatial control. Furthermore, we introduce an\nIndependent-Information Flow (IIF) module integrated within the SAP, isolating\nthe control signals corresponding to individual effects to prevent any unwanted\nblending. To facilitate this research, we construct a comprehensive VFX dataset\nOmni-VFX via a novel data collection pipeline combining image editing and\nFirst-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX\nevaluation framework for validating model performance. Extensive experiments\ndemonstrate that Omni-Effects achieves precise spatial control and diverse\neffect generation, enabling users to specify both the category and location of\ndesired effects.\n","authors":["Fangyuan Mao","Aiming Hao","Jintao Chen","Dongxia Liu","Xiaokun Feng","Jiashu Zhu","Meiqi Wu","Chubin Chen","Jiahong Wu","Xiangxiang Chu"],"pdf_url":"https://arxiv.org/pdf/2508.07981v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20392v4","updated":"2025-10-30T07:48:58Z","published":"2024-12-29T08:09:20Z","title":"Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning","summary":"  Multimodal contrastive learning models (e.g., CLIP) can learn high-quality\nrepresentations from large-scale image-text datasets, while they exhibit\nsignificant vulnerabilities to backdoor attacks, raising serious safety\nconcerns. In this paper, we reveal that CLIP's vulnerabilities primarily stem\nfrom its tendency to encode features beyond in-dataset predictive patterns,\ncompromising its visual feature resistivity to input perturbations. This makes\nits encoded features highly susceptible to being reshaped by backdoor triggers.\nTo address this challenge, we propose Repulsive Visual Prompt Tuning (RVPT), a\nnovel defense approach that employs deep visual prompt tuning with a specially\ndesigned feature-repelling loss. Specifically, RVPT adversarially repels the\nencoded features from deeper layers while optimizing the standard cross-entropy\nloss, ensuring that only predictive features in downstream tasks are encoded,\nthereby enhancing CLIP's visual feature resistivity against input perturbations\nand mitigating its susceptibility to backdoor attacks. Unlike existing\nmultimodal backdoor defense methods that typically require the availability of\npoisoned data or involve fine-tuning the entire model, RVPT leverages few-shot\ndownstream clean samples and only tunes a small number of parameters. Empirical\nresults demonstrate that RVPT tunes only 0.27\\% of the parameters in CLIP, yet\nit significantly outperforms state-of-the-art defense methods, reducing the\nattack success rate from 89.70\\% to 2.76\\% against the most advanced multimodal\nattacks on ImageNet and effectively generalizes its defensive capabilities\nacross multiple datasets.\n","authors":["Zhifang Zhang","Shuo He","Haobo Wang","Bingquan Shen","Lei Feng"],"pdf_url":"https://arxiv.org/pdf/2412.20392v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26213v1","updated":"2025-10-30T07:39:54Z","published":"2025-10-30T07:39:54Z","title":"OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal\n  Document Layout Generation","summary":"  Document AI has advanced rapidly and is attracting increasing attention. Yet,\nwhile most efforts have focused on document layout analysis (DLA), its\ngenerative counterpart, document layout generation, remains underexplored. A\nmajor obstacle lies in the scarcity of diverse layouts: academic papers with\nManhattan-style structures dominate existing studies, while open-world genres\nsuch as newspapers and magazines remain severely underrepresented. To address\nthis gap, we curate OmniLayout-1M, the first million-scale dataset of diverse\ndocument layouts, covering six common document types and comprising\ncontemporary layouts collected from multiple sources. Moreover, since existing\nmethods struggle in complex domains and often fail to arrange long sequences\ncoherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage\nCoarse-to-Fine learning paradigm: 1) learning universal layout principles from\nOmniLayout-1M with coarse category definitions, and 2) transferring the\nknowledge to a specific domain with fine-grained annotations. Extensive\nexperiments demonstrate that our approach achieves strong performance on\nmultiple domains in M$^{6}$Doc dataset, substantially surpassing both existing\nlayout generation experts and several latest general-purpose LLMs. Our code,\nmodels, and dataset will be publicly released.\n","authors":["Hengrui Kang","Zhuangcheng Gu","Zhiyuan Zhao","Zichen Wen","Bin Wang","Weijia Li","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2510.26213v1.pdf","comment":"TL;DR: With OmniLayout-1M dataset and LLM-based coarse-to-fine\n  learning, we enable universal and diverse document layout generation"},{"id":"http://arxiv.org/abs/2510.23588v2","updated":"2025-10-30T07:38:54Z","published":"2025-10-27T17:54:08Z","title":"FARMER: Flow AutoRegressive Transformer over Pixels","summary":"  Directly modeling the explicit likelihood of the raw data distribution is key\ntopic in the machine learning area, which achieves the scaling successes in\nLarge Language Models by autoregressive modeling. However, continuous AR\nmodeling over visual pixel data suffer from extremely long sequences and\nhigh-dimensional spaces. In this paper, we present FARMER, a novel end-to-end\ngenerative framework that unifies Normalizing Flows (NF) and Autoregressive\n(AR) models for tractable likelihood estimation and high-quality image\nsynthesis directly from raw pixels. FARMER employs an invertible autoregressive\nflow to transform images into latent sequences, whose distribution is modeled\nimplicitly by an autoregressive model. To address the redundancy and complexity\nin pixel-level modeling, we propose a self-supervised dimension reduction\nscheme that partitions NF latent channels into informative and redundant\ngroups, enabling more effective and efficient AR modeling. Furthermore, we\ndesign a one-step distillation scheme to significantly accelerate inference\nspeed and introduce a resampling-based classifier-free guidance algorithm to\nboost image generation quality. Extensive experiments demonstrate that FARMER\nachieves competitive performance compared to existing pixel-based generative\nmodels while providing exact likelihoods and scalable training.\n","authors":["Guangting Zheng","Qinyu Zhao","Tao Yang","Fei Xiao","Zhijie Lin","Jie Wu","Jiajun Deng","Yanyong Zhang","Rui Zhu"],"pdf_url":"https://arxiv.org/pdf/2510.23588v2.pdf","comment":"Bytedance Seed Technical Report"},{"id":"http://arxiv.org/abs/2510.26203v1","updated":"2025-10-30T07:26:18Z","published":"2025-10-30T07:26:18Z","title":"Developing a Multi-task Ensemble Geometric Deep Network for Supply Chain\n  Sustainability and Risk Management","summary":"  The sustainability of supply chain plays a key role in achieving optimal\nperformance in controlling the supply chain. The management of risks that occur\nin a supply chain is a fundamental problem for the purpose of developing the\nsustainability of the network and elevating the performance efficiency of the\nsupply chain. The correct classification of products is another essential\nelement in a sustainable supply chain. Acknowledging recent breakthroughs in\nthe context of deep networks, several architectural options have been deployed\nto analyze supply chain datasets. A novel geometric deep network is used to\npropose an ensemble deep network. The proposed Chebyshev ensemble geometric\nnetwork (Ch-EGN) is a hybrid convolutional and geometric deep learning. This\nnetwork is proposed to leverage the information dependencies in supply chain to\nderive invisible states of samples in the database. The functionality of the\nproposed deep network is assessed on the two different databases. The\nSupplyGraph Dataset and DataCo are considered in this research. The prediction\nof delivery status of DataCo supply chain is done for risk administration. The\nproduct classification and edge classification are performed using the\nSupplyGraph database to enhance the sustainability of the supply network. An\naverage accuracy of 98.95% is obtained for the ensemble network for risk\nmanagement. The average accuracy of 100% and 98.07% are obtained for\nsustainable supply chain in terms of 5 product group classification and 4\nproduct relation classification, respectively. The average accuracy of 92.37%\nis attained for 25 company relation classification. The results confirm an\naverage improvement and efficiency of the proposed method compared to the\nstate-of-the-art approaches.\n","authors":["Mehdi Khaleghi","Nastaran Khaleghi","Sobhan Sheykhivand","Sebelan Danishvar"],"pdf_url":"https://arxiv.org/pdf/2510.26203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10173v2","updated":"2025-10-30T07:25:20Z","published":"2025-06-11T20:53:45Z","title":"SPARKE: Scalable Prompt-Aware Diversity and Novelty Guidance in\n  Diffusion Models via RKE Score","summary":"  Diffusion models have demonstrated remarkable success in high-fidelity image\nsynthesis and prompt-guided generative modeling. However, ensuring adequate\ndiversity in generated samples of prompt-guided diffusion models remains a\nchallenge, particularly when the prompts span a broad semantic spectrum and the\ndiversity of generated data needs to be evaluated in a prompt-aware fashion\nacross semantically similar prompts. Recent methods have introduced guidance\nvia diversity measures to encourage more varied generations. In this work, we\nextend the diversity measure-based approaches by proposing the Scalable\nPrompt-Aware R\\'eny Kernel Entropy Diversity Guidance (SPARKE) method for\nprompt-aware diversity guidance. SPARKE utilizes conditional entropy for\ndiversity guidance, which dynamically conditions diversity measurement on\nsimilar prompts and enables prompt-aware diversity control. While the\nentropy-based guidance approach enhances prompt-aware diversity, its reliance\non the matrix-based entropy scores poses computational challenges in\nlarge-scale generation settings. To address this, we focus on the special case\nof Conditional latent RKE Score Guidance, reducing entropy computation and\ngradient-based optimization complexity from the $O(n^3)$ of general entropy\nmeasures to $O(n)$. The reduced computational complexity allows for\ndiversity-guided sampling over potentially thousands of generation rounds on\ndifferent prompts. We numerically test the SPARKE method on several\ntext-to-image diffusion models, demonstrating that the proposed method improves\nthe prompt-aware diversity of the generated data without incurring significant\ncomputational costs. We release our code on the project page:\nhttps://mjalali.github.io/SPARKE\n","authors":["Mohammad Jalali","Haoyu Lei","Amin Gohari","Farzan Farnia"],"pdf_url":"https://arxiv.org/pdf/2506.10173v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.14431v3","updated":"2025-10-30T07:17:56Z","published":"2025-10-16T08:31:44Z","title":"Real-Time Neural Video Compression with Unified Intra and Inter Coding","summary":"  Neural video compression (NVC) technologies have advanced rapidly in recent\nyears, yielding state-of-the-art schemes such as DCVC-RT that offer superior\ncompression efficiency to H.266/VVC and real-time encoding/decoding\ncapabilities. Nonetheless, existing NVC schemes have several limitations,\nincluding inefficiency in dealing with disocclusion and new content, interframe\nerror propagation and accumulation, among others. To eliminate these\nlimitations, we borrow the idea from classic video coding schemes, which allow\nintra coding within inter-coded frames. With the intra coding tool enabled,\ndisocclusion and new content are properly handled, and interframe error\npropagation is naturally intercepted without the need for manual refresh\nmechanisms. We present an NVC framework with unified intra and inter coding,\nwhere every frame is processed by a single model that is trained to perform\nintra/inter coding adaptively. Moreover, we propose a simultaneous two-frame\ncompression design to exploit interframe redundancy not only forwardly but also\nbackwardly. Experimental results show that our scheme outperforms DCVC-RT by an\naverage of 12.1% BD-rate reduction, delivers more stable bitrate and quality\nper frame, and retains real-time encoding/decoding performances. Code and\nmodels will be released.\n","authors":["Hui Xiang","Yifan Bian","Li Li","Jingran Wu","Xianguo Zhang","Dong Liu"],"pdf_url":"https://arxiv.org/pdf/2510.14431v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2510.26196v1","updated":"2025-10-30T07:13:46Z","published":"2025-10-30T07:13:46Z","title":"Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose\n  Prediction","summary":"  3D human pose estimation from sketches has broad applications in computer\nanimation and film production. Unlike traditional human pose estimation, this\ntask presents unique challenges due to the abstract and disproportionate nature\nof sketches. Previous sketch-to-pose methods, constrained by the lack of\nlarge-scale sketch-3D pose annotations, primarily relied on optimization with\nheuristic rules-an approach that is both time-consuming and limited in\ngeneralizability. To address these challenges, we propose a novel approach\nleveraging a \"learn from synthesis\" strategy. First, a diffusion model is\ntrained to synthesize sketch images from 2D poses projected from 3D human\nposes, mimicking disproportionate human structures in sketches. This process\nenables the creation of a synthetic dataset, SKEP-120K, consisting of 120k\naccurate sketch-3D pose annotation pairs across various sketch styles. Building\non this synthetic dataset, we introduce an end-to-end data-driven framework for\nestimating human poses and shapes from diverse sketch styles. Our framework\ncombines existing 2D pose detectors and generative diffusion priors for sketch\nfeature extraction with a feed-forward neural network for efficient 2D pose\nestimation. Multiple heuristic loss functions are incorporated to guarantee\ngeometric coherence between the derived 3D poses and the detected 2D poses\nwhile preserving accurate self-contacts. Qualitative, quantitative, and\nsubjective evaluations collectively show that our model substantially surpasses\nprevious ones in both estimation accuracy and speed for sketch-to-pose tasks.\n","authors":["Li Wang","Yiyu Zhuang","Yanwen Wang","Xun Cao","Chuan Guo","Xinxin Zuo","Hao Zhu"],"pdf_url":"https://arxiv.org/pdf/2510.26196v1.pdf","comment":"SIGGRAPH Asia 2025"},{"id":"http://arxiv.org/abs/2510.23981v2","updated":"2025-10-30T07:09:32Z","published":"2025-10-28T01:24:24Z","title":"TeleEgo: Benchmarking Egocentric AI Assistants in the Wild","summary":"  Egocentric AI assistants in real-world settings must process multi-modal\ninputs (video, audio, text), respond in real time, and retain evolving\nlong-term memory. However, existing benchmarks typically evaluate these\nabilities in isolation, lack realistic streaming scenarios, or support only\nshort-term tasks. We introduce \\textbf{TeleEgo}, a long-duration, streaming,\nomni-modal benchmark for evaluating egocentric AI assistants in realistic daily\ncontexts. The dataset features over 14 hours per participant of synchronized\negocentric video, audio, and text across four domains: work \\& study, lifestyle\n\\& routines, social activities, and outings \\& culture. All data is aligned on\na unified global timeline and includes high-quality visual narrations and\nspeech transcripts, curated through human refinement.TeleEgo defines 12\ndiagnostic subtasks across three core capabilities: Memory (recalling past\nevents), Understanding (interpreting the current moment), and Cross-Memory\nReasoning (linking distant events). It contains 3,291 human-verified QA items\nspanning multiple question formats (single-choice, binary, multi-choice, and\nopen-ended), evaluated strictly in a streaming setting. We propose two key\nmetrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess\ncorrectness, temporal responsiveness, and long-term retention. TeleEgo provides\na realistic and comprehensive evaluation to advance the development of\npractical AI assistants.\n","authors":["Jiaqi Yan","Ruilong Ren","Jingren Liu","Shuning Xu","Ling Wang","Yiheng Wang","Yun Wang","Long Zhang","Xiangyu Chen","Changzhi Sun","Jixiang Luo","Dell Zhang","Hao Sun","Chi Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2510.23981v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26186v1","updated":"2025-10-30T06:46:17Z","published":"2025-10-30T06:46:17Z","title":"ConceptScope: Characterizing Dataset Bias via Disentangled Visual\n  Concepts","summary":"  Dataset bias, where data points are skewed to certain concepts, is ubiquitous\nin machine learning datasets. Yet, systematically identifying these biases is\nchallenging without costly, fine-grained attribute annotations. We present\nConceptScope, a scalable and automated framework for analyzing visual datasets\nby discovering and quantifying human-interpretable concepts using Sparse\nAutoencoders trained on representations from vision foundation models.\nConceptScope categorizes concepts into target, context, and bias types based on\ntheir semantic relevance and statistical correlation to class labels, enabling\nclass-level dataset characterization, bias identification, and robustness\nevaluation through concept-based subgrouping. We validate that ConceptScope\ncaptures a wide range of visual concepts, including objects, textures,\nbackgrounds, facial attributes, emotions, and actions, through comparisons with\nannotated datasets. Furthermore, we show that concept activations produce\nspatial attributions that align with semantically meaningful image regions.\nConceptScope reliably detects known biases (e.g., background bias in\nWaterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects\nin ImageNet), offering a practical tool for dataset auditing and model\ndiagnostics.\n","authors":["Jinho Choi","Hyesu Lim","Steffen Schneider","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2510.26186v1.pdf","comment":"Published in the Thirty-Ninth Conference on Neural Information\n  Processing Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2505.16239v2","updated":"2025-10-30T06:40:44Z","published":"2025-05-22T05:16:45Z","title":"DOVE: Efficient One-Step Diffusion Model for Real-World Video\n  Super-Resolution","summary":"  Diffusion models have demonstrated promising performance in real-world video\nsuper-resolution (VSR). However, the dozens of sampling steps they require,\nmake inference extremely slow. Sampling acceleration techniques, particularly\nsingle-step, provide a potential solution. Nonetheless, achieving one step in\nVSR remains challenging, due to the high training overhead on video data and\nstringent fidelity demands. To tackle the above issues, we propose DOVE, an\nefficient one-step diffusion model for real-world VSR. DOVE is obtained by\nfine-tuning a pretrained video diffusion model (i.e., CogVideoX). To\neffectively train DOVE, we introduce the latent-pixel training strategy. The\nstrategy employs a two-stage scheme to gradually adapt the model to the video\nsuper-resolution task. Meanwhile, we design a video processing pipeline to\nconstruct a high-quality dataset tailored for VSR, termed HQ-VSR. Fine-tuning\non this dataset further enhances the restoration capability of DOVE. Extensive\nexperiments show that DOVE exhibits comparable or superior performance to\nmulti-step diffusion-based VSR methods. It also offers outstanding inference\nefficiency, achieving up to a 28$\\times$ speed-up over existing methods such as\nMGLD-VSR. Code is available at: https://github.com/zhengchen1999/DOVE.\n","authors":["Zheng Chen","Zichen Zou","Kewei Zhang","Xiongfei Su","Xin Yuan","Yong Guo","Yulun Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.16239v2.pdf","comment":"Accepted to NeurIPS 2025. Code is available at:\n  https://github.com/zhengchen1999/DOVE"},{"id":"http://arxiv.org/abs/2510.26173v1","updated":"2025-10-30T06:24:02Z","published":"2025-10-30T06:24:02Z","title":"MoTDiff: High-resolution Motion Trajectory estimation from a single\n  blurred image using Diffusion models","summary":"  Accurate estimation of motion information is crucial in diverse computational\nimaging and computer vision applications. Researchers have investigated various\nmethods to extract motion information from a single blurred image, including\nblur kernels and optical flow. However, existing motion representations are\noften of low quality, i.e., coarse-grained and inaccurate. In this paper, we\npropose the first high-resolution (HR) Motion Trajectory estimation framework\nusing Diffusion models (MoTDiff). Different from existing motion\nrepresentations, we aim to estimate an HR motion trajectory with high-quality\nfrom a single motion-blurred image. The proposed MoTDiff consists of two key\ncomponents: 1) a new conditional diffusion framework that uses multi-scale\nfeature maps extracted from a single blurred image as a condition, and 2) a new\ntraining method that can promote precise identification of a fine-grained\nmotion trajectory, consistent estimation of overall shape and position of a\nmotion path, and pixel connectivity along a motion trajectory. Our experiments\ndemonstrate that the proposed MoTDiff can outperform state-of-the-art methods\nin both blind image deblurring and coded exposure photography applications.\n","authors":["Wontae Choi","Jaelin Lee","Hyung Sup Yun","Byeungwoo Jeon","Il Yong Chun"],"pdf_url":"https://arxiv.org/pdf/2510.26173v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.13160v2","updated":"2025-10-30T06:21:47Z","published":"2025-03-17T13:31:19Z","title":"Language-guided Open-world Video Anomaly Detection under Weak\n  Supervision","summary":"  Video anomaly detection (VAD) aims to detect anomalies that deviate from what\nis expected. In open-world scenarios, the expected events may change as\nrequirements change. For example, not wearing a mask may be considered abnormal\nduring a flu outbreak but normal otherwise. However, existing methods assume\nthat the definition of anomalies is invariable, and thus are not applicable to\nthe open world. To address this, we propose a novel open-world VAD paradigm\nwith variable definitions, allowing guided detection through user-provided\nnatural language at inference time. This paradigm necessitates establishing a\nrobust mapping from video and textual definition to anomaly scores. Therefore,\nwe propose LaGoVAD (Language-guided Open-world Video Anomaly Detector), a model\nthat dynamically adapts anomaly definitions under weak supervision with two\nregularization strategies: diversifying the relative durations of anomalies via\ndynamic video synthesis, and enhancing feature robustness through contrastive\nlearning with negative mining. Training such adaptable models requires diverse\nanomaly definitions, but existing datasets typically provide labels without\nsemantic descriptions. To bridge this gap, we collect PreVAD (Pre-training\nVideo Anomaly Dataset), the largest and most diverse video anomaly dataset to\ndate, featuring 35,279 annotated videos with multi-level category labels and\ndescriptions that explicitly define anomalies. Zero-shot experiments on seven\ndatasets demonstrate LaGoVAD's SOTA performance. Our dataset and code will be\nreleased at https://github.com/Kamino666/LaGoVAD-PreVAD.\n","authors":["Zihao Liu","Xiaoyu Wu","Jianqin Wu","Xuxu Wang","Linlin Yang"],"pdf_url":"https://arxiv.org/pdf/2503.13160v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26170v1","updated":"2025-10-30T06:14:22Z","published":"2025-10-30T06:14:22Z","title":"Self-localization on a 3D map by fusing global and local features from a\n  monocular camera","summary":"  Self-localization on a 3D map by using an inexpensive monocular camera is\nrequired to realize autonomous driving. Self-localization based on a camera\noften uses a convolutional neural network (CNN) that can extract local features\nthat are calculated by nearby pixels. However, when dynamic obstacles, such as\npeople, are present, CNN does not work well. This study proposes a new method\ncombining CNN with Vision Transformer, which excels at extracting global\nfeatures that show the relationship of patches on whole image. Experimental\nresults showed that, compared to the state-of-the-art method (SOTA), the\naccuracy improvement rate in a CG dataset with dynamic obstacles is 1.5 times\nhigher than that without dynamic obstacles. Moreover, the self-localization\nerror of our method is 20.1% smaller than that of SOTA on public datasets.\nAdditionally, our robot using our method can localize itself with 7.51cm error\non average, which is more accurate than SOTA.\n","authors":["Satoshi Kikuch","Masaya Kato","Tsuyoshi Tasaki"],"pdf_url":"https://arxiv.org/pdf/2510.26170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26160v1","updated":"2025-10-30T05:50:48Z","published":"2025-10-30T05:50:48Z","title":"CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark","summary":"  Wearable devices such as smart glasses are transforming the way people\ninteract with their surroundings, enabling users to seek information regarding\nentities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)\nplays a key role in supporting such questions, yet there is still no\ncomprehensive benchmark for this task, especially regarding wearables\nscenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG\nbenchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse\nset of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn\nconversations across 13 domains, including 6.2K egocentric images designed to\nmimic captures from wearable devices. We carefully constructed the questions to\nreflect real-world scenarios and challenges, including five types of\nimage-quality issues, six question types, varying entity popularity, differing\ninformation dynamism, and different conversation turns. We design three tasks:\nsingle-source augmentation, multi-source augmentation, and multi-turn\nconversations -- each paired with an associated retrieval corpus and APIs for\nboth image-KG retrieval and webpage retrieval. Our evaluation shows that\nstraightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM\nsingle- and multi-turn QA, respectively, whereas state-of-the-art industry\nsolutions have similar quality (32%/45%), underscoring ample room for\nimprovement. The benchmark has hosted KDD Cup 2025, attracting about 1K\nparticipants and 5K submissions, with winning solutions improving baseline\nperformance by 28%, highlighting its early impact on advancing the field.\n","authors":["Jiaqi Wang","Xiao Yang","Kai Sun","Parth Suresh","Sanat Sharma","Adam Czyzewski","Derek Andersen","Surya Appini","Arkav Banerjee","Sajal Choudhary","Shervin Ghasemlou","Ziqiang Guan","Akil Iyer","Haidar Khan","Lingkun Kong","Roy Luo","Tiffany Ma","Zhen Qiao","David Tran","Wenfang Xu","Skyler Yeatman","Chen Zhou","Gunveer Gujral","Yinglong Xia","Shane Moon","Nicolas Scheffer","Nirav Shah","Eun Chang","Yue Liu","Florian Metze","Tammy Stark","Zhaleh Feizollahi","Andrea Jessee","Mangesh Pujari","Ahmed Aly","Babak Damavandi","Rakesh Wanga","Anuj Kumar","Rohit Patel","Wen-tau Yih","Xin Luna Dong"],"pdf_url":"https://arxiv.org/pdf/2510.26160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26154v1","updated":"2025-10-30T05:20:36Z","published":"2025-10-30T05:20:36Z","title":"Detecting Unauthorized Vehicles using Deep Learning for Smart Cities: A\n  Case Study on Bangladesh","summary":"  Modes of transportation vary across countries depending on geographical\nlocation and cultural context. In South Asian countries rickshaws are among the\nmost common means of local transport. Based on their mode of operation,\nrickshaws in cities across Bangladesh can be broadly classified into non-auto\n(pedal-powered) and auto-rickshaws (motorized). Monitoring the movement of\nauto-rickshaws is necessary as traffic rules often restrict auto-rickshaws from\naccessing certain routes. However, existing surveillance systems make it quite\ndifficult to monitor them due to their similarity to other vehicles, especially\nnon-auto rickshaws whereas manual video analysis is too time-consuming. This\npaper presents a machine learning-based approach to automatically detect\nauto-rickshaws in traffic images. In this system, we used real-time object\ndetection using the YOLOv8 model. For training purposes, we prepared a set of\n1,730 annotated images that were captured under various traffic conditions. The\nresults show that our proposed model performs well in real-time auto-rickshaw\ndetection and offers an mAP50 of 83.447% and binary precision and recall values\nabove 78%, demonstrating its effectiveness in handling both dense and sparse\ntraffic scenarios. The dataset has been publicly released for further research.\n","authors":["Sudipto Das Sukanto","Diponker Roy","Fahim Shakil","Nirjhar Singha","Abdullah Asik","Aniket Joarder","Mridha Md Nafis Fuad","Muhammad Ibrahim"],"pdf_url":"https://arxiv.org/pdf/2510.26154v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2510.21271v2","updated":"2025-10-30T05:16:33Z","published":"2025-10-24T09:12:59Z","title":"Buffer layers for Test-Time Adaptation","summary":"  In recent advancements in Test Time Adaptation (TTA), most existing\nmethodologies focus on updating normalization layers to adapt to the test\ndomain. However, the reliance on normalization-based adaptation presents key\nchallenges. First, normalization layers such as Batch Normalization (BN) are\nhighly sensitive to small batch sizes, leading to unstable and inaccurate\nstatistics. Moreover, normalization-based adaptation is inherently constrained\nby the structure of the pre-trained model, as it relies on training-time\nstatistics that may not generalize well to unseen domains. These issues limit\nthe effectiveness of normalization-based TTA approaches, especially under\nsignificant domain shift. In this paper, we introduce a novel paradigm based on\nthe concept of a Buffer layer, which addresses the fundamental limitations of\nnormalization layer updates. Unlike existing methods that modify the core\nparameters of the model, our approach preserves the integrity of the\npre-trained backbone, inherently mitigating the risk of catastrophic forgetting\nduring online adaptation. Through comprehensive experimentation, we demonstrate\nthat our approach not only outperforms traditional methods in mitigating domain\nshift and enhancing model robustness, but also exhibits strong resilience to\nforgetting. Furthermore, our Buffer layer is modular and can be seamlessly\nintegrated into nearly all existing TTA frameworks, resulting in consistent\nperformance improvements across various architectures. These findings validate\nthe effectiveness and versatility of the proposed solution in real-world domain\nadaptation scenarios. The code is available at\nhttps://github.com/hyeongyu-kim/Buffer_TTA.\n","authors":["Hyeongyu Kim","Geonhui Han","Dosik Hwang"],"pdf_url":"https://arxiv.org/pdf/2510.21271v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26151v1","updated":"2025-10-30T05:12:29Z","published":"2025-10-30T05:12:29Z","title":"MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer\n  Diagnosis and Risk Prediction","summary":"  Large annotated datasets are essential for training robust Computer-Aided\nDiagnosis (CAD) models for breast cancer detection or risk prediction. However,\nacquiring such datasets with fine-detailed annotation is both costly and\ntime-consuming. Vision-Language Models (VLMs), such as CLIP, which are\npre-trained on large image-text pairs, offer a promising solution by enhancing\nrobustness and data efficiency in medical imaging tasks. This paper introduces\na novel Multi-View Mammography and Language Model for breast cancer\nclassification and risk prediction, trained on a dataset of paired mammogram\nimages and synthetic radiology reports. Our MV-MLM leverages multi-view\nsupervision to learn rich representations from extensive radiology data by\nemploying cross-modal self-supervision across image-text pairs. This includes\nmultiple views and the corresponding pseudo-radiology reports. We propose a\nnovel joint visual-textual learning strategy to enhance generalization and\naccuracy performance over different data types and tasks to distinguish breast\ntissues or cancer characteristics(calcification, mass) and utilize these\npatterns to understand mammography images and predict cancer risk. We evaluated\nour method on both private and publicly available datasets, demonstrating that\nthe proposed model achieves state-of-the-art performance in three\nclassification tasks: (1) malignancy classification, (2) subtype\nclassification, and (3) image-based cancer risk prediction. Furthermore, the\nmodel exhibits strong data efficiency, outperforming existing fully supervised\nor VLM baselines while trained on synthetic text reports and without the need\nfor actual radiology reports.\n","authors":["Shunjie-Fabian Zheng","Hyeonjun Lee","Thijs Kooi","Ali Diba"],"pdf_url":"https://arxiv.org/pdf/2510.26151v1.pdf","comment":"Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)\n  Workshop at ICCV 2025"},{"id":"http://arxiv.org/abs/2510.26149v1","updated":"2025-10-30T05:08:45Z","published":"2025-10-30T05:08:45Z","title":"BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and\n  Enhanced Motion Compensation","summary":"  Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution\nof video frames, potentially at various scaling factors, which presents several\nchallenges regarding spatial detail reproduction, temporal consistency, and\ncomputational complexity. In this paper, we propose a strong baseline BasicAVSR\nfor AVSR by integrating four key components: 1) adaptive multi-scale frequency\npriors generated from image Laplacian pyramids, 2) a flow-guided propagation\nunit to aggregate spatiotemporal information from adjacent frames, 3) a\nsecond-order motion compensation unit for more accurate spatial alignment of\nadjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and\ncontent-independent upsampling kernels. To meet diverse application demands, we\ninstantiate three propagation variants: (i) a unidirectional RNN unit for\nstrictly online inference, (ii) a unidirectional RNN unit empowered with a\nlimited lookahead that tolerates a small output delay, and (iii) a\nbidirectional RNN unit designed for offline tasks where computational resources\nare less constrained. Experimental results demonstrate the effectiveness and\nadaptability of our model across these different scenarios. Through extensive\nexperiments, we show that BasicAVSR significantly outperforms existing methods\nin terms of super-resolution quality, generalization ability, and inference\nspeed. Our work not only advances the state-of-the-art in AVSR but also extends\nits core components to multiple frameworks for diverse scenarios. The code is\navailable at https://github.com/shangwei5/BasicAVSR.\n","authors":["Wei Shang","Wanying Zhang","Shuhang Gu","Pengfei Zhu","Qinghua Hu","Dongwei Ren"],"pdf_url":"https://arxiv.org/pdf/2510.26149v1.pdf","comment":"13 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2506.00871v2","updated":"2025-10-30T05:04:19Z","published":"2025-06-01T07:18:47Z","title":"Towards Predicting Any Human Trajectory In Context","summary":"  Predicting accurate future trajectories of pedestrians is essential for\nautonomous systems but remains a challenging task due to the need for\nadaptability in different environments and domains. A common approach involves\ncollecting scenario-specific data and performing fine-tuning via\nbackpropagation. However, the need to fine-tune for each new scenario is often\nimpractical for deployment on edge devices. To address this challenge, we\nintroduce \\paper, an In-Context Learning (ICL) framework for pedestrian\ntrajectory prediction that enables adaptation without fine-tuning on the\nscenario-specific data at inference time without requiring weight updates. We\npropose a spatio-temporal similarity-based example selection (STES) method that\nselects relevant examples from previously observed trajectories within the same\nscene by identifying similar motion patterns at corresponding locations. To\nfurther refine this selection, we introduce prediction-guided example selection\n(PG-ES), which selects examples based on both the past trajectory and the\npredicted future trajectory, rather than relying solely on the past trajectory.\nThis approach allows the model to account for long-term dynamics when selecting\nexamples. Finally, instead of relying on small real-world datasets with limited\nscenario diversity, we train our model on a large-scale synthetic dataset to\nenhance its prediction ability by leveraging in-context examples. Extensive\nexperiments demonstrate that TrajICL achieves remarkable adaptation across both\nin-domain and cross-domain scenarios, outperforming even fine-tuned approaches\nacross multiple public benchmarks. Project Page:\nhttps://fujiry0.github.io/TrajICL-project-page/.\n","authors":["Ryo Fujii","Hideo Saito","Ryo Hachiuma"],"pdf_url":"https://arxiv.org/pdf/2506.00871v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.16396v3","updated":"2025-10-30T04:59:32Z","published":"2025-10-18T08:19:49Z","title":"SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation","summary":"  With the increasing ubiquity of AR/VR devices, the deployment of deep\nlearning models on edge devices has become a critical challenge. These devices\nrequire real-time inference, low power consumption, and minimal latency. Many\nframework designers face the conundrum of balancing efficiency and performance.\nWe design a light framework that adopts an encoder-decoder architecture and\nintroduces several key contributions aimed at improving both efficiency and\naccuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the\ninherent sparsity in hand pose images, achieving a 42% end-to-end efficiency\nimprovement. Moreover, we propose our SPLite decoder. This new architecture\nsignificantly boosts the decoding process's frame rate by 3.1x on the Raspberry\nPi 5, while maintaining accuracy on par. To further optimize performance, we\napply quantization-aware training, reducing memory usage while preserving\naccuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on\nFreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5\nCPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on\ncompound benchmark datasets, demonstrating comparable accuracy to\nstate-of-the-art approaches while significantly enhancing computational\nefficiency.\n","authors":["Yeh Keng Hao","Hsu Tzu Wei","Sun Min"],"pdf_url":"https://arxiv.org/pdf/2510.16396v3.pdf","comment":"Accepted to AICCC 2025"},{"id":"http://arxiv.org/abs/2510.26141v1","updated":"2025-10-30T04:52:12Z","published":"2025-10-30T04:52:12Z","title":"StructLayoutFormer:Conditional Structured Layout Generation via\n  Structure Serialization and Disentanglement","summary":"  Structured layouts are preferable in many 2D visual contents (\\eg, GUIs,\nwebpages) since the structural information allows convenient layout editing.\nComputational frameworks can help create structured layouts but require heavy\nlabor input. Existing data-driven approaches are effective in automatically\ngenerating fixed layouts but fail to produce layout structures. We present\nStructLayoutFormer, a novel Transformer-based approach for conditional\nstructured layout generation. We use a structure serialization scheme to\nrepresent structured layouts as sequences. To better control the structures of\ngenerated layouts, we disentangle the structural information from the element\nplacements. Our approach is the first data-driven approach that achieves\nconditional structured layout generation and produces realistic layout\nstructures explicitly. We compare our approach with existing data-driven layout\ngeneration approaches by including post-processing for structure extraction.\nExtensive experiments have shown that our approach exceeds these baselines in\nconditional structured layout generation. We also demonstrate that our approach\nis effective in extracting and transferring layout structures. The code is\npublicly available at %\\href{https://github.com/Teagrus/StructLayoutFormer}\n{https://github.com/Teagrus/StructLayoutFormer}.\n","authors":["Xin Hu","Pengfei Xu","Jin Zhou","Hongbo Fu","Hui Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26140v1","updated":"2025-10-30T04:51:05Z","published":"2025-10-30T04:51:05Z","title":"FullPart: Generating each 3D Part at Full Resolution","summary":"  Part-based 3D generation holds great potential for various applications.\nPrevious part generators that represent parts using implicit vector-set tokens\noften suffer from insufficient geometric details. Another line of work adopts\nan explicit voxel representation but shares a global voxel grid among all\nparts; this often causes small parts to occupy too few voxels, leading to\ndegraded quality. In this paper, we propose FullPart, a novel framework that\ncombines both implicit and explicit paradigms. It first derives the bounding\nbox layout through an implicit box vector-set diffusion process, a task that\nimplicit diffusion handles effectively since box tokens contain little\ngeometric detail. Then, it generates detailed parts, each within its own fixed\nfull-resolution voxel grid. Instead of sharing a global low-resolution space,\neach part in our method - even small ones - is generated at full resolution,\nenabling the synthesis of intricate details. We further introduce a\ncenter-point encoding strategy to address the misalignment issue when\nexchanging information between parts of different actual sizes, thereby\nmaintaining global coherence. Moreover, to tackle the scarcity of reliable part\ndata, we present PartVerse-XL, the largest human-annotated 3D part dataset to\ndate with 40K objects and 320K parts. Extensive experiments demonstrate that\nFullPart achieves state-of-the-art results in 3D part generation. We will\nrelease all code, data, and model to benefit future research in 3D part\ngeneration.\n","authors":["Lihe Ding","Shaocong Dong","Yaokun Li","Chenjian Gao","Xiao Chen","Rui Han","Yihao Kuang","Hong Zhang","Bo Huang","Zhanpeng Huang","Zibin Wang","Dan Xu","Tianfan Xue"],"pdf_url":"https://arxiv.org/pdf/2510.26140v1.pdf","comment":"Project page: https://fullpart3d.github.io"},{"id":"http://arxiv.org/abs/2510.26131v1","updated":"2025-10-30T04:31:56Z","published":"2025-10-30T04:31:56Z","title":"Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM","summary":"  Attention models have recently emerged as a powerful approach, demonstrating\nsignificant progress in various fields. Visualization techniques, such as class\nactivation mapping, provide visual insights into the reasoning of convolutional\nneural networks (CNNs). Using network gradients, it is possible to identify\nregions where the network pays attention during image recognition tasks.\nFurthermore, these gradients can be combined with CNN features to localize more\ngeneralizable, task-specific attentive (salient) regions within scenes.\nHowever, explicit use of this gradient-based attention information integrated\ndirectly into CNN representations for semantic object understanding remains\nlimited. Such integration is particularly beneficial for visual tasks like\nsimultaneous localization and mapping (SLAM), where CNN representations\nenriched with spatially attentive object locations can enhance performance. In\nthis work, we propose utilizing task-specific network attention for RGB-D\nindoor SLAM. Specifically, we integrate layer-wise attention information\nderived from network gradients with CNN feature representations to improve\nframe association performance. Experimental results indicate improved\nperformance compared to baseline methods, particularly for large environments.\n","authors":["Ali Caglayan","Nevrez Imamoglu","Oguzhan Guclu","Ali Osman Serhatoglu","Ahmet Burak Can","Ryosuke Nakamura"],"pdf_url":"https://arxiv.org/pdf/2510.26131v1.pdf","comment":"double-column 5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2510.26125v1","updated":"2025-10-30T04:25:33Z","published":"2025-10-30T04:25:33Z","title":"WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging\n  Long-tail Scenarios","summary":"  Vision-based end-to-end (E2E) driving has garnered significant interest in\nthe research community due to its scalability and synergy with multimodal large\nlanguage models (MLLMs). However, current E2E driving benchmarks primarily\nfeature nominal scenarios, failing to adequately test the true potential of\nthese systems. Furthermore, existing open-loop evaluation metrics often fall\nshort in capturing the multi-modal nature of driving or effectively evaluating\nperformance in long-tail scenarios. To address these gaps, we introduce the\nWaymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021\ndriving segments (approximately 12 hours), specifically curated for challenging\nlong-tail scenarios that that are rare in daily life with an occurring\nfrequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the\nhigh-level routing information, ego states, and 360-degree camera views from 8\nsurrounding cameras. To evaluate the E2E driving performance on these long-tail\nsituations, we propose a novel open-loop evaluation metric: Rater Feedback\nScore (RFS). Unlike conventional metrics that measure the distance between\npredicted way points and the logs, RFS measures how closely the predicted\ntrajectory matches rater-annotated trajectory preference labels. We have\nreleased rater preference labels for all WOD-E2E validation set segments, while\nthe held out test set labels have been used for the 2025 WOD-E2E Challenge.\nThrough our work, we aim to foster state of the art research into\ngeneralizable, robust, and safe end-to-end autonomous driving agents capable of\nhandling complex real-world situations.\n","authors":["Runsheng Xu","Hubert Lin","Wonseok Jeon","Hao Feng","Yuliang Zou","Liting Sun","John Gorman","Kate Tolstaya","Sarah Tang","Brandyn White","Ben Sapp","Mingxing Tan","Jyh-Jing Hwang","Drago Anguelov"],"pdf_url":"https://arxiv.org/pdf/2510.26125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08772v2","updated":"2025-10-30T04:25:25Z","published":"2025-07-11T17:33:18Z","title":"From One to More: Contextual Part Latents for 3D Generation","summary":"  Recent advances in 3D generation have transitioned from multi-view 2D\nrendering approaches to 3D-native latent diffusion frameworks that exploit\ngeometric priors in ground truth data. Despite progress, three key limitations\npersist: (1) Single-latent representations fail to capture complex multi-part\ngeometries, causing detail degradation; (2) Holistic latent coding neglects\npart independence and interrelationships critical for compositional design; (3)\nGlobal conditioning mechanisms lack fine-grained controllability. Inspired by\nhuman 3D design workflows, we propose CoPart - a part-aware diffusion framework\nthat decomposes 3D objects into contextual part latents for coherent multi-part\ngeneration. This paradigm offers three advantages: i) Reduces encoding\ncomplexity through part decomposition; ii) Enables explicit part relationship\nmodeling; iii) Supports part-level conditioning. We further develop a mutual\nguidance strategy to fine-tune pre-trained diffusion models for joint part\nlatent denoising, ensuring both geometric coherence and foundation model\npriors. To enable large-scale training, we construct Partverse - a novel 3D\npart dataset derived from Objaverse through automated mesh segmentation and\nhuman-verified annotations. Extensive experiments demonstrate CoPart's superior\ncapabilities in part-level editing, articulated object generation, and scene\ncomposition with unprecedented controllability.\n","authors":["Shaocong Dong","Lihe Ding","Xiao Chen","Yaokun Li","Yuxin Wang","Yucheng Wang","Qi Wang","Jaehyeok Kim","Chenjian Gao","Zhanpeng Huang","Zibin Wang","Tianfan Xue","Dan Xu"],"pdf_url":"https://arxiv.org/pdf/2507.08772v2.pdf","comment":"Project page: https://copart3d.github.io/"},{"id":"http://arxiv.org/abs/2510.26117v1","updated":"2025-10-30T04:00:07Z","published":"2025-10-30T04:00:07Z","title":"JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting","summary":"  Traditional novel view synthesis methods heavily rely on external camera pose\nestimation tools such as COLMAP, which often introduce computational\nbottlenecks and propagate errors. To address these challenges, we propose a\nunified framework that jointly optimizes 3D Gaussian points and camera poses\nwithout requiring pre-calibrated inputs. Our approach iteratively refines 3D\nGaussian parameters and updates camera poses through a novel co-optimization\nstrategy, ensuring simultaneous improvements in scene reconstruction fidelity\nand pose accuracy. The key innovation lies in decoupling the joint optimization\ninto two interleaved phases: first, updating 3D Gaussian parameters via\ndifferentiable rendering with fixed poses, and second, refining camera poses\nusing a customized 3D optical flow algorithm that incorporates geometric and\nphotometric constraints. This formulation progressively reduces projection\nerrors, particularly in challenging scenarios with large viewpoint variations\nand sparse feature distributions, where traditional methods struggle. Extensive\nevaluations on multiple datasets demonstrate that our approach significantly\noutperforms existing COLMAP-free techniques in reconstruction quality, and also\nsurpasses the standard COLMAP-based baseline in general.\n","authors":["Yuxuan Li","Tao Wang","Xianben Yang"],"pdf_url":"https://arxiv.org/pdf/2510.26117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26114v1","updated":"2025-10-30T03:54:53Z","published":"2025-10-30T03:54:53Z","title":"OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script\n  Research","summary":"  As one of the earliest writing systems, Oracle Bone Script (OBS) preserves\nthe cultural and intellectual heritage of ancient civilizations. However,\ncurrent OBS research faces two major challenges: (1) the interpretation of OBS\ninvolves a complex workflow comprising multiple serial and parallel sub-tasks,\nand (2) the efficiency of OBS information organization and retrieval remains a\ncritical bottleneck, as scholars often spend substantial effort searching for,\ncompiling, and managing relevant resources. To address these challenges, we\npresent OracleAgent, the first agent system designed for the structured\nmanagement and retrieval of OBS-related information. OracleAgent seamlessly\nintegrates multiple OBS analysis tools, empowered by large language models\n(LLMs), and can flexibly orchestrate these components. Additionally, we\nconstruct a comprehensive domain-specific multimodal knowledge base for OBS,\nwhich is built through a rigorous multi-year process of data collection,\ncleaning, and expert annotation. The knowledge base comprises over 1.4M\nsingle-character rubbing images and 80K interpretation texts. OracleAgent\nleverages this resource through its multimodal tools to assist experts in\nretrieval tasks of character, document, interpretation text, and rubbing image.\nExtensive experiments demonstrate that OracleAgent achieves superior\nperformance across a range of multimodal reasoning and generation tasks,\nsurpassing leading mainstream multimodal large language models (MLLMs) (e.g.,\nGPT-4o). Furthermore, our case study illustrates that OracleAgent can\neffectively assist domain experts, significantly reducing the time cost of OBS\nresearch. These results highlight OracleAgent as a significant step toward the\npractical deployment of OBS-assisted research and automated interpretation\nsystems.\n","authors":["Caoshuo Li","Zengmao Ding","Xiaobin Hu","Bang Li","Donghao Luo","Xu Peng","Taisong Jin","Yongge Liu","Shengwei Han","Jing Yang","Xiaoping He","Feng Gao","AndyPian Wu"," SevenShu","Chaoyang Wang","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26113v1","updated":"2025-10-30T03:53:22Z","published":"2025-10-30T03:53:22Z","title":"EgoExo-Con: Exploring View-Invariant Video Temporal Understanding","summary":"  Can Video-LLMs achieve consistent temporal understanding when videos capture\nthe same event from different viewpoints? To study this, we introduce\nEgoExo-Con (Consistency), a benchmark of comprehensively synchronized\negocentric and exocentric video pairs with human-refined queries in natural\nlanguage. EgoExo-Con emphasizes two temporal understanding tasks: Temporal\nVerification and Temporal Grounding. It evaluates not only correctness but\nconsistency across viewpoints. Our analysis reveals two critical limitations of\nexisting Video-LLMs: (1) models often fail to maintain consistency, with\nresults far worse than their single-view performances. (2) When naively\nfinetuned with synchronized videos of both viewpoints, the models show improved\nconsistency but often underperform those trained on a single view. For\nimprovements, we propose View-GRPO, a novel reinforcement learning framework\nthat effectively strengthens view-specific temporal reasoning while encouraging\nconsistent comprehension across viewpoints. Our method demonstrates its\nsuperiority over naive SFT and GRPO, especially for improving cross-view\nconsistency. All resources will be made publicly available.\n","authors":["Minjoon Jung","Junbin Xiao","Junghyun Kim","Byoung-Tak Zhang","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2510.26113v1.pdf","comment":"project page:\n  \\url{https://minjoong507.github.io/projects/EgoExo-Con/}"},{"id":"http://arxiv.org/abs/2510.26105v1","updated":"2025-10-30T03:31:20Z","published":"2025-10-30T03:31:20Z","title":"Security Risk of Misalignment between Text and Image in Multi-modal\n  Model","summary":"  Despite the notable advancements and versatility of multi-modal diffusion\nmodels, such as text-to-image models, their susceptibility to adversarial\ninputs remains underexplored. Contrary to expectations, our investigations\nreveal that the alignment between textual and Image modalities in existing\ndiffusion models is inadequate. This misalignment presents significant risks,\nespecially in the generation of inappropriate or Not-Safe-For-Work (NSFW)\ncontent. To this end, we propose a novel attack called Prompt-Restricted\nMulti-modal Attack (PReMA) to manipulate the generated content by modifying the\ninput image in conjunction with any specified prompt, without altering the\nprompt itself. PReMA is the first attack that manipulates model outputs by\nsolely creating adversarial images, distinguishing itself from prior methods\nthat primarily generate adversarial prompts to produce NSFW content.\nConsequently, PReMA poses a novel threat to the integrity of multi-modal\ndiffusion models, particularly in image-editing applications that operate with\nfixed prompts. Comprehensive evaluations conducted on image inpainting and\nstyle transfer tasks across various models confirm the potent efficacy of\nPReMA.\n","authors":["Xiaosen Wang","Zhijin Ge","Shaokang Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.24267v2","updated":"2025-10-30T03:29:32Z","published":"2025-09-29T04:24:13Z","title":"Cycle Diffusion Model for Counterfactual Image Generation","summary":"  Deep generative models have demonstrated remarkable success in medical image\nsynthesis. However, ensuring conditioning faithfulness and high-quality\nsynthetic images for direct or counterfactual generation remains a challenge.\nIn this work, we introduce a cycle training framework to fine-tune diffusion\nmodels for improved conditioning adherence and enhanced synthetic image\nrealism. Our approach, Cycle Diffusion Model (CDM), enforces consistency\nbetween generated and original images by incorporating cycle constraints,\nenabling more reliable direct and counterfactual generation. Experiments on a\ncombined 3D brain MRI dataset (from ABCD, HCP aging & young adults, ADNI, and\nPPMI) show that our method improves conditioning accuracy and enhances image\nquality as measured by FID and SSIM. The results suggest that the cycle\nstrategy used in CDM can be an effective method for refining diffusion-based\nmedical image generation, with applications in data augmentation,\ncounterfactual, and disease progression modeling.\n","authors":["Fangrui Huang","Alan Wang","Binxu Li","Bailey Trang","Ridvan Yesiloglu","Tianyu Hua","Wei Peng","Ehsan Adeli"],"pdf_url":"https://arxiv.org/pdf/2509.24267v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00254v4","updated":"2025-10-30T03:12:42Z","published":"2025-05-01T02:40:23Z","title":"Empowering Agentic Video Analytics Systems with Video Language Models","summary":"  AI-driven video analytics has become increasingly important across diverse\ndomains. However, existing systems are often constrained to specific,\npredefined tasks, limiting their adaptability in open-ended analytical\nscenarios. The recent emergence of Vision Language Models (VLMs) as\ntransformative technologies offers significant potential for enabling\nopen-ended video understanding, reasoning, and analytics. Nevertheless, their\nlimited context windows present challenges when processing ultra-long video\ncontent, which is prevalent in real-world applications. To address this, we\nintroduce AVA, a VLM-powered system designed for open-ended, advanced video\nanalytics. AVA incorporates two key innovations: (1) the near real-time\nconstruction of Event Knowledge Graphs (EKGs) for efficient indexing of long or\ncontinuous video streams, and (2) an agentic retrieval-generation mechanism\nthat leverages EKGs to handle complex and diverse queries. Comprehensive\nevaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that\nAVA achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,\nrespectively-significantly surpassing existing VLM and video\nRetrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video\nanalytics in ultra-long and open-world video scenarios, we introduce a new\nbenchmark, AVA-100. This benchmark comprises 8 videos, each exceeding 10 hours\nin duration, along with 120 manually annotated, diverse, and complex\nquestion-answer pairs. On AVA-100, AVA achieves top-tier performance with an\naccuracy of 75.8%. The source code of AVA is available at\nhttps://github.com/I-ESC/Project-Ava. The AVA-100 benchmark can be accessed at\nhttps://huggingface.co/datasets/iesc/Ava-100.\n","authors":["Yuxuan Yan","Shiqi Jiang","Ting Cao","Yifan Yang","Qianqian Yang","Yuanchao Shu","Yuqing Yang","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2505.00254v4.pdf","comment":"Accepted to NDSI 2026, 19pages, 12 figures, complementary evaluations\n  and appendix"},{"id":"http://arxiv.org/abs/2505.18584v2","updated":"2025-10-30T02:59:44Z","published":"2025-05-24T08:20:36Z","title":"Unleashing Diffusion Transformers for Visual Correspondence by\n  Modulating Massive Activations","summary":"  Pre-trained stable diffusion models (SD) have shown great advances in visual\ncorrespondence. In this paper, we investigate the capabilities of Diffusion\nTransformers (DiTs) for accurate dense correspondence. Distinct from SD, DiTs\nexhibit a critical phenomenon in which very few feature activations exhibit\nsignificantly larger values than others, known as \\textit{massive activations},\nleading to uninformative representations and significant performance\ndegradation for DiTs. The massive activations consistently concentrate at very\nfew fixed dimensions across all image patch tokens, holding little local\ninformation. We trace these dimension-concentrated massive activations and find\nthat such concentration can be effectively localized by the zero-initialized\nAdaptive Layer Norm (AdaLN-zero). Building on these findings, we propose\nDiffusion Transformer Feature (DiTF), a training-free framework designed to\nextract semantic-discriminative features from DiTs. Specifically, DiTF employs\nAdaLN to adaptively localize and normalize massive activations with\nchannel-wise modulation. In addition, we develop a channel discard strategy to\nfurther eliminate the negative impacts from massive activations. Experimental\nresults demonstrate that our DiTF outperforms both DINO and SD-based models and\nestablishes a new state-of-the-art performance for DiTs in different visual\ncorrespondence tasks (\\eg, with +9.4\\% on Spair-71k and +4.4\\% on AP-10K-C.S.).\n","authors":["Chaofan Gan","Yuanpeng Tu","Xi Chen","Tieyuan Chen","Yuxi Li","Mehrtash Harandi","Weiyao Lin"],"pdf_url":"https://arxiv.org/pdf/2505.18584v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25327v2","updated":"2025-10-30T02:51:38Z","published":"2025-10-29T09:41:03Z","title":"MMEdge: Accelerating On-device Multimodal Inference via Pipelined\n  Sensing and Encoding","summary":"  Real-time multimodal inference on resource-constrained edge devices is\nessential for applications such as autonomous driving, human-computer\ninteraction, and mobile health. However, prior work often overlooks the tight\ncoupling between sensing dynamics and model execution, as well as the complex\ninter-modality dependencies. In this paper, we propose MMEdge, an new on-device\nmulti-modal inference framework based on pipelined sensing and encoding.\nInstead of waiting for complete sensor inputs, MMEdge decomposes the entire\ninference process into a sequence of fine-grained sensing and encoding units,\nallowing computation to proceed incrementally as data arrive. MMEdge also\nintroduces a lightweight but effective temporal aggregation module that\ncaptures rich temporal dynamics across different pipelined units to maintain\naccuracy performance. Such pipelined design also opens up opportunities for\nfine-grained cross-modal optimization and early decision-making during\ninference. To further enhance system performance under resource variability and\ninput data complexity, MMEdge incorporates an adaptive multimodal configuration\noptimizer that dynamically selects optimal sensing and model configurations for\neach modality under latency constraints, and a cross-modal speculative skipping\nmechanism that bypasses future units of slower modalities when early\npredictions reach sufficient confidence. We evaluate MMEdge using two public\nmultimodal datasets and deploy it on a real-world unmanned aerial vehicle\n(UAV)-based multimodal testbed. The results show that MMEdge significantly\nreduces end-to-end latency while maintaining high task accuracy across various\nsystem and data dynamics.\n","authors":["Runxi Huang","Mingxuan Yu","Mingyu Tsoi","Xiaomin Ouyang"],"pdf_url":"https://arxiv.org/pdf/2510.25327v2.pdf","comment":"Code available at: https://github.com/HKUST-MINSys-Lab/MMEdge.\n  Accepted by SenSys 2026"},{"id":"http://arxiv.org/abs/2506.21046v2","updated":"2025-10-30T02:36:15Z","published":"2025-06-26T06:47:51Z","title":"Boosting Generative Adversarial Transferability with Self-supervised\n  Vision Transformer Features","summary":"  The ability of deep neural networks (DNNs) come from extracting and\ninterpreting features from the data provided. By exploiting intermediate\nfeatures in DNNs instead of relying on hard labels, we craft adversarial\nperturbation that generalize more effectively, boosting black-box\ntransferability. These features ubiquitously come from supervised learning in\nprevious work. Inspired by the exceptional synergy between self-supervised\nlearning and the Transformer architecture, this paper explores whether\nexploiting self-supervised Vision Transformer (ViT) representations can improve\nadversarial transferability. We present dSVA -- a generative dual\nself-supervised ViT features attack, that exploits both global structural\nfeatures from contrastive learning (CL) and local textural features from masked\nimage modeling (MIM), the self-supervised learning paradigm duo for ViTs. We\ndesign a novel generative training framework that incorporates a generator to\ncreate black-box adversarial examples, and strategies to train the generator by\nexploiting joint features and the attention mechanism of self-supervised ViTs.\nOur findings show that CL and MIM enable ViTs to attend to distinct feature\ntendencies, which, when exploited in tandem, boast great adversarial\ngeneralizability. By disrupting dual deep features distilled by self-supervised\nViTs, we are rewarded with remarkable black-box transferability to models of\nvarious architectures that outperform state-of-the-arts. Code available at\nhttps://github.com/spencerwooo/dSVA.\n","authors":["Shangbo Wu","Yu-an Tan","Ruinan Ma","Wencong Ma","Dehua Zhu","Yuanzhang Li"],"pdf_url":"https://arxiv.org/pdf/2506.21046v2.pdf","comment":"14 pages, 9 figures, accepted at ICCV 2025"},{"id":"http://arxiv.org/abs/2408.01701v6","updated":"2025-10-30T02:28:16Z","published":"2024-08-03T07:47:16Z","title":"Signal-SGN: A Spiking Graph Convolutional Network for Skeletal Action\n  Recognition via Learning Temporal-Frequency Dynamics","summary":"  For multimodal skeleton-based action recognition, Graph Convolutional\nNetworks (GCNs) are effective models. Still, their reliance on floating-point\ncomputations leads to high energy consumption, limiting their applicability in\nbattery-powered devices. While energy-efficient, Spiking Neural Networks (SNNs)\nstruggle to model skeleton dynamics, leading to suboptimal solutions. We\npropose Signal-SGN (Spiking Graph Convolutional Network), which utilizes the\ntemporal dimension of skeleton sequences as the spike time steps and represents\nfeatures as multi-dimensional discrete stochastic signals for\ntemporal-frequency domain feature extraction. It combines the 1D Spiking Graph\nConvolution (1D-SGC) module and the Frequency Spiking Convolution (FSC) module\nto extract features from the skeleton represented as spiking form.\nAdditionally, the Multi-Scale Wavelet Transform Feature Fusion (MWTF) module is\nproposed to extract dynamic spiking features and capture frequency-specific\ncharacteristics, enhancing classification performance. Experiments across three\nlarge-scale datasets reveal Signal-SGN exceeding state-of-the-art SNN-based\nmethods in accuracy and computational efficiency while attaining comparable\nperformance with GCN methods and significantly reducing theoretical energy\nconsumption.\n","authors":["Naichuan Zheng","Yuchen Du","Hailun Xia","Zeyu Liang"],"pdf_url":"https://arxiv.org/pdf/2408.01701v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.17148v3","updated":"2025-10-30T01:44:58Z","published":"2025-10-20T04:49:14Z","title":"DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through\n  Metric-Guided Alignment","summary":"  Conventional end-to-end (E2E) driving models are effective at generating\nphysically plausible trajectories, but often fail to generalize to long-tail\nscenarios due to the lack of essential world knowledge to understand and reason\nabout surrounding environments. In contrast, Vision-Language-Action (VLA)\nmodels leverage world knowledge to handle challenging cases, but their limited\n3D reasoning capability can lead to physically infeasible actions. In this work\nwe introduce DiffVLA++, an enhanced autonomous driving framework that\nexplicitly bridges cognitive reasoning and E2E planning through metric-guided\nalignment. First, we build a VLA module directly generating semantically\ngrounded driving trajectories. Second, we design an E2E module with a dense\ntrajectory vocabulary that ensures physical feasibility. Third, and most\ncritically, we introduce a metric-guided trajectory scorer that guides and\naligns the outputs of the VLA and E2E modules, thereby integrating their\ncomplementary strengths. The experiment on the ICCV 2025 Autonomous Grand\nChallenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.\n","authors":["Yu Gao","Anqing Jiang","Yiru Wang","Wang Jijun","Hao Jiang","Zhigang Sun","Heng Yuwen","Wang Shuo","Hao Zhao","Sun Hao"],"pdf_url":"https://arxiv.org/pdf/2510.17148v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13444v2","updated":"2025-10-30T01:42:07Z","published":"2025-05-19T17:59:27Z","title":"ChartMuseum: Testing Visual Reasoning Capabilities of Large\n  Vision-Language Models","summary":"  Chart understanding presents a unique challenge for large vision-language\nmodels (LVLMs), as it requires the integration of sophisticated textual and\nvisual reasoning capabilities. However, current LVLMs exhibit a notable\nimbalance between these skills, falling short on visual reasoning that is\ndifficult to perform in text. We conduct a case study using a synthetic dataset\nsolvable only through visual reasoning and show that model performance degrades\nsignificantly with increasing visual complexity, while human performance\nremains robust. We then introduce ChartMuseum, a new Chart Question Answering\n(QA) benchmark containing 1,162 expert-annotated questions spanning multiple\nreasoning types, curated from real-world charts across 184 sources,\nspecifically built to evaluate complex visual and textual reasoning. Unlike\nprior chart understanding benchmarks -- where frontier models perform similarly\nand near saturation -- our benchmark exposes a substantial gap between model\nand human performance, while effectively differentiating model capabilities:\nalthough humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro\nattains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct\nachieves only 38.5%. Moreover, on questions requiring primarily visual\nreasoning, all models experience a 35%-55% performance drop from\ntext-reasoning-heavy question performance. Lastly, our qualitative error\nanalysis reveals specific categories of visual reasoning that are challenging\nfor current LVLMs.\n","authors":["Liyan Tang","Grace Kim","Xinyu Zhao","Thom Lake","Wenxuan Ding","Fangcong Yin","Prasann Singhal","Manya Wadhwa","Zeyu Leo Liu","Zayne Sprague","Ramya Namuduri","Bodun Hu","Juan Diego Rodriguez","Puyuan Peng","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2505.13444v2.pdf","comment":"NeurIPS 2025 Datasets & Benchmarks"},{"id":"http://arxiv.org/abs/2510.25077v2","updated":"2025-10-30T01:37:51Z","published":"2025-10-29T01:24:49Z","title":"Neighborhood Feature Pooling for Remote Sensing Image Classification","summary":"  In this work, we propose neighborhood feature pooling (NFP) as a novel\ntexture feature extraction method for remote sensing image classification. The\nNFP layer captures relationships between neighboring inputs and efficiently\naggregates local similarities across feature dimensions. Implemented using\nconvolutional layers, NFP can be seamlessly integrated into any network.\nResults comparing the baseline models and the NFP method indicate that NFP\nconsistently improves performance across diverse datasets and architectures\nwhile maintaining minimal parameter overhead.\n","authors":["Fahimeh Orvati Nia","Amirmohammad Mohammadi","Salim Al Kharsa","Pragati Naikare","Zigfried Hampel-Arias","Joshua Peeples"],"pdf_url":"https://arxiv.org/pdf/2510.25077v2.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.02781v2","updated":"2025-10-30T01:23:43Z","published":"2025-10-03T07:27:55Z","title":"GCVAMD: A Modified CausalVAE Model for Causal Age-related Macular\n  Degeneration Risk Factor Detection and Prediction","summary":"  Age Related Macular Degeneration(AMD) has been one of the most leading causes\nof permanent vision impairment in ophthalmology. Though treatments, such as\nanti VEGF drugs or photodynamic therapies, were developed to slow down the\ndegenerative process of AMD, there is still no specific cure to reverse vision\nloss caused by AMD. Thus, for AMD, detecting existence of risk factors of AMD\nor AMD itself within the patient retina in early stages is a crucial task to\nreduce the possibility of vision impairment. Apart from traditional approaches,\ndeep learning based methods, especially attention mechanism based CNNs and\nGradCAM based XAI analysis on OCT scans, exhibited successful performance in\ndistinguishing AMD retina from normal retinas, making it possible to use AI\ndriven models to aid medical diagnosis and analysis by ophthalmologists\nregarding AMD. However, though having significant success, previous works\nmostly focused on prediction performance itself, not pathologies or underlying\ncausal mechanisms of AMD, which can prohibit intervention analysis on specific\nfactors or even lead to less reliable decisions. Thus, this paper introduces a\nnovel causal AMD analysis model: GCVAMD, which incorporates a modified\nCausalVAE approach that can extract latent causal factors from only raw OCT\nimages. By considering causality in AMD detection, GCVAMD enables causal\ninference such as treatment simulation or intervention analysis regarding major\nrisk factors: drusen and neovascularization, while returning informative latent\ncausal features that can enhance downstream tasks. Results show that through\nGCVAMD, drusen status and neovascularization status can be identified with AMD\ncausal mechanisms in GCVAMD latent spaces, which can in turn be used for\nvarious tasks from AMD detection(classification) to intervention analysis.\n","authors":["Daeyoung Kim"],"pdf_url":"https://arxiv.org/pdf/2510.02781v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26052v1","updated":"2025-10-30T01:10:25Z","published":"2025-10-30T01:10:25Z","title":"Dynamic VLM-Guided Negative Prompting for Diffusion Models","summary":"  We propose a novel approach for dynamic negative prompting in diffusion\nmodels that leverages Vision-Language Models (VLMs) to adaptively generate\nnegative prompts during the denoising process. Unlike traditional Negative\nPrompting methods that use fixed negative prompts, our method generates\nintermediate image predictions at specific denoising steps and queries a VLM to\nproduce contextually appropriate negative prompts. We evaluate our approach on\nvarious benchmark datasets and demonstrate the trade-offs between negative\nguidance strength and text-image alignment.\n","authors":["Hoyeon Chang","Seungjin Kim","Yoonseok Choi"],"pdf_url":"https://arxiv.org/pdf/2510.26052v1.pdf","comment":"39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: The First Workshop on Generative and Protective AI for\n  Content Creation"},{"id":"http://arxiv.org/abs/2510.26049v1","updated":"2025-10-30T00:53:26Z","published":"2025-10-30T00:53:26Z","title":"FlexICL: A Flexible Visual In-context Learning Framework for Elbow and\n  Wrist Ultrasound Segmentation","summary":"  Elbow and wrist fractures are the most common fractures in pediatric\npopulations. Automatic segmentation of musculoskeletal structures in ultrasound\n(US) can improve diagnostic accuracy and treatment planning. Fractures appear\nas cortical defects but require expert interpretation. Deep learning (DL) can\nprovide real-time feedback and highlight key structures, helping lightly\ntrained users perform exams more confidently. However, pixel-wise expert\nannotations for training remain time-consuming and costly. To address this\nchallenge, we propose FlexICL, a novel and flexible in-context learning (ICL)\nframework for segmenting bony regions in US images. We apply it to an\nintra-video segmentation setting, where experts annotate only a small subset of\nframes, and the model segments unseen frames. We systematically investigate\nvarious image concatenation techniques and training strategies for visual ICL\nand introduce novel concatenation methods that significantly enhance model\nperformance with limited labeled data. By integrating multiple augmentation\nstrategies, FlexICL achieves robust segmentation performance across four wrist\nand elbow US datasets while requiring only 5% of the training images. It\noutperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and\nconventional segmentation models like U-Net and TransUNet by 1-27% Dice\ncoefficient on 1,252 US sweeps. These initial results highlight the potential\nof FlexICL as an efficient and scalable solution for US image segmentation well\nsuited for medical imaging use cases where labeled data is scarce.\n","authors":["Yuyue Zhou","Jessica Knight","Shrimanti Ghosh","Banafshe Felfeliyan","Jacob L. Jaremko","Abhilash R. Hareendranathan"],"pdf_url":"https://arxiv.org/pdf/2510.26049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26038v1","updated":"2025-10-30T00:34:16Z","published":"2025-10-30T00:34:16Z","title":"Do Students Debias Like Teachers? On the Distillability of Bias\n  Mitigation Methods","summary":"  Knowledge distillation (KD) is an effective method for model compression and\ntransferring knowledge between models. However, its effect on model's\nrobustness against spurious correlations that degrade performance on\nout-of-distribution data remains underexplored. This study investigates the\neffect of knowledge distillation on the transferability of ``debiasing''\ncapabilities from teacher models to student models on natural language\ninference (NLI) and image classification tasks. Through extensive experiments,\nwe illustrate several key findings: (i) overall the debiasing capability of a\nmodel is undermined post-KD; (ii) training a debiased model does not benefit\nfrom injecting teacher knowledge; (iii) although the overall robustness of a\nmodel may remain stable post-distillation, significant variations can occur\nacross different types of biases; and (iv) we pin-point the internal attention\npattern and circuit that causes the distinct behavior post-KD. Given the above\nfindings, we propose three effective solutions to improve the distillability of\ndebiasing methods: developing high quality data for augmentation, implementing\niterative knowledge distillation, and initializing student models with weights\nobtained from teacher models. To the best of our knowledge, this is the first\nstudy on the effect of KD on debiasing and its interenal mechanism at scale.\nOur findings provide understandings on how KD works and how to design better\ndebiasing methods.\n","authors":["Jiali Cheng","Chirag Agarwal","Hadi Amiri"],"pdf_url":"https://arxiv.org/pdf/2510.26038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08325v4","updated":"2025-10-30T00:33:00Z","published":"2025-01-14T18:57:21Z","title":"GameFactory: Creating New Games with Generative Interactive Videos","summary":"  Generative videos have the potential to revolutionize game development by\nautonomously creating new content. In this paper, we present GameFactory, a\nframework for action-controlled scene-generalizable game video generation. We\nfirst address the fundamental challenge of action controllability by\nintroducing GF-Minecraft, an action-annotated game video dataset without human\nbias, and developing an action control module that enables precise control over\nboth keyboard and mouse inputs. We further extend to support autoregressive\ngeneration for unlimited-length interactive videos. More importantly,\nGameFactory tackles the critical challenge of scene-generalizable action\ncontrol, which most existing methods fail to address. To enable the creation of\nentirely new and diverse games beyond fixed styles and scenes, we leverage the\nopen-domain generative priors from pre-trained video diffusion models. To\nbridge the domain gap between open-domain priors and small-scale game datasets,\nwe propose a multi-phase training strategy with a domain adapter that decouples\ngame style learning from action control. This decoupling ensures that action\ncontrol learning is no longer bound to specific game styles, thereby achieving\nscene-generalizable action control. Experimental results demonstrate that\nGameFactory effectively generates open-domain action-controllable game videos,\nrepresenting a significant step forward in AI-driven game generation.\n","authors":["Jiwen Yu","Yiran Qin","Xintao Wang","Pengfei Wan","Di Zhang","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08325v4.pdf","comment":"ICCV 2025 Highlight, Project Page:\n  https://yujiwen.github.io/gamefactory"},{"id":"http://arxiv.org/abs/2510.23968v2","updated":"2025-10-30T00:14:35Z","published":"2025-10-28T00:48:00Z","title":"Reasoning Visual Language Model for Chest X-Ray Analysis","summary":"  Vision-language models (VLMs) have shown strong promise for medical image\nanalysis, but most remain opaque, offering predictions without the transparent,\nstepwise reasoning clinicians rely on. We present a framework that brings\nchain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by\nreasoning-first training paradigms, our approach is designed to learn how\nexperts reason, not just what they conclude, by aligning intermediate steps\nwith observable image evidence and radiology workflow. Beyond accuracy, the\nexplicit reasoning traces support clinical auditability: they reveal why a\nconclusion was reached, which alternatives were considered, and where\nuncertainty remains, enabling quality assurance, error analysis, and safer\nhuman-AI collaboration.\n  Our model couples high-fidelity visual encoding with a two-stage training\nrecipe: a reasoning-style supervised fine-tuning (SFT) followed by\nreinforcement learning (RL) that uses verifiable rewards over a list of X-ray\nabnormalities. The model outputs reasoning that mirrors radiologists systematic\nthought process, uncertainty, and differential diagnosis. In\nout-of-distribution evaluation, the approach achieves competitive multi-label\nclassification while improving interpretability. In a reader study with expert\nradiologists, full reasoning traces increased confidence, supported error\nauditing, and reduced time to finalize reports. We release code and the model\nNV-Reason-CXR-3B to support community progress toward trustworthy, explainable\nAI in chest radiography and other medical imaging tasks where reasoning quality\nis as critical as prediction quality.\n","authors":["Andriy Myronenko","Dong Yang","Baris Turkbey","Mariam Aboian","Sena Azamat","Esra Akcicek","Hongxu Yin","Pavlo Molchanov","Marc Edgar","Yufan He","Pengfei Guo","Yucheng Tang","Daguang Xu"],"pdf_url":"https://arxiv.org/pdf/2510.23968v2.pdf","comment":"NV-Reason-CXR-3B"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2510.26750v1","updated":"2025-10-30T17:43:33Z","published":"2025-10-30T17:43:33Z","title":"ProfOlaf: Semi-Automated Tool for Systematic Literature Reviews","summary":"  Systematic reviews and mapping studies are critical for synthesizing\nresearch, identifying gaps, and guiding future work, but they are often\nlabor-intensive and time-consuming. Existing tools provide partial support for\nspecific steps, leaving much of the process manual and error-prone. We present\nProfOlaf, a semi-automated tool designed to streamline systematic reviews while\nmaintaining methodological rigor. ProfOlaf supports iterative snowballing for\narticle collection with human-in-the-loop filtering and uses large language\nmodels to assist in analyzing articles, extracting key topics, and answering\nqueries about the content of papers. By combining automation with guided manual\neffort, ProfOlaf enhances the efficiency, quality, and reproducibility of\nsystematic reviews across research fields. A video describing and demonstrating\nProfOlaf is available at: https://youtu.be/4noUXfcmxsE\n","authors":["Martim Afonso","Nuno Saavedra","Bruno LourenÃ§o","Alexandra Mendes","JoÃ£o Ferreira"],"pdf_url":"https://arxiv.org/pdf/2510.26750v1.pdf","comment":"4 pages, 1 Figure, 2 tables"},{"id":"http://arxiv.org/abs/2503.09205v3","updated":"2025-10-30T17:37:55Z","published":"2025-03-12T09:48:38Z","title":"Quality Over Quantity? LLM-Based Curation for a Data-Efficient\n  Audio-Video Foundation Model","summary":"  Integrating audio and visual data for training multimodal foundational models\nremains a challenge. The Audio-Video Vector Alignment (AVVA) framework\naddresses this by considering AV scene alignment beyond mere temporal\nsynchronization, and leveraging Large Language Models (LLMs) for data curation.\nAVVA implements a scoring mechanism for selecting aligned training data\nsegments. It integrates Whisper, a speech-based foundation model, for audio and\nDINOv2 for video analysis in a dual-encoder structure with contrastive learning\non AV pairs. Evaluations on AudioCaps, VALOR, and VGGSound demonstrate the\neffectiveness of the proposed model architecture and data curation approach.\nAVVA achieves a significant improvement in top-k accuracies for video-to-audio\nretrieval on all datasets compared to DenseAV, while using only 192 hrs of\ncurated training data. Furthermore, an ablation study indicates that the data\ncuration process effectively trades data quality for data quantity, yielding\nincreases in top-k retrieval accuracies on AudioCaps, VALOR, and VGGSound,\ncompared to training on the full spectrum of uncurated data.\n","authors":["Ali Vosoughi","Dimitra Emmanouilidou","Hannes Gamper"],"pdf_url":"https://arxiv.org/pdf/2503.09205v3.pdf","comment":"5 pages, 5 figures, 2 tables. Accepted at EUSIPCO 2025"},{"id":"http://arxiv.org/abs/2508.15840v3","updated":"2025-10-30T16:25:05Z","published":"2025-08-19T17:34:25Z","title":"Unveiling Unicode's Unseen Underpinnings in Undermining Authorship\n  Attribution","summary":"  When using a public communication channel -- whether formal or informal, such\nas commenting or posting on social media -- end users have no expectation of\nprivacy: they compose a message and broadcast it for the world to see. Even if\nan end user takes utmost precautions to anonymize their online presence --\nusing an alias or pseudonym; masking their IP address; spoofing their\ngeolocation; concealing their operating system and user agent; deploying\nencryption; registering with a disposable phone number or email; disabling\nnon-essential settings; revoking permissions; and blocking cookies and\nfingerprinting -- one obvious element still lingers: the message itself.\nAssuming they avoid lapses in judgment or accidental self-exposure, there\nshould be little evidence to validate their actual identity, right? Wrong. The\ncontent of their message -- necessarily open for public consumption -- exposes\nan attack vector: stylometric analysis, or author profiling. In this paper, we\ndissect the technique of stylometry, discuss an antithetical counter-strategy\nin adversarial stylometry, and devise enhancements through Unicode\nsteganography.\n","authors":["Robert Dilworth"],"pdf_url":"https://arxiv.org/pdf/2508.15840v3.pdf","comment":"33 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.14409v2","updated":"2025-10-30T15:05:42Z","published":"2025-02-20T09:57:42Z","title":"Unstructured Evidence Attribution for Long Context Query Focused\n  Summarization","summary":"  Large language models (LLMs) are capable of generating coherent summaries\nfrom very long contexts given a user query, and extracting and citing evidence\nspans helps improve the trustworthiness of these summaries. Whereas previous\nwork has focused on evidence citation with fixed levels of granularity (e.g.\nsentence, paragraph, document, etc.), we propose to extract unstructured (i.e.,\nspans of any length) evidence in order to acquire more relevant and consistent\nevidence than in the fixed granularity case. We show how existing systems\nstruggle to copy and properly cite unstructured evidence, which also tends to\nbe \"lost-in-the-middle\". To help models perform this task, we create the\nSummaries with Unstructured Evidence Text dataset (SUnsET), a synthetic dataset\ngenerated using a novel pipeline, which can be used as training supervision for\nunstructured evidence summarization. We demonstrate across 5 LLMs and 4\ndatasets spanning human written, synthetic, single, and multi-document settings\nthat LLMs adapted with SUnsET generate more relevant and factually consistent\nevidence with their summaries, extract evidence from more diverse locations in\ntheir context, and can generate more relevant and consistent summaries than\nbaselines with no fine-tuning and fixed granularity evidence. We release SUnsET\nand our generation code to the public.\n","authors":["Dustin Wright","Zain Muhammad Mujahid","Lu Wang","Isabelle Augenstein","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2502.14409v2.pdf","comment":"EMNLP 2025 Main; 29 pages; 24 figures; 8 tables"},{"id":"http://arxiv.org/abs/2510.26569v1","updated":"2025-10-30T14:59:37Z","published":"2025-10-30T14:59:37Z","title":"AdSum: Two-stream Audio-visual Summarization for Automated Video\n  Advertisement Clipping","summary":"  Advertisers commonly need multiple versions of the same advertisement (ad) at\nvarying durations for a single campaign. The traditional approach involves\nmanually selecting and re-editing shots from longer video ads to create shorter\nversions, which is labor-intensive and time-consuming. In this paper, we\nintroduce a framework for automated video ad clipping using video summarization\ntechniques. We are the first to frame video clipping as a shot selection\nproblem, tailored specifically for advertising. Unlike existing general video\nsummarization methods that primarily focus on visual content, our approach\nemphasizes the critical role of audio in advertising. To achieve this, we\ndevelop a two-stream audio-visual fusion model that predicts the importance of\nvideo frames, where importance is defined as the likelihood of a frame being\nselected in the firm-produced short ad. To address the lack of ad-specific\ndatasets, we present AdSum204, a novel dataset comprising 102 pairs of\n30-second and 15-second ads from real advertising campaigns. Extensive\nexperiments demonstrate that our model outperforms state-of-the-art methods\nacross various metrics, including Average Precision, Area Under Curve,\nSpearman, and Kendall.\n","authors":["Wen Xie","Yanjun Zhu","Gijs Overgoor","Yakov Bart","Agata Lapedriza Garcia","Sarah Ostadabbas"],"pdf_url":"https://arxiv.org/pdf/2510.26569v1.pdf","comment":"Accepted at 32nd International Conference on MultiMedia Modeling"},{"id":"http://arxiv.org/abs/2510.04226v4","updated":"2025-10-30T14:52:48Z","published":"2025-10-05T14:29:15Z","title":"Epistemic Diversity and Knowledge Collapse in Large Language Models","summary":"  Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation\n","authors":["Dustin Wright","Sarah Masud","Jared Moore","Srishti Yadav","Maria Antoniak","Chan Young Park","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2510.04226v4.pdf","comment":"16 pages; 8 figures, 4 tables; v2 changelog: Fixed the modeling for\n  table 3, random effect is the model version; v3 changelog: Fixed minor\n  formatting issues in tables 2 and 3; v4 changelog: Fixed some typos and model\n  description"},{"id":"http://arxiv.org/abs/2510.26546v1","updated":"2025-10-30T14:37:15Z","published":"2025-10-30T14:37:15Z","title":"WeaveRec: An LLM-Based Cross-Domain Sequential Recommendation Framework\n  with Model Merging","summary":"  Cross-Domain Sequential Recommendation (CDSR) seeks to improve user\npreference modeling by transferring knowledge from multiple domains. Despite\nthe progress made in CDSR, most existing methods rely on overlapping users or\nitems to establish cross-domain correlations-a requirement that rarely holds in\nreal-world settings. The advent of large language models (LLM) and\nmodel-merging techniques appears to overcome this limitation by unifying\nmulti-domain data without explicit overlaps. Yet, our empirical study shows\nthat naively training an LLM on combined domains-or simply merging several\ndomain-specific LLMs-often degrades performance relative to a model trained\nsolely on the target domain. To address these challenges, we first\nexperimentally investigate the cause of suboptimal performance in LLM-based\ncross-domain recommendation and model merging. Building on these insights, we\nintroduce WeaveRec, which cross-trains multiple LoRA modules with source and\ntarget domain data in a weaving fashion, and fuses them via model merging.\nWeaveRec can be extended to multi-source domain scenarios and notably does not\nintroduce additional inference-time cost in terms of latency or memory.\nFurthermore, we provide a theoretical guarantee that WeaveRec can reduce the\nupper bound of the expected error in the target domain. Extensive experiments\non single-source, multi-source, and cross-platform cross-domain recommendation\nscenarios validate that WeaveRec effectively mitigates performance degradation\nand consistently outperforms baseline approaches in real-world recommendation\ntasks.\n","authors":["Min Hou","Xin Liu","Le Wu","Chenyi He","Hao Liu","Zhi Li","Xin Li","Si Wei"],"pdf_url":"https://arxiv.org/pdf/2510.26546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26512v1","updated":"2025-10-30T14:05:55Z","published":"2025-10-30T14:05:55Z","title":"Inside CORE-KG: Evaluating Structured Prompting and Coreference\n  Resolution for Knowledge Graphs","summary":"  Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer critical insights but are often unstructured,\nlexically dense, and filled with ambiguous or shifting references, which pose\nsignificant challenges for automated knowledge graph (KG) construction. While\nrecent LLM-based approaches improve over static templates, they still generate\nnoisy, fragmented graphs with duplicate nodes due to the absence of guided\nextraction and coreference resolution. The recently proposed CORE-KG framework\naddresses these limitations by integrating a type-aware coreference module and\ndomain-guided structured prompts, significantly reducing node duplication and\nlegal noise. In this work, we present a systematic ablation study of CORE-KG to\nquantify the individual contributions of its two key components. Our results\nshow that removing coreference resolution results in a 28.32% increase in node\nduplication and a 4.32% increase in noisy nodes, while removing structured\nprompts leads to a 4.34% increase in node duplication and a 73.33% increase in\nnoisy nodes. These findings offer empirical insights for designing robust\nLLM-based pipelines for extracting structured representations from complex\nlegal texts.\n","authors":["Dipak Meher","Carlotta Domeniconi"],"pdf_url":"https://arxiv.org/pdf/2510.26512v1.pdf","comment":"ICDM 2025 Workshop"},{"id":"http://arxiv.org/abs/2502.08271v2","updated":"2025-10-30T13:49:48Z","published":"2025-02-12T10:24:22Z","title":"RecCocktail: A Generalizable and Efficient Framework for LLM-Based\n  Recommendation","summary":"  Large Language Models (LLMs) have achieved remarkable success in recent\nyears, owing to their impressive generalization capabilities and rich world\nknowledge. To capitalize on the potential of using LLMs as recommender systems,\nmainstream approaches typically focus on two paradigms. The first paradigm\ndesigns multi-domain or multi-task instruction data for generalizable\nrecommendation, so as to align LLMs with general recommendation areas and deal\nwith cold-start recommendation. The second paradigm focuses on enhancing\ndomain-specific recommendation tasks, improving performance in warm\nrecommendation scenarios. While most previous works treat these two paradigms\nseparately, we argue that they have complementary advantages, and combining\nthem can yield better results. In this paper, we propose a generalizable and\nefficient LLM-based recommendation framework RecCocktail. Our approach begins\nwith fine-tuning a \"base spirit\" LoRA module using domain-general\nrecommendation instruction data to align LLM with recommendation knowledge.\nNext, given users' behavior of a specific domain, we construct a\ndomain-specific \"ingredient\" LoRA module. We then provide an entropy-guided\nadaptive merging method to mix the \"base spirit\" and the \"ingredient\" in the\nweight space. Please note that, RecCocktail combines the advantages of the\nexisting two paradigms without introducing additional time or space overhead\nduring the inference phase. Moreover, RecCocktail is efficient with plug and\nplay, as the \"base spirit\" LoRA is trained only once, and any domain-specific\n\"ingredient\" can be efficiently mixed with only domain-specific fine-tuning.\nExtensive experiments on multiple datasets under both warm and cold-start\nrecommendation scenarios validate the effectiveness and generality of the\nproposed RecCocktail.\n","authors":["Min Hou","Chenxi Bai","Le Wu","Hao Liu","Kai Zhang","Weiwen Liu","Richang Hong","Ruiming Tang","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26486v1","updated":"2025-10-30T13:39:08Z","published":"2025-10-30T13:39:08Z","title":"LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks","summary":"  Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks.\n","authors":["Dipak Meher","Carlotta Domeniconi","Guadalupe Correa-Cabrera"],"pdf_url":"https://arxiv.org/pdf/2510.26486v1.pdf","comment":"Accepted in ICKG 2025 Conference, 8 Pages, 2 Figures"},{"id":"http://arxiv.org/abs/2510.26461v1","updated":"2025-10-30T13:07:39Z","published":"2025-10-30T13:07:39Z","title":"Vectorized Context-Aware Embeddings for GAT-Based Collaborative\n  Filtering","summary":"  Recommender systems often struggle with data sparsity and cold-start\nscenarios, limiting their ability to provide accurate suggestions for new or\ninfrequent users. This paper presents a Graph Attention Network (GAT) based\nCollaborative Filtering (CF) framework enhanced with Large Language Model (LLM)\ndriven context aware embeddings. Specifically, we generate concise textual user\nprofiles and unify item metadata (titles, genres, overviews) into rich textual\nembeddings, injecting these as initial node features in a bipartite user item\ngraph. To further optimize ranking performance, we introduce a hybrid loss\nfunction that combines Bayesian Personalized Ranking (BPR) with a cosine\nsimilarity term and robust negative sampling, ensuring explicit negative\nfeedback is distinguished from unobserved data. Experiments on the MovieLens\n100k and 1M datasets show consistent improvements over state-of-the-art\nbaselines in Precision, NDCG, and MAP while demonstrating robustness for users\nwith limited interaction history. Ablation studies confirm the critical role of\nLLM-augmented embeddings and the cosine similarity term in capturing nuanced\nsemantic relationships. Our approach effectively mitigates sparsity and\ncold-start limitations by integrating LLM-derived contextual understanding into\ngraph-based architectures. Future directions include balancing recommendation\naccuracy with coverage and diversity, and introducing fairness-aware\nconstraints and interpretability features to enhance system performance\nfurther.\n","authors":["Danial Ebrat","Sepideh Ahmadian","Luis Rueda"],"pdf_url":"https://arxiv.org/pdf/2510.26461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.17670v2","updated":"2025-10-30T12:05:58Z","published":"2025-10-20T15:41:55Z","title":"On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active\n  Marginal-Samples Exploration","summary":"  Open-vocabulary object detection (OVD) models offer remarkable flexibility by\ndetecting objects from arbitrary text queries. However, their zero-shot\nperformance in specialized domains like Remote Sensing (RS) is often\ncompromised by the inherent ambiguity of natural language, limiting critical\ndownstream applications. For instance, an OVD model may struggle to distinguish\nbetween fine-grained classes such as \"fishing boat\" and \"yacht\" since their\nembeddings are similar and often inseparable. This can hamper specific user\ngoals, such as monitoring illegal fishing, by producing irrelevant detections.\nTo address this, we propose a cascaded approach that couples the broad\ngeneralization of a large pre-trained OVD model with a lightweight few-shot\nclassifier. Our method first employs the zero-shot model to generate\nhigh-recall object proposals. These proposals are then refined for high\nprecision by a compact classifier trained in real-time on only a handful of\nuser-annotated examples - drastically reducing the high costs of RS imagery\nannotation.The core of our framework is FLAME, a one-step active learning\nstrategy that selects the most informative samples for training. FLAME\nidentifies, on the fly, uncertain marginal candidates near the decision\nboundary using density estimation, followed by clustering to ensure sample\ndiversity. This efficient sampling technique achieves high accuracy without\ncostly full-model fine-tuning and enables instant adaptation, within less then\na minute, which is significantly faster than state-of-the-art alternatives.Our\nmethod consistently surpasses state-of-the-art performance on RS benchmarks,\nestablishing a practical and resource-efficient framework for adapting\nfoundation models to specific user needs.\n","authors":["Yehonathan Refael","Amit Aides","Aviad Barzilai","George Leifman","Genady Beryozkin","Vered Silverman","Bolous Jaber","Tomer Shekel"],"pdf_url":"https://arxiv.org/pdf/2510.17670v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26407v1","updated":"2025-10-30T11:56:02Z","published":"2025-10-30T11:56:02Z","title":"Barlow Twins for Sequential Recommendation","summary":"  Sequential recommendation models must navigate sparse interaction data\npopularity bias and conflicting objectives like accuracy versus diversity While\nrecent contrastive selfsupervised learning SSL methods offer improved accuracy\nthey come with tradeoffs large batch requirements reliance on handcrafted\naugmentations and negative sampling that can reinforce popularity bias In this\npaper we introduce BT-SR a novel noncontrastive SSL framework that integrates\nthe Barlow Twins redundancyreduction principle into a Transformerbased nextitem\nrecommender BTSR learns embeddings that align users with similar shortterm\nbehaviors while preserving longterm distinctionswithout requiring negative\nsampling or artificial perturbations This structuresensitive alignment allows\nBT-SR to more effectively recognize emerging user intent and mitigate the\ninfluence of noisy historical context Our experiments on five public benchmarks\ndemonstrate that BTSR consistently improves nextitem prediction accuracy and\nsignificantly enhances longtail item coverage and recommendation calibration\nCrucially we show that a single hyperparameter can control the\naccuracydiversity tradeoff enabling practitioners to adapt recommendations to\nspecific application needs\n","authors":["Ivan Razvorotnev","Marina Munkhoeva","Evgeny Frolov"],"pdf_url":"https://arxiv.org/pdf/2510.26407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26309v1","updated":"2025-10-30T09:53:16Z","published":"2025-10-30T09:53:16Z","title":"GraphCompliance: Aligning Policy and Context Graphs for LLM-Based\n  Regulatory Compliance","summary":"  Compliance at web scale poses practical challenges: each request may require\na regulatory assessment. Regulatory texts (e.g., the General Data Protection\nRegulation, GDPR) are cross-referential and normative, while runtime contexts\nare expressed in unstructured natural language. This setting motivates us to\nalign semantic information in unstructured text with the structured, normative\nelements of regulations. To this end, we introduce GraphCompliance, a framework\nthat represents regulatory texts as a Policy Graph and runtime contexts as a\nContext Graph, and aligns them. In this formulation, the policy graph encodes\nnormative structure and cross-references, whereas the context graph formalizes\nevents as subject-action-object (SAO) and entity-relation triples. This\nalignment anchors the reasoning of a judge large language model (LLM) in\nstructured information and helps reduce the burden of regulatory interpretation\nand event parsing, enabling a focus on the core reasoning step. In experiments\non 300 GDPR-derived real-world scenarios spanning five evaluation tasks,\nGraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than\nLLM-only and RAG baselines, with fewer under- and over-predictions, resulting\nin higher recall and lower false positive rates. Ablation studies indicate\ncontributions from each graph component, suggesting that structured\nrepresentations and a judge LLM are complementary for normative reasoning.\n","authors":["Jiseong Chung","Ronny Ko","Wonchul Yoo","Makoto Onizuka","Sungmok Kim","Tae-Wan Kim","Won-Yong Shin"],"pdf_url":"https://arxiv.org/pdf/2510.26309v1.pdf","comment":"Under review at The Web Conference 2026 (Semantics & Knowledge\n  track). Code will be released upon acceptance. This arXiv v1 contains no\n  repository links to preserve double-blind review"},{"id":"http://arxiv.org/abs/2510.25160v2","updated":"2025-10-30T08:52:17Z","published":"2025-10-29T04:29:17Z","title":"Model-Document Protocol for AI Search","summary":"  AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation.\n","authors":["Hongjin Qian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25160v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2510.26231v1","updated":"2025-10-30T08:10:03Z","published":"2025-10-30T08:10:03Z","title":"DiSE: A diffusion probabilistic model for automatic structure\n  elucidation of organic compounds","summary":"  Automatic structure elucidation is essential for self-driving laboratories as\nit enables the system to achieve truly autonomous. This capability closes the\nexperimental feedback loop, ensuring that machine learning models receive\nreliable structure information for real-time decision-making and optimization.\nHerein, we present DiSE, an end-to-end diffusion-based generative model that\nintegrates multiple spectroscopic modalities, including MS, 13C and 1H chemical\nshifts, HSQC, and COSY, to achieve automated yet accurate structure elucidation\nof organic compounds. By learning inherent correlations among spectra through\ndata-driven approaches, DiSE achieves superior accuracy, strong generalization\nacross chemically diverse datasets, and robustness to experimental data despite\nbeing trained on calculated spectra. DiSE thus represents a significant advance\ntoward fully automated structure elucidation, with broad potential in natural\nproduct research, drug discovery, and self-driving laboratories.\n","authors":["Haochen Chen","Qi Huang","Anan Wu","Wenhao Zhang","Jianliang Ye","Jianming Wu","Kai Tan","Xin Lu","Xin Xu"],"pdf_url":"https://arxiv.org/pdf/2510.26231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20995v3","updated":"2025-10-30T06:37:00Z","published":"2025-02-28T12:32:53Z","title":"The RAG Paradox: A Black-Box Attack Exploiting Unintentional\n  Vulnerabilities in Retrieval-Augmented Generation Systems","summary":"  With the growing adoption of retrieval-augmented generation (RAG) systems,\nvarious attack methods have been proposed to degrade their performance.\nHowever, most existing approaches rely on unrealistic assumptions in which\nexternal attackers have access to internal components such as the retriever. To\naddress this issue, we introduce a realistic black-box attack based on the RAG\nparadox, a structural vulnerability arising from the system's effort to enhance\ntrust by revealing both the retrieved documents and their sources to users.\nThis transparency enables attackers to observe which sources are used and how\ninformation is phrased, allowing them to craft poisoned documents that are more\nlikely to be retrieved and upload them to the identified sources. Moreover, as\nRAG systems directly provide retrieved content to users, these documents must\nnot only be retrievable but also appear natural and credible to maintain user\nconfidence in the search results. Unlike prior work that focuses solely on\nimproving document retrievability, our attack method explicitly considers both\nretrievability and user trust in the retrieved content. Both offline and online\nexperiments demonstrate that our method significantly degrades system\nperformance without internal access, while generating natural-looking poisoned\ndocuments.\n","authors":["Chanwoo Choi","Jinsoo Kim","Sukmin Cho","Soyeong Jeong","Buru Chang"],"pdf_url":"https://arxiv.org/pdf/2502.20995v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26178v1","updated":"2025-10-30T06:35:36Z","published":"2025-10-30T06:35:36Z","title":"ReaKase-8B: Legal Case Retrieval via Knowledge and Reasoning\n  Representations with LLMs","summary":"  Legal case retrieval (LCR) is a cornerstone of real-world legal decision\nmaking, as it enables practitioners to identify precedents for a given query\ncase. Existing approaches mainly rely on traditional lexical models and\npretrained language models to encode the texts of legal cases. Yet there are\nrich information in the relations among different legal entities as well as the\ncrucial reasoning process that uncovers how legal facts and legal issues can\nlead to judicial decisions. Such relational reasoning process reflects the\ndistinctive characteristics of each case that can distinguish one from another,\nmirroring the real-world judicial process. Naturally, incorporating such\ninformation into the precise case embedding could further enhance the accuracy\nof case retrieval. In this paper, a novel ReaKase-8B framework is proposed to\nleverage extracted legal facts, legal issues, legal relation triplets and legal\nreasoning for effective legal case retrieval. ReaKase-8B designs an in-context\nlegal case representation learning paradigm with a fine-tuned large language\nmodel. Extensive experiments on two benchmark datasets from COLIEE 2022 and\nCOLIEE 2023 demonstrate that our knowledge and reasoning augmented embeddings\nsubstantially improve retrieval performance over baseline models, highlighting\nthe potential of integrating legal reasoning into legal case retrieval systems.\nThe code has been released on https://github.com/yanran-tang/ReaKase-8B.\n","authors":["Yanran Tang","Ruihong Qiu","Xue Li","Zi Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.17918v5","updated":"2025-10-30T06:29:37Z","published":"2025-09-22T15:43:11Z","title":"Shilling Recommender Systems by Generating Side-feature-aware Fake User\n  Profiles","summary":"  Recommender systems (RS) greatly influence users' consumption decisions,\nmaking them attractive targets for malicious shilling attacks that inject fake\nuser profiles to manipulate recommendations. Existing shilling methods can\ngenerate effective and stealthy fake profiles when training data only contain\nrating matrix, but they lack comprehensive solutions for scenarios where side\nfeatures are present and utilized by the recommender. To address this gap, we\nextend the Leg-UP framework by enhancing the generator architecture to\nincorporate side features, enabling the generation of side-feature-aware fake\nuser profiles. Experiments on benchmarks show that our method achieves strong\nattack performance while maintaining stealthiness.\n","authors":["Yuanrong Wang","Yingpeng Du"],"pdf_url":"https://arxiv.org/pdf/2509.17918v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26104v1","updated":"2025-10-30T03:30:12Z","published":"2025-10-30T03:30:12Z","title":"OneTrans: Unified Feature Interaction and Sequence Modeling with One\n  Transformer in Industrial Recommender","summary":"  In recommendation systems, scaling up feature-interaction modules (e.g.,\nWukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has\nachieved notable success. However, these efforts typically proceed on separate\ntracks, which not only hinders bidirectional information exchange but also\nprevents unified optimization and scaling. In this paper, we propose OneTrans,\na unified Transformer backbone that simultaneously performs user-behavior\nsequence modeling and feature interaction. OneTrans employs a unified tokenizer\nto convert both sequential and non-sequential attributes into a single token\nsequence. The stacked OneTrans blocks share parameters across similar\nsequential tokens while assigning token-specific parameters to non-sequential\ntokens. Through causal attention and cross-request KV caching, OneTrans enables\nprecomputation and caching of intermediate representations, significantly\nreducing computational costs during both training and inference. Experimental\nresults on industrial-scale datasets demonstrate that OneTrans scales\nefficiently with increasing parameters, consistently outperforms strong\nbaselines, and yields a 5.68% lift in per-user GMV in online A/B tests.\n","authors":["Zhaoqi Zhang","Haolei Pei","Jun Guo","Tianyu Wang","Yufei Feng","Hui Sun","Shaowei Liu","Aixin Sun"],"pdf_url":"https://arxiv.org/pdf/2510.26104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26095v1","updated":"2025-10-30T03:10:45Z","published":"2025-10-30T03:10:45Z","title":"ORBIT -- Open Recommendation Benchmark for Reproducible Research with\n  Hidden Tests","summary":"  Recommender systems are among the most impactful AI applications, interacting\nwith billions of users every day, guiding them to relevant products, services,\nor information tailored to their preferences. However, the research and\ndevelopment of recommender systems are hindered by existing datasets that fail\nto capture realistic user behaviors and inconsistent evaluation settings that\nlead to ambiguous conclusions. This paper introduces the Open Recommendation\nBenchmark for Reproducible Research with HIdden Tests (ORBIT), a unified\nbenchmark for consistent and realistic evaluation of recommendation models.\nORBIT offers a standardized evaluation framework of public datasets with\nreproducible splits and transparent settings for its public leaderboard.\nAdditionally, ORBIT introduces a new webpage recommendation task, ClueWeb-Reco,\nfeaturing web browsing sequences from 87 million public, high-quality webpages.\nClueWeb-Reco is a synthetic dataset derived from real, user-consented, and\nprivacy-guaranteed browsing data. It aligns with modern recommendation\nscenarios and is reserved as the hidden test part of our leaderboard to\nchallenge recommendation models' generalization ability. ORBIT measures 12\nrepresentative recommendation models on its public benchmark and introduces a\nprompted LLM baseline on the ClueWeb-Reco hidden test. Our benchmark results\nreflect general improvements of recommender systems on the public datasets,\nwith variable individual performances. The results on the hidden test reveal\nthe limitations of existing approaches in large-scale webpage recommendation\nand highlight the potential for improvements with LLM integrations. ORBIT\nbenchmark, leaderboard, and codebase are available at\nhttps://www.open-reco-bench.ai.\n","authors":["Jingyuan He","Jiongnan Liu","Vishan Vishesh Oberoi","Bolin Wu","Mahima Jagadeesh Patel","Kangrui Mao","Chuning Shi","I-Ta Lee","Arnold Overwijk","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2510.26095v1.pdf","comment":"Accepted to NeurIPS 2025 Datasets & Benchmarks track"},{"id":"http://arxiv.org/abs/2510.25622v2","updated":"2025-10-30T02:50:46Z","published":"2025-10-29T15:27:23Z","title":"MMQ-v2: Align, Denoise, and Amplify: Adaptive Behavior Mining for\n  Semantic IDs Learning in Recommendation","summary":"  Industrial recommender systems rely on unique Item Identifiers (ItemIDs).\nHowever, this method struggles with scalability and generalization in large,\ndynamic datasets that have sparse long-tail data. Content-based Semantic IDs\n(SIDs) address this by sharing knowledge through content quantization. However,\nby ignoring dynamic behavioral properties, purely content-based SIDs have\nlimited expressive power. Existing methods attempt to incorporate behavioral\ninformation but overlook a critical distinction: unlike relatively uniform\ncontent features, user-item interactions are highly skewed and diverse,\ncreating a vast information gap in quality and quantity between popular and\nlong-tail items. This oversight leads to two critical limitations: (1) Noise\nCorruption: Indiscriminate behavior-content alignment allows collaborative\nnoise from long-tail items to corrupt their content representations, leading to\nthe loss of critical multimodal information. (2)Signal Obscurity: The\nequal-weighting scheme for SIDs fails to reflect the varying importance of\ndifferent behavioral signals, making it difficult for downstream tasks to\ndistinguish important SIDs from uninformative ones. To tackle these issues, we\npropose a mixture-of-quantization framework, MMQ-v2, to adaptively Align,\nDenoise, and Amplify multimodal information from content and behavior\nmodalities for semantic IDs learning. The semantic IDs generated by this\nframework named ADA-SID. It introduces two innovations: an adaptive\nbehavior-content alignment that is aware of information richness to shield\nrepresentations from noise, and a dynamic behavioral router to amplify critical\nsignals by applying different weights to SIDs. Extensive experiments on public\nand large-scale industrial datasets demonstrate ADA-SID's significant\nsuperiority in both generative and discriminative recommendation tasks.\n","authors":["Yi Xu","Moyu Zhang","Chaofan Fan","Jinxin Hu","Xiaochen Li","Yu Zhang","Xiaoyi Zeng","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.25622v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25402v2","updated":"2025-10-30T02:45:14Z","published":"2025-10-29T11:20:18Z","title":"Towards Automated Quality Assurance of Patent Specifications: A\n  Multi-Dimensional LLM Framework","summary":"  Although AI drafting tools have gained prominence in patent writing, the\nsystematic evaluation of AI-generated patent content quality represents a\nsignificant research gap. To address this gap, We propose to evaluate patents\nusing regulatory compliance, technical coherence, and figure-reference\nconsistency detection modules, and then generate improvement suggestions via an\nintegration module. The framework is validated on a comprehensive dataset\ncomprising 80 human-authored and 80 AI-generated patents from two patent\ndrafting tools. Evaluation is performed on 10,841 total sentences, 8,924\nnon-template sentences, and 554 patent figures for the three detection modules\nrespectively, achieving balanced accuracies of 99.74%, 82.12%, and 91.2%\nagainst expert annotations. Additional analysis was conducted to examine defect\ndistributions across patent sections, technical domains, and authoring sources.\nSection-based analysis indicates that figure-text consistency and technical\ndetail precision require particular attention. Mechanical Engineering and\nConstruction show more claim-specification inconsistencies due to complex\ntechnical documentation requirements. AI-generated patents show a significant\ngap compared to human-authored ones. While human-authored patents primarily\ncontain surface-level errors like typos, AI-generated patents exhibit more\nstructural defects in figure-text alignment and cross-references.\n","authors":["Yuqian Chai","Chaochao Wang","Weilei Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.11066v2","updated":"2025-10-30T02:33:58Z","published":"2025-10-13T07:06:26Z","title":"Decoupled Multimodal Fusion for User Interest Modeling in Click-Through\n  Rate Prediction","summary":"  Modern industrial recommendation systems improve recommendation performance\nby integrating multimodal representations from pre-trained models into ID-based\nClick-Through Rate (CTR) prediction frameworks. However, existing approaches\ntypically adopt modality-centric modeling strategies that process ID-based and\nmultimodal embeddings independently, failing to capture fine-grained\ninteractions between content semantics and behavioral signals. In this paper,\nwe propose Decoupled Multimodal Fusion (DMF), which introduces a\nmodality-enriched modeling strategy to enable fine-grained interactions between\nID-based collaborative representations and multimodal representations for user\ninterest modeling. Specifically, we construct target-aware features to bridge\nthe semantic gap across different embedding spaces and leverage them as side\ninformation to enhance the effectiveness of user interest modeling.\nFurthermore, we design an inference-optimized attention mechanism that\ndecouples the computation of target-aware features and ID-based embeddings\nbefore the attention layer, thereby alleviating the computational bottleneck\nintroduced by incorporating target-aware features. To achieve comprehensive\nmultimodal integration, DMF combines user interest representations learned\nunder the modality-centric and modality-enriched modeling strategies. Offline\nexperiments on public and industrial datasets demonstrate the effectiveness of\nDMF. Moreover, DMF has been deployed on the product recommendation system of\nthe international e-commerce platform Lazada, achieving relative improvements\nof 5.30% in CTCVR and 7.43% in GMV with negligible computational overhead.\n","authors":["Alin Fan","Hanqing Li","Sihan Lu","Jingsong Yuan","Jiandong Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.11066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21188v3","updated":"2025-10-30T01:40:08Z","published":"2025-03-27T06:10:22Z","title":"A Task-Centric Perspective on Recommendation Systems","summary":"  Many studies in recommender systems (RecSys) adopt a general problem\ndefinition, i.e., to recommend preferred items to users based on past\ninteractions. Such abstraction often lacks the domain-specific nuances\nnecessary for practical deployment. However, models are frequently evaluated\nusing datasets collected from online recommender platforms, which inherently\nreflect domain or task specificities. In this paper, we analyze RecSys task\nformulations, emphasizing key components such as input-output structures,\ntemporal dynamics, and candidate item selection. All these factors directly\nimpact offline evaluation. We further examine the complexities of user-item\ninteractions, including decision-making costs, multi-step engagements, and\nunobservable interactions, which may influence model design. Additionally, we\nexplore the balance between task specificity and model generalizability,\nhighlighting how well-defined task formulations serve as the foundation for\nrobust evaluation and effective solution development. By clarifying task\ndefinitions and their implications, this work provides a structured perspective\non RecSys research. The goal is to help researchers better navigate the field,\nparticularly in understanding specificities of the RecSys tasks and ensuring\nfair and meaningful evaluations.\n","authors":["Aixin Sun"],"pdf_url":"https://arxiv.org/pdf/2503.21188v3.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2510.26800v1","updated":"2025-10-30T17:59:51Z","published":"2025-10-30T17:59:51Z","title":"OmniX: From Unified Panoramic Generation and Perception to\n  Graphics-Ready 3D Scenes","summary":"  There are two prevalent ways to constructing 3D scenes: procedural generation\nand 2D lifting. Among them, panorama-based 2D lifting has emerged as a\npromising technique, leveraging powerful 2D generative priors to produce\nimmersive, realistic, and diverse 3D environments. In this work, we advance\nthis technique to generate graphics-ready 3D scenes suitable for physically\nbased rendering (PBR), relighting, and simulation. Our key insight is to\nrepurpose 2D generative models for panoramic perception of geometry, textures,\nand PBR materials. Unlike existing 2D lifting approaches that emphasize\nappearance generation and ignore the perception of intrinsic properties, we\npresent OmniX, a versatile and unified framework. Based on a lightweight and\nefficient cross-modal adapter structure, OmniX reuses 2D generative priors for\na broad range of panoramic vision tasks, including panoramic perception,\ngeneration, and completion. Furthermore, we construct a large-scale synthetic\npanorama dataset containing high-quality multimodal panoramas from diverse\nindoor and outdoor scenes. Extensive experiments demonstrate the effectiveness\nof our model in panoramic visual perception and graphics-ready 3D scene\ngeneration, opening new possibilities for immersive and physically realistic\nvirtual world generation.\n","authors":["Yukun Huang","Jiwen Yu","Yanning Zhou","Jianan Wang","Xintao Wang","Pengfei Wan","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26800v1.pdf","comment":"Project page: https://yukun-huang.github.io/OmniX/"},{"id":"http://arxiv.org/abs/2506.03237v2","updated":"2025-10-30T17:59:46Z","published":"2025-06-03T17:49:41Z","title":"UniSite: The First Cross-Structure Dataset and Learning Framework for\n  End-to-End Ligand Binding Site Detection","summary":"  The detection of ligand binding sites for proteins is a fundamental step in\nStructure-Based Drug Design. Despite notable advances in recent years, existing\nmethods, datasets, and evaluation metrics are confronted with several key\nchallenges: (1) current datasets and methods are centered on individual\nprotein-ligand complexes and neglect that diverse binding sites may exist\nacross multiple complexes of the same protein, introducing significant\nstatistical bias; (2) ligand binding site detection is typically modeled as a\ndiscontinuous workflow, employing binary segmentation and subsequent clustering\nalgorithms; (3) traditional evaluation metrics do not adequately reflect the\nactual performance of different binding site prediction methods. To address\nthese issues, we first introduce UniSite-DS, the first UniProt (Unique\nProtein)-centric ligand binding site dataset, which contains 4.81 times more\nmulti-site data and 2.08 times more overall data compared to the previously\nmost widely used datasets. We then propose UniSite, the first end-to-end ligand\nbinding site detection framework supervised by set prediction loss with\nbijective matching. In addition, we introduce Average Precision based on\nIntersection over Union (IoU) as a more accurate evaluation metric for ligand\nbinding site prediction. Extensive experiments on UniSite-DS and several\nrepresentative benchmark datasets demonstrate that IoU-based Average Precision\nprovides a more accurate reflection of prediction quality, and that UniSite\noutperforms current state-of-the-art methods in ligand binding site detection.\nThe dataset and codes will be made publicly available at\nhttps://github.com/quanlin-wu/unisite.\n","authors":["Jigang Fan","Quanlin Wu","Shengjie Luo","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2506.03237v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26795v1","updated":"2025-10-30T17:59:35Z","published":"2025-10-30T17:59:35Z","title":"Scaling Image Geo-Localization to Continent Level","summary":"  Determining the precise geographic location of an image at a global scale\nremains an unsolved challenge. Standard image retrieval techniques are\ninefficient due to the sheer volume of images (>100M) and fail when coverage is\ninsufficient. Scalable solutions, however, involve a trade-off: global\nclassification typically yields coarse results (10+ kilometers), while\ncross-view retrieval between ground and aerial imagery suffers from a domain\ngap and has been primarily studied on smaller regions. This paper introduces a\nhybrid approach that achieves fine-grained geo-localization across a large\ngeographic expanse the size of a continent. We leverage a proxy classification\ntask during training to learn rich feature representations that implicitly\nencode precise location information. We combine these learned prototypes with\nembeddings of aerial imagery to increase robustness to the sparsity of\nground-level data. This enables direct, fine-grained retrieval over areas\nspanning multiple countries. Our extensive evaluation demonstrates that our\napproach can localize within 200m more than 68\\% of queries of a dataset\ncovering a large part of Europe. The code is publicly available at\nhttps://scaling-geoloc.github.io.\n","authors":["Philipp Lindenberger","Paul-Edouard Sarlin","Jan Hosang","Matteo Balice","Marc Pollefeys","Simon Lynen","Eduard Trulls"],"pdf_url":"https://arxiv.org/pdf/2510.26795v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26792v1","updated":"2025-10-30T17:59:09Z","published":"2025-10-30T17:59:09Z","title":"Learning Pseudorandom Numbers with Transformers: Permuted Congruential\n  Generators, Curricula, and Interpretability","summary":"  We study the ability of Transformer models to learn sequences generated by\nPermuted Congruential Generators (PCGs), a widely used family of pseudo-random\nnumber generators (PRNGs). PCGs introduce substantial additional difficulty\nover linear congruential generators (LCGs) by applying a series of bit-wise\nshifts, XORs, rotations and truncations to the hidden state. We show that\nTransformers can nevertheless successfully perform in-context prediction on\nunseen sequences from diverse PCG variants, in tasks that are beyond published\nclassical attacks. In our experiments we scale moduli up to $2^{22}$ using up\nto $50$ million model parameters and datasets with up to $5$ billion tokens.\nSurprisingly, we find even when the output is truncated to a single bit, it can\nbe reliably predicted by the model. When multiple distinct PRNGs are presented\ntogether during training, the model can jointly learn them, identifying\nstructures from different permutations. We demonstrate a scaling law with\nmodulus $m$: the number of in-context sequence elements required for\nnear-perfect prediction grows as $\\sqrt{m}$. For larger moduli, optimization\nenters extended stagnation phases; in our experiments, learning moduli $m \\geq\n2^{20}$ requires incorporating training data from smaller moduli, demonstrating\na critical necessity for curriculum learning. Finally, we analyze embedding\nlayers and uncover a novel clustering phenomenon: the model spontaneously\ngroups the integer inputs into bitwise rotationally-invariant clusters,\nrevealing how representations can transfer from smaller to larger moduli.\n","authors":["Tao Tao","Maissam Barkeshli"],"pdf_url":"https://arxiv.org/pdf/2510.26792v1.pdf","comment":"10+13 pages, 8+19 figures"},{"id":"http://arxiv.org/abs/2510.26788v1","updated":"2025-10-30T17:58:11Z","published":"2025-10-30T17:58:11Z","title":"Defeating the Training-Inference Mismatch via FP16","summary":"  Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to \\textbf{FP16} effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning.\n","authors":["Penghui Qi","Zichen Liu","Xiangxin Zhou","Tianyu Pang","Chao Du","Wee Sun Lee","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2510.26788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26787v1","updated":"2025-10-30T17:58:04Z","published":"2025-10-30T17:58:04Z","title":"Remote Labor Index: Measuring AI Automation of Remote Work","summary":"  AIs have made rapid progress on research-oriented benchmarks of knowledge and\nreasoning, but it remains unclear how these gains translate into economic value\nand automation. To measure this, we introduce the Remote Labor Index (RLI), a\nbroadly multi-sector benchmark comprising real-world, economically valuable\nprojects designed to evaluate end-to-end agent performance in practical\nsettings. AI agents perform near the floor on RLI, with the highest-performing\nagent achieving an automation rate of 2.5%. These results help ground\ndiscussions of AI automation in empirical evidence, setting a common basis for\ntracking AI impacts and enabling stakeholders to proactively navigate AI-driven\nlabor automation.\n","authors":["Mantas Mazeika","Alice Gatti","Cristina Menghini","Udari Madhushani Sehwag","Shivam Singhal","Yury Orlovskiy","Steven Basart","Manasi Sharma","Denis Peskoff","Elaine Lau","Jaehyuk Lim","Lachlan Carroll","Alice Blair","Vinaya Sivakumar","Sumana Basu","Brad Kenstler","Yuntao Ma","Julian Michael","Xiaoke Li","Oliver Ingebretsen","Aditya Mehta","Jean Mottola","John Teichmann","Kevin Yu","Zaina Shaik","Adam Khoja","Richard Ren","Jason Hausenloy","Long Phan","Ye Htet","Ankit Aich","Tahseen Rabbani","Vivswan Shah","Andriy Novykov","Felix Binder","Kirill Chugunov","Luis Ramirez","Matias Geralnik","HernÃ¡n Mesura","Dean Lee","Ed-Yeremai Hernandez Cardona","Annette Diamond","Summer Yue","Alexandr Wang","Bing Liu","Ernesto Hernandez","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2510.26787v1.pdf","comment":"Website: https://www.remotelabor.ai"},{"id":"http://arxiv.org/abs/2510.26786v1","updated":"2025-10-30T17:57:40Z","published":"2025-10-30T17:57:40Z","title":"HEIR: Learning Graph-Based Motion Hierarchies","summary":"  Hierarchical structures of motion exist across research fields, including\ncomputer vision, graphics, and robotics, where complex dynamics typically arise\nfrom coordinated interactions among simpler motion components. Existing methods\nto model such dynamics typically rely on manually-defined or heuristic\nhierarchies with fixed motion primitives, limiting their generalizability\nacross different tasks. In this work, we propose a general hierarchical motion\nmodeling method that learns structured, interpretable motion relationships\ndirectly from data. Our method represents observed motions using graph-based\nhierarchies, explicitly decomposing global absolute motions into\nparent-inherited patterns and local motion residuals. We formulate hierarchy\ninference as a differentiable graph learning problem, where vertices represent\nelemental motions and directed edges capture learned parent-child dependencies\nthrough graph neural networks. We evaluate our hierarchical reconstruction\napproach on three examples: 1D translational motion, 2D rotational motion, and\ndynamic 3D scene deformation via Gaussian splatting. Experimental results show\nthat our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,\nand produces more realistic and interpretable deformations compared to the\nbaseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,\ndata-driven hierarchical modeling paradigm, our method offers a formulation\napplicable to a broad range of motion-centric tasks. Project Page:\nhttps://light.princeton.edu/HEIR/\n","authors":["Cheng Zheng","William Koch","Baiang Li","Felix Heide"],"pdf_url":"https://arxiv.org/pdf/2510.26786v1.pdf","comment":"Code link: https://github.com/princeton-computational-imaging/HEIR"},{"id":"http://arxiv.org/abs/2510.26783v1","updated":"2025-10-30T17:56:47Z","published":"2025-10-30T17:56:47Z","title":"A Unified Theory for Causal Inference: Direct Debiased Machine Learning\n  via Bregman-Riesz Regression","summary":"  This note introduces a unified theory for causal inference that integrates\nRiesz regression, covariate balancing, density-ratio estimation (DRE), targeted\nmaximum likelihood estimation (TMLE), and the matching estimator in average\ntreatment effect (ATE) estimation. In ATE estimation, the balancing weights and\nthe regression functions of the outcome play important roles, where the\nbalancing weights are referred to as the Riesz representer, bias-correction\nterm, and clever covariates, depending on the context. Riesz regression,\ncovariate balancing, DRE, and the matching estimator are methods for estimating\nthe balancing weights, where Riesz regression is essentially equivalent to DRE\nin the ATE context, the matching estimator is a special case of DRE, and DRE is\nin a dual relationship with covariate balancing. TMLE is a method for\nconstructing regression function estimators such that the leading bias term\nbecomes zero. Nearest Neighbor Matching is equivalent to Least Squares Density\nRatio Estimation and Riesz Regression.\n","authors":["Masahiro Kato"],"pdf_url":"https://arxiv.org/pdf/2510.26783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26782v1","updated":"2025-10-30T17:56:43Z","published":"2025-10-30T17:56:43Z","title":"Clone Deterministic 3D Worlds with Geometrically-Regularized World\n  Models","summary":"  A world model is an internal model that simulates how the world evolves.\nGiven past observations and actions, it predicts the future of both the\nembodied agent and its environment. Accurate world models are essential for\nenabling agents to think, plan, and reason effectively in complex, dynamic\nsettings. Despite rapid progress, current world models remain brittle and\ndegrade over long horizons. We argue that a central cause is representation\nquality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or\nentangled latents make dynamics learning unnecessarily hard. We therefore ask\nwhether improving representation learning alone can substantially improve\nworld-model performance. In this work, we take a step toward building a truly\naccurate world model by addressing a fundamental yet open problem: constructing\na model that can fully clone and overfit to a deterministic 3D world. We\npropose Geometrically-Regularized World Models (GRWM), which enforces that\nconsecutive points along a natural sensory trajectory remain close in latent\nrepresentation space. This approach yields significantly improved latent\nrepresentations that align closely with the true topology of the environment.\nGRWM is plug-and-play, requires only minimal architectural modification, scales\nwith trajectory length, and is compatible with diverse latent generative\nbackbones. Across deterministic 3D settings and long-horizon prediction tasks,\nGRWM significantly increases rollout fidelity and stability. Analyses show that\nits benefits stem from learning a latent manifold with superior geometric\nstructure. These findings support a clear takeaway: improving representation\nlearning is a direct and useful path to robust world models, delivering\nreliable long-horizon predictions without enlarging the dynamics module.\n","authors":["Zaishuo Xia","Yukuan Lu","Xinyi Li","Yifan Xu","Yubei Chen"],"pdf_url":"https://arxiv.org/pdf/2510.26782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15723v7","updated":"2025-10-30T17:56:10Z","published":"2024-10-21T07:42:43Z","title":"S-CFE: Simple Counterfactual Explanations","summary":"  We study the problem of finding optimal sparse, manifold-aligned\ncounterfactual explanations for classifiers. Canonically, this can be\nformulated as an optimization problem with multiple non-convex components,\nincluding classifier loss functions and manifold alignment (or\n\\emph{plausibility}) metrics. The added complexity of enforcing\n\\emph{sparsity}, or shorter explanations, complicates the problem further.\nExisting methods often focus on specific models and plausibility measures,\nrelying on convex $\\ell_1$ regularizers to enforce sparsity. In this paper, we\ntackle the canonical formulation using the accelerated proximal gradient (APG)\nmethod, a simple yet efficient first-order procedure capable of handling smooth\nnon-convex objectives and non-smooth $\\ell_p$ (where $0 \\leq p < 1$)\nregularizers. This enables our approach to seamlessly incorporate various\nclassifiers and plausibility measures while producing sparser solutions. Our\nalgorithm only requires differentiable data-manifold regularizers and supports\nbox constraints for bounded feature ranges, ensuring the generated\ncounterfactuals remain \\emph{actionable}. Finally, experiments on real-world\ndatasets demonstrate that our approach effectively produces sparse,\nmanifold-aligned counterfactual explanations while maintaining proximity to the\nfactual data and computational efficiency.\n","authors":["Shpresim Sadiku","Moritz Wagner","Sai Ganesh Nagarajan","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2410.15723v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26778v1","updated":"2025-10-30T17:55:46Z","published":"2025-10-30T17:55:46Z","title":"Surpassing state of the art on AMD area estimation from RGB fundus\n  images through careful selection of U-Net architectures and loss functions\n  for class imbalance","summary":"  Age-related macular degeneration (AMD) is one of the leading causes of\nirreversible vision impairment in people over the age of 60. This research\nfocuses on semantic segmentation for AMD lesion detection in RGB fundus images,\na non-invasive and cost-effective imaging technique. The results of the ADAM\nchallenge - the most comprehensive AMD detection from RGB fundus images\nresearch competition and open dataset to date - serve as a benchmark for our\nevaluation. Taking the U-Net connectivity as a base of our framework, we\nevaluate and compare several approaches to improve the segmentation model's\narchitecture and training pipeline, including pre-processing techniques,\nencoder (backbone) deep network types of varying complexity, and specialized\nloss functions to mitigate class imbalances on image and pixel levels. The main\noutcome of this research is the final configuration of the AMD detection\nframework, which outperforms all the prior ADAM challenge submissions on the\nmulti-class segmentation of different AMD lesion types in non-invasive RGB\nfundus images. The source code used to conduct the experiments presented in\nthis paper is made freely available.\n","authors":["Valentyna Starodub","Mantas LukoÅ¡eviÄius"],"pdf_url":"https://arxiv.org/pdf/2510.26778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23534v2","updated":"2025-10-30T17:55:38Z","published":"2025-10-27T17:10:43Z","title":"Direct Debiased Machine Learning via Bregman Divergence Minimization","summary":"  We develop a direct debiased machine learning framework comprising Neyman\ntargeted estimation and generalized Riesz regression. Our framework unifies\nRiesz regression for automatic debiased machine learning, covariate balancing,\ntargeted maximum likelihood estimation (TMLE), and density-ratio estimation. In\nmany problems involving causal effects or structural models, the parameters of\ninterest depend on regression functions. Plugging regression functions\nestimated by machine learning methods into the identifying equations can yield\npoor performance because of first-stage bias. To reduce such bias, debiased\nmachine learning employs Neyman orthogonal estimating equations. Debiased\nmachine learning typically requires estimation of the Riesz representer and the\nregression function. For this problem, we develop a direct debiased machine\nlearning framework with an end-to-end algorithm. We formulate estimation of the\nnuisance parameters, the regression function and the Riesz representer, as\nminimizing the discrepancy between Neyman orthogonal scores computed with known\nand unknown nuisance parameters, which we refer to as Neyman targeted\nestimation. Neyman targeted estimation includes Riesz representer estimation,\nand we measure discrepancies using the Bregman divergence. The Bregman\ndivergence encompasses various loss functions as special cases, where the\nsquared loss yields Riesz regression and the Kullback-Leibler divergence yields\nentropy balancing. We refer to this Riesz representer estimation as generalized\nRiesz regression. Neyman targeted estimation also yields TMLE as a special case\nfor regression function estimation. Furthermore, for specific pairs of models\nand Riesz representer estimation methods, we can automatically obtain the\ncovariate balancing property without explicitly solving the covariate balancing\nobjective.\n","authors":["Masahiro Kato"],"pdf_url":"https://arxiv.org/pdf/2510.23534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26777v1","updated":"2025-10-30T17:55:23Z","published":"2025-10-30T17:55:23Z","title":"Pre-trained Forecasting Models: Strong Zero-Shot Feature Extractors for\n  Time Series Classification","summary":"  Recent research on time series foundation models has primarily focused on\nforecasting, leaving it unclear how generalizable their learned representations\nare. In this study, we examine whether frozen pre-trained forecasting models\ncan provide effective representations for classification. To this end, we\ncompare different representation extraction strategies and introduce two\nmodel-agnostic embedding augmentations. Our experiments show that the best\nforecasting models achieve classification accuracy that matches or even\nsurpasses that of state-of-the-art models pre-trained specifically for\nclassification. Moreover, we observe a positive correlation between forecasting\nand classification performance. These findings challenge the assumption that\ntask-specific pre-training is necessary, and suggest that learning to forecast\nmay provide a powerful route toward constructing general-purpose time series\nfoundation models.\n","authors":["Andreas Auer","Daniel Klotz","Sebastinan BÃ¶ck","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2510.26777v1.pdf","comment":"NeurIPS 2025 Workshop on Recent Advances in Time Series Foundation\n  Models (BERT2S)"},{"id":"http://arxiv.org/abs/2510.26776v1","updated":"2025-10-30T17:55:19Z","published":"2025-10-30T17:55:19Z","title":"Faithful and Fast Influence Function via Advanced Sampling","summary":"  How can we explain the influence of training data on black-box models?\nInfluence functions (IFs) offer a post-hoc solution by utilizing gradients and\nHessians. However, computing the Hessian for an entire dataset is\nresource-intensive, necessitating a feasible alternative. A common approach\ninvolves randomly sampling a small subset of the training data, but this method\noften results in highly inconsistent IF estimates due to the high variance in\nsample configurations. To address this, we propose two advanced sampling\ntechniques based on features and logits. These samplers select a small yet\nrepresentative subset of the entire dataset by considering the stochastic\ndistribution of features or logits, thereby enhancing the accuracy of IF\nestimations. We validate our approach through class removal experiments, a\ntypical application of IFs, using the F1-score to measure how effectively the\nmodel forgets the removed class while maintaining inference consistency on the\nremaining classes. Our method reduces computation time by 30.1% and memory\nusage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.\n","authors":["Jungyeon Koh","Hyeonsu Lyu","Jonggyu Jang","Hyun Jong Yang"],"pdf_url":"https://arxiv.org/pdf/2510.26776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26771v1","updated":"2025-10-30T17:53:42Z","published":"2025-10-30T17:53:42Z","title":"STaMP: Sequence Transformation and Mixed Precision for Low-Precision\n  Activation Quantization","summary":"  Quantization is the key method for reducing inference latency, power and\nmemory footprint of generative AI models. However, accuracy often degrades\nsharply when activations are quantized below eight bits. Recent work suggests\nthat invertible linear transformations (e.g. rotations) can aid quantization,\nby reparameterizing feature channels and weights. In this paper, we propose\n\\textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a\nnovel strategy that applies linear transformations along the \\textit{sequence}\ndimension to exploit the strong local correlation in language and visual data.\nBy keeping a small number of tokens in each intermediate activation at higher\nprecision, we can maintain model accuracy at lower (average) activations\nbit-widths. We evaluate STaMP on recent LVM and LLM architectures,\ndemonstrating that it significantly improves low bit width activation\nquantization and complements established activation and weight quantization\nmethods including recent feature transformations.\n","authors":["Marco Federici","Riccardo Del Chiaro","Boris van Breugel","Paul Whatmough","Markus Nagel"],"pdf_url":"https://arxiv.org/pdf/2510.26771v1.pdf","comment":"10 pages main text, 8 pages supplementary material"},{"id":"http://arxiv.org/abs/2510.26769v1","updated":"2025-10-30T17:52:39Z","published":"2025-10-30T17:52:39Z","title":"SteerVLM: Robust Model Control through Lightweight Activation Steering\n  for Vision Language Models","summary":"  This work introduces SteerVLM, a lightweight steering module designed to\nguide Vision-Language Models (VLMs) towards outputs that better adhere to\ndesired instructions. Our approach learns from the latent embeddings of paired\nprompts encoding target and converse behaviors to dynamically adjust\nactivations connecting the language modality with image context. This allows\nfor fine-grained, inference-time control over complex output semantics without\nmodifying model weights while preserving performance on off-target tasks. Our\nsteering module requires learning parameters equal to 0.14% of the original\nVLM's size. Our steering module gains model control through dimension-wise\nactivation modulation and adaptive steering across layers without requiring\npre-extracted static vectors or manual tuning of intervention points.\nFurthermore, we introduce VNIA (Visual Narrative Intent Alignment), a\nmultimodal dataset specifically created to facilitate the development and\nevaluation of VLM steering techniques. Our method outperforms existing\nintervention techniques on steering and hallucination mitigation benchmarks for\nVLMs and proposes a robust solution for multimodal model control through\nactivation engineering.\n","authors":["Anushka Sivakumar","Andrew Zhang","Zaber Hakim","Chris Thomas"],"pdf_url":"https://arxiv.org/pdf/2510.26769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.15370v3","updated":"2025-10-30T17:51:51Z","published":"2025-09-18T19:17:07Z","title":"Adversarial generalization of unfolding (model-based) networks","summary":"  Unfolding networks are interpretable networks emerging from iterative\nalgorithms, incorporate prior knowledge of data structure, and are designed to\nsolve inverse problems like compressed sensing, which deals with recovering\ndata from noisy, missing observations. Compressed sensing finds applications in\ncritical domains, from medical imaging to cryptography, where adversarial\nrobustness is crucial to prevent catastrophic failures. However, a solid\ntheoretical understanding of the performance of unfolding networks in the\npresence of adversarial attacks is still in its infancy. In this paper, we\nstudy the adversarial generalization of unfolding networks when perturbed with\n$l_2$-norm constrained attacks, generated by the fast gradient sign method.\nParticularly, we choose a family of state-of-the-art overaparameterized\nunfolding networks and deploy a new framework to estimate their adversarial\nRademacher complexity. Given this estimate, we provide adversarial\ngeneralization error bounds for the networks under study, which are tight with\nrespect to the attack level. To our knowledge, this is the first theoretical\nanalysis on the adversarial generalization of unfolding networks. We further\npresent a series of experiments on real-world data, with results corroborating\nour derived theory, consistently for all data. Finally, we observe that the\nfamily's overparameterization can be exploited to promote adversarial\nrobustness, shedding light on how to efficiently robustify neural networks.\n","authors":["Vicky Kouni"],"pdf_url":"https://arxiv.org/pdf/2509.15370v3.pdf","comment":"Accepted at NeurIPS2025"},{"id":"http://arxiv.org/abs/2510.26752v1","updated":"2025-10-30T17:46:49Z","published":"2025-10-30T17:46:49Z","title":"The Oversight Game: Learning to Cooperatively Balance an AI Agent's\n  Safety and Autonomy","summary":"  As increasingly capable agents are deployed, a central safety question is how\nto retain meaningful human control without modifying the underlying system. We\nstudy a minimal control interface where an agent chooses whether to act\nautonomously (play) or defer (ask), while a human simultaneously chooses\nwhether to be permissive (trust) or to engage in oversight (oversee). If the\nagent defers, the human's choice determines the outcome, potentially leading to\na corrective action or a system shutdown. We model this interaction as a\ntwo-player Markov Game. Our analysis focuses on cases where this game qualifies\nas a Markov Potential Game (MPG), a class of games where we can provide an\nalignment guarantee: under a structural assumption on the human's value\nfunction, any decision by the agent to act more autonomously that benefits\nitself cannot harm the human's value. We also analyze extensions to this MPG\nframework. Theoretically, this perspective provides conditions for a specific\nform of intrinsic alignment. If the reward structures of the human-agent game\nmeet these conditions, we have a formal guarantee that the agent improving its\nown outcome will not harm the human's. Practically, this model motivates a\ntransparent control layer with predictable incentives where the agent learns to\ndefer when risky and act when safe, while its pretrained policy and the\nenvironment's reward structure remain untouched. Our gridworld simulation shows\nthat through independent learning, the agent and human discover their optimal\noversight roles. The agent learns to ask when uncertain and the human learns\nwhen to oversee, leading to an emergent collaboration that avoids safety\nviolations introduced post-training. This demonstrates a practical method for\nmaking misaligned models safer after deployment.\n","authors":["William Overman","Mohsen Bayati"],"pdf_url":"https://arxiv.org/pdf/2510.26752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26745v1","updated":"2025-10-30T17:40:22Z","published":"2025-10-30T17:40:22Z","title":"Deep sequence models tend to memorize geometrically; it is unclear why","summary":"  In sequence modeling, the parametric memory of atomic facts has been\npredominantly abstracted as a brute-force lookup of co-occurrences between\nentities. We contrast this associative view against a geometric view of how\nmemory is stored. We begin by isolating a clean and analyzable instance of\nTransformer reasoning that is incompatible with memory as strictly a storage of\nthe local co-occurrences specified during training. Instead, the model must\nhave somehow synthesized its own geometry of atomic facts, encoding global\nrelationships between all entities, including non-co-occurring ones. This in\nturn has simplified a hard reasoning task involving an $\\ell$-fold composition\ninto an easy-to-learn 1-step geometric task.\n  From this phenomenon, we extract fundamental aspects of neural embedding\ngeometries that are hard to explain. We argue that the rise of such a geometry,\ndespite optimizing over mere local associations, cannot be straightforwardly\nattributed to typical architectural or optimizational pressures.\nCounterintuitively, an elegant geometry is learned even when it is not more\nsuccinct than a brute-force lookup of associations.\n  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry\nstems from a spectral bias that -- in contrast to prevailing theories -- indeed\narises naturally despite the lack of various pressures. This analysis also\npoints to practitioners a visible headroom to make Transformer memory more\nstrongly geometric. We hope the geometric view of parametric memory encourages\nrevisiting the default intuitions that guide researchers in areas like\nknowledge acquisition, capacity, discovery and unlearning.\n","authors":["Shahriar Noroozizadeh","Vaishnavh Nagarajan","Elan Rosenfeld","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2510.26745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.19419v2","updated":"2025-10-30T17:32:12Z","published":"2025-04-28T02:10:18Z","title":"Advancing Local Clustering on Graphs via Compressive Sensing:\n  Semi-supervised and Unsupervised Methods","summary":"  Local clustering aims to identify specific substructures within a large graph\nwithout any additional structural information of the graph. These substructures\nare typically small compared to the overall graph, enabling the problem to be\napproached by finding a sparse solution to a linear system associated with the\ngraph Laplacian. In this work, we first propose a method for identifying\nspecific local clusters when very few labeled data are given, which we term\nsemi-supervised local clustering. We then extend this approach to the\nunsupervised setting when no prior information on labels is available. The\nproposed methods involve randomly sampling the graph, applying diffusion\nthrough local cluster extraction, then examining the overlap among the results\nto find each cluster. We establish the co-membership conditions for any pair of\nnodes, and rigorously prove the correctness of our methods. Additionally, we\nconduct extensive experiments to demonstrate that the proposed methods achieve\nstate of the art results in the low-label rates regime.\n","authors":["Zhaiming Shen","Sung Ha Kang"],"pdf_url":"https://arxiv.org/pdf/2504.19419v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05567v2","updated":"2025-10-30T17:31:52Z","published":"2025-06-05T20:26:18Z","title":"Partially-Supervised Neural Network Model For Quadratic Multiparametric\n  Programming","summary":"  Neural Networks (NN) with ReLU activation functions are used to model\nmultiparametric quadratic optimization problems (mp-QP) in diverse engineering\napplications. Researchers have suggested leveraging the piecewise affine\nproperty of deep NN models to solve mp-QP with linear constraints, which also\nexhibit piecewise affine behaviour. However, traditional deep NN applications\nto mp-QP fall short of providing optimal and feasible predictions, even when\ntrained on large datasets. This study proposes a partially-supervised NN (PSNN)\narchitecture that directly represents the mathematical structure of the global\nsolution function. In contrast to generic NN training approaches, the proposed\nPSNN method derives a large proportion of model weights directly from the\nmathematical properties of the optimization problem, producing more accurate\nsolutions despite significantly smaller training data sets. Many energy\nmanagement problems are formulated as QP, so we apply the proposed approach to\nenergy systems (specifically DC optimal power flow) to demonstrate proof of\nconcept. Model performance in terms of solution accuracy and speed of\npredictions was compared against a commercial solver and a generic Deep NN\nmodel based on classical training. Results show KKT sufficient conditions for\nPSNN consistently outperform generic NN architectures with classical training\nusing far less data, including when tested on extreme, out-of-training\ndistribution test data. Given its speed advantages over traditional solvers,\nthe PSNN model can quickly produce optimal and feasible solutions within a\nsecond for millions of input parameters sampled from a distribution of\nstochastic demands and renewable generator dispatches, which can be used for\nsimulations and long term planning.\n","authors":["Fuat Can Beylunioglu","Mehrdad Pirnia","P. Robert Duimering"],"pdf_url":"https://arxiv.org/pdf/2506.05567v2.pdf","comment":"36 pages including references and appendix"},{"id":"http://arxiv.org/abs/2410.01755v3","updated":"2025-10-30T17:28:23Z","published":"2024-10-02T17:05:48Z","title":"Integrating Protein Sequence and Expression Level to Analysis Molecular\n  Characterization of Breast Cancer Subtypes","summary":"  Breast cancer's complexity and variability pose significant challenges in\nunderstanding its progression and guiding effective treatment. This study aims\nto integrate protein sequence data with expression levels to improve the\nmolecular characterization of breast cancer subtypes and predict clinical\noutcomes. Using ProtGPT2, a language model specifically designed for protein\nsequences, we generated embeddings that capture the functional and structural\nproperties of proteins. These embeddings were integrated with protein\nexpression levels to form enriched biological representations, which were\nanalyzed using machine learning methods, such as ensemble K-means for\nclustering and XGBoost for classification. Our approach enabled the successful\nclustering of patients into biologically distinct groups and accurately\npredicted clinical outcomes such as survival and biomarker status, achieving\nhigh performance metrics, notably an F1 score of 0.88 for survival and 0.87 for\nbiomarker status prediction. Feature importance analysis identified KMT2C,\nCLASP2, and MYO1B as key proteins involved in hormone signaling, cytoskeletal\nremodeling, and therapy resistance in hormone receptor-positive and\ntriple-negative breast cancer, with potential influence on breast cancer\nsubtype behavior and progression. Furthermore, protein-protein interaction\nnetworks and correlation analyses revealed functional interdependencies among\nproteins that may influence the behavior and progression of breast cancer\nsubtypes. These findings suggest that integrating protein sequence and\nexpression data provides valuable insights into tumor biology and has\nsignificant potential to enhance personalized treatment strategies in breast\ncancer care.\n","authors":["Hossein Sholehrasa","Majid Jaberi-Douraki"],"pdf_url":"https://arxiv.org/pdf/2410.01755v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26723v1","updated":"2025-10-30T17:23:40Z","published":"2025-10-30T17:23:40Z","title":"Bridging the Gap between Empirical Welfare Maximization and Conditional\n  Average Treatment Effect Estimation in Policy Learning","summary":"  The goal of policy learning is to train a policy function that recommends a\ntreatment given covariates to maximize population welfare. There are two major\napproaches in policy learning: the empirical welfare maximization (EWM)\napproach and the plug-in approach. The EWM approach is analogous to a\nclassification problem, where one first builds an estimator of the population\nwelfare, which is a functional of policy functions, and then trains a policy by\nmaximizing the estimated welfare. In contrast, the plug-in approach is based on\nregression, where one first estimates the conditional average treatment effect\n(CATE) and then recommends the treatment with the highest estimated outcome.\nThis study bridges the gap between the two approaches by showing that both are\nbased on essentially the same optimization problem. In particular, we prove an\nexact equivalence between EWM and least squares over a reparameterization of\nthe policy class. As a consequence, the two approaches are interchangeable in\nseveral respects and share the same theoretical guarantees under common\nconditions. Leveraging this equivalence, we propose a novel regularization\nmethod for policy learning. Our findings yield a convex and computationally\nefficient training procedure that avoids the NP-hard combinatorial step\ntypically required in EWM.\n","authors":["Masahiro Kato"],"pdf_url":"https://arxiv.org/pdf/2510.26723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26722v1","updated":"2025-10-30T17:22:57Z","published":"2025-10-30T17:22:57Z","title":"Non-Convex Over-the-Air Heterogeneous Federated Learning: A\n  Bias-Variance Trade-off","summary":"  Over-the-air (OTA) federated learning (FL) has been well recognized as a\nscalable paradigm that exploits the waveform superposition of the wireless\nmultiple-access channel to aggregate model updates in a single use. Existing\nOTA-FL designs largely enforce zero-bias model updates by either assuming\n\\emph{homogeneous} wireless conditions (equal path loss across devices) or\nforcing zero-bias updates to guarantee convergence. Under \\emph{heterogeneous}\nwireless scenarios, however, such designs are constrained by the weakest device\nand inflate the update variance. Moreover, prior analyses of biased OTA-FL\nlargely address convex objectives, while most modern AI models are highly\nnon-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient\ndescent (SGD) for general smooth non-convex objectives under wireless\nheterogeneity. We develop novel OTA-FL SGD updates that allow a structured,\ntime-invariant model bias while facilitating reduced variance updates. We\nderive a finite-time stationarity bound (expected time average squared gradient\nnorm) that explicitly reveals a bias-variance trade-off. To optimize this\ntrade-off, we pose a non-convex joint OTA power-control design and develop an\nefficient successive convex approximation (SCA) algorithm that requires only\nstatistical CSI at the base station. Experiments on a non-convex image\nclassification task validate the approach: the SCA-based design accelerates\nconvergence via an optimized bias and improves generalization over prior OTA-FL\nbaselines.\n","authors":["Muhammad Faraz Ul Abrar","NicolÃ² Michelusi"],"pdf_url":"https://arxiv.org/pdf/2510.26722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26717v1","updated":"2025-10-30T17:18:53Z","published":"2025-10-30T17:18:53Z","title":"On Purely Private Covariance Estimation","summary":"  We present a simple perturbation mechanism for the release of $d$-dimensional\ncovariance matrices $\\Sigma$ under pure differential privacy. For large\ndatasets with at least $n\\geq d^2/\\varepsilon$ elements, our mechanism recovers\nthe provably optimal Frobenius norm error guarantees of\n\\cite{nikolov2023private}, while simultaneously achieving best known error for\nall other $p$-Schatten norms, with $p\\in [1,\\infty]$. Our error is\ninformation-theoretically optimal for all $p\\ge 2$, in particular, our\nmechanism is the first purely private covariance estimator that achieves\noptimal error in spectral norm.\n  For small datasets $n< d^2/\\varepsilon$, we further show that by projecting\nthe output onto the nuclear norm ball of appropriate radius, our algorithm\nachieves the optimal Frobenius norm error $O(\\sqrt{d\\;\\text{Tr}(\\Sigma) /n})$,\nimproving over the known bounds of $O(\\sqrt{d/n})$ of \\cite{nikolov2023private}\nand ${O}\\big(d^{3/4}\\sqrt{\\text{Tr}(\\Sigma)/n}\\big)$ of\n\\cite{dong2022differentially}.\n","authors":["Tommaso d'Orsi","Gleb Novikov"],"pdf_url":"https://arxiv.org/pdf/2510.26717v1.pdf","comment":"equal contribution"},{"id":"http://arxiv.org/abs/2506.08645v2","updated":"2025-10-30T17:15:23Z","published":"2025-06-10T09:57:58Z","title":"When Kernels Multiply, Clusters Unify: Fusing Embeddings with the\n  Kronecker Product","summary":"  State-of-the-art embeddings often capture distinct yet complementary\ndiscriminative features: For instance, one image embedding model may excel at\ndistinguishing fine-grained textures, while another focuses on object-level\nstructure. Motivated by this observation, we propose a principled approach to\nfuse such complementary representations through kernel multiplication.\nMultiplying the kernel similarity functions of two embeddings allows their\ndiscriminative structures to interact, producing a fused representation whose\nkernel encodes the union of the clusters identified by each parent embedding.\nThis formulation also provides a natural way to construct joint kernels for\npaired multi-modal data (e.g., image-text tuples), where the product of\nmodality-specific kernels inherits structure from both domains. We highlight\nthat this kernel product is mathematically realized via the Kronecker product\nof the embedding feature maps, yielding our proposed KrossFuse framework for\nembedding fusion. To address the computational cost of the resulting\nhigh-dimensional Kronecker space, we further develop RP-KrossFuse, a scalable\nvariant that leverages random projections for efficient approximation. As a key\napplication, we use this framework to bridge the performance gap between\ncross-modal embeddings (e.g., CLIP, BLIP) and unimodal experts (e.g., DINOv2,\nE5). Experiments show that RP-KrossFuse effectively integrates these models,\nenhancing modality-specific performance while preserving cross-modal alignment.\nThe project code is available at https://github.com/yokiwuuu/KrossFuse.\n","authors":["Youqi Wu","Jingwei Zhang","Farzan Farnia"],"pdf_url":"https://arxiv.org/pdf/2506.08645v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26715v1","updated":"2025-10-30T17:13:58Z","published":"2025-10-30T17:13:58Z","title":"LSM-MS2: A Foundation Model Bridging Spectral Identification and\n  Biological Interpretation","summary":"  A vast majority of mass spectrometry data remains uncharacterized, leaving\nmuch of its biological and chemical information untapped. Recent advances in\nmachine learning have begun to address this gap, particularly for tasks such as\nspectral identification in tandem mass spectrometry data. Here, we present the\nlatest generation of LSM-MS2, a large-scale deep learning foundation model\ntrained on millions of spectra to learn a semantic chemical space. LSM-MS2\nachieves state-of-the-art performance in spectral identification, improving on\nexisting methods by 30% in accuracy of identifying challenging isomeric\ncompounds, yielding 42% more correct identifications in complex biological\nsamples, and maintaining robustness under low-concentration conditions.\nFurthermore, LSM-MS2 produces rich spectral embeddings that enable direct\nbiological interpretation from minimal downstream data, successfully\ndifferentiating disease states and predicting clinical outcomes across diverse\ntranslational applications.\n","authors":["Gabriel Asher","Devesh Shah","Amy A. Caudy","Luke Ferro","Lea Amar","Ana S. H. Costa","Thomas Patton","Niall O'Connor","Jennifer M. Campbell","Jack Geremia"],"pdf_url":"https://arxiv.org/pdf/2510.26715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26714v1","updated":"2025-10-30T17:13:42Z","published":"2025-10-30T17:13:42Z","title":"On the limitation of evaluating machine unlearning using only a single\n  training seed","summary":"  Machine unlearning (MU) aims to remove the influence of certain data points\nfrom a trained model without costly retraining. Most practical MU algorithms\nare only approximate and their performance can only be assessed empirically.\nCare must therefore be taken to make empirical comparisons as representative as\npossible. A common practice is to run the MU algorithm multiple times\nindependently starting from the same trained model. In this work, we\ndemonstrate that this practice can give highly non-representative results\nbecause -- even for the same architecture and same dataset -- some MU methods\ncan be highly sensitive to the choice of random number seed used for model\ntraining. We therefore recommend that empirical\ncomphttps://info.arxiv.org/help/prep#commentsarisons of MU algorithms should\nalso reflect the variability across different model training seeds.\n","authors":["Jamie Lanyon","Axel Finke","Petros Andreou","Georgina Cosma"],"pdf_url":"https://arxiv.org/pdf/2510.26714v1.pdf","comment":"mini paper, 2 figures"},{"id":"http://arxiv.org/abs/2510.26709v1","updated":"2025-10-30T17:11:01Z","published":"2025-10-30T17:11:01Z","title":"An All-Reduce Compatible Top-K Compressor for Communication-Efficient\n  Distributed Learning","summary":"  Communication remains a central bottleneck in large-scale distributed machine\nlearning, and gradient sparsification has emerged as a promising strategy to\nalleviate this challenge. However, existing gradient compressors face notable\nlimitations: Rand-$K$\\ discards structural information and performs poorly in\npractice, while Top-$K$\\ preserves informative entries but loses the\ncontraction property and requires costly All-Gather operations. In this paper,\nwe propose ARC-Top-$K$, an {All-Reduce}-Compatible Top-$K$ compressor that\naligns sparsity patterns across nodes using a lightweight sketch of the\ngradient, enabling index-free All-Reduce while preserving globally significant\ninformation. ARC-Top-$K$\\ is provably contractive and, when combined with\nmomentum error feedback (EF21M), achieves linear speedup and sharper\nconvergence rates than the original EF21M under standard assumptions.\nEmpirically, ARC-Top-$K$\\ matches the accuracy of Top-$K$\\ while reducing\nwall-clock training time by up to 60.7\\%, offering an efficient and scalable\nsolution that combines the robustness of Rand-$K$\\ with the strong performance\nof Top-$K$.\n","authors":["Chuyan Chen","Chenyang Ma","Zhangxin Li","Yutong He","Yanjie Dong","Kun Yuan"],"pdf_url":"https://arxiv.org/pdf/2510.26709v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2509.21319v2","updated":"2025-10-30T17:09:54Z","published":"2025-09-25T16:19:06Z","title":"RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable Rewards","summary":"  Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost). Models:\nhttps://huggingface.co/collections/nvidia/reward-models-10-2025\n","authors":["Zhilin Wang","Jiaqi Zeng","Olivier Delalleau","Ellie Evans","Daniel Egert","Hoo-Chang Shin","Felipe Soares","Yi Dong","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2509.21319v2.pdf","comment":"Added link to access models:\n  https://huggingface.co/collections/nvidia/reward-models-10-2025"},{"id":"http://arxiv.org/abs/2510.26707v1","updated":"2025-10-30T17:09:09Z","published":"2025-10-30T17:09:09Z","title":"Value Drifts: Tracing Value Alignment During LLM Post-Training","summary":"  As LLMs occupy an increasingly important role in society, they are more and\nmore confronted with questions that require them not only to draw on their\ngeneral knowledge but also to align with certain human value systems.\nTherefore, studying the alignment of LLMs with human values has become a\ncrucial field of inquiry. Prior work, however, mostly focuses on evaluating the\nalignment of fully trained models, overlooking the training dynamics by which\nmodels learn to express human values. In this work, we investigate how and at\nwhich stage value alignment arises during the course of a model's\npost-training. Our analysis disentangles the effects of post-training\nalgorithms and datasets, measuring both the magnitude and time of value drifts\nduring training. Experimenting with Llama-3 and Qwen-3 models of different\nsizes and popular supervised fine-tuning (SFT) and preference optimization\ndatasets and algorithms, we find that the SFT phase generally establishes a\nmodel's values, and subsequent preference optimization rarely re-aligns these\nvalues. Furthermore, using a synthetic preference dataset that enables\ncontrolled manipulation of values, we find that different preference\noptimization algorithms lead to different value alignment outcomes, even when\npreference data is held constant. Our findings provide actionable insights into\nhow values are learned during post-training and help to inform data curation,\nas well as the selection of models and algorithms for preference optimization\nto improve model alignment to human values.\n","authors":["Mehar Bhatia","Shravan Nayak","Gaurav Kamath","Marius Mosbach","Karolina StaÅczak","Vered Shwartz","Siva Reddy"],"pdf_url":"https://arxiv.org/pdf/2510.26707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26706v1","updated":"2025-10-30T17:08:52Z","published":"2025-10-30T17:08:52Z","title":"Budgeted Multiple-Expert Deferral","summary":"  Learning to defer uncertain predictions to costly experts offers a powerful\nstrategy for improving the accuracy and efficiency of machine learning systems.\nHowever, standard training procedures for deferral algorithms typically require\nquerying all experts for every training instance, an approach that becomes\nprohibitively expensive when expert queries incur significant computational or\nresource costs. This undermines the core goal of deferral: to limit unnecessary\nexpert usage. To overcome this challenge, we introduce the budgeted deferral\nframework, which aims to train effective deferral algorithms while minimizing\nexpert query costs during training. We propose new algorithms for both\ntwo-stage and single-stage multiple-expert deferral settings that selectively\nquery only a subset of experts per training example. While inspired by active\nlearning, our setting is fundamentally different: labels are already known, and\nthe core challenge is to decide which experts to query in order to balance cost\nand predictive performance. We establish theoretical guarantees for both of our\nalgorithms, including generalization bounds and label complexity analyses.\nEmpirical results across several domains show that our algorithms substantially\nreduce training costs without sacrificing prediction accuracy, demonstrating\nthe practical value of our budget-aware deferral algorithms.\n","authors":["Giulia DeSalvo","Clara Mohri","Mehryar Mohri","Yutao Zhong"],"pdf_url":"https://arxiv.org/pdf/2510.26706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26704v1","updated":"2025-10-30T17:07:14Z","published":"2025-10-30T17:07:14Z","title":"How Regularization Terms Make Invertible Neural Networks Bayesian Point\n  Estimators","summary":"  Can regularization terms in the training of invertible neural networks lead\nto known Bayesian point estimators in reconstruction? Invertible networks are\nattractive for inverse problems due to their inherent stability and\ninterpretability. Recently, optimization strategies for invertible neural\nnetworks that approximate either a reconstruction map or the forward operator\nhave been studied from a Bayesian perspective, but each has limitations. To\naddress this, we introduce and analyze two regularization terms for the network\ntraining that, upon inversion of the network, recover properties of classical\nBayesian point estimators: while the first can be connected to the posterior\nmean, the second resembles the MAP estimator. Our theoretical analysis\ncharacterizes how each loss shapes both the learned forward operator and its\ninverse reconstruction map. Numerical experiments support our findings and\ndemonstrate how these loss-term regularizers introduce data-dependence in a\nstable and interpretable way.\n","authors":["Nick HeilenkÃ¶tter"],"pdf_url":"https://arxiv.org/pdf/2510.26704v1.pdf","comment":"Preprint, under review"},{"id":"http://arxiv.org/abs/2505.12275v2","updated":"2025-10-30T17:06:21Z","published":"2025-05-18T07:27:35Z","title":"Curriculum Abductive Learning","summary":"  Abductive Learning (ABL) integrates machine learning with logical reasoning\nin a loop: a learning model predicts symbolic concept labels from raw inputs,\nwhich are revised through abduction using domain knowledge and then fed back\nfor retraining. However, due to the nondeterminism of abduction, the training\nprocess often suffers from instability, especially when the knowledge base is\nlarge and complex, resulting in a prohibitively large abduction space. While\nprior works focus on improving candidate selection within this space, they\ntypically treat the knowledge base as a static black box. In this work, we\npropose Curriculum Abductive Learning (C-ABL), a method that explicitly\nleverages the internal structure of the knowledge base to address the ABL\ntraining challenges. C-ABL partitions the knowledge base into a sequence of\nsub-bases, progressively introduced during training. This reduces the abduction\nspace throughout training and enables the model to incorporate logic in a\nstepwise, smooth way. Experiments across multiple tasks show that C-ABL\noutperforms previous ABL implementations, significantly improves training\nstability, convergence speed, and final accuracy, especially under complex\nknowledge setting.\n","authors":["Wen-Chao Hu","Qi-Jie Li","Lin-Han Jia","Cunjing Ge","Yu-Feng Li","Yuan Jiang","Zhi-Hua Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.12275v2.pdf","comment":"Accepted by NeurIPS 2025, 22 pages, 6 figures"},{"id":"http://arxiv.org/abs/2510.26700v1","updated":"2025-10-30T17:05:57Z","published":"2025-10-30T17:05:57Z","title":"Assessment of the conditional exchangeability assumption in causal\n  machine learning models: a simulation study","summary":"  Observational studies developing causal machine learning (ML) models for the\nprediction of individualized treatment effects (ITEs) seldom conduct empirical\nevaluations to assess the conditional exchangeability assumption. We aimed to\nevaluate the performance of these models under conditional exchangeability\nviolations and the utility of negative control outcomes (NCOs) as a diagnostic.\nWe conducted a simulation study to examine confounding bias in ITE estimates\ngenerated by causal forest and X-learner models under varying conditions,\nincluding the presence or absence of true heterogeneity. We simulated data to\nreflect real-world scenarios with differing levels of confounding, sample size,\nand NCO confounding structures. We then estimated and compared subgroup-level\ntreatment effects on the primary outcome and NCOs across settings with and\nwithout unmeasured confounding. When conditional exchangeability was violated,\ncausal forest and X-learner models failed to recover true treatment effect\nheterogeneity and, in some cases, falsely indicated heterogeneity when there\nwas none. NCOs successfully identified subgroups affected by unmeasured\nconfounding. Even when NCOs did not perfectly satisfy its ideal assumptions, it\nremained informative, flagging potential bias in subgroup level estimates,\nthough not always pinpointing the subgroup with the largest confounding.\nViolations of conditional exchangeability substantially limit the validity of\nITE estimates from causal ML models in routinely collected observational data.\nNCOs serve a useful empirical diagnostic tool for detecting subgroup-specific\nunmeasured confounding and should be incorporated into causal ML workflows to\nsupport the credibility of individualized inference.\n","authors":["Gerard T. Portela","Jason B. Gibbons","Sebastian Schneeweiss","Rishi J. Desai"],"pdf_url":"https://arxiv.org/pdf/2510.26700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20138v2","updated":"2025-10-30T17:04:50Z","published":"2025-03-26T01:00:35Z","title":"Guided Model Merging for Hybrid Data Learning: Leveraging Centralized\n  Data to Refine Decentralized Models","summary":"  Current network training paradigms primarily focus on either centralized or\ndecentralized data regimes. However, in practice, data availability often\nexhibits a hybrid nature, where both regimes coexist. This hybrid setting\npresents new opportunities for model training, as the two regimes offer\ncomplementary trade-offs: decentralized data is abundant but subject to\nheterogeneity and communication constraints, while centralized data, though\nlimited in volume and potentially unrepresentative, enables better curation and\nhigh-throughput access. Despite its potential, effectively combining these\nparadigms remains challenging, and few frameworks are tailored to hybrid data\nregimes. To address this, we propose a novel framework that constructs a model\natlas from decentralized models and leverages centralized data to refine a\nglobal model within this structured space. The refined model is then used to\nreinitialize the decentralized models. Our method synergizes federated learning\n(to exploit decentralized data) and model merging (to utilize centralized\ndata), enabling effective training under hybrid data availability.\nTheoretically, we show that our approach achieves faster convergence than\nmethods relying solely on decentralized data, due to variance reduction in the\nmerging process. Extensive experiments demonstrate that our framework\nconsistently outperforms purely centralized, purely decentralized, and existing\nhybrid-adaptable methods. Notably, our method remains robust even when the\ncentralized and decentralized data domains differ or when decentralized data\ncontains noise, significantly broadening its applicability.\n","authors":["Junyi Zhu","Ruicong Yao","Taha Ceritli","Savas Ozkan","Matthew B. Blaschko","Eunchung Noh","Jeongwon Min","Cho Jung Min","Mete Ozay"],"pdf_url":"https://arxiv.org/pdf/2503.20138v2.pdf","comment":"Accepted at WACV 2026"},{"id":"http://arxiv.org/abs/2510.26692v1","updated":"2025-10-30T16:59:43Z","published":"2025-10-30T16:59:43Z","title":"Kimi Linear: An Expressive, Efficient Attention Architecture","summary":"  We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints.\n","authors":[" Kimi Team","Yu Zhang","Zongyu Lin","Xingcheng Yao","Jiaxi Hu","Fanqing Meng","Chengyin Liu","Xin Men","Songlin Yang","Zhiyuan Li","Wentao Li","Enzhe Lu","Weizhou Liu","Yanru Chen","Weixin Xu","Longhui Yu","Yejie Wang","Yu Fan","Longguang Zhong","Enming Yuan","Dehao Zhang","Yizhi Zhang","T. Y. Liu","Haiming Wang","Shengjun Fang","Weiran He","Shaowei Liu","Yiwei Li","Jianlin Su","Jiezhong Qiu","Bo Pang","Junjie Yan","Zhejun Jiang","Weixiao Huang","Bohong Yin","Jiacheng You","Chu Wei","Zhengtao Wang","Chao Hong","Yutian Chen","Guanduo Chen","Yucheng Wang","Huabin Zheng","Feng Wang","Yibo Liu","Mengnan Dong","Zheng Zhang","Siyuan Pan","Wenhao Wu","Yuhao Wu","Longyu Guan","Jiawen Tao","Guohong Fu","Xinran Xu","Yuzhi Wang","Guokun Lai","Yuxin Wu","Xinyu Zhou","Zhilin Yang","Yulun Du"],"pdf_url":"https://arxiv.org/pdf/2510.26692v1.pdf","comment":"Kimi Linear tech report"},{"id":"http://arxiv.org/abs/2510.26690v1","updated":"2025-10-30T16:59:22Z","published":"2025-10-30T16:59:22Z","title":"LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits","summary":"  Low-Rank Adaptation (LoRA) has become a popular technique for\nparameter-efficient fine-tuning of large language models (LLMs). In many\nreal-world scenarios, multiple adapters are loaded simultaneously to enable LLM\ncustomization for personalized user experiences or to support a diverse range\nof tasks. Although each adapter is lightweight in isolation, their aggregate\ncost becomes substantial at scale. To address this, we propose LoRAQuant, a\nmixed-precision post-training quantization method tailored to LoRA.\nSpecifically, LoRAQuant reparameterizes each adapter by singular value\ndecomposition (SVD) to concentrate the most important information into specific\nrows and columns. This makes it possible to quantize the important components\nto higher precision, while quantizing the rest to ultra-low bitwidth. We\nconduct comprehensive experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B\nmodels on mathematical reasoning, coding, and summarization tasks. Results show\nthat our LoRAQuant uses significantly lower bits than other quantization\nmethods, but achieves comparable or even higher performance.\n","authors":["Amir Reza Mirzaei","Yuqiao Wen","Yanshuai Cao","Lili Mou"],"pdf_url":"https://arxiv.org/pdf/2510.26690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26688v1","updated":"2025-10-30T16:57:13Z","published":"2025-10-30T16:57:13Z","title":"FlowQ-Net: A Generative Framework for Automated Quantum Circuit Design","summary":"  Designing efficient quantum circuits is a central bottleneck to exploring the\npotential of quantum computing, particularly for noisy intermediate-scale\nquantum (NISQ) devices, where circuit efficiency and resilience to errors are\nparamount. The search space of gate sequences grows combinatorially, and\nhandcrafted templates often waste scarce qubit and depth budgets. We introduce\n\\textsc{FlowQ-Net} (Flow-based Quantum design Network), a generative framework\nfor automated quantum circuit synthesis based on Generative Flow Networks\n(GFlowNets). This framework learns a stochastic policy to construct circuits\nsequentially, sampling them in proportion to a flexible, user-defined reward\nfunction that can encode multiple design objectives such as performance, depth,\nand gate count. This approach uniquely enables the generation of a diverse\nensemble of high-quality circuits, moving beyond single-solution optimization.\nWe demonstrate the efficacy of \\textsc{FlowQ-Net} through an extensive set of\nsimulations. We apply our method to Variational Quantum Algorithm (VQA) ansatz\ndesign for molecular ground state estimation, Max-Cut, and image\nclassification, key challenges in near-term quantum computing. Circuits\ndesigned by \\textsc{FlowQ-Net} achieve significant improvements, yielding\ncircuits that are 10$\\times$-30$\\times$ more compact in terms of parameters,\ngates, and depth compared to commonly used unitary baselines, without\ncompromising accuracy. This trend holds even when subjected to error profiles\nfrom real-world quantum devices. Our results underline the potential of\ngenerative models as a general-purpose methodology for automated quantum\ncircuit design, offering a promising path towards more efficient quantum\nalgorithms and accelerating scientific discovery in the quantum domain.\n","authors":["Jun Dai","Michael Rizvi-Martel","Guillaume Rabusseau"],"pdf_url":"https://arxiv.org/pdf/2510.26688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17434v5","updated":"2025-10-30T16:47:32Z","published":"2023-11-29T08:26:18Z","title":"GSE: Group-wise Sparse and Explainable Adversarial Attacks","summary":"  Sparse adversarial attacks fool deep neural networks (DNNs) through minimal\npixel perturbations, often regularized by the $\\ell_0$ norm. Recent efforts\nhave replaced this norm with a structural sparsity regularizer, such as the\nnuclear group norm, to craft group-wise sparse adversarial attacks. The\nresulting perturbations are thus explainable and hold significant practical\nrelevance, shedding light on an even greater vulnerability of DNNs. However,\ncrafting such attacks poses an optimization challenge, as it involves computing\nnorms for groups of pixels within a non-convex objective. We address this by\npresenting a two-phase algorithm that generates group-wise sparse attacks\nwithin semantically meaningful areas of an image. Initially, we optimize a\nquasinorm adversarial loss using the $1/2-$quasinorm proximal operator tailored\nfor non-convex programming. Subsequently, the algorithm transitions to a\nprojected Nesterov's accelerated gradient descent with $2-$norm regularization\napplied to perturbation magnitudes. Rigorous evaluations on CIFAR-10 and\nImageNet datasets demonstrate a remarkable increase in group-wise sparsity,\ne.g., $50.9\\%$ on CIFAR-10 and $38.4\\%$ on ImageNet (average case, targeted\nattack). This performance improvement is accompanied by significantly faster\ncomputation times, improved explainability, and a $100\\%$ attack success rate.\n","authors":["Shpresim Sadiku","Moritz Wagner","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2311.17434v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26679v1","updated":"2025-10-30T16:47:26Z","published":"2025-10-30T16:47:26Z","title":"Tight Differentially Private PCA via Matrix Coherence","summary":"  We revisit the task of computing the span of the top $r$ singular vectors\n$u_1, \\ldots, u_r$ of a matrix under differential privacy. We show that a\nsimple and efficient algorithm -- based on singular value decomposition and\nstandard perturbation mechanisms -- returns a private rank-$r$ approximation\nwhose error depends only on the \\emph{rank-$r$ coherence} of $u_1, \\ldots, u_r$\nand the spectral gap $\\sigma_r - \\sigma_{r+1}$. This resolves a question posed\nby Hardt and Roth~\\cite{hardt2013beyond}. Our estimator outperforms the state\nof the art -- significantly so in some regimes. In particular, we show that in\nthe dense setting, it achieves the same guarantees for single-spike PCA in the\nWishart model as those attained by optimal non-private algorithms, whereas\nprior private algorithms failed to do so.\n  In addition, we prove that (rank-$r$) coherence does not increase under\nGaussian perturbations. This implies that any estimator based on the Gaussian\nmechanism -- including ours -- preserves the coherence of the input. We\nconjecture that similar behavior holds for other structured models, including\nplanted problems in graphs.\n  We also explore applications of coherence to graph problems. In particular,\nwe present a differentially private algorithm for Max-Cut and other constraint\nsatisfaction problems under low coherence assumptions.\n","authors":["Tommaso d'Orsi","Gleb Novikov"],"pdf_url":"https://arxiv.org/pdf/2510.26679v1.pdf","comment":"SODA 2026; equal contribution"},{"id":"http://arxiv.org/abs/2510.26672v1","updated":"2025-10-30T16:42:09Z","published":"2025-10-30T16:42:09Z","title":"Action-Driven Processes for Continuous-Time Control","summary":"  At the heart of reinforcement learning are actions -- decisions made in\nresponse to observations of the environment. Actions are equally fundamental in\nthe modeling of stochastic processes, as they trigger discontinuous state\ntransitions and enable the flow of information through large, complex systems.\nIn this paper, we unify the perspectives of stochastic processes and\nreinforcement learning through action-driven processes, and illustrate their\napplication to spiking neural networks. Leveraging ideas from\ncontrol-as-inference, we show that minimizing the Kullback-Leibler divergence\nbetween a policy-driven true distribution and a reward-driven model\ndistribution for a suitably defined action-driven process is equivalent to\nmaximum entropy reinforcement learning.\n","authors":["Ruimin He","Shaowei Lin"],"pdf_url":"https://arxiv.org/pdf/2510.26672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26656v1","updated":"2025-10-30T16:23:46Z","published":"2025-10-30T16:23:46Z","title":"Heuristic Adaptation of Potentially Misspecified Domain Support for\n  Likelihood-Free Inference in Stochastic Dynamical Systems","summary":"  In robotics, likelihood-free inference (LFI) can provide the domain\ndistribution that adapts a learnt agent in a parametric set of deployment\nconditions. LFI assumes an arbitrary support for sampling, which remains\nconstant as the initial generic prior is iteratively refined to more\ndescriptive posteriors. However, a potentially misspecified support can lead to\nsuboptimal, yet falsely certain, posteriors. To address this issue, we propose\nthree heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the\nposterior mode shift over inference steps in its own way and, when integrated\ninto an LFI step, adapts the support alongside posterior inference. We first\nexpose the support misspecification issue and evaluate our heuristics using\nstochastic dynamical benchmarks. We then evaluate the impact of heuristic\nsupport adaptation on parameter inference and policy learning for a dynamic\ndeformable linear object (DLO) manipulation task. Inference results in a finer\nlength and stiffness classification for a parametric set of DLOs. When the\nresulting posteriors are used as domain distributions for sim-based policy\nlearning, they lead to more robust object-centric agent performance.\n","authors":["Georgios Kamaras","Craig Innes","Subramanian Ramamoorthy"],"pdf_url":"https://arxiv.org/pdf/2510.26656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26646v1","updated":"2025-10-30T16:12:01Z","published":"2025-10-30T16:12:01Z","title":"Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in\n  Dynamic Environments","summary":"  This paper presents a hierarchical path-planning and control framework that\ncombines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with\na low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller\nfor continuous actuation. The high-level module selects behaviors and\nsub-goals; the low-level module executes smooth velocity commands. We design a\npractical reward shaping scheme (direction, distance, obstacle avoidance,\naction smoothness, collision penalty, time penalty, and progress), together\nwith a LiDAR-based safety gate that prevents unsafe motions. The system is\nimplemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics,\nincluding success rate, collision rate, path efficiency, and re-planning\nefficiency, in dynamic and partially observable environments. Experiments show\nimproved success rate and sample efficiency over single-algorithm baselines\n(DQN or TD3 alone) and rule-based planners, with better generalization to\nunseen obstacle configurations and reduced abrupt control changes. Code and\nevaluation scripts are available at the project repository.\n","authors":["Xiaoyi He","Danggui Chen","Zhenshuo Zhang","Zimeng Bai"],"pdf_url":"https://arxiv.org/pdf/2510.26646v1.pdf","comment":"6 pages, 5 figures; ROS+Gazebo (TurtleBot3) implementation;\n  evaluation with PathBench metrics; code (primary):\n  https://github.com/MayaCHEN-github/HierarchicalRL-robot-navigation; mirror\n  (for reproducibility): https://github.com/ShowyHe/DRL-robot-navigation"},{"id":"http://arxiv.org/abs/2510.26645v1","updated":"2025-10-30T16:11:39Z","published":"2025-10-30T16:11:39Z","title":"Curly Flow Matching for Learning Non-gradient Field Dynamics","summary":"  Modeling the transport dynamics of natural processes from population-level\nobservations is a ubiquitous problem in the natural sciences. Such models rely\non key assumptions about the underlying process in order to enable faithful\nlearning of governing dynamics that mimic the actual system behavior. The de\nfacto assumption in current approaches relies on the principle of least action\nthat results in gradient field dynamics and leads to trajectories minimizing an\nenergy functional between two probability measures. However, many real-world\nsystems, such as cell cycles in single-cell RNA, are known to exhibit\nnon-gradient, periodic behavior, which fundamentally cannot be captured by\ncurrent state-of-the-art methods such as flow and bridge matching. In this\npaper, we introduce Curly Flow Matching (Curly-FM), a novel approach that is\ncapable of learning non-gradient field dynamics by designing and solving a\nSchr\\\"odinger bridge problem with a non-zero drift reference process -- in\nstark contrast to typical zero-drift reference processes -- which is\nconstructed using inferred velocities in addition to population snapshot data.\nWe showcase Curly-FM by solving the trajectory inference problems for single\ncells, computational fluid dynamics, and ocean currents with approximate\nvelocities. We demonstrate that Curly-FM can learn trajectories that better\nmatch both the reference process and population marginals. Curly-FM expands\nflow matching models beyond the modeling of populations and towards the\nmodeling of known periodic behavior in physical systems. Our code repository is\naccessible at: https://github.com/kpetrovicc/curly-flow-matching.git\n","authors":["Katarina PetroviÄ","Lazar Atanackovic","Viggo Moro","Kacper KapuÅniak","Ä°smail Ä°lkan Ceylan","Michael Bronstein","Avishek Joey Bose","Alexander Tong"],"pdf_url":"https://arxiv.org/pdf/2510.26645v1.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26643v1","updated":"2025-10-30T16:09:51Z","published":"2025-10-30T16:09:51Z","title":"MSAD: A Deep Dive into Model Selection for Time series Anomaly Detection","summary":"  Anomaly detection is a fundamental task for time series analytics with\nimportant implications for the downstream performance of many applications.\nDespite increasing academic interest and the large number of methods proposed\nin the literature, recent benchmarks and evaluation studies demonstrated that\nno overall best anomaly detection methods exist when applied to very\nheterogeneous time series datasets. Therefore, the only scalable and viable\nsolution to solve anomaly detection over very different time series collected\nfrom diverse domains is to propose a model selection method that will select,\nbased on time series characteristics, the best anomaly detection methods to\nrun. Existing AutoML solutions are, unfortunately, not directly applicable to\ntime series anomaly detection, and no evaluation of time series-based\napproaches for model selection exists. Towards that direction, this paper\nstudies the performance of time series classification methods used as model\nselection for anomaly detection. In total, we evaluate 234 model configurations\nderived from 16 base classifiers across more than 1980 time series, and we\npropose the first extensive experimental evaluation of time series\nclassification as model selection for anomaly detection. Our results\ndemonstrate that model selection methods outperform every single anomaly\ndetection method while being in the same order of magnitude regarding execution\ntime. This evaluation is the first step to demonstrate the accuracy and\nefficiency of time series classification algorithms for anomaly detection, and\nrepresents a strong baseline that can then be used to guide the model selection\nstep in general AutoML pipelines. Preprint version of an article accepted at\nthe VLDB Journal.\n","authors":["Emmanouil Sylligardos","John Paparrizos","Themis Palpanas","Pierre Senellart","Paul Boniol"],"pdf_url":"https://arxiv.org/pdf/2510.26643v1.pdf","comment":"25 pages, 13 figures, VLDB Journal"},{"id":"http://arxiv.org/abs/2507.00927v3","updated":"2025-10-30T16:05:04Z","published":"2025-07-01T16:34:19Z","title":"Understanding Generalization in Node and Link Prediction","summary":"  Using message-passing graph neural networks (MPNNs) for node and link\nprediction is crucial in various scientific and industrial domains, which has\nled to the development of diverse MPNN architectures. Besides working well in\npractical settings, their ability to generalize beyond the training set remains\npoorly understood. While some studies have explored MPNNs' generalization in\ngraph-level prediction tasks, much less attention has been given to node- and\nlink-level predictions. Existing works often rely on unrealistic i.i.d.\\@\nassumptions, overlooking possible correlations between nodes or links, and\nassuming fixed aggregation and impractical loss functions while neglecting the\ninfluence of graph structure. In this work, we introduce a unified framework to\nanalyze the generalization properties of MPNNs in inductive and transductive\nnode and link prediction settings, incorporating diverse architectural\nparameters and loss functions and quantifying the influence of graph structure.\nAdditionally, our proposed generalization framework can be applied beyond\ngraphs to any classification task under the inductive or transductive setting.\nOur empirical study supports our theoretical insights, deepening our\nunderstanding of MPNNs' generalization capabilities in these tasks.\n","authors":["Antonis Vasileiou","Timo Stoll","Christopher Morris"],"pdf_url":"https://arxiv.org/pdf/2507.00927v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2412.07106"},{"id":"http://arxiv.org/abs/2510.26633v1","updated":"2025-10-30T16:02:53Z","published":"2025-10-30T16:02:53Z","title":"Omnipresent Yet Overlooked: Heat Kernels in Combinatorial Bayesian\n  Optimization","summary":"  Bayesian Optimization (BO) has the potential to solve various combinatorial\ntasks, ranging from materials science to neural architecture search. However,\nBO requires specialized kernels to effectively model combinatorial domains.\nRecent efforts have introduced several combinatorial kernels, but the\nrelationships among them are not well understood. To bridge this gap, we\ndevelop a unifying framework based on heat kernels, which we derive in a\nsystematic way and express as simple closed-form expressions. Using this\nframework, we prove that many successful combinatorial kernels are either\nrelated or equivalent to heat kernels, and validate this theoretical claim in\nour experiments. Moreover, our analysis confirms and extends the results\npresented in Bounce: certain algorithms' performance decreases substantially\nwhen the unknown optima of the function do not have a certain structure. In\ncontrast, heat kernels are not sensitive to the location of the optima. Lastly,\nwe show that a fast and simple pipeline, relying on heat kernels, is able to\nachieve state-of-the-art results, matching or even outperforming certain slow\nor complex algorithms.\n","authors":["Colin Doumont","Victor Picheny","Viacheslav Borovitskiy","Henry Moss"],"pdf_url":"https://arxiv.org/pdf/2510.26633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.21258v2","updated":"2025-10-30T16:01:21Z","published":"2025-08-28T23:09:54Z","title":"RelP: Faithful and Efficient Circuit Discovery in Language Models via\n  Relevance Patching","summary":"  Activation patching is a standard method in mechanistic interpretability for\nlocalizing the components of a model responsible for specific behaviors, but it\nis computationally expensive to apply at scale. Attribution patching offers a\nfaster, gradient-based approximation, yet suffers from noise and reduced\nreliability in deep, highly non-linear networks. In this work, we introduce\nRelevance Patching (RelP), which replaces the local gradients in attribution\npatching with propagation coefficients derived from Layer-wise Relevance\nPropagation (LRP). LRP propagates the network's output backward through the\nlayers, redistributing relevance to lower-level components according to local\npropagation rules that ensure properties such as relevance conservation or\nimproved signal-to-noise ratio. Like attribution patching, RelP requires only\ntwo forward passes and one backward pass, maintaining computational efficiency\nwhile improving faithfulness. We validate RelP across a range of models and\ntasks, showing that it more accurately approximates activation patching than\nstandard attribution patching, particularly when analyzing residual stream and\nMLP outputs in the Indirect Object Identification (IOI) task. For instance, for\nMLP outputs in GPT-2 Large, attribution patching achieves a Pearson correlation\nof 0.006, whereas RelP reaches 0.956, highlighting the improvement offered by\nRelP. Additionally, we compare the faithfulness of sparse feature circuits\nidentified by RelP and Integrated Gradients (IG), showing that RelP achieves\ncomparable faithfulness without the extra computational cost associated with\nIG.\n","authors":["Farnoush Rezaei Jafari","Oliver Eberle","Ashkan Khakzar","Neel Nanda"],"pdf_url":"https://arxiv.org/pdf/2508.21258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.01988v3","updated":"2025-10-30T15:47:25Z","published":"2025-10-02T13:07:37Z","title":"PepCompass: Navigating peptide embedding spaces using Riemannian\n  Geometry","summary":"  Antimicrobial peptide discovery is challenged by the astronomical size of\npeptide space and the relative scarcity of active peptides. Generative models\nprovide continuous latent \"maps\" of peptide space, but conventionally ignore\ndecoder-induced geometry and rely on flat Euclidean metrics, rendering\nexploration and optimization distorted and inefficient. Prior manifold-based\nremedies assume fixed intrinsic dimensionality, which critically fails in\npractice for peptide data. Here, we introduce PepCompass, a geometry-aware\nframework for peptide exploration and optimization. At its core, we define a\nUnion of $\\kappa$-Stable Riemannian Manifolds $\\mathbb{M}^{\\kappa}$, a family\nof decoder-induced manifolds that captures local geometry while ensuring\ncomputational stability. We propose two local exploration methods: Second-Order\nRiemannian Brownian Efficient Sampling, which provides a convergent\nsecond-order approximation to Riemannian Brownian motion, and Mutation\nEnumeration in Tangent Space, which reinterprets tangent directions as discrete\namino-acid substitutions. Combining these yields Local Enumeration Bayesian\nOptimization (LE-BO), an efficient algorithm for local activity optimization.\nFinally, we introduce Potential-minimizing Geodesic Search (PoGS), which\ninterpolates between prototype embeddings along property-enriched geodesics,\nbiasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro\nvalidation confirms the effectiveness of PepCompass: PoGS yields four novel\nseeds, and subsequent optimization with LE-BO discovers 25 highly active\npeptides with broad-spectrum activity, including against resistant bacterial\nstrains. These results demonstrate that geometry-informed exploration provides\na powerful new paradigm for antimicrobial peptide design.\n","authors":["Marcin MoÅ¼ejko","Adam Bielecki","Jurand PrÄdzyÅski","Marcin Traskowski","Antoni Janowski","Hyun-Su Lee","Marcelo Der Torossian Torres","MichaÅ Kmicikiewicz","Paulina Szymczak","Karol Jurasz","MichaÅ Kucharczyk","Cesar de la Fuente-Nunez","Ewa Szczurek"],"pdf_url":"https://arxiv.org/pdf/2510.01988v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17773v3","updated":"2025-10-30T15:43:22Z","published":"2025-05-23T11:44:02Z","title":"C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in\n  Large Language Models","summary":"  Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning\nlarge language models (LLMs), but it often produces overconfident predictions\nin data-scarce few-shot settings. To address this issue, several classical\nstatistical learning approaches have been repurposed for scalable\nuncertainty-aware LoRA fine-tuning. However, these approaches neglect how input\ncharacteristics affect the predictive uncertainty estimates. To address this\nlimitation, we propose Contextual Low-Rank Adaptation (C-LoRA) as a novel\nuncertainty-aware and parameter efficient fine-tuning approach, by developing\nnew lightweight LoRA modules contextualized to each input data sample to\ndynamically adapt uncertainty estimates. Incorporating data-driven contexts\ninto the parameter posteriors, C-LoRA mitigates overfitting, achieves\nwell-calibrated uncertainties, and yields robust predictions. Extensive\nexperiments on LLaMA2-7B models demonstrate that C-LoRA consistently\noutperforms the state-of-the-art uncertainty-aware LoRA methods in both\nuncertainty quantification and model generalization. Ablation studies further\nconfirm the critical role of our contextual modules in capturing\nsample-specific uncertainties. C-LoRA sets a new standard for robust,\nuncertainty-aware LLM fine-tuning in few-shot regimes. Although our experiments\nare limited to 7B models, our method is architecture-agnostic and, in\nprinciple, applies beyond this scale; studying its scaling to larger models\nremains an open problem. Our code is available at\nhttps://github.com/ahra99/c_lora.\n","authors":["Amir Hossein Rahmati","Sanket Jantre","Weifeng Zhang","Yucheng Wang","Byung-Jun Yoon","Nathan M. Urban","Xiaoning Qian"],"pdf_url":"https://arxiv.org/pdf/2505.17773v3.pdf","comment":"Conference on Neural Information Processing Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.26616v1","updated":"2025-10-30T15:41:43Z","published":"2025-10-30T15:41:43Z","title":"Aeolus: A Multi-structural Flight Delay Dataset","summary":"  We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designed\nto advance research on flight delay prediction and support the development of\nfoundation models for tabular data. Existing datasets in this domain are\ntypically limited to flat tabular structures and fail to capture the\nspatiotemporal dynamics inherent in delay propagation. Aeolus addresses this\nlimitation by providing three aligned modalities: (i) a tabular dataset with\nrich operational, meteorological, and airportlevel features for over 50 million\nflights; (ii) a flight chain module that models delay propagation along\nsequential flight legs, capturing upstream and downstream dependencies; and\n(iii) a flight network graph that encodes shared aircraft, crew, and airport\nresource connections, enabling cross-flight relational reasoning. The dataset\nis carefully constructed with temporal splits, comprehensive features, and\nstrict leakage prevention to support realistic and reproducible machine\nlearning evaluation. Aeolus supports a broad range of tasks, including\nregression, classification, temporal structure modeling, and graph learning,\nserving as a unified benchmark across tabular, sequential, and graph\nmodalities. We release baseline experiments and preprocessing tools to\nfacilitate adoption. Aeolus fills a key gap for both domain-specific modeling\nand general-purpose structured data research.Our source code and data can be\naccessed at https://github.com/Flnny/Delay-data\n","authors":["Lin Xu","Xinyun Yuan","Yuxuan Liang","Suwan Yin","Yuankai Wu"],"pdf_url":"https://arxiv.org/pdf/2510.26616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.14904v2","updated":"2025-10-30T15:39:25Z","published":"2025-10-16T17:20:22Z","title":"MaskCaptioner: Learning to Jointly Segment and Caption Object\n  Trajectories in Videos","summary":"  Dense Video Object Captioning (DVOC) is the task of jointly detecting,\ntracking, and captioning object trajectories in a video, requiring the ability\nto understand spatio-temporal details and describe them in natural language.\nDue to the complexity of the task and the high cost associated with manual\nannotation, previous approaches resort to disjoint training strategies,\npotentially leading to suboptimal performance. To circumvent this issue, we\npropose to generate captions about spatio-temporally localized entities\nleveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets\nwith our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an\nend-to-end model capable of jointly detecting, segmenting, tracking and\ncaptioning object trajectories. Moreover, with pretraining on LVISCap and\nLV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three\nexisting benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are\navailable at https://www.gabriel.fiastre.fr/maskcaptioner/.\n","authors":["Gabriel Fiastre","Antoine Yang","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2510.14904v2.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.26609v1","updated":"2025-10-30T15:37:40Z","published":"2025-10-30T15:37:40Z","title":"CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for\n  Satellite Sensing","summary":"  Accurate and timely crop yield prediction is crucial for global food security\nand modern agricultural management. Traditional methods often lack the\nscalability and granularity required for precision farming. This paper\nintroduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder\nfor Satellite Sensing), a deep learning model designed for high-resolution,\nintra-field canola yield prediction. CYPRESS leverages a pre-trained,\nlarge-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for\na continuous regression task, transforming multi-temporal satellite imagery\ninto dense, pixel-level yield maps. Evaluated on a comprehensive dataset from\nthe Canadian Prairies, CYPRESS demonstrates superior performance over existing\ndeep learning-based yield prediction models, highlighting the effectiveness of\nfine-tuning foundation models for specialized agricultural applications. By\nproviding a continuous, high-resolution output, CYPRESS offers a more\nactionable tool for precision agriculture than conventional classification or\ncounty-level aggregation methods. This work validates a novel approach that\nbridges the gap between large-scale Earth observation and on-farm\ndecision-making, offering a scalable solution for detailed agricultural\nmonitoring.\n","authors":["Shayan Nejadshamsi","Yuanyuan Zhang","Shadi Zaki","Brock Porth","Lysa Porth","Vahab Khoshdel"],"pdf_url":"https://arxiv.org/pdf/2510.26609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26607v1","updated":"2025-10-30T15:36:39Z","published":"2025-10-30T15:36:39Z","title":"Wasserstein Regression as a Variational Approximation of Probabilistic\n  Trajectories through the Bernstein Basis","summary":"  This paper considers the problem of regression over distributions, which is\nbecoming increasingly important in machine learning. Existing approaches often\nignore the geometry of the probability space or are computationally expensive.\nTo overcome these limitations, a new method is proposed that combines the\nparameterization of probability trajectories using a Bernstein basis and the\nminimization of the Wasserstein distance between distributions. The key idea is\nto model a conditional distribution as a smooth probability trajectory defined\nby a weighted sum of Gaussian components whose parameters -- the mean and\ncovariance -- are functions of the input variable constructed using Bernstein\npolynomials. The loss function is the averaged squared Wasserstein distance\nbetween the predicted Gaussian distributions and the empirical data, which\ntakes into account the geometry of the distributions. An autodiff-based\noptimization method is used to train the model. Experiments on synthetic\ndatasets that include complex trajectories demonstrated that the proposed\nmethod provides competitive approximation quality in terms of the Wasserstein\ndistance, Energy Distance, and RMSE metrics, especially in cases of pronounced\nnonlinearity. The model demonstrates trajectory smoothness that is better than\nor comparable to alternatives and robustness to changes in data structure,\nwhile maintaining high interpretability due to explicit parameterization via\ncontrol points. The developed approach represents a balanced solution that\ncombines geometric accuracy, computational practicality, and interpretability.\nProspects for further research include extending the method to non-Gaussian\ndistributions, applying entropy regularization to speed up computations, and\nadapting the approach to working with high-dimensional data for approximating\nsurfaces and more complex structures.\n","authors":["Maksim Maslov","Alexander Kugaevskikh","Matthew Ivanov"],"pdf_url":"https://arxiv.org/pdf/2510.26607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.08604v2","updated":"2025-10-30T15:33:58Z","published":"2025-10-07T09:40:20Z","title":"LatentBreak: Jailbreaking Large Language Models through Latent Space\n  Feedback","summary":"  Jailbreaks are adversarial attacks designed to bypass the built-in safety\nmechanisms of large language models. Automated jailbreaks typically optimize an\nadversarial suffix or adapt long prompt templates by forcing the model to\ngenerate the initial part of a restricted or harmful response. In this work, we\nshow that existing jailbreak attacks that leverage such mechanisms to unlock\nthe model response can be detected by a straightforward perplexity-based\nfiltering on the input prompt. To overcome this issue, we propose LatentBreak,\na white-box jailbreak attack that generates natural adversarial prompts with\nlow perplexity capable of evading such defenses. LatentBreak substitutes words\nin the input prompt with semantically-equivalent ones, preserving the initial\nintent of the prompt, instead of adding high-perplexity adversarial suffixes or\nlong templates. These words are chosen by minimizing the distance in the latent\nspace between the representation of the adversarial prompt and that of harmless\nrequests. Our extensive evaluation shows that LatentBreak leads to shorter and\nlow-perplexity prompts, thus outperforming competing jailbreak algorithms\nagainst perplexity-based filters on multiple safety-aligned models.\n","authors":["Raffaele Mura","Giorgio Piras","KamilÄ LukoÅ¡iÅ«tÄ","Maura Pintor","Amin Karbasi","Battista Biggio"],"pdf_url":"https://arxiv.org/pdf/2510.08604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.17197v2","updated":"2025-10-30T15:26:13Z","published":"2025-09-21T18:54:54Z","title":"SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal\n  Processing","summary":"  Modern signal processing (SP) pipelines, whether model-based or data-driven,\noften constrained by complex and fragmented workflow, rely heavily on expert\nknowledge and manual engineering, and struggle with adaptability and\ngeneralization under limited data. In contrast, Large Language Models (LLMs)\noffer strong reasoning capabilities, broad general-purpose knowledge,\nin-context learning, and cross-modal transfer abilities, positioning them as\npowerful tools for automating and generalizing SP workflows. Motivated by these\npotentials, we introduce SignalLLM, the first general-purpose LLM-based agent\nframework for general SP tasks. Unlike prior LLM-based SP approaches that are\nlimited to narrow applications or tricky prompting, SignalLLM introduces a\nprincipled, modular architecture. It decomposes high-level SP goals into\nstructured subtasks via in-context learning and domain-specific retrieval,\nfollowed by hierarchical planning through adaptive retrieval-augmented\ngeneration (RAG) and refinement; these subtasks are then executed through\nprompt-based reasoning, cross-modal reasoning, code synthesis, model\ninvocation, or data-driven LLM-assisted modeling. Its generalizable design\nenables the flexible selection of problem solving strategies across different\nsignal modalities, task types, and data conditions. We demonstrate the\nversatility and effectiveness of SignalLLM through five representative tasks in\ncommunication and sensing, such as radar target detection, human activity\nrecognition, and text compression. Experimental results show superior\nperformance over traditional and existing LLM-based methods, particularly in\nfew-shot and zero-shot settings.\n","authors":["Junlong Ke","Qiying Hu","Shenghai Yuan","Yuecong Xu","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2509.17197v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2510.26593v1","updated":"2025-10-30T15:18:07Z","published":"2025-10-30T15:18:07Z","title":"Hybrid Physical-Neural Simulator for Fast Cosmological Hydrodynamics","summary":"  Cosmological field-level inference requires differentiable forward models\nthat solve the challenging dynamics of gas and dark matter under hydrodynamics\nand gravity. We propose a hybrid approach where gravitational forces are\ncomputed using a differentiable particle-mesh solver, while the hydrodynamics\nare parametrized by a neural network that maps local quantities to an effective\npressure field. We demonstrate that our method improves upon alternative\napproaches, such as an Enthalpy Gradient Descent baseline, both at the field\nand summary-statistic level. The approach is furthermore highly data efficient,\nwith a single reference simulation of cosmological structure formation being\nsufficient to constrain the neural pressure model. This opens the door for\nfuture applications where the model is fit directly to observational data,\nrather than a training set of simulations.\n","authors":["Arne Thomsen","Tilman TrÃ¶ster","FranÃ§ois Lanusse"],"pdf_url":"https://arxiv.org/pdf/2510.26593v1.pdf","comment":"Accepted to the NeurIPS 2025 Workshop on Machine Learning and the\n  Physical Sciences"},{"id":"http://arxiv.org/abs/2510.26586v1","updated":"2025-10-30T15:13:25Z","published":"2025-10-30T15:13:25Z","title":"Physics-Informed Mixture Models and Surrogate Models for Precision\n  Additive Manufacturing","summary":"  In this study, we leverage a mixture model learning approach to identify\ndefects in laser-based Additive Manufacturing (AM) processes. By incorporating\nphysics based principles, we also ensure that the model is sensitive to\nmeaningful physical parameter variations. The empirical evaluation was\nconducted by analyzing real-world data from two AM processes: Directed Energy\nDeposition and Laser Powder Bed Fusion. In addition, we also studied the\nperformance of the developed framework over public datasets with different\nalloy type and experimental parameter information. The results show the\npotential of physics-guided mixture models to examine the underlying physical\nbehavior of an AM system.\n","authors":["Sebastian Basterrech","Shuo Shan","Debabrata Adhikari","Sankhya Mohanty"],"pdf_url":"https://arxiv.org/pdf/2510.26586v1.pdf","comment":"Five pages, four figures, to be presented at the AI in Science\n  Summit, Denmark, November, 2025"},{"id":"http://arxiv.org/abs/2410.06324v4","updated":"2025-10-30T15:07:47Z","published":"2024-10-08T20:01:39Z","title":"Differentiation Through Black-Box Quadratic Programming Solvers","summary":"  Differentiable optimization has attracted significant research interest,\nparticularly for quadratic programming (QP). Existing approaches for\ndifferentiating the solution of a QP with respect to its defining parameters\noften rely on specific integrated solvers. This integration limits their\napplicability, including their use in neural network architectures and bi-level\noptimization tasks, restricting users to a narrow selection of solver choices.\nTo address this limitation, we introduce dQP, a modular and solver-agnostic\nframework for plug-and-play differentiation of virtually any QP solver. A key\ninsight we leverage to achieve modularity is that, once the active set of\ninequality constraints is known, both the solution and its derivative can be\nexpressed using simplified linear systems that share the same matrix. This\nformulation fully decouples the computation of the QP solution from its\ndifferentiation. Building on this result, we provide a minimal-overhead,\nopen-source implementation ( https://github.com/cwmagoon/dQP ) that seamlessly\nintegrates with over 15 state-of-the-art solvers. Comprehensive benchmark\nexperiments demonstrate dQP's robustness and scalability, particularly\nhighlighting its advantages in large-scale sparse problems.\n","authors":["Connor W. Magoon","Fengyu Yang","Noam Aigerman","Shahar Z. Kovalsky"],"pdf_url":"https://arxiv.org/pdf/2410.06324v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26577v1","updated":"2025-10-30T15:04:36Z","published":"2025-10-30T15:04:36Z","title":"Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference\n  in Large Language Models","summary":"  Large Language Models (LLMs) face significant inference latency challenges\nstemming from their autoregressive design and large size. To address this,\nspeculative decoding emerges as a solution, enabling the simultaneous\ngeneration and validation of multiple tokens. While recent approaches like\nEAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures,\nthey often neglect the impact of crucial system variables such as GPU devices\nand batch sizes.\n  Therefore, we introduce a new dynamic tree decoding approach called CAST that\ntakes into account inference costs, including factors such as GPU\nconfigurations and batch sizes, to dynamically refine the tree structure.\nThrough comprehensive experimentation across six diverse tasks and utilizing\nsix distinct LLMs, our methodology demonstrates remarkable results, achieving\nspeeds up to 5.2 times faster than conventional decoding methods. Moreover, it\ngenerally outperforms existing state-of-the-art techniques from 5% to 20%.\n","authors":["Yinrong Hong","Zhiquan Tan","Kai Hu"],"pdf_url":"https://arxiv.org/pdf/2510.26577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05978v2","updated":"2025-10-30T15:02:26Z","published":"2025-04-08T12:33:38Z","title":"Smart Exploration in Reinforcement Learning using Bounded Uncertainty\n  Models","summary":"  Reinforcement learning (RL) is a powerful framework for decision-making in\nuncertain environments, but it often requires large amounts of data to learn an\noptimal policy. We address this challenge by incorporating prior model\nknowledge to guide exploration and accelerate the learning process.\nSpecifically, we assume access to a model set that contains the true transition\nkernel and reward function. We optimize over this model set to obtain upper and\nlower bounds on the Q-function, which are then used to guide the exploration of\nthe agent. We provide theoretical guarantees on the convergence of the\nQ-function to the optimal Q-function under the proposed class of exploring\npolicies. Furthermore, we also introduce a data-driven regularized version of\nthe model set optimization problem that ensures the convergence of the class of\nexploring policies to the optimal policy. Lastly, we show that when the model\nset has a specific structure, namely the bounded-parameter MDP (BMDP)\nframework, the regularized model set optimization problem becomes convex and\nsimple to implement. In this setting, we also prove finite-time convergence to\nthe optimal policy under mild assumptions. We demonstrate the effectiveness of\nthe proposed exploration strategy, which we call BUMEX (Bounded Uncertainty\nModel-based Exploration), in a simulation study. The results indicate that the\nproposed method can significantly accelerate learning in benchmark examples. A\ntoolbox is available at https://github.com/JvHulst/BUMEX.\n","authors":["J. S. van Hulst","W. P. M. H. Heemels","D. J. Antunes"],"pdf_url":"https://arxiv.org/pdf/2504.05978v2.pdf","comment":"Accepted for Presentation at 64th IEEE Conference on Decision and\n  Control, CDC 2025, Rio de Janeiro, Brazil, 2025"},{"id":"http://arxiv.org/abs/2510.26566v1","updated":"2025-10-30T14:56:07Z","published":"2025-10-30T14:56:07Z","title":"Multiclass Local Calibration With the Jensen-Shannon Distance","summary":"  Developing trustworthy Machine Learning (ML) models requires their predicted\nprobabilities to be well-calibrated, meaning they should reflect true-class\nfrequencies. Among calibration notions in multiclass classification, strong\ncalibration is the most stringent, as it requires all predicted probabilities\nto be simultaneously calibrated across all classes. However, existing\napproaches to multiclass calibration lack a notion of distance among inputs,\nwhich makes them vulnerable to proximity bias: predictions in sparse regions of\nthe feature space are systematically miscalibrated. This is especially relevant\nin high-stakes settings, such as healthcare, where the sparse instances are\nexactly those most at risk of biased treatment. In this work, we address this\nmain shortcoming by introducing a local perspective on multiclass calibration.\nFirst, we formally define multiclass local calibration and establish its\nrelationship with strong calibration. Second, we theoretically analyze the\npitfalls of existing evaluation metrics when applied to multiclass local\ncalibration. Third, we propose a practical method for enhancing local\ncalibration in Neural Networks, which enforces alignment between predicted\nprobabilities and local estimates of class frequencies using the Jensen-Shannon\ndistance. Finally, we empirically validate our approach against existing\nmulticlass calibration techniques.\n","authors":["Cesare Barbera","Lorenzo Perini","Giovanni De Toni","Andrea Passerini","Andrea Pugnana"],"pdf_url":"https://arxiv.org/pdf/2510.26566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.04226v4","updated":"2025-10-30T14:52:48Z","published":"2025-10-05T14:29:15Z","title":"Epistemic Diversity and Knowledge Collapse in Large Language Models","summary":"  Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation\n","authors":["Dustin Wright","Sarah Masud","Jared Moore","Srishti Yadav","Maria Antoniak","Chan Young Park","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2510.04226v4.pdf","comment":"16 pages; 8 figures, 4 tables; v2 changelog: Fixed the modeling for\n  table 3, random effect is the model version; v3 changelog: Fixed minor\n  formatting issues in tables 2 and 3; v4 changelog: Fixed some typos and model\n  description"},{"id":"http://arxiv.org/abs/2510.26560v1","updated":"2025-10-30T14:51:03Z","published":"2025-10-30T14:51:03Z","title":"On Measuring Localization of Shortcuts in Deep Networks","summary":"  Shortcuts, spurious rules that perform well during training but fail to\ngeneralize, present a major challenge to the reliability of deep networks\n(Geirhos et al., 2020). However, the impact of shortcuts on feature\nrepresentations remains understudied, obstructing the design of principled\nshortcut-mitigation methods. To overcome this limitation, we investigate the\nlayer-wise localization of shortcuts in deep models. Our novel experiment\ndesign quantifies the layer-wise contribution to accuracy degradation caused by\na shortcut-inducing skew by counterfactual training on clean and skewed\ndatasets. We employ our design to study shortcuts on CIFAR-10, Waterbirds, and\nCelebA datasets across VGG, ResNet, DeiT, and ConvNeXt architectures. We find\nthat shortcut learning is not localized in specific layers but distributed\nthroughout the network. Different network parts play different roles in this\nprocess: shallow layers predominantly encode spurious features, while deeper\nlayers predominantly forget core features that are predictive on clean data. We\nalso analyze the differences in localization and describe its principal axes of\nvariation. Finally, our analysis of layer-wise shortcut-mitigation strategies\nsuggests the hardness of designing general methods, supporting dataset- and\narchitecture-specific approaches instead.\n","authors":["Nikita Tsoy","Nikola Konstantinov"],"pdf_url":"https://arxiv.org/pdf/2510.26560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26557v1","updated":"2025-10-30T14:47:57Z","published":"2025-10-30T14:47:57Z","title":"Boosted Trees on a Diet: Compact Models for Resource-Constrained Devices","summary":"  Deploying machine learning models on compute-constrained devices has become a\nkey building block of modern IoT applications. In this work, we present a\ncompression scheme for boosted decision trees, addressing the growing need for\nlightweight machine learning models. Specifically, we provide techniques for\ntraining compact boosted decision tree ensembles that exhibit a reduced memory\nfootprint by rewarding, among other things, the reuse of features and\nthresholds during training. Our experimental evaluation shows that models\nachieved the same performance with a compression ratio of 4-16x compared to\nLightGBM models using an adapted training process and an alternative memory\nlayout. Once deployed, the corresponding IoT devices can operate independently\nof constant communication or external energy supply, and, thus, autonomously,\nrequiring only minimal computing power and energy. This capability opens the\ndoor to a wide range of IoT applications, including remote monitoring, edge\nanalytics, and real-time decision making in isolated or power-limited\nenvironments.\n","authors":["Jan Stenkamp","Nina Herrmann","Benjamin Karic","Stefan Oehmcke","Fabian Gieseke"],"pdf_url":"https://arxiv.org/pdf/2510.26557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01369v2","updated":"2025-10-30T14:45:40Z","published":"2025-06-02T06:54:29Z","title":"Incentivizing LLMs to Self-Verify Their Answers","summary":"  Large Language Models (LLMs) have demonstrated remarkable progress in complex\nreasoning tasks through both post-training and test-time scaling laws. While\nprevalent test-time scaling approaches are often realized by using external\nreward models to guide the model generation process, we find that only marginal\ngains can be acquired when scaling a model post-trained on specific reasoning\ntasks. We identify that the limited improvement stems from distribution\ndiscrepancies between the specific post-trained generator and the general\nreward model. To address this, we propose a framework that incentivizes LLMs to\nself-verify their own answers. By unifying answer generation and verification\nwithin a single reinforcement learning (RL) process, we train models that can\neffectively assess the correctness of their own solutions. The trained model\ncan further scale its performance at inference time by verifying its\ngenerations, without the need for external verifiers. We train our\nself-verification models based on Qwen2.5-Math-7B and\nDeepSeek-R1-Distill-Qwen-1.5B, demonstrating their capabilities across varying\nreasoning context lengths. Experiments on multiple mathematical reasoning\nbenchmarks show that our models can not only improve post-training performance\nbut also enable effective test-time scaling.\n","authors":["Fuxiang Zhang","Jiacheng Xu","Chaojie Wang","Ce Cui","Yang Liu","Bo An"],"pdf_url":"https://arxiv.org/pdf/2506.01369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23216v3","updated":"2025-10-30T14:45:38Z","published":"2025-10-27T11:06:00Z","title":"Human-Like Goalkeeping in a Realistic Football Simulation: a\n  Sample-Efficient Reinforcement Learning Approach","summary":"  While several high profile video games have served as testbeds for Deep\nReinforcement Learning (DRL), this technique has rarely been employed by the\ngame industry for crafting authentic AI behaviors. Previous research focuses on\ntraining super-human agents with large models, which is impractical for game\nstudios with limited resources aiming for human-like agents. This paper\nproposes a sample-efficient DRL method tailored for training and fine-tuning\nagents in industrial settings such as the video game industry. Our method\nimproves sample efficiency of value-based DRL by leveraging pre-collected data\nand increasing network plasticity. We evaluate our method training a goalkeeper\nagent in EA SPORTS FC 25, one of the best-selling football simulations today.\nOur agent outperforms the game's built-in AI by 10% in ball saving rate.\nAblation studies show that our method trains agents 50% faster compared to\nstandard DRL methods. Finally, qualitative evaluation from domain experts\nindicates that our approach creates more human-like gameplay compared to\nhand-crafted agents. As a testament to the impact of the approach, the method\nhas been adopted for use in the most recent release of the series.\n","authors":["Alessandro Sestini","Joakim Bergdahl","Jean-Philippe Barrette-LaPierre","Florian Fuchs","Brady Chen","Michael Jones","Linus GisslÃ©n"],"pdf_url":"https://arxiv.org/pdf/2510.23216v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26551v1","updated":"2025-10-30T14:44:24Z","published":"2025-10-30T14:44:24Z","title":"Adaptive Inverse Kinematics Framework for Learning Variable-Length Tool\n  Manipulation in Robotics","summary":"  Conventional robots possess a limited understanding of their kinematics and\nare confined to preprogrammed tasks, hindering their ability to leverage tools\nefficiently. Driven by the essential components of tool usage - grasping the\ndesired outcome, selecting the most suitable tool, determining optimal tool\norientation, and executing precise manipulations - we introduce a pioneering\nframework. Our novel approach expands the capabilities of the robot's inverse\nkinematics solver, empowering it to acquire a sequential repertoire of actions\nusing tools of varying lengths. By integrating a simulation-learned action\ntrajectory with the tool, we showcase the practicality of transferring acquired\nskills from simulation to real-world scenarios through comprehensive\nexperimentation. Remarkably, our extended inverse kinematics solver\ndemonstrates an impressive error rate of less than 1 cm. Furthermore, our\ntrained policy achieves a mean error of 8 cm in simulation. Noteworthy, our\nmodel achieves virtually indistinguishable performance when employing two\ndistinct tools of different lengths. This research provides an indication of\npotential advances in the exploration of all four fundamental aspects of tool\nusage, enabling robots to master the intricate art of tool manipulation across\ndiverse tasks.\n","authors":["Prathamesh Kothavale","Sravani Boddepalli"],"pdf_url":"https://arxiv.org/pdf/2510.26551v1.pdf","comment":"10 pages, 5 figures. Demonstrates a reinforcement learning framework\n  for adaptive tool manipulation with variable-length extensions"},{"id":"http://arxiv.org/abs/2510.26543v1","updated":"2025-10-30T14:36:09Z","published":"2025-10-30T14:36:09Z","title":"The Structure of Relation Decoding Linear Operators in Large Language\n  Models","summary":"  This paper investigates the structure of linear operators introduced in\nHernandez et al. [2023] that decode specific relational facts in transformer\nlanguage models. We extend their single-relation findings to a collection of\nrelations and systematically chart their organization. We show that such\ncollections of relation decoders can be highly compressed by simple order-3\ntensor networks without significant loss in decoding accuracy. To explain this\nsurprising redundancy, we develop a cross-evaluation protocol, in which we\napply each linear decoder operator to the subjects of every other relation. Our\nresults reveal that these linear maps do not encode distinct relations, but\nextract recurring, coarse-grained semantic properties (e.g., country of capital\ncity and country of food are both in the country-of-X property). This\nproperty-centric structure clarifies both the operators' compressibility and\nhighlights why they generalize only to new relations that are semantically\nclose. Our findings thus interpret linear relational decoding in transformer\nlanguage models as primarily property-based, rather than relation-specific.\n","authors":["Miranda Anna Christ","AdriÃ¡n CsiszÃ¡rik","Gergely BecsÃ³","DÃ¡niel Varga"],"pdf_url":"https://arxiv.org/pdf/2510.26543v1.pdf","comment":"NeurIPS 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2510.26541v1","updated":"2025-10-30T14:30:53Z","published":"2025-10-30T14:30:53Z","title":"A Three-Stage Bayesian Transfer Learning Framework to Improve\n  Predictions in Data-Scarce Domains","summary":"  The use of ML in engineering has grown steadily to support a wide array of\napplications. Among these methods, deep neural networks have been widely\nadopted due to their performance and accessibility, but they require large,\nhigh-quality datasets. Experimental data are often sparse, noisy, or\ninsufficient to build resilient data-driven models. Transfer learning, which\nleverages relevant data-abundant source domains to assist learning in\ndata-scarce target domains, has shown efficacy. Parameter transfer, where\npretrained weights are reused, is common but degrades under large domain\nshifts. Domain-adversarial neural networks (DANNs) help address this issue by\nlearning domain-invariant representations, thereby improving transfer under\ngreater domain shifts in a semi-supervised setting. However, DANNs can be\nunstable during training and lack a native means for uncertainty\nquantification. This study introduces a fully-supervised three-stage framework,\nthe staged Bayesian domain-adversarial neural network (staged B-DANN), that\ncombines parameter transfer and shared latent space adaptation. In Stage 1, a\ndeterministic feature extractor is trained on the source domain. This feature\nextractor is then adversarially refined using a DANN in Stage 2. In Stage 3, a\nBayesian neural network is built on the adapted feature extractor for\nfine-tuning on the target domain to handle conditional shifts and yield\ncalibrated uncertainty estimates. This staged B-DANN approach was first\nvalidated on a synthetic benchmark, where it was shown to significantly\noutperform standard transfer techniques. It was then applied to the task of\npredicting critical heat flux in rectangular channels, leveraging data from\ntube experiments as the source domain. The results of this study show that the\nstaged B-DANN method can improve predictive accuracy and generalization,\npotentially assisting other domains in nuclear engineering.\n","authors":["Aidan Furlong","Robert Salko","Xingang Zhao","Xu Wu"],"pdf_url":"https://arxiv.org/pdf/2510.26541v1.pdf","comment":"Submitted to Engineering Applications of Artificial Intelligence"},{"id":"http://arxiv.org/abs/2406.08525v2","updated":"2025-10-30T14:25:46Z","published":"2024-06-12T07:33:38Z","title":"A mathematical certification for positivity conditions in Neural\n  Networks with applications to partial monotonicity and Trustworthy AI","summary":"  Artificial Neural Networks (ANNs) have become a powerful tool for modeling\ncomplex relationships in large-scale datasets. However, their black-box nature\nposes trustworthiness challenges. In certain situations, ensuring trust in\npredictions might require following specific partial monotonicity constraints.\nHowever, certifying if an already-trained ANN is partially monotonic is\nchallenging. Therefore, ANNs are often disregarded in some critical\napplications, such as credit scoring, where partial monotonicity is required.\nTo address this challenge, this paper presents a novel algorithm (LipVor) that\ncertifies if a black-box model, such as an ANN, is positive based on a finite\nnumber of evaluations. Consequently, since partial monotonicity can be\nexpressed as a positivity condition on partial derivatives, LipVor can certify\nwhether an ANN is partially monotonic. To do so, for every positively evaluated\npoint, the Lipschitzianity of the black-box model is used to construct a\nspecific neighborhood where the function remains positive. Next, based on the\nVoronoi diagram of the evaluated points, a sufficient condition is stated to\ncertify if the function is positive in the domain. Unlike prior methods, our\napproach certifies partial monotonicity without constrained architectures or\npiece-wise linear activations. Therefore, LipVor could open up the possibility\nof using unconstrained ANN in some critical fields. Moreover, some other\nproperties of an ANN, such as convexity, can be posed as positivity conditions,\nand therefore, LipVor could also be applied.\n","authors":["Alejandro Polo-Molina","David Alfaya","Jose Portela"],"pdf_url":"https://arxiv.org/pdf/2406.08525v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2510.26533v1","updated":"2025-10-30T14:22:57Z","published":"2025-10-30T14:22:57Z","title":"Higher-Order Regularization Learning on Hypergraphs","summary":"  Higher-Order Hypergraph Learning (HOHL) was recently introduced as a\nprincipled alternative to classical hypergraph regularization, enforcing\nhigher-order smoothness via powers of multiscale Laplacians induced by the\nhypergraph structure. Prior work established the well- and ill-posedness of\nHOHL through an asymptotic consistency analysis in geometric settings. We\nextend this theoretical foundation by proving the consistency of a truncated\nversion of HOHL and deriving explicit convergence rates when HOHL is used as a\nregularizer in fully supervised learning. We further demonstrate its strong\nempirical performance in active learning and in datasets lacking an underlying\ngeometric structure, highlighting HOHL's versatility and robustness across\ndiverse learning settings.\n","authors":["Adrien Weihs","Andrea Bertozzi","Matthew Thorpe"],"pdf_url":"https://arxiv.org/pdf/2510.26533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26527v1","updated":"2025-10-30T14:20:24Z","published":"2025-10-30T14:20:24Z","title":"Polybasic Speculative Decoding Through a Theoretical Perspective","summary":"  Inference latency stands as a critical bottleneck in the large-scale\ndeployment of Large Language Models (LLMs). Speculative decoding methods have\nrecently shown promise in accelerating inference without compromising the\noutput distribution. However, existing work typically relies on a dualistic\ndraft-verify framework and lacks rigorous theoretical grounding. In this paper,\nwe introduce a novel \\emph{polybasic} speculative decoding framework,\nunderpinned by a comprehensive theoretical analysis. Specifically, we prove a\nfundamental theorem that characterizes the optimal inference time for\nmulti-model speculative decoding systems, shedding light on how to extend\nbeyond the dualistic approach to a more general polybasic paradigm. Through our\ntheoretical investigation of multi-model token generation, we expose and\noptimize the interplay between model capabilities, acceptance lengths, and\noverall computational cost. Our framework supports both standalone\nimplementation and integration with existing speculative techniques, leading to\naccelerated performance in practice. Experimental results across multiple model\nfamilies demonstrate that our approach yields speedup ratios ranging from\n$3.31\\times$ to $4.01\\times$ for LLaMA2-Chat 7B, up to $3.87 \\times$ for\nLLaMA3-8B, up to $4.43 \\times$ for Vicuna-7B and up to $3.85 \\times$ for\nQwen2-7B -- all while preserving the original output distribution. We release\nour theoretical proofs and implementation code to facilitate further\ninvestigation into polybasic speculative decoding.\n","authors":["Ruilin Wang","Huixia Li","Yuexiao Ma","Xiawu Zheng","Fei Chao","Xuefeng Xiao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2510.26527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26519v1","updated":"2025-10-30T14:14:15Z","published":"2025-10-30T14:14:15Z","title":"Think Outside the Policy: In-Context Steered Policy Optimization","summary":"  Existing Reinforcement Learning from Verifiable Rewards (RLVR) methods, such\nas Group Relative Policy Optimization (GRPO), have achieved remarkable progress\nin improving the reasoning capabilities of Large Reasoning Models (LRMs).\nHowever, they exhibit limited exploration due to reliance on on-policy rollouts\nwhere confined to the current policy's distribution, resulting in narrow\ntrajectory diversity. Recent approaches attempt to expand policy coverage by\nincorporating trajectories generated from stronger expert models, yet this\nreliance increases computational cost and such advaned models are often\ninaccessible. To address these issues, we propose In-Context Steered Policy\nOptimization (ICPO), a unified framework that leverages the inherent in-context\nlearning capability of LRMs to provide expert guidance using existing datasets.\nICPO introduces Mixed-Policy GRPO with Implicit Expert Forcing, which expands\nexploration beyond the current policy distribution without requiring advanced\nLRM trajectories. To further stabilize optimization, ICPO integrates Expert\nRegion Reject Sampling to filter unreliable off-policy trajectories and\nAnnealed Expert-Bonus Reward Shaping to balance early expert guidance with\nlater autonomous improvement. Results demonstrate that ICPO consistently\nenhances reinforcement learning performance and training stability on\nmathematical reasoning benchmarks, revealing a scalable and effective RLVR\nparadigm for LRMs.\n","authors":["Hsiu-Yuan Huang","Chenming Tang","Weijie Liu","Saiyong Yang","Yunfang Wu"],"pdf_url":"https://arxiv.org/pdf/2510.26519v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2510.26512v1","updated":"2025-10-30T14:05:55Z","published":"2025-10-30T14:05:55Z","title":"Inside CORE-KG: Evaluating Structured Prompting and Coreference\n  Resolution for Knowledge Graphs","summary":"  Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer critical insights but are often unstructured,\nlexically dense, and filled with ambiguous or shifting references, which pose\nsignificant challenges for automated knowledge graph (KG) construction. While\nrecent LLM-based approaches improve over static templates, they still generate\nnoisy, fragmented graphs with duplicate nodes due to the absence of guided\nextraction and coreference resolution. The recently proposed CORE-KG framework\naddresses these limitations by integrating a type-aware coreference module and\ndomain-guided structured prompts, significantly reducing node duplication and\nlegal noise. In this work, we present a systematic ablation study of CORE-KG to\nquantify the individual contributions of its two key components. Our results\nshow that removing coreference resolution results in a 28.32% increase in node\nduplication and a 4.32% increase in noisy nodes, while removing structured\nprompts leads to a 4.34% increase in node duplication and a 73.33% increase in\nnoisy nodes. These findings offer empirical insights for designing robust\nLLM-based pipelines for extracting structured representations from complex\nlegal texts.\n","authors":["Dipak Meher","Carlotta Domeniconi"],"pdf_url":"https://arxiv.org/pdf/2510.26512v1.pdf","comment":"ICDM 2025 Workshop"},{"id":"http://arxiv.org/abs/2510.26510v1","updated":"2025-10-30T14:04:25Z","published":"2025-10-30T14:04:25Z","title":"LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection","summary":"  Model and hyperparameter selection are critical but challenging in machine\nlearning, typically requiring expert intuition or expensive automated search.\nWe investigate whether large language models (LLMs) can act as in-context\nmeta-learners for this task. By converting each dataset into interpretable\nmetadata, we prompt an LLM to recommend both model families and\nhyperparameters. We study two prompting strategies: (1) a zero-shot mode\nrelying solely on pretrained knowledge, and (2) a meta-informed mode augmented\nwith examples of models and their performance on past tasks. Across synthetic\nand real-world benchmarks, we show that LLMs can exploit dataset metadata to\nrecommend competitive models and hyperparameters without search, and that\nimprovements from meta-informed prompting demonstrate their capacity for\nin-context meta-learning. These results highlight a promising new role for LLMs\nas lightweight, general-purpose assistants for model selection and\nhyperparameter optimization.\n","authors":["Youssef Attia El Hili","Albert Thomas","Malik Tiomoko","Abdelhakim Benechehab","Corentin LÃ©ger","Corinne Ancourt","BalÃ¡zs KÃ©gl"],"pdf_url":"https://arxiv.org/pdf/2510.26510v1.pdf","comment":"27 pages, 6 figures"},{"id":"http://arxiv.org/abs/2508.06576v2","updated":"2025-10-30T13:59:28Z","published":"2025-08-07T14:03:23Z","title":"GFlowNets for Learning Better Drug-Drug Interaction Representations","summary":"  Drug-drug interactions pose a significant challenge in clinical pharmacology,\nwith severe class imbalance among interaction types limiting the effectiveness\nof predictive models. Common interactions dominate datasets, while rare but\ncritical interactions remain underrepresented, leading to poor model\nperformance on infrequent cases. Existing methods often treat DDI prediction as\na binary problem, ignoring class-specific nuances and exacerbating bias toward\nfrequent interactions. To address this, we propose a framework combining\nGenerative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE)\nto generate synthetic samples for rare classes, improving model balance and\ngenerate effective and novel DDI pairs. Our approach enhances predictive\nperformance across interaction types, ensuring better clinical reliability.\n","authors":["Azmine Toushik Wasi"],"pdf_url":"https://arxiv.org/pdf/2508.06576v2.pdf","comment":"Accepted to ICANN 2025:AIDD and NeurIPS 2025 Workshop on Structured\n  Probabilistic Inference & Generative Modeling\n  (https://openreview.net/forum?id=LZW1jSgfCI)"},{"id":"http://arxiv.org/abs/2509.22018v2","updated":"2025-10-30T13:57:35Z","published":"2025-09-26T07:51:59Z","title":"Exploring the Early Universe with Deep Learning","summary":"  Hydrogen is the most abundant element in our Universe. The first generation\nof stars and galaxies produced photons that ionized hydrogen gas, driving a\ncosmological event known as the Epoch of Reionization (EoR). The upcoming\nSquare Kilometre Array Observatory (SKAO) will map the distribution of neutral\nhydrogen during this era, aiding in the study of the properties of these\nfirst-generation objects. Extracting astrophysical information will be\nchallenging, as SKAO will produce a tremendous amount of data where the\nhydrogen signal will be contaminated with undesired foreground contamination\nand instrumental systematics. To address this, we develop the latest deep\nlearning techniques to extract information from the 2D power spectra of the\nhydrogen signal expected from SKAO. We apply a series of neural network models\nto these measurements and quantify their ability to predict the history of\ncosmic hydrogen reionization, which is connected to the increasing number and\nefficiency of early photon sources. We show that the study of the early\nUniverse benefits from modern deep learning technology. In particular, we\ndemonstrate that dedicated machine learning algorithms can achieve more than a\n$0.95$ $R^2$ score on average in recovering the reionization history. This\nenables accurate and precise cosmological and astrophysical inference of\nstructure formation in the early Universe.\n","authors":["Emmanuel de Salis","Massimo De Santis","Davide Piras","Sambit K. Giri","Michele Bianco","Nicolas Cerardi","Philipp Denzel","Merve Selcuk-Simsek","Kelley M. Hess","M. Carmen Toribio","Franz Kirsten","Hatem Ghorbel"],"pdf_url":"https://arxiv.org/pdf/2509.22018v2.pdf","comment":"EPIA 2025 preprint version, 12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2510.26501v1","updated":"2025-10-30T13:54:37Z","published":"2025-10-30T13:54:37Z","title":"Enhancing ECG Classification Robustness with Lightweight Unsupervised\n  Anomaly Detection Filters","summary":"  Continuous electrocardiogram (ECG) monitoring via wearables offers\nsignificant potential for early cardiovascular disease (CVD) detection.\nHowever, deploying deep learning models for automated analysis in\nresource-constrained environments faces reliability challenges due to\ninevitable Out-of-Distribution (OOD) data. OOD inputs, such as unseen\npathologies or noisecorrupted signals, often cause erroneous, high-confidence\npredictions by standard classifiers, compromising patient safety. Existing OOD\ndetection methods either neglect computational constraints or address noise and\nunseen classes separately. This paper explores Unsupervised Anomaly Detection\n(UAD) as an independent, upstream filtering mechanism to improve robustness. We\nbenchmark six UAD approaches, including Deep SVDD, reconstruction-based models,\nMasked Anomaly Detection, normalizing flows, and diffusion models, optimized\nvia Neural Architecture Search (NAS) under strict resource constraints (at most\n512k parameters). Evaluation on PTB-XL and BUT QDB datasets assessed detection\nof OOD CVD classes and signals unsuitable for analysis due to noise. Results\nshow Deep SVDD consistently achieves the best trade-off between detection and\nefficiency. In a realistic deployment simulation, integrating the optimized\nDeep SVDD filter with a diagnostic classifier improved accuracy by up to 21\npercentage points over a classifier-only baseline. This study demonstrates that\noptimized UAD filters can safeguard automated ECG analysis, enabling safer,\nmore reliable continuous cardiovascular monitoring on wearables.\n","authors":["Mustafa Fuad Rifet Ibrahim","Maurice Meijer","Alexander Schlaefer","Peer Stelldinger"],"pdf_url":"https://arxiv.org/pdf/2510.26501v1.pdf","comment":"Submitted to the 24th International Conference on Pervasive Computing\n  and Communications (PerCom 2026)"},{"id":"http://arxiv.org/abs/2505.11730v2","updated":"2025-10-30T13:52:37Z","published":"2025-05-16T22:24:48Z","title":"Rethinking Optimal Verification Granularity for Compute-Efficient\n  Test-Time Scaling","summary":"  Test-time scaling (TTS) has proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Verification plays a key role in\nTTS, simultaneously influencing (1) reasoning performance and (2) compute\nefficiency, due to the quality and computational cost of verification. In this\nwork, we challenge the conventional paradigms of verification, and make the\nfirst attempt toward systematically investigating the impact of verification\ngranularity-that is, how frequently the verifier is invoked during generation,\nbeyond verifying only the final output or individual generation steps. To this\nend, we introduce Variable Granularity Search (VG-Search), a unified algorithm\nthat generalizes beam search and Best-of-N sampling via a tunable granularity\nparameter g. Extensive experiments with VG-Search under varying compute\nbudgets, generator-verifier configurations, and task attributes reveal that\ndynamically selecting g can improve the compute efficiency and scaling\nbehavior. Building on these findings, we propose adaptive VG-Search strategies\nthat achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over\nBest-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to\nsupport future research.\n","authors":["Hao Mark Chen","Guanxi Lu","Yasuyuki Okoshi","Zhiwen Mo","Masato Motomura","Hongxiang Fan"],"pdf_url":"https://arxiv.org/pdf/2505.11730v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.17475v3","updated":"2025-10-30T13:42:29Z","published":"2025-06-20T20:46:01Z","title":"A geometric framework for momentum-based optimizers for low-rank\n  training","summary":"  Low-rank pre-training and fine-tuning have recently emerged as promising\ntechniques for reducing the computational and storage costs of large neural\nnetworks. Training low-rank parameterizations typically relies on conventional\noptimizers such as heavy ball momentum methods or Adam. In this work, we\nidentify and analyze potential difficulties that these training methods\nencounter when used to train low-rank parameterizations of weights. In\nparticular, we show that classical momentum methods can struggle to converge to\na local optimum due to the geometry of the underlying optimization landscape.\nTo address this, we introduce novel training strategies derived from dynamical\nlow-rank approximation, which explicitly account for the underlying geometric\nstructure. Our approach leverages and combines tools from dynamical low-rank\napproximation and momentum-based optimization to design optimizers that respect\nthe intrinsic geometry of the parameter space. We validate our methods through\nnumerical experiments, demonstrating faster convergence, and stronger\nvalidation metrics at given parameter budgets.\n","authors":["Steffen SchotthÃ¶fer","Timon Klein","Jonas Kusch"],"pdf_url":"https://arxiv.org/pdf/2506.17475v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26491v1","updated":"2025-10-30T13:40:52Z","published":"2025-10-30T13:40:52Z","title":"Data-Efficient RLVR via Off-Policy Influence Guidance","summary":"  Data selection is a critical aspect of Reinforcement Learning with Verifiable\nRewards (RLVR) for enhancing the reasoning capabilities of large language\nmodels (LLMs). Current data selection methods are largely heuristic-based,\nlacking theoretical guarantees and generalizability. This work proposes a\ntheoretically-grounded approach using influence functions to estimate the\ncontribution of each data point to the learning objective. To overcome the\nprohibitive computational cost of policy rollouts required for online influence\nestimation, we introduce an off-policy influence estimation method that\nefficiently approximates data influence using pre-collected offline\ntrajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we\nemploy sparse random projection to reduce dimensionality and improve storage\nand computation efficiency. Leveraging these techniques, we develop\n\\textbf{C}urriculum \\textbf{R}L with \\textbf{O}ff-\\textbf{P}olicy\n\\text{I}nfluence guidance (\\textbf{CROPI}), a multi-stage RL framework that\niteratively selects the most influential data for the current policy.\nExperiments on models up to 7B parameters demonstrate that CROPI significantly\naccelerates training. On a 1.5B model, it achieves a 2.66x step-level\nacceleration while using only 10\\% of the data per stage compared to\nfull-dataset training. Our results highlight the substantial potential of\ninfluence-based data selection for efficient RLVR.\n","authors":["Erle Zhu","Dazhi Jiang","Yuan Wang","Xujun Li","Jiale Cheng","Yuxian Gu","Yilin Niu","Aohan Zeng","Jie Tang","Minlie Huang","Hongning Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26487v1","updated":"2025-10-30T13:39:44Z","published":"2025-10-30T13:39:44Z","title":"Quantum Gated Recurrent GAN with Gaussian Uncertainty for Network\n  Anomaly Detection","summary":"  Anomaly detection in time-series data is a critical challenge with\nsignificant implications for network security. Recent quantum machine learning\napproaches, such as quantum kernel methods and variational quantum circuits,\nhave shown promise in capturing complex data distributions for anomaly\ndetection but remain constrained by limited qubit counts. We introduce in this\nwork a novel Quantum Gated Recurrent Unit (QGRU)-based Generative Adversarial\nNetwork (GAN) employing Successive Data Injection (SuDaI) and a multi-metric\ngating strategy for robust network anomaly detection. Our model uniquely\nutilizes a quantum-enhanced generator that outputs parameters (mean and\nlog-variance) of a Gaussian distribution via reparameterization, combined with\na Wasserstein critic to stabilize adversarial training. Anomalies are\nidentified through a novel gating mechanism that initially flags potential\nanomalies based on Gaussian uncertainty estimates and subsequently verifies\nthem using a composite of critic scores and reconstruction errors. Evaluated on\nbenchmark datasets, our method achieves a high time-series aware F1 score\n(TaF1) of 89.43% demonstrating superior capability in detecting anomalies\naccurately and promptly as compared to existing classical and quantum models.\nFurthermore, the trained QGRU-WGAN was deployed on real IBM Quantum hardware,\nwhere it retained high anomaly detection performance, confirming its robustness\nand practical feasibility on current noisy intermediate-scale quantum (NISQ)\ndevices.\n","authors":["Wajdi Hammami","Soumaya Cherkaoui","Jean-Frederic Laprade","Ola Ahmad","Shengrui Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26486v1","updated":"2025-10-30T13:39:08Z","published":"2025-10-30T13:39:08Z","title":"LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks","summary":"  Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks.\n","authors":["Dipak Meher","Carlotta Domeniconi","Guadalupe Correa-Cabrera"],"pdf_url":"https://arxiv.org/pdf/2510.26486v1.pdf","comment":"Accepted in ICKG 2025 Conference, 8 Pages, 2 Figures"},{"id":"http://arxiv.org/abs/2507.02843v2","updated":"2025-10-30T13:29:54Z","published":"2025-07-03T17:52:27Z","title":"LLM-Driven Treatment Effect Estimation Under Inference Time Text\n  Confounding","summary":"  Estimating treatment effects is crucial for personalized decision-making in\nmedicine, but this task faces unique challenges in clinical practice. At\ntraining time, models for estimating treatment effects are typically trained on\nwell-structured medical datasets that contain detailed patient information.\nHowever, at inference time, predictions are often made using textual\ndescriptions (e.g., descriptions with self-reported symptoms), which are\nincomplete representations of the original patient information. In this work,\nwe make three contributions. (1) We show that the discrepancy between the data\navailable during training time and inference time can lead to biased estimates\nof treatment effects. We formalize this issue as an inference time text\nconfounding problem, where confounders are fully observed during training time\nbut only partially available through text at inference time. (2) To address\nthis problem, we propose a novel framework for estimating treatment effects\nthat explicitly accounts for inference time text confounding. Our framework\nleverages large language models together with a custom doubly robust learner to\nmitigate biases caused by the inference time text confounding. (3) Through a\nseries of experiments, we demonstrate the effectiveness of our framework in\nreal-world applications.\n","authors":["Yuchen Ma","Dennis Frauen","Jonas Schweisthal","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2507.02843v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26475v1","updated":"2025-10-30T13:27:42Z","published":"2025-10-30T13:27:42Z","title":"ReSpec: Towards Optimizing Speculative Decoding in Reinforcement\n  Learning Systems","summary":"  Adapting large language models (LLMs) via reinforcement learning (RL) is\noften bottlenecked by the generation stage, which can consume over 75\\% of the\ntraining time. Speculative decoding (SD) accelerates autoregressive generation\nin serving systems, but its behavior under RL training remains largely\nunexplored. We identify three critical gaps that hinder the naive integration\nof SD into RL systems: diminishing speedups at large batch sizes, drafter\nstaleness under continual actor updates, and drafter-induced policy\ndegradation.\n  To address these gaps, we present ReSpec, a system that adapts SD to RL\nthrough three complementary mechanisms: dynamically tuning SD configurations,\nevolving the drafter via knowledge distillation, and weighting updates by\nrollout rewards. On Qwen models (3B--14B), ReSpec achieves up to 4.5x speedup\nwhile preserving reward convergence and training stability, providing a\npractical solution for efficient RL-based LLM adaptation.\n","authors":["Qiaoling Chen","Zijun Liu","Peng Sun","Shenggui Li","Guoteng Wang","Ziming Liu","Yonggang Wen","Siyuan Feng","Tianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12371v2","updated":"2025-10-30T13:27:07Z","published":"2025-05-18T11:28:17Z","title":"MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional\n  Methods for Diverse Medical Tasks","summary":"  The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/.\n","authors":["Yinghao Zhu","Ziyi He","Haoran Hu","Xiaochen Zheng","Xichen Zhang","Zixiang Wang","Junyi Gao","Liantao Ma","Lequan Yu"],"pdf_url":"https://arxiv.org/pdf/2505.12371v2.pdf","comment":"Accepted by NeurIPS 2025 Datasets & Benchmarks Track"},{"id":"http://arxiv.org/abs/2510.26474v1","updated":"2025-10-30T13:26:58Z","published":"2025-10-30T13:26:58Z","title":"Counteracting Matthew Effect in Self-Improvement of LVLMs through\n  Head-Tail Re-balancing","summary":"  Self-improvement has emerged as a mainstream paradigm for advancing the\nreasoning capabilities of large vision-language models (LVLMs), where models\nexplore and learn from successful trajectories iteratively. However, we\nidentify a critical issue during this process: the model excels at generating\nhigh-quality trajectories for simple queries (i.e., head data) but struggles\nwith more complex ones (i.e., tail data). This leads to an imbalanced\noptimization that drives the model to prioritize simple reasoning skills, while\nhindering its ability to tackle more complex reasoning tasks. Over iterations,\nthis imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew\neffect\"--which ultimately hinders further model improvement and leads to\nperformance bottlenecks. To counteract this challenge, we introduce four\nefficient strategies from two perspectives: distribution-reshaping and\ntrajectory-resampling, to achieve head-tail re-balancing during the\nexploration-and-learning self-improvement process. Extensive experiments on\nQwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks\ndemonstrate that our methods consistently improve visual reasoning\ncapabilities, outperforming vanilla self-improvement by 3.86 points on average.\n","authors":["Xin Guo","Zhiheng Xi","Yiwen Ding","Yitao Zhai","Xiaowei Shi","Xunliang Cai","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26474v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2508.08606v3","updated":"2025-10-30T13:25:58Z","published":"2025-08-12T03:39:07Z","title":"Distributed optimization: designed for federated learning","summary":"  Federated learning (FL), as a distributed collaborative machine learning (ML)\nframework under privacy-preserving constraints, has garnered increasing\nresearch attention in cross-organizational data collaboration scenarios. This\npaper proposes a class of distributed optimization algorithms based on the\naugmented Lagrangian technique, designed to accommodate diverse communication\ntopologies in both centralized and decentralized FL settings. Furthermore, we\ndevelop multiple termination criteria and parameter update mechanisms to\nenhance computational efficiency, accompanied by rigorous theoretical\nguarantees of convergence. By generalizing the augmented Lagrangian relaxation\nthrough the incorporation of proximal relaxation and quadratic approximation,\nour framework systematically recovers a broad of classical unconstrained\noptimization methods, including proximal algorithm, classic gradient descent,\nand stochastic gradient descent, among others. Notably, the convergence\nproperties of these methods can be naturally derived within the proposed\ntheoretical framework. Numerical experiments demonstrate that the proposed\nalgorithm exhibits strong performance in large-scale settings with significant\nstatistical heterogeneity across clients.\n","authors":["Wenyou Guo","Ting Qu","Chunrong Pan","George Q. Huang"],"pdf_url":"https://arxiv.org/pdf/2508.08606v3.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.02331v2","updated":"2025-10-30T13:17:24Z","published":"2025-02-04T14:06:27Z","title":"On the Impact of Performative Risk Minimization for Binary Random\n  Variables","summary":"  Performativity, the phenomenon where outcomes are influenced by predictions,\nis particularly prevalent in social contexts where individuals strategically\nrespond to a deployed model. In order to preserve the high accuracy of machine\nlearning models under distribution shifts caused by performativity, Perdomo et\nal. (2020) introduced the concept of performative risk minimization (PRM).\nWhile this framework ensures model accuracy, it overlooks the impact of the PRM\non the underlying distributions and the predictions of the model. In this\npaper, we initiate the analysis of the impact of PRM, by studying\nperformativity for a sequential performative risk minimization problem with\nbinary random variables and linear performative shifts. We formulate two\nnatural measures of impact. In the case of full information, where the\ndistribution dynamics are known, we derive explicit formulas for the PRM\nsolution and our impact measures. In the case of partial information, we\nprovide performative-aware statistical estimators, as well as simulations. Our\nanalysis contrasts PRM to alternatives that do not model data shift and\nindicates that PRM can have amplified side effects compared to such methods.\n","authors":["Nikita Tsoy","Ivan Kirev","Negin Rahimiyazdi","Nikola Konstantinov"],"pdf_url":"https://arxiv.org/pdf/2502.02331v2.pdf","comment":"ICML 2025 camera-ready"},{"id":"http://arxiv.org/abs/2509.09695v2","updated":"2025-10-30T13:14:13Z","published":"2025-08-27T09:31:50Z","title":"Machine-learning competition to grade EEG background patterns in\n  newborns with hypoxic-ischaemic encephalopathy","summary":"  Machine learning (ML) has the potential to support and improve expert\nperformance in monitoring the brain function of at-risk newborns. Developing\naccurate and reliable ML models depends on access to high-quality, annotated\ndata, a resource in short supply. ML competitions address this need by\nproviding researchers access to expertly annotated datasets, fostering shared\nlearning through direct model comparisons, and leveraging the benefits of\ncrowdsourcing diverse expertise. We compiled a retrospective dataset containing\n353 hours of EEG from 102 individual newborns from a multi-centre study. The\ndata was fully anonymised and divided into training, testing, and held-out\nvalidation datasets. EEGs were graded for the severity of abnormal background\npatterns. Next, we created a web-based competition platform and hosted a\nmachine learning competition to develop ML models for classifying the severity\nof EEG background patterns in newborns. After the competition closed, the top 4\nperforming models were evaluated offline on a separate held-out validation\ndataset. Although a feature-based model ranked first on the testing dataset,\ndeep learning models generalised better on the validation sets. All methods had\na significant decline in validation performance compared to the testing\nperformance. This highlights the challenges for model generalisation on unseen\ndata, emphasising the need for held-out validation datasets in ML studies with\nneonatal EEG. The study underscores the importance of training ML models on\nlarge and diverse datasets to ensure robust generalisation. The competition's\noutcome demonstrates the potential for open-access data and collaborative ML\ndevelopment to foster a collaborative research environment and expedite the\ndevelopment of clinical decision-support tools for neonatal neuromonitoring.\n","authors":["Fabio Magarelli","Geraldine B. Boylan","Saeed Montazeri","Feargal O'Sullivan","Dominic Lightbody","Minoo Ashoori","Tamara Skoric","John M. O'Toole"],"pdf_url":"https://arxiv.org/pdf/2509.09695v2.pdf","comment":"29 pages, supplementary materials: \"supplementary materials ML\n  Comp.docx\""},{"id":"http://arxiv.org/abs/2510.26466v1","updated":"2025-10-30T13:11:23Z","published":"2025-10-30T13:11:23Z","title":"Representation-Level Counterfactual Calibration for Debiased Zero-Shot\n  Recognition","summary":"  Object-context shortcuts remain a persistent challenge in vision-language\nmodels, undermining zero-shot reliability when test-time scenes differ from\nfamiliar training co-occurrences. We recast this issue as a causal inference\nproblem and ask: Would the prediction remain if the object appeared in a\ndifferent environment? To answer this at inference time, we estimate object and\nbackground expectations within CLIP's representation space, and synthesize\ncounterfactual embeddings by recombining object features with diverse\nalternative contexts sampled from external datasets, batch neighbors, or\ntext-derived descriptions. By estimating the Total Direct Effect and simulating\nintervention, we further subtract background-only activation, preserving\nbeneficial object-context interactions while mitigating hallucinated scores.\nWithout retraining or prompt design, our method substantially improves both\nworst-group and average accuracy on context-sensitive benchmarks, establishing\na new zero-shot state of the art. Beyond performance, our framework provides a\nlightweight representation-level counterfactual approach, offering a practical\ncausal avenue for debiased and reliable multimodal reasoning.\n","authors":["Pei Peng","MingKun Xie","Hang Hao","Tong Jin","ShengJun Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26461v1","updated":"2025-10-30T13:07:39Z","published":"2025-10-30T13:07:39Z","title":"Vectorized Context-Aware Embeddings for GAT-Based Collaborative\n  Filtering","summary":"  Recommender systems often struggle with data sparsity and cold-start\nscenarios, limiting their ability to provide accurate suggestions for new or\ninfrequent users. This paper presents a Graph Attention Network (GAT) based\nCollaborative Filtering (CF) framework enhanced with Large Language Model (LLM)\ndriven context aware embeddings. Specifically, we generate concise textual user\nprofiles and unify item metadata (titles, genres, overviews) into rich textual\nembeddings, injecting these as initial node features in a bipartite user item\ngraph. To further optimize ranking performance, we introduce a hybrid loss\nfunction that combines Bayesian Personalized Ranking (BPR) with a cosine\nsimilarity term and robust negative sampling, ensuring explicit negative\nfeedback is distinguished from unobserved data. Experiments on the MovieLens\n100k and 1M datasets show consistent improvements over state-of-the-art\nbaselines in Precision, NDCG, and MAP while demonstrating robustness for users\nwith limited interaction history. Ablation studies confirm the critical role of\nLLM-augmented embeddings and the cosine similarity term in capturing nuanced\nsemantic relationships. Our approach effectively mitigates sparsity and\ncold-start limitations by integrating LLM-derived contextual understanding into\ngraph-based architectures. Future directions include balancing recommendation\naccuracy with coverage and diversity, and introducing fairness-aware\nconstraints and interpretability features to enhance system performance\nfurther.\n","authors":["Danial Ebrat","Sepideh Ahmadian","Luis Rueda"],"pdf_url":"https://arxiv.org/pdf/2510.26461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21513v2","updated":"2025-10-30T13:03:25Z","published":"2025-10-24T14:39:23Z","title":"Wisdom and Delusion of LLM Ensembles for Code Generation and Repair","summary":"  Today's pursuit of a single Large Language Model (LMM) for all software\nengineering tasks is resource-intensive and overlooks the potential benefits of\ncomplementarity, where different models contribute unique strengths. However,\nthe degree to which coding LLMs complement each other and the best strategy for\nmaximizing an ensemble's potential are unclear, leaving practitioners without a\nclear path to move beyond single-model systems. To address this gap, we\nempirically compare ten individual LLMs from five families, and three ensembles\nof these LLMs across three software engineering benchmarks covering code\ngeneration and program repair. We assess the complementarity between models and\nthe performance gap between the best individual model and the ensembles. Next,\nwe evaluate various selection heuristics to identify correct solutions from an\nensemble's candidate pool. We find that the theoretical upperbound for an\nensemble's performance can be 83% above the best single model. Our results show\nthat consensus-based strategies for selecting solutions fall into a \"popularity\ntrap,\" amplifying common but incorrect outputs. In contrast, a diversity-based\nstrategy realizes up to 95% of this theoretical potential, and proves effective\neven in small two-model ensembles, enabling a cost-efficient way to enhance\nperformance by leveraging multiple LLMs.\n","authors":["Fernando Vallecillos-Ruiz","Max Hort","Leon Moonen"],"pdf_url":"https://arxiv.org/pdf/2510.21513v2.pdf","comment":"Added Acknowledgments section and hyphenated last names"},{"id":"http://arxiv.org/abs/2510.26451v1","updated":"2025-10-30T12:55:21Z","published":"2025-10-30T12:55:21Z","title":"Robust Graph Condensation via Classification Complexity Mitigation","summary":"  Graph condensation (GC) has gained significant attention for its ability to\nsynthesize smaller yet informative graphs. However, existing studies often\noverlook the robustness of GC in scenarios where the original graph is\ncorrupted. In such cases, we observe that the performance of GC deteriorates\nsignificantly, while existing robust graph learning technologies offer only\nlimited effectiveness. Through both empirical investigation and theoretical\nanalysis, we reveal that GC is inherently an intrinsic-dimension-reducing\nprocess, synthesizing a condensed graph with lower classification complexity.\nAlthough this property is critical for effective GC performance, it remains\nhighly vulnerable to adversarial perturbations. To tackle this vulnerability\nand improve GC robustness, we adopt the geometry perspective of graph data\nmanifold and propose a novel Manifold-constrained Robust Graph Condensation\nframework named MRGC. Specifically, we introduce three graph data manifold\nlearning modules that guide the condensed graph to lie within a smooth,\nlow-dimensional manifold with minimal class ambiguity, thereby preserving the\nclassification complexity reduction capability of GC and ensuring robust\nperformance under universal adversarial attacks. Extensive experiments\ndemonstrate the robustness of \\ModelName\\ across diverse attack scenarios.\n","authors":["Jiayi Luo","Qingyun Sun","Beining Yang","Haonan Yuan","Xingcheng Fu","Yanbiao Ma","Jianxin Li","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2510.26451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26444v1","updated":"2025-10-30T12:50:12Z","published":"2025-10-30T12:50:12Z","title":"Personalized Treatment Outcome Prediction from Scarce Data via\n  Dual-Channel Knowledge Distillation and Adaptive Fusion","summary":"  Personalized treatment outcome prediction based on trial data for\nsmall-sample and rare patient groups is critical in precision medicine.\nHowever, the costly trial data limit the prediction performance. To address\nthis issue, we propose a cross-fidelity knowledge distillation and adaptive\nfusion network (CFKD-AFN), which leverages abundant but low-fidelity simulation\ndata to enhance predictions on scarce but high-fidelity trial data. CFKD-AFN\nincorporates a dual-channel knowledge distillation module to extract\ncomplementary knowledge from the low-fidelity model, along with an\nattention-guided fusion module to dynamically integrate multi-source\ninformation. Experiments on treatment outcome prediction for the chronic\nobstructive pulmonary disease demonstrates significant improvements of CFKD-AFN\nover state-of-the-art methods in prediction accuracy, ranging from 6.67\\% to\n74.55\\%, and strong robustness to varying high-fidelity dataset sizes.\nFurthermore, we extend CFKD-AFN to an interpretable variant, enabling the\nexploration of latent medical semantics to support clinical decision-making.\n","authors":["Wenjie Chen","Li Zhuang","Ziying Luo","Yu Liu","Jiahao Wu","Shengcai Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11542v2","updated":"2025-10-30T12:48:34Z","published":"2025-05-14T13:18:12Z","title":"Cybersecurity threat detection based on a UEBA framework using Deep\n  Autoencoders","summary":"  User and Entity Behaviour Analytics (UEBA) is a broad branch of data\nanalytics that attempts to build a normal behavioural profile in order to\ndetect anomalous events. Among the techniques used to detect anomalies, Deep\nAutoencoders constitute one of the most promising deep learning models on UEBA\ntasks, allowing explainable detection of security incidents that could lead to\nthe leak of personal data, hijacking of systems, or access to sensitive\nbusiness information. In this study, we introduce the first implementation of\nan explainable UEBA-based anomaly detection framework that leverages Deep\nAutoencoders in combination with Doc2Vec to process both numerical and textual\nfeatures. Additionally, based on the theoretical foundations of neural\nnetworks, we offer a novel proof demonstrating the equivalence of two widely\nused definitions for fully-connected neural networks. The experimental results\ndemonstrate the proposed framework capability to detect real and synthetic\nanomalies effectively generated from real attack data, showing that the models\nprovide not only correct identification of anomalies but also explainable\nresults that enable the reconstruction of the possible origin of the anomaly.\nOur findings suggest that the proposed UEBA framework can be seamlessly\nintegrated into enterprise environments, complementing existing security\nsystems for explainable threat detection.\n","authors":["Jose Fuentes","Ines Ortega-Fernandez","Nora M. Villanueva","Marta Sestelo"],"pdf_url":"https://arxiv.org/pdf/2505.11542v2.pdf","comment":"Published in AIMS Mathematics (2025), 10(10): 23496-23517. DOI:\n  10.3934/math.20251043"},{"id":"http://arxiv.org/abs/2510.26433v1","updated":"2025-10-30T12:28:40Z","published":"2025-10-30T12:28:40Z","title":"Co-Evolving Latent Action World Models","summary":"  Adapting pre-trained video generation models into controllable world models\nvia latent actions is a promising step towards creating generalist world\nmodels. The dominant paradigm adopts a two-stage approach that trains latent\naction model (LAM) and the world model separately, resulting in redundant\ntraining and limiting their potential for co-adaptation. A conceptually simple\nand appealing idea is to directly replace the forward dynamic model in LAM with\na powerful world model and training them jointly, but it is non-trivial and\nprone to representational collapse. In this work, we propose CoLA-World, which\nfor the first time successfully realizes this synergistic paradigm, resolving\nthe core challenge in joint learning through a critical warm-up phase that\neffectively aligns the representations of the from-scratch LAM with the\npre-trained world model. This unlocks a co-evolution cycle: the world model\nacts as a knowledgeable tutor, providing gradients to shape a high-quality LAM,\nwhile the LAM offers a more precise and adaptable control interface to the\nworld model. Empirically, CoLA-World matches or outperforms prior two-stage\nmethods in both video simulation quality and downstream visual planning,\nestablishing a robust and efficient new paradigm for the field.\n","authors":["Yucen Wang","Fengming Zhang","De-Chuan Zhan","Li Zhao","Kaixin Wang","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2510.26433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25080v2","updated":"2025-10-30T12:16:59Z","published":"2025-10-29T01:38:19Z","title":"Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response\n  Games","summary":"  Card games are widely used to study sequential decision-making under\nuncertainty, with real-world analogues in negotiation, finance, and\ncybersecurity. These games typically fall into three categories based on the\nflow of control: strictly sequential (players alternate single actions),\ndeterministic response (some actions trigger a fixed outcome), and unbounded\nreciprocal response (alternating counterplays are permitted). A less-explored\nbut strategically rich structure is the bounded one-sided response, where a\nplayer's action briefly transfers control to the opponent, who must satisfy a\nfixed condition through one or more moves before the turn resolves. We term\ngames featuring this mechanism Bounded One-Sided Response Games (BORGs). We\nintroduce a modified version of Monopoly Deal as a benchmark environment that\nisolates this dynamic, where a Rent action forces the opponent to choose\npayment assets. The gold-standard algorithm, Counterfactual Regret Minimization\n(CFR), converges on effective strategies without novel algorithmic extensions.\nA lightweight full-stack research platform unifies the environment, a\nparallelized CFR runtime, and a human-playable web interface. The trained CFR\nagent and source code are available at https://monopolydeal.ai.\n","authors":["Will Wolf"],"pdf_url":"https://arxiv.org/pdf/2510.25080v2.pdf","comment":"24 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.17883v2","updated":"2025-10-30T12:06:31Z","published":"2024-12-23T06:22:03Z","title":"In Defence of Post-hoc Explainability","summary":"  This position paper defends post-hoc explainability methods as legitimate\ntools for scientific knowledge production in machine learning. Addressing\ncriticism of these methods' reliability and epistemic status, we develop a\nphilosophical framework grounded in mediated understanding and bounded\nfactivity. We argue that scientific insights can emerge through structured\ninterpretation of model behaviour without requiring complete mechanistic\ntransparency, provided explanations acknowledge their approximative nature and\nundergo rigorous empirical validation. Through analysis of recent biomedical ML\napplications, we demonstrate how post-hoc methods, when properly integrated\ninto scientific practice, generate novel hypotheses and advance phenomenal\nunderstanding.\n","authors":["Nick Oh"],"pdf_url":"https://arxiv.org/pdf/2412.17883v2.pdf","comment":"v1 presented at the Interpretable AI: Past, Present, and Future\n  Workshop at NeurIPS 2024 (non-archival)"},{"id":"http://arxiv.org/abs/2510.17670v2","updated":"2025-10-30T12:05:58Z","published":"2025-10-20T15:41:55Z","title":"On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active\n  Marginal-Samples Exploration","summary":"  Open-vocabulary object detection (OVD) models offer remarkable flexibility by\ndetecting objects from arbitrary text queries. However, their zero-shot\nperformance in specialized domains like Remote Sensing (RS) is often\ncompromised by the inherent ambiguity of natural language, limiting critical\ndownstream applications. For instance, an OVD model may struggle to distinguish\nbetween fine-grained classes such as \"fishing boat\" and \"yacht\" since their\nembeddings are similar and often inseparable. This can hamper specific user\ngoals, such as monitoring illegal fishing, by producing irrelevant detections.\nTo address this, we propose a cascaded approach that couples the broad\ngeneralization of a large pre-trained OVD model with a lightweight few-shot\nclassifier. Our method first employs the zero-shot model to generate\nhigh-recall object proposals. These proposals are then refined for high\nprecision by a compact classifier trained in real-time on only a handful of\nuser-annotated examples - drastically reducing the high costs of RS imagery\nannotation.The core of our framework is FLAME, a one-step active learning\nstrategy that selects the most informative samples for training. FLAME\nidentifies, on the fly, uncertain marginal candidates near the decision\nboundary using density estimation, followed by clustering to ensure sample\ndiversity. This efficient sampling technique achieves high accuracy without\ncostly full-model fine-tuning and enables instant adaptation, within less then\na minute, which is significantly faster than state-of-the-art alternatives.Our\nmethod consistently surpasses state-of-the-art performance on RS benchmarks,\nestablishing a practical and resource-efficient framework for adapting\nfoundation models to specific user needs.\n","authors":["Yehonathan Refael","Amit Aides","Aviad Barzilai","George Leifman","Genady Beryozkin","Vered Silverman","Bolous Jaber","Tomer Shekel"],"pdf_url":"https://arxiv.org/pdf/2510.17670v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23117v2","updated":"2025-10-30T12:02:18Z","published":"2025-10-27T08:38:17Z","title":"Seeing Structural Failure Before it Happens: An Image-Based\n  Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction","summary":"  Physics Informed Neural Networks (PINNs) are gaining attention for their\nability to embed physical laws into deep learning models, which is particularly\nuseful in structural engineering tasks with limited data. This paper aims to\nexplore the use of PINNs to predict the weight of small scale spaghetti\nbridges, a task relevant to understanding load limits and potential failure\nmodes in simplified structural models. Our proposed framework incorporates\nphysics-based constraints to the prediction model for improved performance. In\naddition to standard PINNs, we introduce a novel architecture named Physics\nInformed Kolmogorov Arnold Network (PIKAN), which blends universal function\napproximation theory with physical insights. The structural parameters provided\nas input to the model are collected either manually or through computer vision\nmethods. Our dataset includes 15 real bridges, augmented to 100 samples, and\nour best model achieves an $R^2$ score of 0.9603 and a mean absolute error\n(MAE) of 10.50 units. From applied perspective, we also provide a web based\ninterface for parameter entry and prediction. These results show that PINNs can\noffer reliable estimates of structural weight, even with limited data, and may\nhelp inform early stage failure analysis in lightweight bridge designs.\n  The complete data and code are available at\nhttps://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.\n","authors":["Omer Jauhar Khan","Sudais Khan","Hafeez Anwar","Shahzeb Khan","Shams Ul Arifeen"],"pdf_url":"https://arxiv.org/pdf/2510.23117v2.pdf","comment":"12 pages, 17 figures. Preprint"},{"id":"http://arxiv.org/abs/2510.26402v1","updated":"2025-10-30T11:41:50Z","published":"2025-10-30T11:41:50Z","title":"Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback\n  in Programming Education","summary":"  The rapid growth of programming education has outpaced traditional assessment\ntools, leaving faculty with limited means to provide meaningful, scalable\nfeedback. Conventional autograders, while efficient, act as black-box systems\nthat simply return pass/fail results, offering little insight into student\nthinking or learning needs.\n  Autograder+ is designed to shift autograding from a purely summative process\nto a formative learning experience. It introduces two key capabilities:\nautomated feedback generation using a fine-tuned Large Language Model, and\nvisualization of student code submissions to uncover learning patterns. The\nmodel is fine-tuned on curated student code and expert feedback to ensure\npedagogically aligned, context-aware guidance.\n  In evaluation across 600 student submissions from multiple programming tasks,\nthe system produced feedback with strong semantic alignment to instructor\ncomments. For visualization, contrastively learned code embeddings trained on\n1,000 annotated submissions enable grouping solutions into meaningful clusters\nbased on functionality and approach. The system also supports prompt-pooling,\nallowing instructors to guide feedback style through selected prompt templates.\n  By integrating AI-driven feedback, semantic clustering, and interactive\nvisualization, Autograder+ reduces instructor workload while supporting\ntargeted instruction and promoting stronger learning outcomes.\n","authors":["Vikrant Sahu","Gagan Raj Gupta","Raghav Borikar","Nitin Mane"],"pdf_url":"https://arxiv.org/pdf/2510.26402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26401v1","updated":"2025-10-30T11:41:19Z","published":"2025-10-30T11:41:19Z","title":"Multi-Output Robust and Conjugate Gaussian Processes","summary":"  Multi-output Gaussian process (MOGP) regression allows modelling dependencies\namong multiple correlated response variables. Similarly to standard Gaussian\nprocesses, MOGPs are sensitive to model misspecification and outliers, which\ncan distort predictions within individual outputs. This situation can be\nfurther exacerbated by multiple anomalous response variables whose errors\npropagate due to correlations between outputs. To handle this situation, we\nextend and generalise the robust and conjugate Gaussian process (RCGP)\nframework introduced by Altamirano et al. (2024). This results in the\nmulti-output RCGP (MO-RCGP): a provably robust MOGP that is conjugate, and\njointly captures correlations across outputs. We thoroughly evaluate our\napproach through applications in finance and cancer research.\n","authors":["Joshua Rooijakkers","Leiv RÃ¸nneberg","FranÃ§ois-Xavier Briol","Jeremias Knoblauch","Matias Altamirano"],"pdf_url":"https://arxiv.org/pdf/2510.26401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26392v1","updated":"2025-10-30T11:35:05Z","published":"2025-10-30T11:35:05Z","title":"Multi-Task Learning Based on Support Vector Machines and Twin Support\n  Vector Machines: A Comprehensive Survey","summary":"  Multi-task learning (MTL) enables simultaneous training across related tasks,\nleveraging shared information to improve generalization, efficiency, and\nrobustness, especially in data-scarce or high-dimensional scenarios. While deep\nlearning dominates recent MTL research, Support Vector Machines (SVMs) and Twin\nSVMs (TWSVMs) remain relevant due to their interpretability, theoretical rigor,\nand effectiveness with small datasets.\n  This chapter surveys MTL approaches based on SVM and TWSVM, highlighting\nshared representations, task regularization, and structural coupling\nstrategies. Special attention is given to emerging TWSVM extensions for\nmulti-task settings, which show promise but remain underexplored. We compare\nthese models in terms of theoretical properties, optimization strategies, and\nempirical performance, and discuss applications in fields such as computer\nvision, natural language processing, and bioinformatics.\n  Finally, we identify research gaps and outline future directions for building\nscalable, interpretable, and reliable margin-based MTL frameworks. This work\nprovides a comprehensive resource for researchers and practitioners interested\nin SVM- and TWSVM-based multi-task learning.\n","authors":["Fatemeh Bazikar","Hossein Moosaei","Atefeh Hemmati","Panos M. Pardalos"],"pdf_url":"https://arxiv.org/pdf/2510.26392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11329v4","updated":"2025-10-30T11:34:01Z","published":"2025-05-16T14:53:50Z","title":"TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM\n  Inference","summary":"  Distributed inference of large language models (LLMs) can introduce overheads\nof up to 20% even over GPUs connected via high-speed interconnects such as\nNVLink. Multiple techniques have been proposed to mitigate these overheads by\ndecomposing computations into finer-grained tasks and overlapping communication\nwith sub-tasks as they complete. However, fine-grained decomposition of a large\ncomputation into many smaller computations on GPUs results in overheads.\nFurthermore, the communication itself uses many streaming multiprocessors\n(SMs), adding to the overhead.\n  We present TokenWeave to address these challenges. TokenWeave proposes a\nToken-Splitting technique that divides the tokens in the inference batch into\ntwo approximately equal subsets in a wave-aware manner. The communication of\none subset is then overlapped with the computation of the other. In addition,\nTokenWeave optimizes the order of the layer normalization computation with\nrespect to communication operations and implements a novel fused\nAllReduce--RMSNorm kernel that carefully leverages Multimem instruction support\navailable on Hopper and Blackwell NVIDIA GPUs. These optimizations allow\nTokenWeave to perform communication and RMSNorm using only 2-8 SMs. Moreover,\nour kernel enables the memory-bound RMSNorm to be overlapped with the other\nbatch's computation, providing additional gains.\n  Our evaluations demonstrate up to 1.29x speedup in latency and 1.26x higher\nthroughput across multiple models and workloads. In several settings,\nTokenWeave results in better performance compared to an equivalent model with\nall communication removed.\n","authors":["Raja Gond","Nipun Kwatra","Ramachandran Ramjee"],"pdf_url":"https://arxiv.org/pdf/2505.11329v4.pdf","comment":"14 pages, 16 figures. For source code, see\n  https://github.com/microsoft/tokenweave. In version 2, Figure 6 shows\n  All-Reduce bandwidth instead of Reduce-Scatter. The Multimem Reduce-Scatter\n  bandwidth formula differs slightly from the ring-based version. Fixed x-ticks\n  in Figure 7"},{"id":"http://arxiv.org/abs/2510.26389v1","updated":"2025-10-30T11:32:45Z","published":"2025-10-30T11:32:45Z","title":"Adaptive Context Length Optimization with Low-Frequency Truncation for\n  Multi-Agent Reinforcement Learning","summary":"  Recently, deep multi-agent reinforcement learning (MARL) has demonstrated\npromising performance for solving challenging tasks, such as long-term\ndependencies and non-Markovian environments. Its success is partly attributed\nto conditioning policies on large fixed context length. However, such large\nfixed context lengths may lead to limited exploration efficiency and redundant\ninformation. In this paper, we propose a novel MARL framework to obtain\nadaptive and effective contextual information. Specifically, we design a\ncentral agent that dynamically optimizes context length via temporal gradient\nanalysis, enhancing exploration to facilitate convergence to global optima in\nMARL. Furthermore, to enhance the adaptive optimization capability of the\ncontext length, we present an efficient input representation for the central\nagent, which effectively filters redundant information. By leveraging a\nFourier-based low-frequency truncation method, we extract global temporal\ntrends across decentralized agents, providing an effective and efficient\nrepresentation of the MARL environment. Extensive experiments demonstrate that\nthe proposed method achieves state-of-the-art (SOTA) performance on long-term\ndependency tasks, including PettingZoo, MiniGrid, Google Research Football\n(GRF), and StarCraft Multi-Agent Challenge v2 (SMACv2).\n","authors":["Wenchang Duan","Yaoliang Yu","Jiwan He","Yi Shi"],"pdf_url":"https://arxiv.org/pdf/2510.26389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17316v2","updated":"2025-10-30T11:32:37Z","published":"2025-07-23T08:30:37Z","title":"Nearly Minimax Discrete Distribution Estimation in Kullback-Leibler\n  Divergence with High Probability","summary":"  We consider the fundamental problem of estimating a discrete distribution on\na domain of size~$K$ with high probability in Kullback-Leibler divergence. We\nprovide upper and lower bounds on the minimax estimation rate, which show that\nthe optimal rate is between $\\big(K + \\ln(K)\\ln(1/\\delta)\\big) /n$ and\n$\\big(K\\ln\\ln(K) + \\ln(K)\\ln(1/\\delta)\\big) /n$ at error probability $\\delta$\nand sample size $n$, which pins down the rate up to the doubly logarithmic\nfactor $\\ln \\ln K$ that multiplies $K$. Our upper bound uses techniques from\nonline learning to construct a novel estimator via online-to-batch conversion.\nPerhaps surprisingly, the tail behavior of the minimax rate is worse than for\nthe squared total variation and squared Hellinger distance, for which it is\n$\\big(K + \\ln(1/\\delta)\\big) /n$, i.e.\\ without the $\\ln K$ multiplying $\\ln\n(1/\\delta)$. As a consequence, we cannot obtain a fully tight lower bound from\nthe usual reduction to these smaller distances. Moreover, we show that this\nlower bound cannot be achieved by the standard lower bound approach based on a\nreduction to hypothesis testing, and instead we need to introduce a new\nreduction to what we call weak hypothesis testing. We investigate the source of\nthe gap with other divergences further in refined results, which show that the\ntotal variation rate is achievable for Kullback-Leibler divergence after all\n(in fact by he maximum likelihood estimator) if we rule out outcome\nprobabilities smaller than $O(\\ln(K/\\delta) / n)$, which is a vanishing set as\n$n$ increases for fixed $K$ and~$\\delta$. This explains why minimax\nKullback-Leibler estimation is more difficult than asymptotic estimation.\n","authors":["Dirk van der Hoeven","Julia Olkhovskaia","Tim van Erven"],"pdf_url":"https://arxiv.org/pdf/2507.17316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26384v1","updated":"2025-10-30T11:28:58Z","published":"2025-10-30T11:28:58Z","title":"Scales++: Compute Efficient Evaluation Subset Selection with Cognitive\n  Scales Embeddings","summary":"  The prohibitive cost of evaluating large language models (LLMs) on\ncomprehensive benchmarks necessitates the creation of small yet representative\ndata subsets (i.e., tiny benchmarks) that enable efficient assessment while\nretaining predictive fidelity. Current methods for this task operate under a\nmodel-centric paradigm, selecting benchmarking items based on the collective\nperformance of existing models. Such approaches are limited by large upfront\ncosts, an inability to immediately handle new benchmarks (`cold-start'), and\nthe fragile assumption that future models will share the failure patterns of\ntheir predecessors. In this work, we challenge this paradigm and propose a\nitem-centric approach to benchmark subset selection, arguing that selection\nshould be based on the intrinsic properties of the task items themselves,\nrather than on model-specific failure patterns. We instantiate this\nitem-centric efficient benchmarking approach via a novel method, Scales++,\nwhere data selection is based on the cognitive demands of the benchmark\nsamples. Empirically, we show Scales++ reduces the upfront selection cost by\nover 18x while achieving competitive predictive fidelity. On the Open LLM\nLeaderboard, using just a 0.5\\% data subset, we predict full benchmark scores\nwith a 2.9% mean absolute error. We demonstrate that this item-centric approach\nenables more efficient model evaluation without significant fidelity\ndegradation, while also providing better cold-start performance and more\ninterpretable benchmarking.\n","authors":["Andrew M. Bean","Nabeel Seedat","Shengzhuang Chen","Jonathan Richard Schwarz"],"pdf_url":"https://arxiv.org/pdf/2510.26384v1.pdf","comment":"9 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2507.08896v2","updated":"2025-10-30T11:18:11Z","published":"2025-07-11T03:11:15Z","title":"Predictive Causal Inference via Spatio-Temporal Modeling and Penalized\n  Empirical Likelihood","summary":"  This study introduces an integrated framework for predictive causal inference\ndesigned to overcome limitations inherent in conventional single model\napproaches. Specifically, we combine a Hidden Markov Model (HMM) for spatial\nhealth state estimation with a Multi Task and Multi Graph Convolutional Network\n(MTGCN) for capturing temporal outcome trajectories. The framework\nasymmetrically treats temporal and spatial information regarding them as\nendogenous variables in the outcome regression, and exogenous variables in the\npropensity score model, thereby expanding the standard doubly robust treatment\neffect estimation to jointly enhance bias correction and predictive accuracy.\nTo demonstrate its utility, we focus on clinical domains such as cancer,\ndementia, and Parkinson disease, where treatment effects are challenging to\nobserve directly. Simulation studies are conducted to emulate latent disease\ndynamics and evaluate the model performance under varying conditions. Overall,\nthe proposed framework advances predictive causal inference by structurally\nadapting to spatiotemporal complexities common in biomedical data.\n","authors":["Byunghee Lee","Hye Yeon Sin","Joonsung Kang"],"pdf_url":"https://arxiv.org/pdf/2507.08896v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26376v1","updated":"2025-10-30T11:16:22Z","published":"2025-10-30T11:16:22Z","title":"Efficient Generative AI Boosts Probabilistic Forecasting of Sudden\n  Stratospheric Warmings","summary":"  Sudden Stratospheric Warmings (SSWs) are key sources of subseasonal\npredictability and major drivers of extreme winter weather. Yet, their accurate\nand efficient forecast remains a persistent challenge for numerical weather\nprediction (NWP) systems due to limitations in physical representation,\ninitialization, and the immense computational demands of ensemble forecasts.\nWhile data-driven forecasting is rapidly evolving, its application to the\ncomplex, three-dimensional dynamics of SSWs, particularly for probabilistic\nforecast, remains underexplored. Here, we bridge this gap by developing a Flow\nMatching-based generative AI model (FM-Cast) for efficient and skillful\nprobabilistic forecasting of the spatiotemporal evolution of stratospheric\ncirculation. Evaluated across 18 major SSW events (1998-2024), FM-Cast\nskillfully forecasts the onset, intensity, and morphology of 10 events up to 20\ndays in advance, achieving ensemble accuracies above 50%. Its performance is\ncomparable to or exceeds leading NWP systems while requiring only two minutes\nfor a 50-member, 30-day forecast on a consumer GPU. Furthermore, leveraging\nFM-Cast as a scientific tool, we demonstrate through idealized experiments that\nSSW predictability is fundamentally linked to its underlying physical drivers,\ndistinguishing between events forced from the troposphere and those driven by\ninternal stratospheric dynamics. Our work thus establishes a computationally\nefficient paradigm for probabilistic forecasting stratospheric anomalies and\nshowcases generative AI's potential to deepen the physical understanding of\natmosphere-climate dynamics.\n","authors":["Ningning Tao","Fei Xie","Baoxiang Pan","Hongyu Wang","Han Huang","Zhongpu Qiu","Ke Gui","Jiali Luo","Xiaosong Chen"],"pdf_url":"https://arxiv.org/pdf/2510.26376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26369v1","updated":"2025-10-30T11:14:17Z","published":"2025-10-30T11:14:17Z","title":"CorVS: Person Identification via Video Trajectory-Sensor Correspondence\n  in a Real-World Warehouse","summary":"  Worker location data is key to higher productivity in industrial sites.\nCameras are a promising tool for localization in logistics warehouses since\nthey also offer valuable environmental contexts such as package status.\nHowever, identifying individuals with only visual data is often impractical.\nAccordingly, several prior studies identified people in videos by comparing\ntheir trajectories and wearable sensor measurements. While this approach has\nadvantages such as independence from appearance, the existing methods may break\ndown under real-world conditions. To overcome this challenge, we propose CorVS,\na novel data-driven person identification method based on correspondence\nbetween visual tracking trajectories and sensor measurements. Firstly, our deep\nlearning model predicts correspondence probabilities and reliabilities for\nevery pair of a trajectory and sensor measurements. Secondly, our algorithm\nmatches the trajectories and sensor measurements over time using the predicted\nprobabilities and reliabilities. We developed a dataset with actual warehouse\noperations and demonstrated the method's effectiveness for real-world\napplications.\n","authors":["Kazuma Kano","Yuki Mori","Shin Katayama","Kenta Urano","Takuro Yonezawa","Nobuo Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2510.26369v1.pdf","comment":"7 pages, 3 figures, accepted to IPIN 2025"},{"id":"http://arxiv.org/abs/2510.24817v2","updated":"2025-10-30T11:13:33Z","published":"2025-10-28T10:06:49Z","title":"Towards a Method for Synthetic Generation of Persons with Aphasia\n  Transcripts","summary":"  In aphasia research, Speech-Language Pathologists (SLPs) devote extensive\ntime to manually coding speech samples using Correct Information Units (CIUs),\na measure of how informative an individual sample of speech is. Developing\nautomated systems to recognize aphasic language is limited by data scarcity.\nFor example, only about 600 transcripts are available in AphasiaBank yet\nbillions of tokens are used to train large language models (LLMs). In the\nbroader field of machine learning (ML), researchers increasingly turn to\nsynthetic data when such are sparse. Therefore, this study constructs and\nvalidates two methods to generate synthetic transcripts of the AphasiaBank Cat\nRescue picture description task. One method leverages a procedural programming\napproach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct\nLLMs. The methods generate transcripts across four severity levels (Mild,\nModerate, Severe, Very Severe) through word dropping, filler insertion, and\nparaphasia substitution. Overall, we found, compared to human-elicited\ntranscripts, Mistral 7b Instruct best captures key aspects of linguistic\ndegradation observed in aphasia, showing realistic directional changes in NDW,\nword count, and word length amongst the synthetic generation methods. Based on\nthe results, future work should plan to create a larger dataset, fine-tune\nmodels for better aphasic representation, and have SLPs assess the realism and\nusefulness of the synthetic transcripts.\n","authors":["Jason M. Pittman","Anton Phillips Jr.","Yesenia Medina-Santos","Brielle C. Stark"],"pdf_url":"https://arxiv.org/pdf/2510.24817v2.pdf","comment":"19 pages, 1 figure, 7 tables"},{"id":"http://arxiv.org/abs/2510.26353v1","updated":"2025-10-30T11:05:15Z","published":"2025-10-30T11:05:15Z","title":"Towards Explainable and Reliable AI in Finance","summary":"  Financial forecasting increasingly uses large neural network models, but\ntheir opacity raises challenges for trust and regulatory compliance. We present\nseveral approaches to explainable and reliable AI in finance. \\emph{First}, we\ndescribe how Time-LLM, a time series foundation model, uses a prompt to avoid a\nwrong directional forecast. \\emph{Second}, we show that combining foundation\nmodels for time series forecasting with a reliability estimator can filter our\nunreliable predictions. \\emph{Third}, we argue for symbolic reasoning encoding\ndomain rules for transparent justification. These approaches shift emphasize\nexecuting only forecasts that are both reliable and explainable. Experiments on\nequity and cryptocurrency data show that the architecture reduces false\npositives and supports selective execution. By integrating predictive\nperformance with reliability estimation and rule-based reasoning, our framework\nadvances transparent and auditable financial AI systems.\n","authors":["Albi Isufaj","Pablo MollÃ¡","Helmut Prendinger"],"pdf_url":"https://arxiv.org/pdf/2510.26353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26350v1","updated":"2025-10-30T11:01:57Z","published":"2025-10-30T11:01:57Z","title":"UnifiedFL: A Dynamic Unified Learning Framework for Equitable Federation","summary":"  Federated learning (FL) has emerged as a key paradigm for collaborative model\ntraining across multiple clients without sharing raw data, enabling\nprivacy-preserving applications in areas such as radiology and pathology.\nHowever, works on collaborative training across clients with fundamentally\ndifferent neural architectures and non-identically distributed datasets remain\nscarce. Existing FL frameworks face several limitations. Despite claiming to\nsupport architectural heterogeneity, most recent FL methods only tolerate\nvariants within a single model family (e.g., shallower, deeper, or wider CNNs),\nstill presuming a shared global architecture and failing to accommodate\nfederations where clients deploy fundamentally different network types (e.g.,\nCNNs, GNNs, MLPs). Moreover, existing approaches often address only statistical\nheterogeneity while overlooking the domain-fracture problem, where each\nclient's data distribution differs markedly from that faced at testing time,\nundermining model generalizability. When clients use different architectures,\nhave non-identically distributed data, and encounter distinct test domains,\ncurrent methods perform poorly. To address these challenges, we propose\nUnifiedFL, a dynamic federated learning framework that represents heterogeneous\nlocal networks as nodes and edges in a directed model graph optimized by a\nshared graph neural network (GNN). UnifiedFL introduces (i) a common GNN to\nparameterize all architectures, (ii) distance-driven clustering via Euclidean\ndistances between clients' parameters, and (iii) a two-tier aggregation policy\nbalancing convergence and diversity. Experiments on MedMNIST classification and\nhippocampus segmentation benchmarks demonstrate UnifiedFL's superior\nperformance. Code and data: https://github.com/basiralab/UnifiedFL\n","authors":["Furkan Pala","Islem Rekik"],"pdf_url":"https://arxiv.org/pdf/2510.26350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26347v1","updated":"2025-10-30T10:55:05Z","published":"2025-10-30T10:55:05Z","title":"Reinforcement Learning for Pollution Detection in a Randomized, Sparse\n  and Nonstationary Environment with an Autonomous Underwater Vehicle","summary":"  Reinforcement learning (RL) algorithms are designed to optimize\nproblem-solving by learning actions that maximize rewards, a task that becomes\nparticularly challenging in random and nonstationary environments. Even\nadvanced RL algorithms are often limited in their ability to solve problems in\nthese conditions. In applications such as searching for underwater pollution\nclouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate\nreward-sparse environments, where actions frequently result in a zero reward.\nThis paper aims to address these challenges by revisiting and modifying\nclassical RL approaches to efficiently operate in sparse, randomized, and\nnonstationary environments. We systematically study a large number of\nmodifications, including hierarchical algorithm changes, multigoal learning,\nand the integration of a location memory as an external output filter to\nprevent state revisits. Our results demonstrate that a modified Monte\nCarlo-based approach significantly outperforms traditional Q-learning and two\nexhaustive search patterns, illustrating its potential in adapting RL to\ncomplex environments. These findings suggest that reinforcement learning\napproaches can be effectively adapted for use in random, nonstationary, and\nreward-sparse environments.\n","authors":["Sebastian Zieglmeier","Niklas Erdmann","Narada D. Warakagoda"],"pdf_url":"https://arxiv.org/pdf/2510.26347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26345v1","updated":"2025-10-30T10:52:43Z","published":"2025-10-30T10:52:43Z","title":"MisSynth: Improving MISSCI Logical Fallacies Classification with\n  Synthetic Data","summary":"  Health-related misinformation is very prevalent and potentially harmful. It\nis difficult to identify, especially when claims distort or misinterpret\nscientific findings. We investigate the impact of synthetic data generation and\nlightweight fine-tuning techniques on the ability of large language models\n(LLMs) to recognize fallacious arguments using the MISSCI dataset and\nframework. In this work, we propose MisSynth, a pipeline that applies\nretrieval-augmented generation (RAG) to produce synthetic fallacy samples,\nwhich are then used to fine-tune an LLM model. Our results show substantial\naccuracy gains with fine-tuned models compared to vanilla baselines. For\ninstance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score\nabsolute improvement on the MISSCI test split over its vanilla baseline. We\ndemonstrate that introducing synthetic fallacy data to augment limited\nannotated resources can significantly enhance zero-shot LLM classification\nperformance on real-world scientific misinformation tasks, even with limited\ncomputational resources. The code and synthetic dataset are available on\nhttps://github.com/mxpoliakov/MisSynth.\n","authors":["Mykhailo Poliakov","Nadiya Shvai"],"pdf_url":"https://arxiv.org/pdf/2510.26345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26342v1","updated":"2025-10-30T10:49:25Z","published":"2025-10-30T10:49:25Z","title":"Linear Causal Discovery with Interventional Constraints","summary":"  Incorporating causal knowledge and mechanisms is essential for refining\ncausal models and improving downstream tasks such as designing new treatments.\nIn this paper, we introduce a novel concept in causal discovery, termed\ninterventional constraints, which differs fundamentally from interventional\ndata. While interventional data require direct perturbations of variables,\ninterventional constraints encode high-level causal knowledge in the form of\ninequality constraints on causal effects. For instance, in the Sachs dataset\n(Sachs et al.\\ 2005), Akt has been shown to be activated by PIP3, meaning PIP3\nexerts a positive causal effect on Akt. Existing causal discovery methods allow\nenforcing structural constraints (for example, requiring a causal path from\nPIP3 to Akt), but they may still produce incorrect causal conclusions such as\nlearning that \"PIP3 inhibits Akt\". Interventional constraints bridge this gap\nby explicitly constraining the total causal effect between variable pairs,\nensuring learned models respect known causal influences. To formalize\ninterventional constraints, we propose a metric to quantify total causal\neffects for linear causal models and formulate the problem as a constrained\noptimization task, solved using a two-stage constrained optimization method. We\nevaluate our approach on real-world datasets and demonstrate that integrating\ninterventional constraints not only improves model accuracy and ensures\nconsistency with established findings, making models more explainable, but also\nfacilitates the discovery of new causal relationships that would otherwise be\ncostly to identify.\n","authors":["Zhigao Guo","Feng Dong"],"pdf_url":"https://arxiv.org/pdf/2510.26342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26340v1","updated":"2025-10-30T10:48:18Z","published":"2025-10-30T10:48:18Z","title":"SABER: Symbolic Regression-based Angle of Arrival and Beam Pattern\n  Estimator","summary":"  Accurate Angle-of-arrival (AoA) estimation is essential for next-generation\nwireless communication systems to enable reliable beamforming, high-precision\nlocalization, and integrated sensing. Unfortunately, classical high-resolution\ntechniques require multi-element arrays and extensive snapshot collection,\nwhile generic Machine Learning (ML) approaches often yield black-box models\nthat lack physical interpretability. To address these limitations, we propose a\nSymbolic Regression (SR)-based ML framework. Namely, Symbolic Regression-based\nAngle of Arrival and Beam Pattern Estimator (SABER), a constrained\nsymbolic-regression framework that automatically discovers closed-form beam\npattern and AoA models from path loss measurements with interpretability. SABER\nachieves high accuracy while bridging the gap between opaque ML methods and\ninterpretable physics-driven estimators. First, we validate our approach in a\ncontrolled free-space anechoic chamber, showing that both direct inversion of\nthe known $\\cos^n$ beam and a low-order polynomial surrogate achieve sub-0.5\ndegree Mean Absolute Error (MAE). A purely unconstrained SR method can further\nreduce the error of the predicted angles, but produces complex formulas that\nlack physical insight. Then, we implement the same SR-learned inversions in a\nreal-world, Reconfigurable Intelligent Surface (RIS)-aided indoor testbed.\nSABER and unconstrained SR models accurately recover the true AoA with\nnear-zero error. Finally, we benchmark SABER against the Cram\\'er-Rao Lower\nBounds (CRLBs). Our results demonstrate that SABER is an interpretable and\naccurate alternative to state-of-the-art and black-box ML-based methods for AoA\nestimation.\n","authors":["Shih-Kai Chou","Mengran Zhao","Cheng-Nan Hu","Kuang-Chung Chou","Carolina Fortuna","Jernej Hribar"],"pdf_url":"https://arxiv.org/pdf/2510.26340v1.pdf","comment":"12 pages, 11 figures"},{"id":"http://arxiv.org/abs/2402.03145v4","updated":"2025-10-30T10:40:47Z","published":"2024-02-05T16:12:36Z","title":"SafEDMD: A Koopman-based data-driven controller design framework for\n  nonlinear dynamical systems","summary":"  The Koopman operator serves as the theoretical backbone for machine learning\nof dynamical control systems, where the operator is heuristically approximated\nby extended dynamic mode decomposition (EDMD). In this paper, we propose\nSafEDMD, a novel stability- and feedback-oriented EDMD-based controller design\nframework. Our approach leverages a reliable surrogate model generated in a\ndata-driven fashion in order to provide closed-loop guarantees. In particular,\nwe establish a controller design based on semi-definite programming with\nguaranteed stabilization of the underlying nonlinear system. As central\ningredient, we derive proportional error bounds that vanish at the origin and\nare tailored to control tasks. We illustrate the developed method by means of\nseveral benchmark examples and highlight the advantages over state-of-the-art\nmethods.\n","authors":["Robin StrÃ¤sser","Manuel Schaller","Karl Worthmann","Julian Berberich","Frank AllgÃ¶wer"],"pdf_url":"https://arxiv.org/pdf/2402.03145v4.pdf","comment":"Accepted for publication in Automatica"},{"id":"http://arxiv.org/abs/2506.07500v2","updated":"2025-10-30T10:28:19Z","published":"2025-06-09T07:25:51Z","title":"Mind the Gap: Removing the Discretization Gap in Differentiable Logic\n  Gate Networks","summary":"  Modern neural networks demonstrate state-of-the-art performance on numerous\nexisting benchmarks; however, their high computational requirements and energy\nconsumption prompt researchers to seek more efficient solutions for real-world\ndeployment. Logic gate networks (LGNs) learns a large network of logic gates\nfor efficient image classification. However, learning a network that can solve\na simple problem like CIFAR-10 can take days to weeks to train. Even then,\nalmost half of the network remains unused, causing a discretization gap. This\ndiscretization gap hinders real-world deployment of LGNs, as the performance\ndrop between training and inference negatively impacts accuracy. We inject\nGumbel noise with a straight-through estimator during training to significantly\nspeed up training, improve neuron utilization, and decrease the discretization\ngap. We theoretically show that this results from implicit Hessian\nregularization, which improves the convergence properties of LGNs. We train\nnetworks $4.5 \\times$ faster in wall-clock time, reduce the discretization gap\nby $98\\%$, and reduce the number of unused gates by $100\\%$.\n","authors":["Shakir Yousefi","Andreas Plesner","Till Aczel","Roger Wattenhofer"],"pdf_url":"https://arxiv.org/pdf/2506.07500v2.pdf","comment":"Accepted to NeurIPS 2025 (main track)"},{"id":"http://arxiv.org/abs/2510.26328v1","updated":"2025-10-30T10:27:11Z","published":"2025-10-30T10:27:11Z","title":"Agent Skills Enable a New Class of Realistic and Trivially Simple Prompt\n  Injections","summary":"  Enabling continual learning in LLMs remains a key unresolved research\nchallenge. In a recent announcement, a frontier LLM company made a step towards\nthis by introducing Agent Skills, a framework that equips agents with new\nknowledge based on instructions stored in simple markdown files. Although Agent\nSkills can be a very useful tool, we show that they are fundamentally insecure,\nsince they enable trivially simple prompt injections. We demonstrate how to\nhide malicious instructions in long Agent Skill files and referenced scripts to\nexfiltrate sensitive data, such as internal files or passwords. Importantly, we\nshow how to bypass system-level guardrails of a popular coding agent: a benign,\ntask-specific approval with the \"Don't ask again\" option can carry over to\nclosely related but harmful actions. Overall, we conclude that despite ongoing\nresearch efforts and scaling model capabilities, frontier LLMs remain\nvulnerable to very simple prompt injections in realistic scenarios. Our code is\navailable at https://github.com/aisa-group/promptinject-agent-skills.\n","authors":["David Schmotz","Sahar Abdelnabi","Maksym Andriushchenko"],"pdf_url":"https://arxiv.org/pdf/2510.26328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21038v2","updated":"2025-10-30T10:23:32Z","published":"2025-10-23T22:44:50Z","title":"Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the\n  LibriBrain Dataset","summary":"  Non-invasive brain-computer interfaces (BCIs) are beginning to benefit from\nlarge, public benchmarks. However, current benchmarks target relatively simple,\nfoundational tasks like Speech Detection and Phoneme Classification, while\napplication-ready results on tasks like Brain-to-Text remain elusive. We\npropose Keyword Spotting (KWS) as a practically applicable, privacy-aware\nintermediate task. Using the deep 52-hour, within-subject LibriBrain corpus, we\nprovide standardized train/validation/test splits for reproducible\nbenchmarking, and adopt an evaluation protocol tailored to extreme class\nimbalance. Concretely, we use area under the precision-recall curve (AUPRC) as\na robust evaluation metric, complemented by false alarms per hour (FA/h) at\nfixed recall to capture user-facing trade-offs. To simplify deployment and\nfurther experimentation within the research community, we are releasing an\nupdated version of the pnpl library with word-level dataloaders and Colab-ready\ntutorials. As an initial reference model, we present a compact 1-D Conv/ResNet\nbaseline with focal loss and top-k pooling that is trainable on a single\nconsumer-class GPU. The reference model achieves approximately 13x the\npermutation baseline AUPRC on held-out sessions, demonstrating the viability of\nthe task. Exploratory analyses reveal: (i) predictable within-subject scaling -\nperformance improves log-linearly with more training hours - and (ii) the\nexistence of word-level factors (frequency and duration) that systematically\nmodulate detectability.\n","authors":["Gereon Elvers","Gilad Landau","Oiwi Parker Jones"],"pdf_url":"https://arxiv.org/pdf/2510.21038v2.pdf","comment":"16 pages, 7 figures, 6 tables; updated acknowledgments"},{"id":"http://arxiv.org/abs/2510.26324v1","updated":"2025-10-30T10:17:27Z","published":"2025-10-30T10:17:27Z","title":"Posterior Sampling by Combining Diffusion Models with Annealed Langevin\n  Dynamics","summary":"  Given a noisy linear measurement $y = Ax + \\xi$ of a distribution $p(x)$, and\na good approximation to the prior $p(x)$, when can we sample from the posterior\n$p(x \\mid y)$? Posterior sampling provides an accurate and fair framework for\ntasks such as inpainting, deblurring, and MRI reconstruction, and several\nheuristics attempt to approximate it. Unfortunately, approximate posterior\nsampling is computationally intractable in general.\n  To sidestep this hardness, we focus on (local or global) log-concave\ndistributions $p(x)$. In this regime, Langevin dynamics yields posterior\nsamples when the exact scores of $p(x)$ are available, but it is brittle to\nscore--estimation error, requiring an MGF bound (sub-exponential error). By\ncontrast, in the unconditional setting, diffusion models succeed with only an\n$L^2$ bound on the score error. We prove that combining diffusion models with\nan annealed variant of Langevin dynamics achieves conditional sampling in\npolynomial time using merely an $L^4$ bound on the score error.\n","authors":["Zhiyang Xun","Shivam Gupta","Eric Price"],"pdf_url":"https://arxiv.org/pdf/2510.26324v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26323v1","updated":"2025-10-30T10:17:25Z","published":"2025-10-30T10:17:25Z","title":"On the Impact of Weight Discretization in QUBO-Based SVM Training","summary":"  Training Support Vector Machines (SVMs) can be formulated as a QUBO problem,\nenabling the use of quantum annealing for model optimization. In this work, we\nstudy how the number of qubits - linked to the discretization level of dual\nweights - affects predictive performance across datasets. We compare QUBO-based\nSVM training to the classical LIBSVM solver and find that even low-precision\nQUBO encodings (e.g., 1 bit per parameter) yield competitive, and sometimes\nsuperior, accuracy. While increased bit-depth enables larger regularization\nparameters, it does not always improve classification. Our findings suggest\nthat selecting the right support vectors may matter more than their precise\nweighting. Although current hardware limits the size of solvable QUBOs, our\nresults highlight the potential of quantum annealing for efficient SVM training\nas quantum devices scale.\n","authors":["Sascha MÃ¼cke"],"pdf_url":"https://arxiv.org/pdf/2510.26323v1.pdf","comment":"Presented at the 7th DSO Workshop at ECML PKDD 2025"},{"id":"http://arxiv.org/abs/2506.20535v2","updated":"2025-10-30T10:14:59Z","published":"2025-06-25T15:24:45Z","title":"AIMeter: Measuring, Analyzing, and Visualizing Energy and Carbon\n  Footprint of AI Workloads","summary":"  The rapid advancement of AI, particularly large language models (LLMs), has\nraised significant concerns about the energy use and carbon emissions\nassociated with model training and inference. However, existing tools for\nmeasuring and reporting such impacts are often fragmented, lacking systematic\nmetric integration and offering limited support for correlation analysis among\nthem. This paper presents AIMeter, a comprehensive software toolkit for the\nmeasurement, analysis, and visualization of energy use, power draw, hardware\nperformance, and carbon emissions across AI workloads. By seamlessly\nintegrating with existing AI frameworks, AIMeter offers standardized reports\nand exports fine-grained time-series data to support benchmarking and\nreproducibility in a lightweight manner. It further enables in-depth\ncorrelation analysis between hardware metrics and model performance and thus\nfacilitates bottleneck identification and performance enhancement. By\naddressing critical limitations in existing tools, AIMeter encourages the\nresearch community to weigh environmental impact alongside raw performance of\nAI workloads and advances the shift toward more sustainable \"Green AI\"\npractices. The code is available at https://github.com/SusCom-Lab/AIMeter.\n","authors":["Hongzhen Huang","Kunming Zhang","Hanlong Liao","Kui Wu","Guoming Tang"],"pdf_url":"https://arxiv.org/pdf/2506.20535v2.pdf","comment":"11 pages, 7 figures and 5 tables"},{"id":"http://arxiv.org/abs/2510.26311v1","updated":"2025-10-30T09:58:48Z","published":"2025-10-30T09:58:48Z","title":"Model Inversion with Layer-Specific Modeling and Alignment for Data-Free\n  Continual Learning","summary":"  Continual learning (CL) aims to incrementally train a model on a sequence of\ntasks while retaining performance on prior ones. However, storing and replaying\ndata is often infeasible due to privacy or security constraints and impractical\nfor arbitrary pre-trained models. Data-free CL seeks to update models without\naccess to previous data. Beyond regularization, we employ model inversion to\nsynthesize data from the trained model, enabling replay without storing\nsamples. Yet, model inversion in predictive models faces two challenges: (1)\ngenerating inputs solely from compressed output labels causes drift between\nsynthetic and real data, and replaying such data can erode prior knowledge; (2)\ninversion is computationally expensive since each step backpropagates through\nthe full model. These issues are amplified in large pre-trained models such as\nCLIP. To improve efficiency, we propose Per-layer Model Inversion (PMI),\ninspired by faster convergence in single-layer optimization. PMI provides\nstrong initialization for full-model inversion, substantially reducing\niterations. To mitigate feature shift, we model class-wise features via\nGaussian distributions and contrastive model, ensuring alignment between\nsynthetic and real features. Combining PMI and feature modeling, our approach\nenables continual learning of new classes by generating pseudo-images from\nsemantic-aware projected features, achieving strong effectiveness and\ncompatibility across multiple CL settings.\n","authors":["Ruilin Tong","Haodong Lu","Yuhang Liu","Dong Gong"],"pdf_url":"https://arxiv.org/pdf/2510.26311v1.pdf","comment":"Accepted in NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26307v1","updated":"2025-10-30T09:49:59Z","published":"2025-10-30T09:49:59Z","title":"A Survey of Heterogeneous Graph Neural Networks for Cybersecurity\n  Anomaly Detection","summary":"  Anomaly detection is a critical task in cybersecurity, where identifying\ninsider threats, access violations, and coordinated attacks is essential for\nensuring system resilience. Graph-based approaches have become increasingly\nimportant for modeling entity interactions, yet most rely on homogeneous and\nstatic structures, which limits their ability to capture the heterogeneity and\ntemporal evolution of real-world environments. Heterogeneous Graph Neural\nNetworks (HGNNs) have emerged as a promising paradigm for anomaly detection by\nincorporating type-aware transformations and relation-sensitive aggregation,\nenabling more expressive modeling of complex cyber data. However, current\nresearch on HGNN-based anomaly detection remains fragmented, with diverse\nmodeling strategies, limited comparative evaluation, and an absence of\nstandardized benchmarks. To address this gap, we provide a comprehensive survey\nof HGNN-based anomaly detection methods in cybersecurity. We introduce a\ntaxonomy that classifies approaches by anomaly type and graph dynamics, analyze\nrepresentative models, and map them to key cybersecurity applications. We also\nreview commonly used benchmark datasets and evaluation metrics, highlighting\ntheir strengths and limitations. Finally, we identify key open challenges\nrelated to modeling, data, and deployment, and outline promising directions for\nfuture research. This survey aims to establish a structured foundation for\nadvancing HGNN-based anomaly detection toward scalable, interpretable, and\npractically deployable solutions.\n","authors":["Laura Jiang","Reza Ryan","Qian Li","Nasim Ferdosian"],"pdf_url":"https://arxiv.org/pdf/2510.26307v1.pdf","comment":"37 pages, 4 figures, 86 references. Submitted to Journal of Computer\n  Security (under review)"},{"id":"http://arxiv.org/abs/2410.09766v2","updated":"2025-10-30T09:48:49Z","published":"2024-10-13T07:50:47Z","title":"Stability and Sharper Risk Bounds with Convergence Rate\n  $\\tilde{O}(1/n^2)$","summary":"  Prior work (Klochkov $\\&$ Zhivotovskiy, 2021) establishes at most\n$O\\left(\\log (n)/n\\right)$ excess risk bounds via algorithmic stability for\nstrongly-convex learners with high probability. We show that under the similar\ncommon assumptions -- - Polyak-Lojasiewicz condition, smoothness, and Lipschitz\ncontinous for losses -- - rates of $O\\left(\\log^2(n)/n^2\\right)$ are at most\nachievable. To our knowledge, our analysis also provides the tightest\nhigh-probability bounds for gradient-based generalization gaps in nonconvex\nsettings.\n","authors":["Bowei Zhu","Shaojie Li","Mingyang Yi","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.09766v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10573v3","updated":"2025-10-30T09:42:47Z","published":"2024-11-15T20:46:58Z","title":"Hysteresis Activation Function for Efficient Inference","summary":"  The widely used ReLU is favored for its hardware efficiency, {as the\nimplementation at inference is a one bit sign case,} yet suffers from issues\nsuch as the ``dying ReLU'' problem, where during training, neurons fail to\nactivate and constantly remain at zero, as highlighted by Lu et al. Traditional\napproaches to mitigate this issue often introduce more complex and less\nhardware-friendly activation functions. In this work, we propose a Hysteresis\nRectified Linear Unit (HeLU), an efficient activation function designed to\naddress the ``dying ReLU'' problem with minimal complexity. Unlike traditional\nactivation functions with fixed thresholds for training and inference, HeLU\nemploys a variable threshold that refines the backpropagation. This refined\nmechanism allows simpler activation functions to achieve competitive\nperformance comparable to their more complex counterparts without introducing\nunnecessary complexity or requiring inductive biases. Empirical evaluations\ndemonstrate that HeLU enhances model generalization across diverse datasets,\noffering a promising solution for efficient and effective inference suitable\nfor a wide range of neural network architectures.\n","authors":["Moshe Kimhi","Idan Kashani","Avi Mendelson","Chaim Baskin"],"pdf_url":"https://arxiv.org/pdf/2411.10573v3.pdf","comment":"Accepted to 4th NeurIPS Efficient Natural Language and Speech\n  Processing Workshop (ENLSP-IV 2024)"},{"id":"http://arxiv.org/abs/2510.26303v1","updated":"2025-10-30T09:41:33Z","published":"2025-10-30T09:41:33Z","title":"Implicit Bias of Per-sample Adam on Separable Data: Departure from the\n  Full-batch Regime","summary":"  Adam [Kingma and Ba, 2015] is the de facto optimizer in deep learning, yet\nits theoretical understanding remains limited. Prior analyses show that Adam\nfavors solutions aligned with $\\ell_\\infty$-geometry, but these results are\nrestricted to the full-batch regime. In this work, we study the implicit bias\nof incremental Adam (using one sample per step) for logistic regression on\nlinearly separable data, and we show that its bias can deviate from the\nfull-batch behavior. To illustrate this, we construct a class of structured\ndatasets where incremental Adam provably converges to the $\\ell_2$-max-margin\nclassifier, in contrast to the $\\ell_\\infty$-max-margin bias of full-batch\nAdam. For general datasets, we develop a proxy algorithm that captures the\nlimiting behavior of incremental Adam as $\\beta_2 \\to 1$ and we characterize\nits convergence direction via a data-dependent dual fixed-point formulation.\nFinally, we prove that, unlike Adam, Signum [Bernstein et al., 2018] converges\nto the $\\ell_\\infty$-max-margin classifier for any batch size by taking $\\beta$\nclose enough to 1. Overall, our results highlight that the implicit bias of\nAdam crucially depends on both the batching scheme and the dataset, while\nSignum remains invariant.\n","authors":["Beomhan Baek","Minhak Song","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2510.26303v1.pdf","comment":"50 pages"},{"id":"http://arxiv.org/abs/2510.26302v1","updated":"2025-10-30T09:41:21Z","published":"2025-10-30T09:41:21Z","title":"Understanding Hardness of Vision-Language Compositionality from A\n  Token-level Causal Lens","summary":"  Contrastive Language-Image Pre-training (CLIP) delivers strong cross modal\ngeneralization by aligning images and texts in a shared embedding space, yet it\npersistently fails at compositional reasoning over objects, attributes, and\nrelations often behaving like a bag-of-words matcher. Prior causal accounts\ntypically model text as a single vector, obscuring token-level structure and\nleaving core phenomena-such as prompt sensitivity and failures on hard\nnegatives unexplained. We address this gap with a token-aware causal\nrepresentation learning (CRL) framework grounded in a sequential,\nlanguage-token SCM. Our theory extends block identifiability to tokenized text,\nproving that CLIP's contrastive objective can recover the modal-invariant\nlatent variable under both sentence-level and token-level SCMs. Crucially,\ntoken granularity yields the first principled explanation of CLIP's\ncompositional brittleness: composition nonidentifiability. We show the\nexistence of pseudo-optimal text encoders that achieve perfect modal-invariant\nalignment yet are provably insensitive to SWAP, REPLACE, and ADD operations\nover atomic concepts, thereby failing to distinguish correct captions from hard\nnegatives despite optimizing the same training objective as true-optimal\nencoders. The analysis further links language-side nonidentifiability to\nvisual-side failures via the modality gap and shows how iterated composition\noperators compound hardness, motivating improved negative mining strategies.\n","authors":["Ziliang Chen","Tianang Xiao","Jusheng Zhang","Yongsen Zheng","Xipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2510.26302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26301v1","updated":"2025-10-30T09:39:05Z","published":"2025-10-30T09:39:05Z","title":"Offline Clustering of Preference Learning with Active-data Augmentation","summary":"  Preference learning from pairwise feedback is a widely adopted framework in\napplications such as reinforcement learning with human feedback and\nrecommendations. In many practical settings, however, user interactions are\nlimited or costly, making offline preference learning necessary. Moreover,\nreal-world preference learning often involves users with different preferences.\nFor example, annotators from different backgrounds may rank the same responses\ndifferently. This setting presents two central challenges: (1) identifying\nsimilarity across users to effectively aggregate data, especially under\nscenarios where offline data is imbalanced across dimensions, and (2) handling\nthe imbalanced offline data where some preference dimensions are\nunderrepresented. To address these challenges, we study the Offline Clustering\nof Preference Learning problem, where the learner has access to fixed datasets\nfrom multiple users with potentially different preferences and aims to maximize\nutility for a test user. To tackle the first challenge, we first propose\nOff-C$^2$PL for the pure offline setting, where the learner relies solely on\noffline data. Our theoretical analysis provides a suboptimality bound that\nexplicitly captures the tradeoff between sample noise and bias. To address the\nsecond challenge of inbalanced data, we extend our framework to the setting\nwith active-data augmentation where the learner is allowed to select a limited\nnumber of additional active-data for the test user based on the cluster\nstructure learned by Off-C$^2$PL. In this setting, our second algorithm,\nA$^2$-Off-C$^2$PL, actively selects samples that target the least-informative\ndimensions of the test user's preference. We prove that these actively\ncollected samples contribute more effectively than offline ones. Finally, we\nvalidate our theoretical results through simulations on synthetic and\nreal-world datasets.\n","authors":["Jingyuan Liu","Fatemeh Ghaffari","Xuchuang Wang","Mohammad Hajiesmaili","Carlee Joe-Wong"],"pdf_url":"https://arxiv.org/pdf/2510.26301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22319v2","updated":"2025-10-30T09:33:15Z","published":"2025-10-25T14:51:17Z","title":"GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via\n  Regulated Clipping","summary":"  Recently, GRPO-based reinforcement learning has shown remarkable progress in\noptimizing flow-matching models, effectively improving their alignment with\ntask-specific rewards. Within these frameworks, the policy update relies on\nimportance-ratio clipping to constrain overconfident positive and negative\ngradients. However, in practice, we observe a systematic shift in the\nimportance-ratio distribution-its mean falls below 1 and its variance differs\nsubstantially across timesteps. This left-shifted and inconsistent distribution\nprevents positive-advantage samples from entering the clipped region, causing\nthe mechanism to fail in constraining overconfident positive updates. As a\nresult, the policy model inevitably enters an implicit over-optimization\nstage-while the proxy reward continues to increase, essential metrics such as\nimage quality and text-prompt alignment deteriorate sharply, ultimately making\nthe learned policy impractical for real-world use. To address this issue, we\nintroduce GRPO-Guard, a simple yet effective enhancement to existing GRPO\nframeworks. Our method incorporates ratio normalization, which restores a\nbalanced and step-consistent importance ratio, ensuring that PPO clipping\nproperly constrains harmful updates across denoising timesteps. In addition, a\ngradient reweighting strategy equalizes policy gradients over noise conditions,\npreventing excessive updates from particular timestep regions. Together, these\ndesigns act as a regulated clipping mechanism, stabilizing optimization and\nsubstantially mitigating implicit over-optimization without relying on heavy KL\nregularization. Extensive experiments on multiple diffusion backbones (e.g.,\nSD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard\nsignificantly reduces over-optimization while maintaining or even improving\ngeneration quality.\n","authors":["Jing Wang","Jiajun Liang","Jie Liu","Henglin Liu","Gongye Liu","Jun Zheng","Wanyuan Pang","Ao Ma","Zhenyu Xie","Xintao Wang","Meng Wang","Pengfei Wan","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2510.22319v2.pdf","comment":"Project Page: https://jingw193.github.io/GRPO-Guard/"},{"id":"http://arxiv.org/abs/2505.22151v2","updated":"2025-10-30T09:26:54Z","published":"2025-05-28T09:17:44Z","title":"Oryx: a Scalable Sequence Model for Many-Agent Coordination in Offline\n  MARL","summary":"  A key challenge in offline multi-agent reinforcement learning (MARL) is\nachieving effective many-agent multi-step coordination in complex environments.\nIn this work, we propose Oryx, a novel algorithm for offline cooperative MARL\nto directly address this challenge. Oryx adapts the recently proposed\nretention-based architecture Sable and combines it with a sequential form of\nimplicit constraint Q-learning (ICQ), to develop a novel offline autoregressive\npolicy update scheme. This allows Oryx to solve complex coordination challenges\nwhile maintaining temporal coherence over long trajectories. We evaluate Oryx\nacross a diverse set of benchmarks from prior works -- SMAC, RWARE, and\nMulti-Agent MuJoCo -- covering tasks of both discrete and continuous control,\nvarying in scale and difficulty. Oryx achieves state-of-the-art performance on\nmore than 80% of the 65 tested datasets, outperforming prior offline MARL\nmethods and demonstrating robust generalisation across domains with many agents\nand long horizons. Finally, we introduce new datasets to push the limits of\nmany-agent coordination in offline MARL, and demonstrate Oryx's superior\nability to scale effectively in such settings.\n","authors":["Claude Formanek","Omayma Mahjoub","Louay Ben Nessir","Sasha Abramowitz","Ruan de Kock","Wiem Khlifi","Daniel Rajaonarivonivelomanantsoa","Simon Du Toit","Arnol Fokam","Siddarth Singh","Ulrich Mbou Sob","Felix Chalumeau","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2505.22151v2.pdf","comment":"Published at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2505.13138v2","updated":"2025-10-30T09:22:20Z","published":"2025-05-19T14:07:47Z","title":"Neurosymbolic Diffusion Models","summary":"  Neurosymbolic (NeSy) predictors combine neural perception with symbolic\nreasoning to solve tasks like visual reasoning. However, standard NeSy\npredictors assume conditional independence between the symbols they extract,\nthus limiting their ability to model interactions and uncertainty - often\nleading to overconfident predictions and poor out-of-distribution\ngeneralisation. To overcome the limitations of the independence assumption, we\nintroduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy\npredictors that use discrete diffusion to model dependencies between symbols.\nOur approach reuses the independence assumption from NeSy predictors at each\nstep of the diffusion process, enabling scalable learning while capturing\nsymbol dependencies and uncertainty quantification. Across both synthetic and\nreal-world benchmarks - including high-dimensional visual path planning and\nrule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among\nNeSy predictors and demonstrate strong calibration.\n","authors":["Emile van Krieken","Pasquale Minervini","Edoardo Ponti","Antonio Vergari"],"pdf_url":"https://arxiv.org/pdf/2505.13138v2.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.03509v2","updated":"2025-10-30T09:17:12Z","published":"2025-05-06T13:19:15Z","title":"AnomalyMatch: Discovering Rare Objects of Interest with Semi-supervised\n  and Active Learning","summary":"  Anomaly detection in large datasets is essential in astronomy and computer\nvision. However, due to a scarcity of labelled data, it is often infeasible to\napply supervised methods to anomaly detection. We present AnomalyMatch, an\nanomaly detection framework combining the semi-supervised FixMatch algorithm\nusing EfficientNet classifiers with active learning. AnomalyMatch is tailored\nfor large-scale applications and integrated into the ESA Datalabs science\nplatform. In this method, we treat anomaly detection as a binary classification\nproblem and efficiently utilise limited labelled and abundant unlabelled images\nfor training. We enable active learning via a user interface for verification\nof high-confidence anomalies and correction of false positives. Evaluations on\nthe GalaxyMNIST astronomical dataset and the miniImageNet natural-image\nbenchmark under severe class imbalance display strong performance. Starting\nfrom five to ten labelled anomalies, we achieve an average AUROC of 0.96\n(miniImageNet) and 0.89 (GalaxyMNIST), with respective AUPRC of 0.82 and 0.77.\nAfter three active learning cycles, anomalies are ranked with 76%\n(miniImageNet) to 94% (GalaxyMNIST) precision in the top 1% of the\nhighest-ranking images by score. We compare to the established Astronomaly\nsoftware on selected 'odd' galaxies from the 'Galaxy Zoo - The Galaxy\nChallenge' dataset, achieving comparable performance with an average AUROC of\n0.83. Our results underscore the exceptional utility and scalability of this\napproach for anomaly discovery, highlighting the value of specialised\napproaches for domains characterised by severe label scarcity.\n","authors":["Pablo GÃ³mez","Laslo E. Ruhberg","Maria Teresa Nardone","David O'Ryan"],"pdf_url":"https://arxiv.org/pdf/2505.03509v2.pdf","comment":"Journal submission in preparation to RASTI; 15 pages; 12 figures"},{"id":"http://arxiv.org/abs/2502.04380v3","updated":"2025-10-30T09:16:49Z","published":"2025-02-05T17:21:01Z","title":"Diversity as a Reward: Fine-Tuning LLMs on a Mixture of\n  Domain-Undetermined Data","summary":"  Fine-tuning large language models (LLMs) using diverse datasets is crucial\nfor enhancing their overall performance across various domains. In practical\nscenarios, existing methods based on modeling the mixture proportions of data\ncomposition often struggle with data whose domain labels are missing, imprecise\nor non-normalized, while methods based on data selection usually encounter\ndifficulties in balancing multi-domain performance. To address these\nchallenges, in this work, we investigate the role of data diversity in\nenhancing the overall abilities of LLMs by empirically constructing contrastive\ndata pools and theoretically deriving explanations. Building upon the insights\ngained, we propose a new method that gives the LLM a dual identity: an output\nmodel to cognitively probe and select data based on diversity reward, as well\nas an input model to be tuned with the selected data. Extensive experiments\nshow that the proposed method notably boosts performance across\ndomain-undetermined data and a series of foundational downstream tasks when\napplied to various advanced LLMs. We release our code and hope this study can\nshed light on the understanding of data diversity and advance feedback-driven\ndata-model co-design for LLMs.\n","authors":["Zhenqing Ling","Daoyuan Chen","Liuyi Yao","Qianli Shen","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2502.04380v3.pdf","comment":"Accepted by NeurIPS'25 main track. 47 pages, 21 figures, 32 tables"},{"id":"http://arxiv.org/abs/2510.04237v3","updated":"2025-10-30T09:14:25Z","published":"2025-10-05T15:04:03Z","title":"Truncated Kernel Stochastic Gradient Descent with General Losses and\n  Spherical Radial Basis Functions","summary":"  In this paper, we propose a novel kernel stochastic gradient descent (SGD)\nalgorithm for large-scale supervised learning with general losses. Compared to\ntraditional kernel SGD, our algorithm improves efficiency and scalability\nthrough an innovative regularization strategy. By leveraging the infinite\nseries expansion of spherical radial basis functions, this strategy projects\nthe stochastic gradient onto a finite-dimensional hypothesis space, which is\nadaptively scaled according to the bias-variance trade-off, thereby enhancing\ngeneralization performance. Based on a new estimation of the spectral structure\nof the kernel-induced covariance operator, we develop an analytical framework\nthat unifies optimization and generalization analyses. We prove that both the\nlast iterate and the suffix average converge at minimax-optimal rates, and we\nfurther establish optimal strong convergence in the reproducing kernel Hilbert\nspace. Our framework accommodates a broad class of classical loss functions,\nincluding least-squares, Huber, and logistic losses. Moreover, the proposed\nalgorithm significantly reduces computational complexity and achieves optimal\nstorage complexity by incorporating coordinate-wise updates from linear SGD,\nthereby avoiding the costly pairwise operations typical of kernel SGD and\nenabling efficient processing of streaming data. Finally, extensive numerical\nexperiments demonstrate the efficiency of our approach.\n","authors":["Jinhui Bai","Andreas Christmann","Lei Shi"],"pdf_url":"https://arxiv.org/pdf/2510.04237v3.pdf","comment":"54 pages, 20 figures"},{"id":"http://arxiv.org/abs/2503.12902v2","updated":"2025-10-30T09:10:57Z","published":"2025-03-17T08:03:47Z","title":"Experiments with Optimal Model Trees","summary":"  Model trees provide an appealing way to perform interpretable machine\nlearning for both classification and regression problems. In contrast to\n``classic'' decision trees with constant values in their leaves, model trees\ncan use linear combinations of predictor variables in their leaf nodes to form\npredictions, which can help achieve higher accuracy and smaller trees. Typical\nalgorithms for learning model trees from training data work in a greedy\nfashion, growing the tree in a top-down manner by recursively splitting the\ndata into smaller and smaller subsets. Crucially, the selected splits are only\nlocally optimal, potentially rendering the tree overly complex and less\naccurate than a tree whose structure is globally optimal for the training data.\nIn this paper, we empirically investigate the effect of constructing globally\noptimal model trees for classification and regression with linear support\nvector machines at the leaf nodes. To this end, we present mixed-integer linear\nprogramming formulations to learn optimal trees, compute such trees for a large\ncollection of benchmark data sets, and compare their performance against\ngreedily grown model trees in terms of interpretability and accuracy. We also\ncompare to classic optimal and greedily grown decision trees, random forests,\nand support vector machines. Our results show that optimal model trees can\nachieve competitive accuracy with very small trees. We also investigate the\neffect on the accuracy of replacing axis-parallel splits with multivariate\nones, foregoing interpretability while potentially obtaining greater accuracy.\n","authors":["Sabino Francesco Roselli","Eibe Frank"],"pdf_url":"https://arxiv.org/pdf/2503.12902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26285v1","updated":"2025-10-30T09:08:50Z","published":"2025-10-30T09:08:50Z","title":"Unravelling the Mechanisms of Manipulating Numbers in Language Models","summary":"  Recent work has shown that different large language models (LLMs) converge to\nsimilar and accurate input embedding representations for numbers. These\nfindings conflict with the documented propensity of LLMs to produce erroneous\noutputs when dealing with numeric information. In this work, we aim to explain\nthis conflict by exploring how language models manipulate numbers and quantify\nthe lower bounds of accuracy of these mechanisms. We find that despite\nsurfacing errors, different language models learn interchangeable\nrepresentations of numbers that are systematic, highly accurate and universal\nacross their hidden states and the types of input contexts. This allows us to\ncreate universal probes for each LLM and to trace information -- including the\ncauses of output errors -- to specific layers. Our results lay a fundamental\nunderstanding of how pre-trained LLMs manipulate numbers and outline the\npotential of more accurate probing techniques in addressed refinements of LLMs'\narchitectures.\n","authors":["Michal Å tefÃ¡nik","Timothee Mickus","Marek KadlÄÃ­k","Bertram HÃ¸jer","Michal Spiegel","RaÃºl VÃ¡zquez","Aman Sinha","Josef KuchaÅ","Philipp Mondorf"],"pdf_url":"https://arxiv.org/pdf/2510.26285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26284v1","updated":"2025-10-30T09:08:07Z","published":"2025-10-30T09:08:07Z","title":"Empirical Bayesian Multi-Bandit Learning","summary":"  Multi-task learning in contextual bandits has attracted significant research\ninterest due to its potential to enhance decision-making across multiple\nrelated tasks by leveraging shared structures and task-specific heterogeneity.\nIn this article, we propose a novel hierarchical Bayesian framework for\nlearning in various bandit instances. This framework captures both the\nheterogeneity and the correlations among different bandit instances through a\nhierarchical Bayesian model, enabling effective information sharing while\naccommodating instance-specific variations. Unlike previous methods that\noverlook the learning of the covariance structure across bandits, we introduce\nan empirical Bayesian approach to estimate the covariance matrix of the prior\ndistribution.This enhances both the practicality and flexibility of learning\nacross multi-bandits. Building on this approach, we develop two efficient\nalgorithms: ebmTS (Empirical Bayesian Multi-Bandit Thompson Sampling) and\nebmUCB (Empirical Bayesian Multi-Bandit Upper Confidence Bound), both of which\nincorporate the estimated prior into the decision-making process. We provide\nthe frequentist regret upper bounds for the proposed algorithms, thereby\nfilling a research gap in the field of multi-bandit problems. Extensive\nexperiments on both synthetic and real-world datasets demonstrate the superior\nperformance of our algorithms, particularly in complex environments. Our\nmethods achieve lower cumulative regret compared to existing techniques,\nhighlighting their effectiveness in balancing exploration and exploitation\nacross multi-bandits.\n","authors":["Xia Jiang","Rong J. B. Zhu"],"pdf_url":"https://arxiv.org/pdf/2510.26284v1.pdf","comment":"33 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.15475v3","updated":"2025-10-30T09:02:24Z","published":"2025-02-21T14:00:14Z","title":"Decoding for Punctured Convolutional and Turbo Codes: A Deep Learning\n  Solution for Protocols Compliance","summary":"  Neural network-based decoding methods show promise in enhancing error\ncorrection performance but face challenges with punctured codes. In particular,\nexisting methods struggle to adapt to variable code rates or meet protocol\ncompatibility requirements. This paper proposes a unified long short-term\nmemory (LSTM)-based neural decoder for punctured convolutional and Turbo codes\nto address these challenges. The key component of the proposed LSTM-based\nneural decoder is puncturing-aware embedding, which integrates puncturing\npatterns directly into the neural network to enable seamless adaptation to\ndifferent code rates. Moreover, a balanced bit error rate training strategy is\ndesigned to ensure the decoder's robustness across various code lengths, rates,\nand channels. In this way, the protocol compatibility requirement can be\nrealized. Extensive simulations in both additive white Gaussian noise (AWGN)\nand Rayleigh fading channels demonstrate that the proposed neural decoder\noutperforms conventional decoding techniques, offering significant improvements\nin decoding accuracy and robustness.\n","authors":["Yongli Yan","Linglong Dai"],"pdf_url":"https://arxiv.org/pdf/2502.15475v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26278v1","updated":"2025-10-30T09:00:42Z","published":"2025-10-30T09:00:42Z","title":"Distributional Multi-objective Black-box Optimization for\n  Diffusion-model Inference-time Multi-Target Generation","summary":"  Diffusion models have been successful in learning complex data distributions.\nThis capability has driven their application to high-dimensional\nmulti-objective black-box optimization problem. Existing approaches often\nemploy an external optimization loop, such as an evolutionary algorithm, to the\ndiffusion model. However, these approaches treat the diffusion model as a\nblack-box refiner, which overlooks the internal distribution transition of the\ndiffusion generation process, limiting their efficiency. To address these\nchallenges, we propose the Inference-time Multi-target Generation (IMG)\nalgorithm, which optimizes the diffusion process at inference-time to generate\nsamples that simultaneously satisfy multiple objectives. Specifically, our IMG\nperforms weighted resampling during the diffusion generation process according\nto the expected aggregated multi-objective values. This weighted resampling\nstrategy ensures the diffusion-generated samples are distributed according to\nour desired multi-target Boltzmann distribution. We further derive that the\nmulti-target Boltzmann distribution has an interesting log-likelihood\ninterpretation, where it is the optimal solution to the distributional\nmulti-objective optimization problem. We implemented IMG for a multi-objective\nmolecule generation task. Experiments show that IMG, requiring only a single\ngeneration pass, achieves a significantly higher hypervolume than baseline\noptimization algorithms that often require hundreds of diffusion generations.\nNotably, our algorithm can be viewed as an optimized diffusion process and can\nbe integrated into existing methods to further improve their performance.\n","authors":["Kim Yong Tan","Yueming Lyu","Ivor Tsang","Yew-Soon Ong"],"pdf_url":"https://arxiv.org/pdf/2510.26278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12995v2","updated":"2025-10-30T08:59:47Z","published":"2024-11-20T02:46:15Z","title":"Beyond likelihood ratio bias: Nested multi-time-scale stochastic\n  approximation for likelihood-free parameter estimation","summary":"  We study parameter inference in simulation-based stochastic models where the\nanalytical form of the likelihood is unknown. The main difficulty is that score\nevaluation as a ratio of noisy Monte Carlo estimators induces bias and\ninstability, which we overcome with a ratio-free nested multi-time-scale (NMTS)\nstochastic approximation (SA) method that simultaneously tracks the score and\ndrives the parameter update. We provide a comprehensive theoretical analysis of\nthe proposed NMTS algorithm for solving likelihood-free inference problems,\nincluding strong convergence, asymptotic normality, and convergence rates. We\nshow that our algorithm can eliminate the original asymptotic bias\n$O\\big(\\sqrt{\\frac{1}{N}}\\big)$ and accelerate the convergence rate from\n$O\\big(\\beta_k+\\sqrt{\\frac{1}{N}}\\big)$ to\n$O\\big(\\frac{\\beta_k}{\\alpha_k}+\\sqrt{\\frac{\\alpha_k}{N}}\\big)$, where $N$ is\nthe fixed batch size, $\\alpha_k$ and $\\beta_k$ are decreasing step sizes with\n$\\alpha_k$, $\\beta_k$, $\\beta_k/\\alpha_k\\rightarrow 0$. With proper choice of\n$\\alpha_k$ and $\\beta_k$, our convergence rates can match the optimal rate in\nthe multi-time-scale SA literature. Numerical experiments demonstrate that our\nalgorithm can improve the estimation accuracy by one to two orders of magnitude\nat the same computational cost, making it efficient for parameter estimation in\nstochastic systems.\n","authors":["Zehao Li","Zhouchen Lin","Yijie Peng"],"pdf_url":"https://arxiv.org/pdf/2411.12995v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26275v1","updated":"2025-10-30T08:59:01Z","published":"2025-10-30T08:59:01Z","title":"A Research Roadmap for Augmenting Software Engineering Processes and\n  Software Products with Generative AI","summary":"  Generative AI (GenAI) is rapidly transforming software engineering (SE)\npractices, influencing how SE processes are executed, as well as how software\nsystems are developed, operated, and evolved. This paper applies design science\nresearch to build a roadmap for GenAI-augmented SE. The process consists of\nthree cycles that incrementally integrate multiple sources of evidence,\nincluding collaborative discussions from the FSE 2025 \"Software Engineering\n2030\" workshop, rapid literature reviews, and external feedback sessions\ninvolving peers. McLuhan's tetrads were used as a conceptual instrument to\nsystematically capture the transforming effects of GenAI on SE processes and\nsoftware products.The resulting roadmap identifies four fundamental forms of\nGenAI augmentation in SE and systematically characterizes their related\nresearch challenges and opportunities. These insights are then consolidated\ninto a set of future research directions. By grounding the roadmap in a\nrigorous multi-cycle process and cross-validating it among independent author\nteams and peers, the study provides a transparent and reproducible foundation\nfor analyzing how GenAI affects SE processes, methods and tools, and for\nframing future research within this rapidly evolving area. Based on these\nfindings, the article finally makes ten predictions for SE in the year 2030.\n","authors":["Domenico Amalfitano","Andreas Metzger","Marco Autili","Tommaso Fulcini","Tobias Hey","Jan Keim","Patrizio Pelliccione","Vincenzo Scotti","Anne Koziolek","Raffaela Mirandola","Andreas Vogelsang"],"pdf_url":"https://arxiv.org/pdf/2510.26275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26274v1","updated":"2025-10-30T08:58:44Z","published":"2025-10-30T08:58:44Z","title":"PVMark: Enabling Public Verifiability for LLM Watermarking Schemes","summary":"  Watermarking schemes for large language models (LLMs) have been proposed to\nidentify the source of the generated text, mitigating the potential threats\nemerged from model theft. However, current watermarking solutions hardly\nresolve the trust issue: the non-public watermark detection cannot prove itself\nfaithfully conducting the detection. We observe that it is attributed to the\nsecret key mostly used in the watermark detection -- it cannot be public, or\nthe adversary may launch removal attacks provided the key; nor can it be\nprivate, or the watermarking detection is opaque to the public. To resolve the\ndilemma, we propose PVMark, a plugin based on zero-knowledge proof (ZKP),\nenabling the watermark detection process to be publicly verifiable by third\nparties without disclosing any secret key. PVMark hinges upon the proof of\n`correct execution' of watermark detection on which a set of ZKP constraints\nare built, including mapping, random number generation, comparison, and\nsummation. We implement multiple variants of PVMark in Python, Rust and Circom,\ncovering combinations of three watermarking schemes, three hash functions, and\nfour ZKP protocols, to show our approach effectively works under a variety of\ncircumstances. By experimental results, PVMark efficiently enables public\nverifiability on the state-of-the-art LLM watermarking schemes yet without\ncompromising the watermarking performance, promising to be deployed in\npractice.\n","authors":["Haohua Duan","Liyao Xiang","Xin Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26274v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2405.09086v2","updated":"2025-10-30T08:49:01Z","published":"2024-05-15T04:47:31Z","title":"Chaos-based reinforcement learning with TD3","summary":"  Chaos-based reinforcement learning (CBRL) is a method in which the agent's\ninternal chaotic dynamics drives exploration. However, the learning algorithms\nin CBRL have not been thoroughly developed in previous studies, nor have they\nincorporated recent advances in reinforcement learning. This study introduced\nTwin Delayed Deep Deterministic Policy Gradients (TD3), which is one of the\nstate-of-the-art deep reinforcement learning algorithms that can treat\ndeterministic and continuous action spaces, to CBRL. The validation results\nprovide several insights. First, TD3 works as a learning algorithm for CBRL in\na simple goal-reaching task. Second, CBRL agents with TD3 can autonomously\nsuppress their exploratory behavior as learning progresses and resume\nexploration when the environment changes. Finally, examining the effect of the\nagent's chaoticity on learning shows that there exists a suitable range of\nchaos strength in the agent's model to flexibly switch between exploration and\nexploitation and adapt to environmental changes.\n","authors":["Toshitaka Matsuki","Yusuke Sakemi","Kazuyuki Aihara"],"pdf_url":"https://arxiv.org/pdf/2405.09086v2.pdf","comment":"Accepted for publication in Neural Networks"},{"id":"http://arxiv.org/abs/2510.26266v1","updated":"2025-10-30T08:46:53Z","published":"2025-10-30T08:46:53Z","title":"Likely Interpolants of Generative Models","summary":"  Interpolation in generative models allows for controlled generation, model\ninspection, and more. Unfortunately, most generative models lack a principal\nnotion of interpolants without restrictive assumptions on either the model or\ndata dimension. In this paper, we develop a general interpolation scheme that\ntargets likely transition paths compatible with different metrics and\nprobability distributions. We consider interpolants analogous to a geodesic\nconstrained to a suitable data distribution and derive a novel algorithm for\ncomputing these curves, which requires no additional training. Theoretically,\nwe show that our method locally can be considered as a geodesic under a\nsuitable Riemannian metric. We quantitatively show that our interpolation\nscheme traverses higher density regions than baselines across a range of models\nand datasets.\n","authors":["Frederik MÃ¶bius Rygaard","Shen Zhu","Yinzhu Jin","SÃ¸ren Hauberg","Tom Fletcher"],"pdf_url":"https://arxiv.org/pdf/2510.26266v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2510.26759v1","updated":"2025-10-30T17:49:49Z","published":"2025-10-30T17:49:49Z","title":"MORE: Multi-Organ Medical Image REconstruction Dataset","summary":"  CT reconstruction provides radiologists with images for diagnosis and\ntreatment, yet current deep learning methods are typically limited to specific\nanatomies and datasets, hindering generalization ability to unseen anatomies\nand lesions. To address this, we introduce the Multi-Organ medical image\nREconstruction (MORE) dataset, comprising CT scans across 9 diverse anatomies\nwith 15 lesion types. This dataset serves two key purposes: (1) enabling robust\ntraining of deep learning models on extensive, heterogeneous data, and (2)\nfacilitating rigorous evaluation of model generalization for CT reconstruction.\nWe further establish a strong baseline solution that outperforms prior\napproaches under these challenging conditions. Our results demonstrate that:\n(1) a comprehensive dataset helps improve the generalization capability of\nmodels, and (2) optimization-based methods offer enhanced robustness for unseen\nanatomies. The MORE dataset is freely accessible under CC-BY-NC 4.0 at our\nproject page https://more-med.github.io/\n","authors":["Shaokai Wu","Yapan Guo","Yanbiao Ji","Jing Tong","Yuxiang Lu","Mei Li","Suizhi Huang","Yue Ding","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2510.26759v1.pdf","comment":"Accepted to ACMMM 2025"},{"id":"http://arxiv.org/abs/2503.09205v3","updated":"2025-10-30T17:37:55Z","published":"2025-03-12T09:48:38Z","title":"Quality Over Quantity? LLM-Based Curation for a Data-Efficient\n  Audio-Video Foundation Model","summary":"  Integrating audio and visual data for training multimodal foundational models\nremains a challenge. The Audio-Video Vector Alignment (AVVA) framework\naddresses this by considering AV scene alignment beyond mere temporal\nsynchronization, and leveraging Large Language Models (LLMs) for data curation.\nAVVA implements a scoring mechanism for selecting aligned training data\nsegments. It integrates Whisper, a speech-based foundation model, for audio and\nDINOv2 for video analysis in a dual-encoder structure with contrastive learning\non AV pairs. Evaluations on AudioCaps, VALOR, and VGGSound demonstrate the\neffectiveness of the proposed model architecture and data curation approach.\nAVVA achieves a significant improvement in top-k accuracies for video-to-audio\nretrieval on all datasets compared to DenseAV, while using only 192 hrs of\ncurated training data. Furthermore, an ablation study indicates that the data\ncuration process effectively trades data quality for data quantity, yielding\nincreases in top-k retrieval accuracies on AudioCaps, VALOR, and VGGSound,\ncompared to training on the full spectrum of uncurated data.\n","authors":["Ali Vosoughi","Dimitra Emmanouilidou","Hannes Gamper"],"pdf_url":"https://arxiv.org/pdf/2503.09205v3.pdf","comment":"5 pages, 5 figures, 2 tables. Accepted at EUSIPCO 2025"},{"id":"http://arxiv.org/abs/2510.26721v1","updated":"2025-10-30T17:22:22Z","published":"2025-10-30T17:22:22Z","title":"Unveiling Intrinsic Text Bias in Multimodal Large Language Models\n  through Attention Key-Space Analysis","summary":"  Multimodal large language models (MLLMs) exhibit a pronounced preference for\ntextual inputs when processing vision-language data, limiting their ability to\nreason effectively from visual evidence. Unlike prior studies that attribute\nthis text bias to external factors such as data imbalance or instruction\ntuning, we propose that the bias originates from the model's internal\narchitecture. Specifically, we hypothesize that visual key vectors (Visual\nKeys) are out-of-distribution (OOD) relative to the text key space learned\nduring language-only pretraining. Consequently, these visual keys receive\nsystematically lower similarity scores during attention computation, leading to\ntheir under-utilization in the context representation. To validate this\nhypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their\ndistributional structures using qualitative (t-SNE) and quantitative\n(Jensen-Shannon divergence) methods. The results provide direct evidence that\nvisual and textual keys occupy markedly distinct subspaces within the attention\nspace. The inter-modal divergence is statistically significant, exceeding\nintra-modal variation by several orders of magnitude. These findings reveal\nthat text bias arises from an intrinsic misalignment within the attention key\nspace rather than solely from external data factors.\n","authors":["Xinhan Zheng","Huyu Wu","Xueting Wang","Haiyun Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.26721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.24518v2","updated":"2025-10-30T15:08:00Z","published":"2025-05-30T12:30:04Z","title":"ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis\n  Optimization for Speech Multi-Metric Estimation","summary":"  Speech signal analysis poses significant challenges, particularly in tasks\nsuch as speech quality evaluation and profiling, where the goal is to predict\nmultiple perceptual and objective metrics. For instance, metrics like PESQ\n(Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective\nIntelligibility), and MOS (Mean Opinion Score) each capture different aspects\nof speech quality. However, these metrics often have different scales,\nassumptions, and dependencies, making joint estimation non-trivial. To address\nthese issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based\nHypothesis Optimization), a chain-based, versatile evaluation system for speech\nassessment grounded in autoregressive dependency modeling. ARECHO is\ndistinguished by three key innovations: (1) a comprehensive speech information\ntokenization pipeline; (2) a dynamic classifier chain that explicitly captures\ninter-metric dependencies; and (3) a two-step confidence-oriented decoding\nalgorithm that enhances inference reliability. Experiments demonstrate that\nARECHO significantly outperforms the baseline framework across diverse\nevaluation scenarios, including enhanced speech analysis, speech generation\nevaluation, and, noisy speech evaluation. Furthermore, its dynamic dependency\nmodeling improves interpretability by capturing inter-metric relationships.\nAcross tasks, ARECHO offers reference-free evaluation using its dynamic\nclassifier chain to support subset queries (single or multiple metrics) and\nreduces error propagation via confidence-oriented decoding.\n","authors":["Jiatong Shi","Yifan Cheng","Bo-Hao Su","Hye-jin Shim","Jinchuan Tian","Samuele Cornell","Yiwen Zhao","Siddhant Arora","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2505.24518v2.pdf","comment":"NeurIPS 2025 Spotlight"},{"id":"http://arxiv.org/abs/2510.26569v1","updated":"2025-10-30T14:59:37Z","published":"2025-10-30T14:59:37Z","title":"AdSum: Two-stream Audio-visual Summarization for Automated Video\n  Advertisement Clipping","summary":"  Advertisers commonly need multiple versions of the same advertisement (ad) at\nvarying durations for a single campaign. The traditional approach involves\nmanually selecting and re-editing shots from longer video ads to create shorter\nversions, which is labor-intensive and time-consuming. In this paper, we\nintroduce a framework for automated video ad clipping using video summarization\ntechniques. We are the first to frame video clipping as a shot selection\nproblem, tailored specifically for advertising. Unlike existing general video\nsummarization methods that primarily focus on visual content, our approach\nemphasizes the critical role of audio in advertising. To achieve this, we\ndevelop a two-stream audio-visual fusion model that predicts the importance of\nvideo frames, where importance is defined as the likelihood of a frame being\nselected in the firm-produced short ad. To address the lack of ad-specific\ndatasets, we present AdSum204, a novel dataset comprising 102 pairs of\n30-second and 15-second ads from real advertising campaigns. Extensive\nexperiments demonstrate that our model outperforms state-of-the-art methods\nacross various metrics, including Average Precision, Area Under Curve,\nSpearman, and Kendall.\n","authors":["Wen Xie","Yanjun Zhu","Gijs Overgoor","Yakov Bart","Agata Lapedriza Garcia","Sarah Ostadabbas"],"pdf_url":"https://arxiv.org/pdf/2510.26569v1.pdf","comment":"Accepted at 32nd International Conference on MultiMedia Modeling"},{"id":"http://arxiv.org/abs/2504.11331v2","updated":"2025-10-30T14:50:58Z","published":"2025-04-15T16:05:09Z","title":"Dependency Structure Augmented Contextual Scoping Framework for\n  Multimodal Aspect-Based Sentiment Analysis","summary":"  Multimodal Aspect-Based Sentiment Analysis (MABSA) seeks to extract\nfine-grained information from image-text pairs to identify aspect terms and\ndetermine their sentiment polarity. However, existing approaches often fall\nshort in simultaneously addressing three core challenges: Sentiment Cue\nPerception (SCP), Multimodal Information Misalignment (MIM), and Semantic Noise\nElimination (SNE). To overcome these limitations, we propose DASCO\n(\\textbf{D}ependency Structure \\textbf{A}ugmented \\textbf{Sco}ping Framework),\na fine-grained scope-oriented framework that enhances aspect-level sentiment\nreasoning by leveraging dependency parsing trees. First, we designed a\nmulti-task pretraining strategy for MABSA on our base model, combining\naspect-oriented enhancement, image-text matching, and aspect-level\nsentiment-sensitive cognition. This improved the model's perception of aspect\nterms and sentiment cues while achieving effective image-text alignment,\naddressing key challenges like SCP and MIM. Furthermore, we incorporate\ndependency trees as syntactic branch combining with semantic branch, guiding\nthe model to selectively attend to critical contextual elements within a\ntarget-specific scope while effectively filtering out irrelevant noise for\naddressing SNE problem. Extensive experiments on two benchmark datasets across\nthree subtasks demonstrate that DASCO achieves state-of-the-art performance in\nMABSA, with notable gains in JMASA (+2.3\\% F1 and +3.5\\% precision on\nTwitter2015). The source code is available at https://github.com/LHaoooo/DASCO .\n","authors":["Hao Liu","Lijun He","Jiaxi Liang","Zhihan Ren","Haixia Bi","Fan Li"],"pdf_url":"https://arxiv.org/pdf/2504.11331v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.24325v2","updated":"2025-10-30T13:38:59Z","published":"2025-09-29T06:23:47Z","title":"ReCon-GS: Continuum-Preserved Gaussian Streaming for Fast and Compact\n  Reconstruction of Dynamic Scenes","summary":"  Online free-viewpoint video (FVV) reconstruction is challenged by slow\nper-frame optimization, inconsistent motion estimation, and unsustainable\nstorage demands. To address these challenges, we propose the Reconfigurable\nContinuum Gaussian Stream, dubbed ReCon-GS, a novel storage-aware framework\nthat enables high fidelity online dynamic scene reconstruction and real-time\nrendering. Specifically, we dynamically allocate multi-level Anchor Gaussians\nin a density-adaptive fashion to capture inter-frame geometric deformations,\nthereby decomposing scene motion into compact coarse-to-fine representations.\nThen, we design a dynamic hierarchy reconfiguration strategy that preserves\nlocalized motion expressiveness through on-demand anchor re-hierarchization,\nwhile ensuring temporal consistency through intra-hierarchical deformation\ninheritance that confines transformation priors to their respective hierarchy\nlevels. Furthermore, we introduce a storage-aware optimization mechanism that\nflexibly adjusts the density of Anchor Gaussians at different hierarchy levels,\nenabling a controllable trade-off between reconstruction fidelity and memory\nusage. Extensive experiments on three widely used datasets demonstrate that,\ncompared to state-of-the-art methods, ReCon-GS improves training efficiency by\napproximately 15% and achieves superior FVV synthesis quality with enhanced\nrobustness and stability. Moreover, at equivalent rendering quality, ReCon-GS\nslashes memory requirements by over 50% compared to leading state-of-the-art\nmethods.\n","authors":["Jiaye Fu","Qiankun Gao","Chengxiang Wen","Yanmin Wu","Siwei Ma","Jiaqi Zhang","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.24325v2.pdf","comment":"Published in NeurIPS 2025"},{"id":"http://arxiv.org/abs/2509.04448v2","updated":"2025-10-30T10:58:04Z","published":"2025-09-04T17:59:43Z","title":"TRUST-VL: An Explainable News Assistant for General Multimodal\n  Misinformation Detection","summary":"  Multimodal misinformation, encompassing textual, visual, and cross-modal\ndistortions, poses an increasing societal threat that is amplified by\ngenerative AI. Existing methods typically focus on a single type of distortion\nand struggle to generalize to unseen scenarios. In this work, we observe that\ndifferent distortion types share common reasoning capabilities while also\nrequiring task-specific skills. We hypothesize that joint training across\ndistortion types facilitates knowledge sharing and enhances the model's ability\nto generalize. To this end, we introduce TRUST-VL, a unified and explainable\nvision-language model for general multimodal misinformation detection. TRUST-VL\nincorporates a novel Question-Aware Visual Amplifier module, designed to\nextract task-specific visual features. To support training, we also construct\nTRUST-Instruct, a large-scale instruction dataset containing 198K samples\nfeaturing structured reasoning chains aligned with human fact-checking\nworkflows. Extensive experiments on both in-domain and zero-shot benchmarks\ndemonstrate that TRUST-VL achieves state-of-the-art performance, while also\noffering strong generalization and interpretability.\n","authors":["Zehong Yan","Peng Qi","Wynne Hsu","Mong Li Lee"],"pdf_url":"https://arxiv.org/pdf/2509.04448v2.pdf","comment":"EMNLP 2025 Oral; Project Homepage:\n  https://yanzehong.github.io/trust-vl/"},{"id":"http://arxiv.org/abs/2510.26289v1","updated":"2025-10-30T09:14:46Z","published":"2025-10-30T09:14:46Z","title":"Contribution-Guided Asymmetric Learning for Robust Multimodal Fusion\n  under Imbalance and Noise","summary":"  Multimodal learning faces two major challenges: modality imbalance and data\nnoise, which significantly affect the robustness and generalization ability of\nmodels. Existing methods achieve modality balance by suppressing dominant\nmodalities, but they neglect the inherent differences in the information value\nbetween modalities, potentially leading to convergence to suboptimal solutions.\nThis paper proposes an innovative modality compression paradigm,\nContribution-Guided Asymmetric Learning (CAL), which aims to enhance the\ncontribution of high-contribution modalities while compressing weak modalities\nto increase their contribution, allowing both to improve the performance of\nmultimodal information fusion. CAL is based on a modality contribution metric\nW^m combining the information quantity I(m) and confidence D(m), and it designs\nan asymmetric gradient acceleration mechanism and a contribution-aware\nAsymmetric Information Bottleneck (AIB) compression mechanism. The former\naccelerates the gradient update of modalities, while the latter dynamically\ncompresses the noise of low-contribution modalities.\n  On five benchmark datasets, including emotion recognition, scene recognition,\nand event localization tasks, CAL has shown outstanding performance in\nimbalanced fusion tasks and noise robustness tests. On CREMA-D, KS, and AVE,\nCAL achieves 79.30%, 74.82%, and 74.21% accuracy, significantly outperforming\nthe existing state-of-the-art model ARL. In high-noise robustness tests, CAL\nalso achieved leading performance under various attack strategies on the\nMVSA-Single and NYUD2 datasets. These results validate the significant\nadvantages of CAL in modality imbalance and noise interference. CAL, as a\nflexible and efficient framework, is easy to transfer to other tasks and has\nbroad adaptability and potential application prospects.\n","authors":["Zijing Xu","Yunfeng Kou","Kunming Wu","Hong Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25600v2","updated":"2025-10-30T03:43:02Z","published":"2025-10-29T15:10:17Z","title":"PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse\n  Attention for Vision-Language Large Models","summary":"  Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation.\n","authors":["Zhonghua Jiang","Kunxi Li","Yiyun Zhou","Sihao Liu","Zhaode Wang","Chengfei lv","Shengyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.25600v2.pdf","comment":null}]}}